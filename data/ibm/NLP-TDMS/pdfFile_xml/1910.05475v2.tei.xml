<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Saliency Guided Self-attention Network for Weakly and Semi-supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
							<email>gongxj@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Saliency Guided Self-attention Network for Weakly and Semi-supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised semantic segmentation (WSSS) using only image-level labels can greatly reduce the annotation cost and therefore has attracted considerable research interest. However, its performance is still inferior to the fully supervised counterparts. To mitigate the performance gap, we propose a saliency guided self-attention network (SGAN) to address the WSSS problem. The introduced self-attention mechanism is able to capture rich and extensive contextual information but may mis-spread attentions to unexpected regions. In order to enable this mechanism to work effectively under weak supervision, we integrate class-agnostic saliency priors into the self-attention mechanism and utilize class-specific attention cues as an additional supervision for SGAN. Our SGAN is able to produce dense and accurate localization cues so that the segmentation performance is boosted. Moreover, by simply replacing the additional supervisions with partially labeled ground-truth, SGAN works effectively for semi-supervised semantic segmentation as well. Experiments on the PASCAL VOC 2012 and COCO datasets show that our approach outperforms all other state-of-the-art methods in both weakly and semi-supervised settings.</p><p>Abstract-Weakly supervised semantic segmentation (WSSS) using only image-level labels can greatly reduce the annotation cost and therefore has attracted considerable research interest. However, its performance is still inferior to the fully supervised counterparts. To mitigate the performance gap, we propose a saliency guided self-attention network (SGAN) to address the WSSS problem. The introduced self-attention mechanism is able to capture rich and extensive contextual information but also may mis-spread attentions to unexpected regions. To enable this mechanism work effectively under weak supervision, we integrate class-agnostic saliency priors into the self-attention mechanism to prevent the attentions on discriminative parts from misspreading to the background. And meanwhile we utilize classspecific attention cues as an additional supervision for SGAN, which reduces the mis-spread of attentions in regions belonging to different foreground categories. The proposed approach is able to produce dense and accurate localization cues, by which the segmentation performance is boosted. Experiments on PASCAL VOC 2012 dataset show that the proposed approach outperforms all other state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Semantic segmentation aims to predict a semantic label for each pixel in an image. Based upon the fundamental Fully Convolutional Networks (FCNs) <ref type="bibr">[1]</ref>, various techniques such as dilated convolution <ref type="bibr">[2]</ref>, spatial pyramid pooling <ref type="bibr">[3]</ref>, and encoder-decoders <ref type="bibr">[4]</ref> have been developed in the last decade. These techniques gradually improve segmentation accuracy via exploiting extensive contextual information. Recently, the self-attention mechanism <ref type="bibr">[5]</ref>, <ref type="bibr">[6]</ref>, <ref type="bibr">[7]</ref> has been successfully employed to capture richer contextual information and boost the segmentation performance further. Although the abovementioned methods have achieved high performance in semantic segmentation, they all work under full supervision. This supervision manner requires a large amount of pixelwise annotations for training, which are very expensive and time-consuming.</p><p>To reduce the annotation burden, different supervision forms such as bounding boxes <ref type="bibr">[8]</ref>, scribbles <ref type="bibr">[9]</ref>, and image-level tags <ref type="bibr">[10]</ref> have been considered for semantic segmentation. Among them, the form of using image-level tags has attracted major attention because of its minimal annotation cost as well as its great challenge. Recent work <ref type="bibr" target="#b43">[11]</ref> has shown that convolutional neural networks (CNNs) have the localization ability even if only image-level tags are used. This observation has inspired many weakly-supervised semantic segmentation (WSSS) researches. However, attentions in the class activation maps (CAMs) <ref type="bibr" target="#b43">[11]</ref> inferred from image classification networks <ref type="bibr">1</ref> Saliency Guided Self-attention Network for Weakly-supervised Semantic Segmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Semantic segmentation aims to predict a semantic label for each pixel in an image. Based upon the fundamental Fully Convolutional Networks (FCNs) <ref type="bibr">[1]</ref>, various techniques such as dilated convolution <ref type="bibr">[2]</ref>, spatial pyramid pooling <ref type="bibr">[3]</ref>, and encoder-decoders <ref type="bibr">[4]</ref> have been developed in the last decade. These techniques gradually improve segmentation accuracy via exploiting more and more extensive contextual information. Recently, the self-attention mechanism <ref type="bibr">[5]</ref>, <ref type="bibr">[6]</ref>, <ref type="bibr">[7]</ref> has been successfully employed to capture richer contextual information and boost the segmentation performance further. Although the above-mentioned methods have achieved high performance in semantic segmentation, they all work under full supervision. This supervision manner requires a large amount of pixel-wise annotations for training, which are very expensive and time-consuming.</p><p>To reduce the annotation burden, different supervision forms such as bounding boxes <ref type="bibr">[8]</ref>, scribbles <ref type="bibr">[9]</ref>, and image-level tags <ref type="bibr">[10]</ref> have been considered for semantic segmentation. Among them, the form of using image-level tags has attracted major attention because of its minimal annotation cost as well as its great challenge. Recent work <ref type="bibr" target="#b43">[11]</ref> has shown that convolutional neural networks (CNNs) have the localization ability even if only image-level tags are used. This observation has inspired many weakly-supervised semantic segmentation (WSSS) researches. However, attentions in the class activation maps (CAMs) <ref type="bibr" target="#b43">[11]</ref> inferred from image classification networks tend to focus on small discriminative parts of objects. The object location cues (also referred to as seeds) retrieved from these CAMs are too sparse to effectively train a segmentation model. Therefore, many efforts have been devoted to recover dense and reliable seeds <ref type="bibr" target="#b44">[12]</ref>, <ref type="bibr">[13]</ref>, <ref type="bibr">[14]</ref>, <ref type="bibr">[15]</ref>, <ref type="bibr">[16]</ref>. In this paper, we aim to take advantage of the self-attention mechanism to mine high-quality seeds. As validated in <ref type="bibr">[6]</ref>, <ref type="bibr">[5]</ref>, this mechanism is able to successfully capture longrange contextual dependencies in fully-supervised semantic segmentation. However, it encounters the following challenges when applied to WSSS. (1) Some foreground objects may always be co-occurrent with the same background, like 'boat' and 'water', leading to a pathological bias <ref type="bibr">[17]</ref>; (2) The global average pooling (GAP), which is commonly used in classification networks to aggregate pixel-wise responses into image-level label scores, encourages all responses to be high; (3) In the self-attention scheme, each pixel directly contributes to all other pixels and vice versa. These factors may result in a mis-spread of attentions from discriminative parts to unexpected regions. Typical examples are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. In the fully-supervised setting, the information of the selected discriminative pixels can be correctly propagated to the pixels belonging to the same category. Contrastively, the discriminative information is diffused to the regions of the background and other categories under weak supervision. tend to focus on small discriminative parts of objects. The object location cues (also referred to as seeds) retrieved from these CAMs are too sparse to effectively train a segmentation model. Therefore, a great amount of effort has been devoted to recover dense and reliable seeds <ref type="bibr" target="#b44">[12]</ref>, <ref type="bibr">[13]</ref>, <ref type="bibr">[14]</ref>, <ref type="bibr">[15]</ref>, <ref type="bibr">[16]</ref>.</p><p>In this paper, we aim to take advantage of the self-attention mechanism to mine high-quality seeds. As validated in <ref type="bibr">[6]</ref>, <ref type="bibr">[5]</ref>, this mechanism is able to successfully capture longrange contextual dependencies in fully-supervised semantic segmentation. However, it encounters the following challenges when applied to WSSS. (1) Some foreground objects may always be co-occurrent with the same background, like 'boat' and 'water', leading to a pathological bias <ref type="bibr">[17]</ref>; (2) The global average pooling (GAP), which is commonly used in classification networks to aggregate pixel-wise responses into image-level label scores, encourages all responses to be high; (3) In the self-attention scheme, each pixel directly contributes to all other pixels and vice versa. These factors may result in a mis-spread of attentions from discriminative parts to unexpected regions. Typical examples are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. In the fully-supervised setting, the information of the selected discriminative pixels can be correctly propagated to the pixels belonging to the same category. Contrastively, the discriminative information is diffused to the regions of the arXiv:1910.05475v2 [cs.CV] 12 Jan 2020 background and other categories under weak supervision.</p><p>To address the above-mentioned problems and enable the self-attention mechanism to capture long-range contextual information correctly under weak supervision, we construct a self-attention network that leverages the class-agnostic saliency as a guidance. A saliency map provides a rough detection of foreground objects so that it can prevent attentions from spreading to unexpected background regions. To further reduce information diffusion among foreground categories, we integrate the class-specific attention cues as additional supervision. The integration of these prior cues is implemented in our network via a joint learning of a seed segmentation branch and an image classification branch. By all these means, our network generates high quality seeds so that the segmentation performance is boosted.</p><p>Our work distinguishes itself from the others as follows:</p><p>• We propose a saliency-guided self-attention network (SGAN) for weakly supervised semantic segmentation. It integrates class-agnostic saliency maps and class-specific attention cues to enable the self-attention mechanism to work effectively under weak supervision. Moreover, these two types of priors are fused adaptively in our SGAN to help the generation of high quality seeds.</p><p>• In our network, both the seed segmentation branch and the image classification branch can produce high quality seed results. The ensemble of two results improves the quality of seeds further.</p><p>• By simply replacing saliency maps and attention cues with partially labeled segmentation ground-truth, SGAN can work effectively for semi-supervised semantic segmentation as well.</p><p>• Our approach achieves state-of-the-art performance on the PASCAL VOC 2012 and COCO datasets in both weakly and semi-supervised settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>A. Weakly-supervised Semantic Segmentation Various supervision forms have been exploited for weaklysupervised semantic segmentation (WSSS). Here, we focus on the works using image-level tags. Most recent methods solve the WSSS problem by first mining reliable seeds and then take them as proxy ground-truth to train segmentation models. Thus, a great amount of effort has been devoted to generate high-quality seeds.</p><p>A group of approaches take the class activation maps (CAMs) <ref type="bibr" target="#b43">[11]</ref> generated from classification networks as initial seeds. Since CAMs only focus on small discriminative regions which are too sparse to effectively supervise a segmentation model, various techniques such as adversarial erasing <ref type="bibr" target="#b44">[12]</ref>, <ref type="bibr">[17]</ref>, <ref type="bibr">[18]</ref>, <ref type="bibr">[19]</ref>, attention accumulation <ref type="bibr">[20]</ref>, and region growing <ref type="bibr">[13]</ref>, <ref type="bibr">[21]</ref>, <ref type="bibr">[22]</ref> have been developed to expand sparse object seeds. Another research line introduces dilated convolutions of different rates <ref type="bibr">[14]</ref>, <ref type="bibr">[16]</ref>, <ref type="bibr">[15]</ref>, <ref type="bibr">[23]</ref> to enlarge receptive fields in classification networks and aggregate multiple attention maps to achieve dense localization cues. In this work, we adopt the self-attention mechanism to capture richer and more extensive contextual information in order to mine high quality seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-attention Mechanism</head><p>The self-attention mechanism <ref type="bibr">[24]</ref> computes the context at each position as a weighted sum of all positions. Its superiority in capturing long-range dependencies has been recently validated by various vision tasks <ref type="bibr">[25]</ref>, <ref type="bibr">[26]</ref>, <ref type="bibr">[5]</ref>, <ref type="bibr">[6]</ref>. Particularly, in semantic segmentation, Yuan and Wang <ref type="bibr">[5]</ref> integrated this mechanism into pyramid structures to capture multi-scale contextual information; Fu et al. <ref type="bibr">[6]</ref> constructed a dual attention network to capture dependencies in both spatial and channel dimensions; Huang et al. <ref type="bibr">[27]</ref> proposed an interlaced sparse approach to improve the efficiency of the self-attention mechanism; and Huang et al. <ref type="bibr">[7]</ref> designed a recurrent criss-cross attention module to efficiently harvest the contextual information. These methods significantly boost the segmentation performance, but all of them perform under full supervision. Although Sun and Li <ref type="bibr">[21]</ref> utilized the selfattention scheme for WSSS, they only used this scheme to learn a saliency detector that is trained also in a fullysupervised manner. In our work, we apply the self-attention scheme to a weakly-supervised scenario which is more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Saliency Guidance for WSSS</head><p>Salient object detection (SOD) <ref type="bibr">[28]</ref> produces class-agnostic saliency maps that distinguish foreground objects from the background. The results of SOD have been extensively used in weakly-supervised semantic segmentation. For instance, many methods <ref type="bibr" target="#b44">[12]</ref>, <ref type="bibr">[13]</ref>, <ref type="bibr">[15]</ref>, <ref type="bibr">[23]</ref>, <ref type="bibr">[17]</ref>, <ref type="bibr">[14]</ref> exploited saliency maps to generate background seeds. Moreover, Wei et al. <ref type="bibr">[29]</ref> adopted a self-paced learning strategy to learn a segmentation model that was initialized under the full supervision of saliency maps of simple images. Sun and Li <ref type="bibr">[21]</ref> utilized saliency maps to guide a CAM-seeded region growing process to expand object regions. Fan et al. <ref type="bibr">[30]</ref> used instance-level saliency maps to construct and partition similarity graphs for WSSS. In addition, Chaudhry et al. <ref type="bibr">[19]</ref>, Oh et al. <ref type="bibr">[31]</ref>, and Wang et al. <ref type="bibr">[32]</ref> combined class-agnostic saliency maps with class-specific attention cues like us to obtain dense seed. But their combinations are implemented in user-defined ways. In contrast, our saliency maps and attention cues are adaptively fused within the proposed self-attention network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED APPROACH</head><p>The proposed approach for weakly and semi-supervised semantic segmentation is divided into two parts: (1) learning a saliency guided self-attention network to generate dense and accurate seeds, and (2) utilizing the high-quality seeds as proxy ground-truth to train a semantic segmentation model. The details are introduced in the followings.</p><p>A. Saliency Guided Self-attention Network 1) Network Architecture: The overview of our proposed saliency guided self-attention network (SGAN) is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. It consists of three components: (1) a CNN backbone to learn deep feature representations; (2) a saliency guided self-attention module that propagates attentions from small discriminative parts to non-discriminative regions via capturing long-range contextual dependencies; (3) an image classification branch together with a seed segmentation branch to supervise the training of the entire network.</p><p>We adopt a slightly modified VGG-16 <ref type="bibr">[10]</ref> network as the backbone for feature extraction. The last two pooling layers are removed in order to increase the resolution of the output feature maps. Note that, unlike previous works <ref type="bibr">[14]</ref>, <ref type="bibr">[23]</ref>, <ref type="bibr">[15]</ref> that enlarge the dilation rate of convolution kernels in conv5 block, we avoid the usage of dilated convolution and instead use the self-attention module to capture more extensive contexts.</p><p>2) Saliency Guided Self-attention Module: This module aims to take advantage of the self-attention mechanism to capture rich contextual information that is essential for discovering integral extent of objects and retrieving high-quality seeds. The self-attention mechanism has demonstrated its effectiveness in capturing long-range dependencies under full supervision <ref type="bibr">[6]</ref>, <ref type="bibr">[5]</ref>. However, simply integrating it into a weakly-supervised network may suffer from a severe misspread problem as introduced in Section I. Thus, we propose to incorporate class-agnostic saliency priors to prohibit the spread of attentions from discriminative object regions to the background.</p><p>We formally describe the saliency guided self-attention module as follows. This module takes the feature map output from the VGG's conv5 block, which is denoted as X ∈ R C×H×W , together with a saliency map as the inputs. With the input feature map, a sequence of spatial matrix operations are performed to generate a spatial attention map P ∈ R N ×N , where N = H × W is the number of positions. More specifically, X is first fed into two 1×1 convolutions for linear embedding and generating a key feature map K ∈ R C×H×W and a query feature map Q ∈ R C×H×W respectively. These two feature maps are further reshaped to R C×N . Then, the spatial attention map P is generated by computing the inner product of channel-wise features from any two positions of K and Q. That is,</p><formula xml:id="formula_0">P ij = K T i Q j ,<label>(1)</label></formula><p>where {i, j} ∈ {1, 2, ..., N } are the indexes of positions, and {K i , Q j } ∈ R C×1 are the channel-wise features. P ij measures the similarity of the features extracted at position i and j.</p><p>Note that different pairwise functions <ref type="bibr">[25]</ref> can be used for the similarity measurement, we take the inner product because it is simple but effective enough. For the input saliency map, we first threshold it to get a binary mask B and reshape it to R N ×1 . After that, a saliency attention map S ∈ R N ×N is computed by</p><formula xml:id="formula_1">S ij = 1(B i == B j ),<label>(2)</label></formula><p>where 1 is an indicator function. It equals one if both positions i and j are salient or non-salient.</p><p>Then, the context attention map D ∈ R N ×N is generated via an element-wise production between the spatial attention map P and the saliency attention map S, followed by a linear normalization:</p><formula xml:id="formula_2">D ij = P ij · S ij N j=1 P ij · S ij .<label>(3)</label></formula><p>Once the context attention map D is obtained, we use it to enhance the original feature map X. Specifically, we reshape X to R C×N and conduct a matrix multiplication between X and the transpose of D. Then we reshape the result back to R C×H×W and perform an element-wise summation with X to obtain the enhanced features E ∈ R C×H×W . That is,</p><formula xml:id="formula_3">E i = γ N j=1 (D ij · X j ) + X i ,<label>(4)</label></formula><p>where γ is a parameter initialized as 0 <ref type="bibr">[6]</ref> and gradually learned in training. Equation <ref type="formula" target="#formula_3">(4)</ref> indicates that each position of E is the sum of similarity-weighted features at all positions and the original features. Therefore, this module captures contextual information from a whole image. By this means, attentions on small discriminative parts of objects can be propagated to non-discriminative object regions, but not to the background because of the guidance of saliency.</p><p>3) Integrating Class-specific Attention Cues: The classagnostic saliency maps introduced above can only roughly separate foreground objects from the background, but provide no information about semantic categories. In order to prevent our SGAN from mis-spreading attentions among objects of different categories, we propose to integrate the class-specific attention cues obtained by the CAM method <ref type="bibr" target="#b43">[11]</ref> from a classification network as additional supervision.</p><p>Specifically, we construct a segmentation branch in our SGAN. It takes the enhanced feature E as the input and goes through a convolutional layer to produce M segmentation maps, each of which corresponds to a foreground category. Meanwhile, we retrieve reliable but sparse foreground object seeds by thresholding the class activation maps obtained from the VGG-16 classification network with a high confidence value (empirically set to 0.3 in this work) and use them to supervise the segmentation maps. The corresponding seed loss L seed is defined by</p><formula xml:id="formula_4">L seed = − 1 z∈Z |Λ z | z∈Z u∈Λz logΦ z,u .<label>(5)</label></formula><p>Here, Z denotes the set of foreground classes that present in an image and Λ z is a set of seed locations corresponding to class z. | · | is the cardinality of the set. Φ z,u denotes the probability of class z at position u in the segmentation maps. Note that, in contrast to the seeding loss defined in <ref type="bibr">[10]</ref>, <ref type="bibr">[13]</ref> that considers both foreground and background categories, our loss only takes into account the foreground classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Training SGAN:</head><p>The network also has an image classification branch that is supervised by image-level labels. Let us denote the classification probability as τ ∈ R M ×1 and the corresponding image-level label as y = [y 1 , ..., y M ] ∈ {1, −1}, which indicates the presence or absence of foreground object categories. Then the classification loss L cls is defined by the sigmoid cross entropy. That is,</p><formula xml:id="formula_5">L cls = − 1 M M m=1 log(y m · (τ m − 1 2 ) + 1 2 ).<label>(6)</label></formula><p>The overall loss for training our saliency guided selfattention network is defined by</p><formula xml:id="formula_6">L = L cls + λL seed ,<label>(7)</label></formula><p>where λ is a weighting factor to balance the two terms. 5) Generating High-quality Seeds: Once the proposed SGAN is trained, we note that there are two possible ways to get class activation maps (CAMs). One is following the common practice <ref type="bibr" target="#b43">[11]</ref> to infer the CAMs from the image classification branch. The other is directly taking the segmentation maps output from the seed segmentation branch. Either may be used to retrieve dense and accurate seeds. But we find out that the combination of them improves the seeds' quality further because they are complementary in some scenarios as will be shown in Section IV-D. Therefore, we take an elementwise summation of these two results to generate the final class activation maps.</p><p>Then, for each foreground class, we retrieve object seeds by thresholding the corresponding class activation map with a high value α. In addition, we retrieve background seeds by thresholding the input saliency map with a low value β. Following <ref type="bibr">[10]</ref>, <ref type="bibr">[13]</ref>, <ref type="bibr">[14]</ref>, we set α = 0.2 and β = 0.06 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training the Segmentation Network</head><p>After obtaining the high-quality seeds, we can use them as proxy ground-truth labels to train an arbitrary semantic segmentation network. In this work, we adopt the balanced seed loss L balance seed proposed in DSRG <ref type="bibr">[13]</ref> for the seed supervision. It is</p><formula xml:id="formula_7">L balance seed = − 1 z∈Z |Λ z | z∈Z u∈Λz logΦ z,u − 1 z∈Z |Λ z | z∈Z u∈Λz logΦ z,u ,<label>(8)</label></formula><p>where Z denotes the background. Z, Λ z , and Φ z,u holds the same definitions as previous.</p><p>We further exploit the boundary constraint loss used in both DSRG <ref type="bibr">[13]</ref> and SEC <ref type="bibr">[10]</ref> to encourage segmentation results to match up with object boundaries. Let us denote I as the input image and R(I, Φ) as the output probability map of the fully-connected CRF. Then the boundary constraint loss is defined as the mean KL-divergence between the segmentation map and the output of CRF:</p><formula xml:id="formula_8">L boundary = 1 N N u=1 M +1 z=1 R z,u (I, Φ)log R z,u (I, Φ) Φ z,u .<label>(9)</label></formula><p>Thus, the total loss for training the segmentation model is L = L balance seed + L boundary .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SGAN Under Semi-supervision</head><p>In the semi-supervised setting, a small number of training images are provided with strong pixel-level labels and the rest have image-level tags only. For these strongly annotated images, we replace their saliency maps with the binary foreground masks derived from the ground-truth annotations, and meanwhile use the ground-truth to supervise the seed segmentation branch. These simple replacements help the proposed SGAN to learn a better model and generate higherquality seeds for all images.</p><p>For the training of the segmentation network, we also have the ground-truth to take place of the generated seeds for these strongly annotated images. Except this, we keep the training loss under semi-supervision the same as that in the weaklysupervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Experimental Setup 1) Dataset and Evaluation Metric: The proposed approach is evaluated on the PASCAL VOC 2012 segmentation benchmark <ref type="bibr">[33]</ref> and the COCO <ref type="bibr">[34]</ref> dataset.</p><p>PASCAL VOC: This dataset provides pixel-wise annotations for 20 object classes and one background class. It contains 1464 images for training, 1449 images for validation and 1456 images for testing. Following the common practice <ref type="bibr">[10]</ref>, <ref type="bibr">[13]</ref>, <ref type="bibr">[14]</ref>, we augment the training set to 10,582 images. Our network is trained on the augmented training set only using image-level annotations and evaluated on the validation set in terms of the mean intersection-over-union (mIoU) criterion. Evaluation results of the test set are obtained by submitting our prediction results to the official PASCAL VOC evaluation server.</p><p>COCO: This dataset contains 80k images for training and 40k images for validation. Our network is trained on the training set with only image-level annotations and evaluated on the validation set in terms of mIoU for 81 categories.</p><p>2) Training Details: The saliency guided self-attention network is built on the VGG-16 network pre-trained on ImageNet. The remaining parameters of our SGAN are randomly initialized. Following <ref type="bibr">[14]</ref>, we use S-Net <ref type="bibr">[28]</ref> to get a class-agnostic saliency map for each image. SGD with mini-batch is used for training. The batch size is set to 15, the momentum is 0.9 and the weight decay is 0.0005. Input images are resized to 321 × 321 and no data augmentation except randomly horizontal flip are adopted. We train the SGAN for 8,000 iterations. The initial learning rate is 0.001 and it is decreased by a factor of 10 every 2,000 iterations.</p><p>The semantic segmentation model is chosen to use the Deeplab-ASPP <ref type="bibr">[2]</ref> network in order to compare with other WSSS works. Both VGG-16 and ResNet-101 backbones are investigated. The batch size is set to 15, the momentum is 0.9 and the weight decay is 0.0005. Input images are resized to 353 × 353 and randomly cropped to 321 × 321 for training. Horizontal flip and color jittering are employed for data augmentation. We train the segmentation model for 12,000 iterations. The initial learning rate is 0.001 and it is decreased by a factor of 0.33 every 2,000 iterations.</p><p>3) Reproducibility: We implement our SGAN on Py-Torch <ref type="bibr" target="#b67">[35]</ref> for training and producing high-quality seeds. We use the official Deeplab-ASPP code implemented on Caffe <ref type="bibr" target="#b68">[36]</ref> for semantic segmentation. All the experiments are conducted on a GTX 1080Ti GPU. The code is available at https://github.com/yaoqi-zd/SGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison to the State of the Art 1) Weakly-supervised Semantic Segmentation: PASCAL VOC:</head><p>We compare our approach with other stateof-the-art WSSS methods that are also supervised only by image-level labels. For fair comparison, we separate the methods into two groups according to the backbones upon which their segmentation models are built, as listed in <ref type="table" target="#tab_0">Table I</ref>. Most of existing methods use saliency maps to retrieve background seeds or even foreground seeds. Therefore, <ref type="table" target="#tab_0">Table I</ref> also marks out if a method uses saliency maps. <ref type="table" target="#tab_0">Table I</ref> shows that our method outperforms all the previous methods on both VGG-16 and ResNet-101 backbones. In particular, AE PSL <ref type="bibr" target="#b44">[12]</ref>, GAIN <ref type="bibr">[17]</ref>, and SeeNet <ref type="bibr">[18]</ref> use erasing techniques to get dense localization cues, which tend to identify true negative regions. AffinityNet <ref type="bibr" target="#b71">[39]</ref>, DSRG <ref type="bibr">[13]</ref>, SGDN <ref type="bibr">[21]</ref>, and SSDD <ref type="bibr">[22]</ref> adopt region growing techniques to expand seeds. It may be hard for them to expand to non-discriminative regions if initial seeds are concentrated on extremely small discriminative parts. OAA <ref type="bibr">[20]</ref> accumulates attention maps during the training procedure which may introduce unexpected attention regions at the early stage when the classifier is weak. MDC <ref type="bibr">[14]</ref>, DFPN <ref type="bibr">[16]</ref>, and FickleNet <ref type="bibr">[15]</ref> use dilated convolutions to retrieve dense seeds, whose receptive fields are not adaptive to image contents and may result in over-expansion. In contrast, our method can achieve dense and accurate seeds, which is benefitted from the self-attention mechanism as well as the design of our SGAN network. It needs to be mentioned that our approach outperforms DSNA <ref type="bibr" target="#b72">[40]</ref>, which uses a spatial attention mechanism, by a great margin. Our approach also performs better than AISI <ref type="bibr">[30]</ref> that leverages strong instance-level saliency information, CIAN <ref type="bibr">[23]</ref> that utilizes cross-image affinities, and SSNet <ref type="bibr" target="#b73">[41]</ref> that jointly learn saliency and segmentation with additional pixel-wise saliency supervision.</p><p>COCO: To further validate the effectiveness of our approach, we conduct experiments on the COCO dataset which is much more challenging than PASCAL VOC. Most existing methods haven't done the experiments on COCO yet, except DSRG <ref type="bibr">[13]</ref>. Therefore, we compare our results to DSRG in <ref type="table" target="#tab_0">Table II</ref>, in which the results of SEC are also quoted from DSRG's paper. <ref type="table" target="#tab_0">Table II</ref> shows that our VGG16-based SGAN surpasses both SEC and DSRG by a large margin. In particular, our method performs excellently on those categories with large scale, such as airplane, bus and train etc, but has difficulties in handling small things that are likely to be indistinguishable with the clustered background, such as baseball glove and spoon etc. 2) Semi-supervised Semantic Segmentation: We compare our approach with other state-of-the-art semi-supervised semantic segmentation methods on PASCAL VOC 2012 dataset under the same setting, where 1.4K images annotated with pixel-level labels and 9K images annotated with image-level tags are available. The comparison results are reported in <ref type="table" target="#tab_0">Table III</ref>, together with the results obtained by the fully supervised Deeplab-ASPP <ref type="bibr">[2]</ref> segmentation network. <ref type="table" target="#tab_0">Table III</ref> shows that our approach not only outperforms all previous methods but also reaches 95.3% of the performance under full supervision. It needs to be mentioned that most previous methods use the 1.4K pixel-level labels for training the semantic segmentation model only, while our approach can easily adopt them to facilitate the training of SGAN and improve the quality of dense seeds for the 9K weakly annotated images, leading to better performance.  <ref type="table" target="#tab_3">Table 3</ref> shows that our approach not only outperforms all previous methods but also reaches 95.3% of the performance under full supervision. It needs to be mentioned that most previous methods use the 1.4K pixellevel labels for training the semantic segmentation model only, while our approach can easily adopt them to facilitate the training of SGAN and improve the quality of dense seeds for the 9K weakly annotated images, leading to better performance.  <ref type="figure" target="#fig_4">Figure 3</ref> shows qualitative segmentation results obtained by the proposed approach on PASCAL VOC dataset. As we can see, our approach produces accurate segmentation results and recovers fine details of object boundaries for images containing scale variation, multiple objects, and complex background. A typical failure case is also presented in the last row, in which the dining table is indistinguishable from the background and thus misidentified as background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ABLATION STUDIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Effectiveness of The Components in SGAN</head><p>To investigate the effectiveness of each component in the saliency guided self-attention network, we conduct a series  The comparison results are listed in <ref type="table">Table 5</ref>, from which we make the following observations: (1) The SGAN-SAL-SEED model that applies the self-attention mechanism directly to a weakly-supervised network degrades the segmentation performance, especially for the categories that are always co-occurrent with the same background, for in-  <ref type="figure" target="#fig_4">Figure 3</ref> shows qualitative segmentation results obtained by the proposed approach on PASCAL VOC dataset. As we can  better than the recent WSSS method SSNet <ref type="bibr" target="#b74">[42]</ref> that jointly learns saliency and segmentation tasks, as shown in <ref type="table">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented a saliency guided selfattention network to address the semantic segmentation problem supervised by image-level labels only. To generate dense and accurate object seeds, we introduced the self-attention mechanism into the weakly-supervised scenario and utilized both class-agnostic saliency maps and class-specific attention cues to enable the mechanism work effectively. Extensive see, our approach produces accurate segmentation results and recovers fine details of object boundaries for images containing scale variation, multiple objects, and complex background. A typical failure case is also presented in the last row, in which the dining table is indistinguishable from the background and thus misidentified as background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Effectiveness of The Components in SGAN:</head><p>To investigate the effectiveness of each component in the saliency guided self-attention network, we conduct a series of experiments in different settings while keeping the VGG-16based segmentation model the same throughout all these experiments. Particularly, the following configurations are in-vestigated: (1) the full model, which is referred to as SGAN;</p><p>(2) the full model but only using the CAMs from the image classification branch, which is denoted as SGAN CLS ; (3) the full model but only using the CAMs from the seed segmentation branch, which is denoted as SGAN SEG ; (4) the model without the segmentation branch and the seed loss, which is denoted as SGAN-SEED; (5) the model without the segmentation branch, the seed loss, and the saliency guidance, which in essence is directly integrating the selfattention mechanism into the modified VGG-16 classification network. We denote this variant as SGAN-SAL-SEED; and (6) the baseline model without our proposed saliency guided self-attention module, which is actually the modified VGG-16 classification network.</p><p>The comparison results are listed in <ref type="table" target="#tab_0">Table IV</ref>, from which we make the following observations: (1) The SGAN-SAL-SEED model that applies the self-attention mechanism directly to a weakly-supervised network degrades the segmentation performance, especially for the categories that are always cooccurrent with the same background, for instance, 'airplane' with 'sky', 'boat' with 'water', 'horse' with 'grass', etc. In such cases, SGAN-SAL-SEED tends to propagate attentions from foreground objects to the concurrent background and generate inaccurate seeds. <ref type="formula" target="#formula_1">(2)</ref> The SGAN-SEED model that uses the proposed saliency guided self-attention module outperforms the baseline model over all categories. (3) After integrating class-specific attention cues, both SGAN CLS and SGAN SEG produce high quality seeds and achieve high segmentation performance. The ensemble of CAMs from two branches, that is SGAN, boosts the performance further. <ref type="bibr">(4)</ref> Compared to the baseline model, our full model boosts the performance significantly for the categories containing objects in large size, like 'bus' (+29.2%) and 'train' (+16.5%), and the categories with large scale variation such as 'person' (+14%). For these categories, the initial localization cues are usually too sparse to delineate the integral object extent. Our model can effectively propagate attentions from small discriminative parts to non-discriminative regions of objects and generate more complete object seeds, leading to much better segmentation performance.</p><p>In order to understand these models more intuitively, we present the class activation maps generated by each variant in <ref type="figure" target="#fig_6">Figure 4</ref>. From this figure we observe that SGAN-SAL-SEED tends to diffuse class-specific attentions to backgrounds and other categories. SGAN-SEED can greatly reduce the diffusion to backgrounds but it cannot prevent the miss-spread among foreground categories. SGAN CLS , SGAN SEG , and SGAN can constrain attentions mostly within the regions of the right class. Moreover, the CAMs obtained by SGAN CLS and SGAN SEG are complementary to each other in some cases such as the 'train' image and the 'sheep' image.</p><p>2) Evaluation of Seeds' Quality: High-quality seeds are obtained by thresholding the class activation maps presented above. Here, we also adopt precision, recall, and the Fmeasure score to evaluate the quality of seeds produced by different variants of our SGAN. The F-measure is defined as the weighted harmonic mean of the precision and recall:</p><formula xml:id="formula_9">F β = (1 + β 2 ) · P recision · Recall β 2 · P recision + Recall ,<label>(10)</label></formula><p>where β 2 is empirically set to be 0.4 to emphasize the importance of precision. <ref type="table" target="#tab_6">Table V</ref> reports the evaluation results. From it we get the following observations: (1) The SGAN-SAL-SEED model that applies the self-attention mechanism directly under weaklysupervised settings degrades the seed's precision drastically and thus leading to poor segmentation performance. <ref type="bibr">(</ref>2) The full model, SGAN, enhances the recall of seeds by a large margin while maintains the precision level, indicating that it can produce dense and accurate seeds.  3) Visualization of Context Attention Maps: To better understand how the self-attention mechanism behaves in our models, <ref type="figure">Figure 5</ref> visualizes the context attention maps learned by different variant of SGAN. Specifically, we select one discriminative pixel in each image and mark it by a yellow '+'. The attentions propagated from the selected pixel to all other pixels are indicated in the corresponding column of the learned context attention map. We reshape the column into the image size and overlay it on the color image for visualization. As shown in <ref type="figure">Figure 5</ref>, simply integrating the self-attention mechanism in a weakly-supervised network tends to mess up the attentions. Saliency priors can prohibit from spreading the attentions to the background. By further integrating the classspecific attention cues, our full model can restrict the attentions propagated mostly to the pixels belonging to the same category with the selected pixel. These maps help us to interpret the CAMs presented in <ref type="figure" target="#fig_6">Figure 4</ref>. 4) Influence of The Weighting Factor λ: The weighting factor λ in the total loss (in Equation <ref type="formula" target="#formula_6">(7)</ref>) of SGAN deter-mines the impact of the seed loss. Without the seed loss, no class-specific attention cues are included and our SGAN cannot handle the problem of mis-spreading attentions among foreground categories. Whereas, putting too much weight on this term may cause inefficient training due to the sparsity of the seeds. Therefore we carry out a set of experiments to check the influence of λ and report the results in <ref type="table" target="#tab_0">Table VI</ref>. It shows that λ = 0.15 leads to the best performance. Therefore, we empirically set this value throughout all other experiments.  <ref type="table" target="#tab_0">Table I</ref>, it is quite common for WSSS methods to take saliency as additional guidance. The reason is that it can provide rough localization of foreground objects and therefore help the propagation of CAMs. In our work, we adopt it to prevent object's discriminative information from spreading to unexpected background regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Influence of Saliency Detectors: As marked out in</head><p>Various saliency detectors, such as DHSNet <ref type="bibr" target="#b77">[45]</ref>, DSS <ref type="bibr">[46]</ref>, and S-Net <ref type="bibr">[28]</ref>, have been adopted in recent WSSS methods <ref type="bibr">[19]</ref>, <ref type="bibr">[18]</ref>, <ref type="bibr">[14]</ref> to produce saliency masks. In our work, saliency masks are generated by S-Net <ref type="bibr">[28]</ref>. We also conduct experiments using DHSNet <ref type="bibr" target="#b77">[45]</ref> and DSS <ref type="bibr">[46]</ref>. <ref type="table" target="#tab_0">Table VII</ref> shows that SGAN is not so sensitive to saliency detectors, because these saliency detection methods perform comparable well on outdoor scenarios but all have difficulties in handling indoor scenes. In addition, we admit that the errors in saliency masks may have negative effects for the WSSS task. But our proposed method is still performing better than the recent WSSS method SSNet <ref type="bibr" target="#b73">[41]</ref> that jointly learns saliency and segmentation tasks, as shown in <ref type="table" target="#tab_0">Table I</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented a saliency guided selfattention network to address the semantic segmentation problem supervised by image-level labels only. To generate dense and accurate object seeds, we introduced the self-attention mechanism into the weakly-supervised scenario and utilized both class-agnostic saliency maps and class-specific attention cues to enable the mechanism work effectively. Extensive experiments on PASCAL VOC 2012 dataset show that the proposed method outperforms the baseline model with a large margin and performs better than all other state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustrations of context attention maps learned by the self-attention scheme. (a) shows images where pixels of interest are marked by '+'. (b) presents the attention maps learned in a fully-supervised segmentation network, in which the pixels belonging to the same category with the selected pixels are highlighted. (c) shows the results learned in a weakly-supervised scenario, in which the information of the selected pixels is mis-spread to unexpected regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Illustrations of context attention maps learned by the self-attention scheme. (a) shows images where pixels of interest are marked by '+'. (b) presents the attention maps learned in a fully-supervised segmentation network, in which the pixels belonging to the same category with the selected pixels are highlighted. (c) shows the results learned in a weakly-supervised scenario, in which the information of the selected pixels is mis-spread to unexpected regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>An overview of the proposed saliency guided self-attention network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 3 :</head><label>3</label><figDesc>Examples of segmentation results on PASCAL VOC 2012 validation set obtained by the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>Examples of segmentation results on PASCAL VOC 2012 validation set obtained by the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 4 :</head><label>4</label><figDesc>Visualization of the class attention maps learned by different variants of our SGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 :</head><label>4</label><figDesc>Visualization of the class attention maps learned by different variants of our SGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FIGURE 5 : 10 Fig. 5 :</head><label>5105</label><figDesc>(3)  The F-measure score shows a strong correlation with the final segmentationAuthor et al.: P (a) Image (b) SGAN-SAL-SEED (c) SGAN-SEED (d) SGAN Visualization of the context attention maps learned by different variants of our SGAN. Visualization of the context attention maps learned by different variants of our SGAN.performance (mIoU). The higher a F-measure is, the better a variant SGAN model can perform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparison of weakly-supervised semantic segmentation methods on PASCAL VOC 2012 validation and test sets in terms of mIoU (%). For the methods using the ResNet backbone for segmentation, most of them use ResNet-101, except these marked with † that use ResNet-38.</figDesc><table><row><cell cols="2">Methods Backbone: VGG-16 network Publication SEC [10] ECCV'16 AF-SS [37] ECCV'16 CBTS [38] CVPR'17 AE PSL [12] CVPR'17 DCSP [19] CVPR'17 GAIN [17] CVPR'18 MCOF [32] CVPR'18 AffinityNet [39] CVPR'18 DSRG [13] CVPR'18 MDC [14] CVPR'18 SeeNet [18] NeurIPS'18 AISI [30] ECCV'18 SGDN [21] PRL'19 DSNA [40] TMM'19 FickleNet [15] CVPR'19 SSNet [41] ICCV'19 OAA [20] ICCV'19 RRM [42] AAAI'20 SGAN(Ours) -Backbone: ResNet network DCSP [19] CVPR'17 MCOF [32] CVPR'18</cell><cell cols="3">Extra guidance ---saliency saliency saliency saliency -saliency saliency saliency instance saliency 61.3 Val 50.7 52.6 52.8 55.0 58.6 55.3 56.2 58.4 59.0 60.4 61.1 saliency 50.5 -55.4 saliency 61.2 saliency 57.1 saliency 63.1 -60.7 saliency 64.2 65.0 1 Test 51.1 52.7 53.7 55.7 59.2 56.8 57.6 60.5 60.4 60.8 60.7 62.1 51.3 56.4 61.8 58.6 62.8 61.0 saliency 60.8 61.8 saliency 60.3 61.2</cell></row><row><cell>AffinityNet  † [39] DSRG [13] SeeNet [18] AISI [30] CIAN [23] DFPN [16] DSNA [40] FickleNet [15]</cell><cell>CVPR'18 CVPR'18 NeurIPS'18 ECCV'18 arXiv'18 TIP'19 TMM'19 CVPR'19</cell><cell cols="2">-saliency saliency instance saliency 63.6 61.7 61.4 63.1 saliency 64.1 -61.9 -58.2 saliency 64.9</cell><cell>63.7 63.2 62.8 64.5 64.7 62.8 60.1 65.3</cell></row><row><cell>SSDD  † [22] OAA [20]</cell><cell>ICCV'19 ICCV'19</cell><cell>-saliency</cell><cell>64.9 65.2</cell><cell>65.5 66.4</cell></row><row><cell>SSENet  † [43] RRM [42] SGAN(Ours)</cell><cell>arXiv'19 AAAI'20 -</cell><cell>--saliency</cell><cell cols="2">63.3 66.3 67.1 67.2 2 64.9 66.5</cell></row></table><note>1 http://host.robots.ox.ac.uk:8080/anonymous/GLCTVA.html2 http://host.robots.ox.ac.uk:8080/anonymous/SINTUJ.html</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Comparison of weakly-supervised semantic segmentation methods on COCO validation set in terms of mIoU (%). All methods are based on the VGG-16 backbone.</figDesc><table><row><cell>class background person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove skateboard surfboard tennis racket bottle</cell><cell>SEC 74.3 43.6 24.2 15.9 52.1 36.6 37.7 30.1 24.1 17.3 16.7 55.9 48.4 25.2 16.4 34.7 57.2 45.2 34.4 40.3 41.4 62.9 59.1 59.8 48.8 0.3 26.0 0.5 6.5 16.7 12.3 1.6 5.3 7.9 9.1 1.0 0.6 7.1 7.7 9.1 13.2</cell><cell>DSRG Ours 80.6 73.9 -53.8 30.4 45.6 22.1 35.5 54.2 67.4 45.2 66.8 38.7 66.0 33.2 65.0 25.9 44.3 20.6 37.4 16.2 16.6 60.4 58.6 51.0 47.1 26.3 53.6 22.3 24.7 41.5 54.5 62.2 73.4 55.6 63.1 42.3 64.9 47.1 60.8 49.3 63.2 67.1 81.3 62.6 77.4 63.2 66.8 54.3 61.3 0.2 9.1 35.3 42.5 0.7 2.9 7.0 3.7 23.4 36.7 13.0 26.1 1.5 4.2 16.3 14.3 9.8 9.0 17.4 14.7 4.8 2.7 1.2 0.2 14.4 16.2 13.5 21.8 6.8 11.8 22.3 24.6</cell><cell>class wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza donut cake chair couch potted plant bed dining table toilet tv laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors teddy bear hair drier toothbrush mean IoU</cell><cell>SEC 22.3 17.9 1.8 1.4 0.6 12.5 43.6 23.6 22.8 44.3 36.8 6.7 31.2 50.9 32.8 12.0 7.8 5.6 6.2 23.4 0.0 38.5 19.2 20.1 3.5 17.5 12.5 32.1 8.2 13.7 0.0 10.8 4.0 0.4 17.8 18.4 16.5 47.0 0.0 2.8 22.4</cell><cell>DSRG Ours 24.0 28.4 20.4 29.3 0.0 14.5 5.0 7.7 0.5 4.1 18.8 19.4 46.4 48.1 24.3 32.1 24.5 40.6 41.2 43.2 35.7 34.2 15.3 23.8 24.9 38.1 56.2 62.5 34.2 49.2 6.9 40.3 9.7 14.7 17.7 22.8 14.3 11.1 32.4 35.8 3.8 6.4 43.6 48.9 25.3 33.5 21.1 36.8 0.9 21.9 20.6 22.1 12.3 42.2 33.0 30.8 11.2 24.7 12.4 24.8 0.0 0.0 17.8 18.2 15.5 24.3 12.3 24.3 20.7 17.6 23.9 11.3 17.3 18.0 46.3 45.4 0.0 0.0 4.5 7.1 26.0 33.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Comparison of semi-supervised semantic segmentation methods on PASCAL VOC 2012 validation set in terms of mIoU. All methods are based on the VGG-16 backbone.</figDesc><table><row><cell>Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS</cell><cell>Methods Deeplab [2] WSSL [44] GAIN [17] MDC [14] DSRG [13] FickleNet [15] SGAN(Ours)</cell><cell>Publication TPAMI'17 ICCV'15 CVPR'18 CVPR'18 CVPR'18 CVPR'19 -</cell><cell>Training Set 1.4K strong 10.6K strong 1.4K strong + 9K weak 1.4K strong + 9K weak 1.4K strong + 9K weak 1.4K strong + 9K weak 1.4K strong + 9K weak 1.4K strong + 9K weak</cell><cell>mIoU 62.5 70.3 64.6 60.5 65.7 64.3 65.8 67.0</cell></row><row><cell>obtained by the fully supervised Deeplab-ASPP [2] segmen-tation network.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>Comparison of semi-supervised semantic segmentation methods on PASCAL VOC 2012 validation set in terms of mIoU. All methods are based on the VGG-16 backbone.</figDesc><table><row><cell>Methods Deeplab [2] WSSL [44] GAIN [17] MDC [14] DSRG [13] FickleNet [15] SGAN(Ours)</cell><cell>Publication TPAMI'17 ICCV'15 CVPR'18 CVPR'18 CVPR'18 CVPR'19 -</cell><cell>Training Set 1.4K strong 10.6K strong 1.4K strong + 9K weak 1.4K strong + 9K weak 1.4K strong + 9K weak 1.4K strong + 9K weak 1.4K strong + 9K weak 1.4K strong + 9K weak</cell><cell>mIoU 62.5 70.3 64.6 60.5 65.7 64.3 65.8 67.0</cell></row><row><cell cols="2">C. QUALITATIVE RESULTS</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison of the proposed model under different settings on VOC 2012 val set in terms of mIoU (%). 68.1 29.8 71.8 56.2 56.3 47.6 69.7 75.4 18.6 60.6 18.3 62.6 62.1 67.1 59.3 34.4 69.7 27.3 58.4 55.0 SGAN-SAL-SEED 45.8 78.7 51.4 22.1 23.5 21.4 62.5 73.8 60.2 80.6 6.6 58.1 4.3 69.5 45.8 65.3 66.1 31.4 35.4 23.7 48.7 35.3 SGAN-SEED 62.4 89.5 75.4 31.0 75.1 60.0 66.3 68.3 73.8 82.3 23.0 74.8 25.1 76.2 69.0 69.1 72.8 40.3 71.5 32.8 73.2 60.6 SGAN CLS 63.7 89.6 75.0 31.8 73.1 61.1 67.4 79.1 75.4 82.3 26.3 75.0 28.5 75.7 67.8 70.1 73.1 45.7 72.5 35.6 73.2 58.6 SGAN SEG 63.4 89.9 80.6 34.6 76.1 60.4 70.0 75.7 72.0 82.8 20.0 76.7 16.8 76.8 71.2 70.1 71.8 45.1 73.5 35.2 75.2 57.Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS</figDesc><table><row><cell>Method Baseline SGAN</cell><cell>mIoU 55.0 64.2</cell><cell>bkg 86.5 2 aero bike bird boat bottle bus car cat chair cow table dog horse motor person plant sheep sofa train tv 89.9 77.7 33.7 75.3 61.7 68.5 76.8 76.3 81.7 28.7 75.8 27.4 75.6 70.0 70.6 73.3 41.8 73.5 35.6 74.9 59.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 7 :</head><label>7</label><figDesc>Influence of saliency detectors to the segmentation performance on PASCAL VOC 2012 validation set.</figDesc><table><row><cell>saliency mask mIoU (%)</cell><cell>S-Net [29] DSS [46] DHSNet [45] 64.2 64.0 63.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>The quality of seeds generated by different variants of our SGAN model.</figDesc><table><row><cell>Method Baseline SGAN-SAL-SEED SGAN-SEED SGAN CLS SGAN SEG SGAN</cell><cell>Precision Recall 75.5 28.4 31.7 47.6 76.1 48.6 74.4 60.0 73.8 57.1 76.4 57.4</cell><cell>F score mIoU 61.5 55.0 33.2 45.8 70.5 62.4 72.0 63.7 70.9 63.4 73.0 64.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Influence of the weighting factor λ to the segmentation performance on PASCAL VOC 2012 validation set.</figDesc><table><row><cell>λ mIoU (%) 62.4 63.9 64.0 64.2 64.1 64.0 63.6 0 0.05 0.1 0.15 0.2 0.25 0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII :</head><label>VII</label><figDesc>Influence of saliency detectors to the segmentation performance on PASCAL VOC 2012 validation set.</figDesc><table><row><cell>saliency mask S-Net [28] DSS [46] DHSNet [45] mIoU (%) 64.2 64.0 63.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to segment under various forms of weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3781" to="3790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7014" to="7023" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7268" to="7277" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semisupervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coarse-to-fine semantic segmentation from image-level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="9215" to="9223" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="547" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discovering class-specific pixels for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2070" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Self-supervised scale equivariant network for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03714</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency guided deep network for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="62" to="68" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cian: Cross-image affinity net for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10842</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huan</surname></persName>
		</author>
		<idno>arXiv:19</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Xiao tion wit on Mult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>mantic s intellige</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Associ segment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Exploi CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1354</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The pa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to segment under various forms of weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3781" to="3790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribblesupervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7014" to="7023" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7268" to="7277" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semisupervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Coarse-to-fine semantic segmentation from image-level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="9215" to="9223" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="547" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Discovering class-specific pixels for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2070" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Saliency guided deep network for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="62" to="68" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Cian: Cross-image affinity net for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10842</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7794" to="7803" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Interlaced sparse self-attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12273</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep salient object detection with dense connections and distraction diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3239" to="3251" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Associating inter-image salient instances for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="367" to="383" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Exploiting saliency for object segmentation from image level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5038" to="5047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Augmented feedback in semantic segmentation under image level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Combining bottom-up, top-down, and smoothness cues for weakly supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3529" to="3538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with imagelevel supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="4981" to="4990" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Decoupled spatial neural attention for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Joint learning of saliency detection and weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Reliability does matter: An end-to-endweakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Self-supervised scale equivariant network for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03714</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Weaklyand semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
