<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bonsai-Net: One-Shot Neural Architecture Search via Differentiable Pruners</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Geada</surname></persName>
							<email>r.geada2@newcastle.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Newcastle University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Prangle</surname></persName>
							<email>dennis.prangle@newcastle.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Newcastle University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Stephen</forename><surname>Mcgough</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Newcastle University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bonsai-Net: One-Shot Neural Architecture Search via Differentiable Pruners</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-shot Neural Architecture Search (NAS) aims to minimize the computational expense of discovering state-ofthe-art models. However, in the past year attention has been drawn to the comparable performance of naïve random search across the same search spaces used by leading NAS algorithms. To address this, we explore the effects of drastically relaxing the NAS search space, and we present Bonsai-Net 1 , an efficient one-shot NAS method to explore our relaxed search space. Bonsai-Net is built around a modified differential pruner and can consistently discover stateof-the-art architectures that are significantly better than random search with fewer parameters than other state-ofthe-art methods. Additionally, Bonsai-Net performs simultaneous model search and training, dramatically reducing the total time it takes to generate fully-trained models from scratch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural Architecture Search (NAS) is a field which contains many promising methods for developing state-of-theart architectures. However, recent work by Yu et al. <ref type="bibr">[7]</ref> and Li &amp; Talwalkar <ref type="bibr" target="#b3">[4]</ref> found the performance of these methods to be barely better, if not worse, than random search. Our hypothesis as to why this occurs mirrors that of Yu et al.; the search space for these methodologies is overconstrained to the extent that any model from the search space would perform well. This is in part due to the saturation of the CIFAR-10 problem, in that all the cumulative research on best practices has leaked into the design of the search spaces. This means that the search spaces exclusively contain excellent CIFAR-10 architectures, which both eliminates any potential benefit of NAS as well as limits the generalizability of these spaces to other problem domains.</p><p>To address these issues, we have designed a new search space that significantly relaxes the restrictions commonly seen in other NAS methods, such as to remove their implicit design biases but also explore novel design patterns not seen in other works. Additionally, we have produced a one-shot NAS method capable of efficiently discovering state-of-the-art models within this new space via differentiable, memory-aware pruners, such as to investigate the efficacy of NAS over broader search spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Search Space</head><p>Our search space is based on the search space of DARTS <ref type="bibr" target="#b4">[5]</ref> and PC-DARTS <ref type="bibr" target="#b5">[6]</ref>, whose models are composed of stacked cells, each cell a directed graph wherein edges are tensor operations and nodes are tensor aggregations. Cells are classified as either a normal cell, meaning that tensor dimensionality remains unchanged throughout, or as a reduction cell, meaning that spatial dimensions are halved while the channel dimension is doubled. However, in both of these papers and in most existing NAS models, each cell C n is restricted to receiving input from the two previous cells C n−1 and C n−2 , and each reduction or normal cell in the model is necessarily homogenous, that is, identical to every other cell of the same type. In their space, there are roughly 6 × 10 11 possible 4 node cells, 2 different cells per model, and 1 possible set of connections between these cells.</p><p>To relax this search space, we allow each cell to receive input from both the previous cell as well as any combination of previous cells, each node can receive input from any combination of previous nodes in the cell, and each edge can contain any combination of the operations in the operation space. Most importantly, each cell in the network is distinct, meaning each cell has a distinct connectivity and operation set. The operation space is equivalent to that of PC-DARTS: identity, 3x3 average pooling, 3x3 max pooling, 3x3 separable and dilated convolutions, and 5x5 separable and dilated convolutions. Therefore, our search space is a significantly larger superset of the DARTS space; an 8 cell model has around 3 × 10 29 possible 4 node cells, 8 different cells per model, and 254 different sets of connections between the cells. LG] 12 Jun 2020</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Differentiable Pruners</head><p>The differentiable pruner was introduced by Kim et al. <ref type="bibr" target="#b1">[2]</ref> in 2019 as a means of pruning FLOPS from neural operations. The authors describe the problem of gating channels via a gate function, where the gate function has a constant derivative of 0 everywhere it is differentiable, meaning that gradient descent cannot be aware of the effects of the gate. To remedy this, the authors add a saw wave S to the gate function G, creating a trainable gate function P :</p><formula xml:id="formula_0">G(w) = 0 w &lt; 0 1 w ≥ 0 (1) S(w) = M w − M w M (2) P (x, w) = (G(w) + S(w))x<label>(3)</label></formula><p>where x is the output tensor of the operation to be pruned, and w is the vector of weights that control the gating effects of the pruner. With large enough M , G(w) + S(w) is effectively 0 or 1 everywhere while still having a constant derivative of 1 everywhere it is differentiable, as visualized in <ref type="figure" target="#fig_2">Figure 1</ref> of the supplementary materials. This lets gradient descent track the effects of gating, and thus allows for gating to be a parameter learned simultaneous with model training. Compression is encouraged by adding a compression term to the loss function, computed as follows:</p><formula xml:id="formula_1">L comp = λ c target − c actual<label>(4)</label></formula><p>where λ is the compression weight (such as to balance this term with the regression or classification loss of the model), c actual is the vector of the model's compression, and c target is the desired compression level. In the original paper, compression is computed as the ratio of the model's current FLOP count to its original FLOP count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Memory-Aware Pruners</head><p>For our purposes, we adapt the differentiable pruner from a channel-wise operation to a generic binary operation, that can gate any connection within the model. The weight vectors of these adapted pruners are of size 1, meaning that the tensors passed through the pruners are either entirely preserved or entirely pruned. In addition, we compute the memory size s of each potentially pruned connection in the network, to be used in the computation of our c actual , which is the vector of the compression of each cell in the model:</p><formula xml:id="formula_2">c actuali = Memory Size Unpruned Operations in C i Memory Size All Operations in C i (5) = #cnx∈Ci j=0 (s j * (G(w j ) + S(w j ))) #cnx∈Ci j=0 s j<label>(6)</label></formula><p>where C i is the ith cell of the model, #cnx is the number of connections in C i , s j is the memory size of the jth connection, and w j the weight value of the pruner along that connection. Therefore, a cell that started with 8 GiB VRAM usage and then pruned down to 6 GiB would have a compression ratio of 6 8 = 0.75. The λ term in L comp as per equation 4 is chosen such that the overall compression penalty does not overpower the classification or regression loss of the network; if we must encourage pruning we want to minimize its effect on the model's performance on the target task. From our experiments, best results occur when the compression loss is around 1% of the classification loss, which typically corresponds to a λ term on the order of 0.01.</p><p>The advantage of memory-aware pruners is that they allow the memory cost of a model to be a directly differentiable parameter. In our experience, GPU memory is a major limiting factor in deployment, and thus being able to directly tune memory cost during training to a variety of memory and hardware constraints is tremendously useful. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NAS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bonsai-Net</head><p>Bonsai-Net is our NAS model that leverages these memory-aware differentiable pruners to discover models in our relaxed search space. Bonsai-Net creates a hypernetwork similar to DARTS, wherein the model is initialized with every possible connection within the search space, called the hyper-connected state. This means that every cell receives the output of the n − 1th cell as well as the combination of every previous cell's output, each node within the cell receives the combination of each previous node, and each edge is the combination of each operation in the operation set. Pruners are put along each intra-cellular and inter-cellular connection, such that Bonsai-Net can eliminate any connection as it sees fit. The hyper-connected state is visualized in <ref type="figure" target="#fig_5">Figure 2</ref> of the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The Bonsai Process</head><p>Due to the immense number of connections in the Bonsai-Net hypernetwork, the entirety of the hyperconnected model cannot be fit into GPU memory at first. Instead, models are divided into groups of consecutive cells called sections, such that each section is roughly the same size and the first section can fit into GPU memory in the hyper-connected state. The model is then trained iteratively; the first section is added to the model along with a temporary classification tower (consisting of global average pooling followed by a fully-connected layer), then the model is trained and pruned until the next section can fit into memory. When a new section is added, the weights of the previous sections are preserved and the old classification tower is converted into an auxiliary tower to help bootstrap the new section.</p><p>We've likened this growing and pruning process to that of growing a bonsai tree, hence the name. Through the use of memory-aware pruners, we can directly encourage Bonsai-Net to free up GPU space for these future sec </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">CIFAR-10 Configuration</head><p>The configuration for our Bonsai-Net models for CIFAR-10 is chosen to be similar to that of DARTS. Models have 8 cells, with reductions cells placed at 1/3rd and 2/3rds of model depth. The initial channel count is 36 with a batch size of 64, and the models are trained for 600 epochs with a cosine-annealed learning rate starting at 0.01. We use a drop-path value of 0.3 <ref type="bibr" target="#b2">[3]</ref>, and data is augmented with the standard set of augmentations and cutout. Compression λ is set to 0.01 at the start of each prune cycle, and is doubled every 16 epochs while the compression target is not met. The cellular composition as well as time spent at each section from an example run of Bonsai-Net on CIFAR-10 is given in table 3 in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Random Search and Ablation Study</head><p>We tested Bonsai-Net against two different levels of randomness, to both evaluate our algorithm as well as perform an ablation study. Both levels are trained for the same number of epochs with the same hyper-parameters as the Bonsai-Net models, to allow for direct comparison. The levels are as follows:</p><p>Level 1: Sections 1 through n − 1 are randomly connected at the same compression level as sections 1 through n − 1 of the Bonsai-Net model. Section n is added at full size. The model is allowed to prune internal connections. This level tests the effectiveness of iteratively building the model up to its final size via the Bonsai process versus random search.</p><p>Level 2: The full model is initialized at the same compression as the fully trained Bonsai-Net model, and is not allowed to prune. This level tests the efficacy of the joint Bonsai and pruning process in generating model architectures versus pure random search. This is equivalent to the random search in the work of Yu et al. and Li &amp; Talkwalkar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Performance and Comparisons</head><p>Test Acc.</p><p>Parameters Bonsai-Net 96.65 ± 0.06% 2.95 ± 0.11 M Random-1 95.27 ± 0.05% 3.89 ± 0.12 M Random-2 95.19 ± 0.13% 3.03 ± 0.01 M <ref type="table">Table 1</ref>: Bonsai-Net performance, parameter counts, and total runtime versus the two levels of random search. Each configuration is tested three times, and the average of these three runs is reported. Models used 7.21 ± 0.14 GiB of GPU memory on average. Error bounds are computed as the standard error of mean. <ref type="table">Table 1</ref> shows that both random searches performed significantly worse than Bonsai-Net, despite using the same amount of GPU memory and roughly similar parameter counts, which indicates the effectiveness of Bonsai-Net in discovering and training models. Bonsai-Net achieves similar mean performance to other state-of-the-art methods as per DARTS, 650K fewer than PC-DARTS, and 1.7M fewer than ENAS. Since the search and train phases of Bonsai-Net are codependent, we report the total time from the commencement of the algorithm to the completion of training, which is typically around 3.3 GPU days on an NVidia 1080Ti, nearly twice as fast as the total runtime of second-order DARTS on the same GPU (6.5 GPU days). Additionally, table 2 demonstrates that de-constricting the search space has indeed removed some of the design bias of the restricted space; the average randomly-selected model in the Bonsai search space performs 1.29 percentage points worse than the average randomly-selected model in the constricted search space of other NAS algorithms. However, despite the de-constricted space, Bonsai-Net still discovers state-of-the-art architectures. While future work is necessary to compare how other NAS methods perform in the Bonsai search space, implementing the other methods into a search space that does not enforce cellular homogeneity may prove to be impossible due to VRAM constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Observations</head><p>While observing the pruning patterns of our models, consistent 'choices' became noticeable. These seem to indicate that the architectural decisions being made are deliberate, learned choices rather than the result of random chance:</p><p>• Models seem to have a preferential ranking of operations, demonstrated by a consistent occurrence frequency of various operations in different types of cells.</p><p>In normal cells, the most frequently chosen operation by far is identity operations, followed by separable convolutions, then max pooling. Dilated convolutions are rarely chosen, around a dozen per model, while average pooling has never been chosen in any recorded run. Operation counts are shown in <ref type="figure">Figures 3 and 4</ref> of the supplementary materials.</p><p>• Average pooling operations are always pruned out of the model entirely. Discovering why this is the case presents an interesting avenue for future investigation.</p><p>• At a higher organizational level, Bonsai-Net tends to build ResNet-esque <ref type="bibr" target="#b0">[1]</ref> patterns within cells, favoring edges that are the summation of identity and separable convolution operations. This is one of the most common learned designs for edges within Bonsai-Net, and is a configuration that is only possible within our search space. Specific examples are shown in <ref type="figure">Figure 5</ref> of the supplementary materials.</p><p>• Cells prefer to receive connections from closer cells rather than further ones. The most frequently chosen input for cell C n is C n−1 , then C n−2 , and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have created an expanded Neural Architecture Search space, and have designed a one-shot NAS method to produce state-of-the-art results over this new space. Our search space presents a more realistic starting point for NAS; when approaching a novel problem, the intuition and design experience that is encoded into the restricted search spaces of other NAS algorithms does not exist, and thus techniques must be able to operate on significantly broader search spaces. Our results demonstrate that our search space is significantly less constricted than that of other NAS methods, but over this expanded and more realistic space our one-shot NAS algorithm Bonsai-Net still produces state-ofthe-art results, while generating fully-trained models in a much faster time and with fewer parameters than other leading algorithms.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bonsai-Net: Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Operation Counts</head><p>To visualize the architectural decisions made by Bonsai-Net models, here we have plotted the number and type of operations that were in each of the three Bonsai-Net models at the end of 600 epochs, grouped by cell type and by cell depth within the model.   <ref type="figure">Figure 3</ref>: Here we see the total operation counts of each normal and reduction cell across three separate Bonsai-Net runs. Notice the similar operation frequencies between cells of the same type across different runs.   <ref type="figure">Figure 4</ref>: Here we see the total operation counts from each cell in three separate Bonsai-Net runs. Notice the frequency similarities between cells at the same depth across the three runs, as well as the drastically different schema for reduction cells. <ref type="figure">Figure 5</ref>: Above is the learned connectivity of the zeroth cell from Model 1 above, after 600 epochs of training. The 'In' node refers to the input of the model, which passes unmodified to the two input nodes of the cell, Input 0 and Input 1. Notice the ResNet-esque patterns appearing at different scales. Across two-nodes, such patterns appear as Input 1 → 1, Input 2 → 2, or 1 → Out, where identity and convolution operations run parallel with each other into a summation node. They also appear across three-node patterns, such as Input 1 → 0 → 2 or 0 → 1 → Out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">By Cell Type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">By Cell Depth</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>arXiv:2006.09264v1 [cs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>R</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :</head><label>1</label><figDesc>tions, via a compression term as per equation 4 added to the loss function. The compression targets for this term are established at the start of training, by determining how much space each new section of the hypernetwork requires. The lengths of the pruning periods for each section are not fixed, but rather depend on the time it takes for the model to achieve the necessary compression. These intermediate training steps are included in Bonsai-Net's training allocation, and thus the total number of training epochs is fixed regardless of how long the model takes to build itself to full size. At this full size, the model trains with no compression term in the loss, meaning that any compression that does occur is solely motivated by performance. The full algorithm is detailed in algorithm 1 below: The Bonsai-Net algorithm For all n ∈ [1, # sections ], determine the compression c n s.t. section s n can be appended to the model; Initialize hyper-connected section s 0 ; for n in [1, # sections ] do while c actual &gt; c n do Train + prune, c target = c n ; Convert classification tower to auxiliary tower; Add hyper-connected section s n ; Add new classification tower; Finish remaining train+prune epochs, c target = None;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1</head><label>1</label><figDesc>Pruner Operation G(w) + S(w) from equation 3 in the main paper is shown below:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 : 1 [ 7 ]</head><label>117</label><figDesc>G(w) + S(w), shown at an unrealistically small M = 40 for clarity. Larger M values (typically around 1e5 in actual models) reduce the magnitude of the saw component to imperceptiblity. Notice how the saw function introduces a consistent gradient to the gate function without drastically affecting the function's overall shape. Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, and Mathieu Salzman. Evaluating the search phase of neural architecture search. CoRR, abs/1902.08142, 2019. A hyperconnected four node cell, the same size as the cells in the Bonsai-Net runs described in the main paper. Pruners are represented by horizontal black boxes. Input 0 is the permanent, hardcoded input from the previous cell, while Input 1 is the prunable input from each previous cell. Between each node is each of the possible operations in the search space, all of which are independently passed through pruners before being summed together and inputted into their target node. The intercellular connectivity for a small four cell model. Pruners are again represented by the horizontal black boxes. Each cell receives a permanent, hard-coded input from the previous cell, as well as a prunable combination of the outputs of each of the previous cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Intra-and inter-cellular hyper-connectivityIn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table 2 ,</head><label>2</label><figDesc>while requiring 350K fewer parameters than</figDesc><table><row><cell>Restricted Search Space Test Acc. ∆ Random Search 96.86 ± 0.17% 0.38% 96.76 ± 0.10% 0.28% 96.62 ± 0.23% 0.14% BayesNAS 95.99 ± 0.25% Model NAO ENAS DARTS -0.44% Random -96.48 ± 0.18% Bonsai Search Space Bonsai-Net 96.65 ± 0.05% 1.45% Random 95.19 ± 0.13% -</cell></row><row><cell>Table 2: Bonsai-Net performance versus random search, compared to other NAS algorithms. Results for other algo-rithms is taken from Yu et al. [7]. Level-2 random search is chosen for comparison as it aligns with the random-search methodology of Yu et al.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>the cellular makeup, total model size, compression targets, and epochs spent training each section are listed for an example run of Bonsai-Net.</figDesc><table><row><cell>Epochs 0 → 7 8 → 51 52 → 59 60 → 75 None 76 → 600 Section Cells Total # Cells c target 0 [N, N] 2 89.4% 1 [R] 3 46.4% 2 [N, N] 5 51.8% 3 [R] 6 41.1% 4 [N, N] 8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The cellular makeup (where 'N' refers to a normal cell, and 'R' a reduction), total cumulative cell count, compression target, and the number of training epochs used for each section from an example Bonsai-Net run on CIFAR-10. The model compressed to 28.4% by epoch 600 solely due to classification performance.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See https://github.com/RobGeada/bonsai-net-lite for code and implementation details.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Plug-in, trainable gate for streamlining arbitrary neural networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaedeok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyoun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun-Joo</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonsuck</forename><surname>Choe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fractalnet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno>abs/1605.07648</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">DARTS: differentiable architecture search. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">PC-DARTS: partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi Tian Amd Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<idno>abs/1907.05737</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
