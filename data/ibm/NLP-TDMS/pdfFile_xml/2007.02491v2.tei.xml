<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dark Matter</orgName>
								<orgName type="institution">AI Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Su</surname></persName>
							<email>sujiang@dm-ai.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Dark Matter</orgName>
								<orgName type="institution">AI Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Dark Matter</orgName>
								<orgName type="institution">AI Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Model Compression; Neural Network Pruning;</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finding out the computational redundant part of a trained Deep Neural Network (DNN) is the key question that pruning algorithms target on. Many algorithms try to predict model performance of the pruned sub-nets by introducing various evaluation methods. But they are either inaccurate or very complicated for general application. In this work, we present a pruning method called EagleEye, in which a simple yet efficient evaluation component based on adaptive batch normalization is applied to unveil a strong correlation between different pruned DNN structures and their final settled accuracy. This strong correlation allows us to fast spot the pruned candidates with highest potential accuracy without actually fine-tuning them. This module is also general to plug-in and improve some existing pruning algorithms. EagleEye achieves better pruning performance than all of the studied pruning algorithms in our experiments. Concretely, to prune MobileNet V1 and ResNet-50, EagleEye outperforms all compared methods by up to 3.8%. Even in the more challenging experiments of pruning the compact model of Mo-bileNet V1, EagleEye achieves the highest accuracy of 70.9% with an overall 50% operations (FLOPs) pruned. All accuracy results are Top-1 ImageNet classification accuracy. Source code and models are accessible to open-source community. 3</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Neural Network (DNN) pruning aims to reduce computational redundancy from a full model with an allowed accuracy range. Pruned models usually result in a smaller energy or hardware resource budget and, therefore, are especially meaningful to the deployment to power-efficient front-end systems. However, how to trim off the parts of a network that make little contribution to the model accuracy is no trivial question.</p><p>DNN pruning can be considered as a searching problem. The searching space consists of all legitimate pruned networks, which are referred as sub-nets or pruning candidates. In such space, how to obtain the sub-net with highest accuracy with reasonably small searching efforts is the core of a pruning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full-size Network</head><p>Pruned Network</p><p>Adaptive BN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity Analysis</head><p>Meta Network Short-term Fine-tuning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Process</head><p>Pruning Fine-tuning (Optional) <ref type="figure">Fig. 1</ref>. A generalized pipeline for pruning tasks. The evaluation process unveils the potential of different pruning strategies and picks the one that most likely to deliver high accuracy after convergence.</p><p>Particularly, an evaluation process can be commonly found in existing pruning pipelines. Such process aims to unveil the potential of sub-nets so that best pruning candidate can be selected to deliver the final pruning strategy. A visual illustration of this generalization is shown in <ref type="figure">Figure 1</ref>. More details about the existing evaluation methods will be discussed throughout this work. An advantage of using an evaluation module is fast decision-making because training all sub-nets, in a large searching space, to convergence for comparison can be very time-consuming and hence impractical.</p><p>However, we found that the evaluation methods in existing works are suboptimal. Concretely, they are either inaccurate or complicated.</p><p>By saying inaccurate, it means the winner sub-nets from the evaluation process do not necessarily deliver high accuracy when they converge <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">19</ref>]. This will be quantitatively proved in Section 4.1 as a correlation problem measured by several commonly used correlation coefficients. To our knowledge, we are the first to introduce correlation-based analysis for sub-net selection in pruning task. Moreover, we demonstrate that the reason such evaluation is inaccurate is the use of sub-optimal statistical values for Batch Normalization (BN) layers <ref type="bibr" target="#b9">[10]</ref>. In this work, we use a so-called adaptive BN technique to fix the issue and effectively reach a higher correlation for our proposed evaluation process.</p><p>By saying complicated, it points to the fact that the evaluation process in some works rely on tricky or computationally intensive components such as a reinforcement learning agent <ref type="bibr" target="#b6">[7]</ref>, auxiliary network training <ref type="bibr" target="#b22">[22]</ref>, knowledge distillation <ref type="bibr" target="#b7">[8]</ref>, and so on. These methods require careful hyper-parameter tuning or extra training efforts on the auxiliary models. These requirements make it potentially difficult to repeat the results and these pruning methods can be time-consuming due to their high algorithmic complexity.</p><p>Above-mentioned issues in current works motivate us to propose a better pruning algorithm that equips with a faster and more accurate evaluation process, which eventually helps to provide the state-of-the-art pruning performance. The main novelty of the proposed EagleEye pruning algorithm is described as below:</p><p>-We point out the reason that a so-called vanilla evaluation step (explained in Section 3.1) widely found in many existing pruning methods leads to poor pruning results. To quantitatively demonstrate the issue, we are the first to introduce a correlation analysis to the domain of pruning algorithm. -We adopt the technique of adaptive batch normalization for pruning purposes in this work to address the issue in the vanilla evaluation step. It is one of the modules in our proposed pruning algorithm called EagleEye. Our proposed algorithm can effectively estimate the converged accuracy for any pruned model in the time of only a few iterations of inference. It is also general enough to plug-in and improve some existing methods for performance improvement.</p><p>-Our experiments show that although EagleEye is simple, it achieves the state-of-the-art pruning performance in comparisons with many more complex approaches. In the ResNet-50 experiments, EagleEye delivers 1.3% to 3.8% higher accuracy than compared algorithms. Even in the challenging task of pruning the compact model of MobileNet V1, EagleEye achieves the highest accuracy of 70.9% with an overall 50% operations (FLOPs) pruned. The results here are ImageNet top-1 classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Pruning was mainly handled by hand-crafted heuristics in early time <ref type="bibr" target="#b12">[13]</ref>. So a pruned candidate network is obtained by human expertise and evaluated by training it to the converged accuracy, which can be very time consuming considering the large number of plausible sub-nets. In later chapters, we will show that the pruning candidate selection is problematic and selected pruned networks cannot necessarily deliver the highest accuracy after fine-tuning. Greedy strategy were introduced to save manual efforts <ref type="bibr" target="#b26">[26]</ref> in more recent time. But it is easy for such strategy to fall into the local optimal caused by the greedy nature. For example, NetAdapt <ref type="bibr" target="#b26">[26]</ref> supposes the layer l t with the least accuracy drop, noted as d t , is greedily pruned at step t. However, there may exist a better pruning strategy where d t &gt; d t , but d t + d t+1 &lt; d t + d t+1 . Our method searches the pruning ratios for all layers together in one single step and therefore avoids this issue. Some other works induce sparsity to weights in training phase for pruning purposes. For example, <ref type="bibr" target="#b25">[25]</ref> introduces group-LASSO to introduce sparsity of the kernels and <ref type="bibr" target="#b21">[21]</ref> regularizes the parameter in batch normalization layer. <ref type="bibr" target="#b23">[23]</ref> ranks the importance of filters based on Taylor expansion and trimmed off the low-ranked ones. The selection standards proposed in these methods are orthogonal to our proposed algorithm. More recently, versatile techniques were proposed to achieve automated and efficient pruning strategies such as reinforcement learning <ref type="bibr" target="#b6">[7]</ref>, generative adversarial learning mechanism <ref type="bibr" target="#b16">[17]</ref> and so on. But the introduced hyper-parameters add difficulty to repeat the experiments and the trail-and-error to get the auxiliary models work well can be time consuming.</p><p>The technique of adjusting BN was used to serve for non-pruning purposes in existing works. <ref type="bibr" target="#b13">[14]</ref> adapts the BN statistics for target domain in domain adaptation tasks. The common point with our work is that we both notice the batch normalization requires an adjustment to adapt models in a new setting where either model or domain changes. But this useful technique has not been particularly used for model pruning purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Training Pruning Evaluation Fine-tuning   <ref type="figure" target="#fig_1">Figure 2</ref>. Pruning is normally applied to a trained full-size network for redundancy removal purposes. An fine-tuning process is then followed up to gain accuracy back from losing parameters in the trimmed filters. In this work, we focus on structured filter pruning approaches, which can be generally formulated as (r1, r2, ..., rL) * = arg min r 1 ,r 2 ,...,r L L(A(r1, r2, ..., rL; w)), s.t. C &lt; constraints,</p><p>where L is the loss function and A is the neural network model. r l is the pruning ratio applied to the l th layer. Given some constraints C such as targeted amount of parameters, operations, or execution latency, a combination of pruning ratios (r 1 , r 2 , ..., r L ), which is referred as pruning strategy, is applied to the full-size model. All possible combinations of the pruning ratios form a searching space. To obtain a compact model with the highest accuracy, one should search through the search space by applying different pruning strategies to the model, fine-tuning each of the pruned model to converged and pick the best one. We consider the pruning task as finding the optimal pruning strategy, denoted as (r 1 , r 2 , ..., r L ) * , that results in the highest converged accuracy of the pruned model. Apart from handcraft designing, different searching methods have been applied in previous work to find the optimal pruning strategy, such as greedy algorithm <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b28">28]</ref>, RL <ref type="bibr" target="#b6">[7]</ref>, and evlolutionary algorithm <ref type="bibr" target="#b20">[20]</ref>. All of the these methods are guided by the evaluation results of the pruning strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>In many published approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">19]</ref> in this domain, pruning candidates directly compare with each other in terms of evaluation accuracy. The sub-nets with higher evaluation accuracy are selected and expected to also deliver high accuracy after fine-tuning. However, such intention can not be necessarily achieved as we notice the sub-nets perform poorly if directly used to do inference. The inference results normally fall into a very low-range accuracy, which is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref> left. An early attempt is to randomly generate pruning rates for MobileNet V1 and apply L1-norm based pruning <ref type="bibr" target="#b12">[13]</ref> for 50 times. The dark red bars form the histogram of accuracy collected from directly doing inference with the pruned candidates in the same way that <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">19]</ref> do before fine-tuning. Because our pruning rates are randomly generated in this early attempt, so the accuracy is very low and only for observation. The gray bars in <ref type="figure" target="#fig_3">Figure 4</ref> shows the situation after fine-tuning these 50 pruned networks. We notice a huge difference in accuracy distribution between these two results. Therefore, there are two questions came up to our mind given above observation. The first question is why removal to filters, especially considered as unimportant filters, can cause such noticeable accuracy degradation although the pruning rates are random? The natural question to ask next is how strongly the low-range accuracy is positively correlated to the final converged accuracy. These two questions triggered our investigation into this commonly used evaluation process, which is called vanilla evaluation in this work.  <ref type="bibr" target="#b8">[9]</ref> during fine-tuning on ImageNet <ref type="bibr" target="#b2">[3]</ref>. Where X axis presents the magnitude of the L1-norm of kernel, Y axis presents the quantity, Z axis presents the fine-tuning epochs.</p><p>Some initial investigations are done to tentatively address the above two questions. <ref type="figure" target="#fig_2">Figure 3</ref> right shows that it might not be the weights that mess up the accuracy at the evaluation stage as only a gentle shift in weight distribution is observed during fine-tuning, but the delivered inference accuracy is very different. On the other side, <ref type="figure" target="#fig_3">Figure 4</ref> left shows that the low-range accuracy indeed presents poor correlation with the fine-tuned accuracy, which means that it can be misleading to use evaluated accuracy to guide the pruning candidates selection.</p><p>Interestingly, we found that it is the batch normalization layer that largely affects the evaluation. Without fine-tuning, pruning candidates have parameters that are a subset of those in the full-size model. So the layer-wise feature map data are also affected by the changed model dimensions. However, vanilla evaluation still uses Batch Normalization (BN) inherited from the full-size model. The outdated statistical values of BN layers eventually drag down the evaluation accuracy to a surprisingly low range and, more importantly, break the correlation between evaluation accuracy and the final converged accuracy of the pruning candidates in the strategy searching space. A brief training, also called fine-tuning, all pruning candidates and then compare them is a more accurate way to carry out the evaluation <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b14">15]</ref>. However, it is very time-consuming to do the training-based evaluation for even single-epoch fine-tuning due to the large scale of the searching space. We give quantitative analysis later in this section to demonstrate this point.</p><p>Firstly, to quantitatively demonstrate the idea of vanilla evaluation and the problems that come with it, we symbolize the original BN <ref type="bibr" target="#b9">[10]</ref> as below:</p><formula xml:id="formula_1">y = γ x − µ √ σ 2 + + β,<label>(2)</label></formula><p>Where β and γ are trainable scale and bias terms. is a term with small value to avoid zero division. For a mini-batch with size N , the statistical values of µ and σ 2 are calculated as below:</p><formula xml:id="formula_2">µB = E[xB] = 1 N N i=1 xi, σ 2 B = V ar[xB] = 1 N − 1 N i=1 (xi − µB) 2 .<label>(3)</label></formula><p>During training, µ and σ 2 are calculated with the moving mean and variance:</p><formula xml:id="formula_3">µ t = mµ t−1 + (1 − m)µ B , σ 2 t = mσ 2 t−1 + (1 − m)σ 2 B ,<label>(4)</label></formula><p>where m is the momentum coefficient and subscript t refers to the number of training iterations. In a typical training pipeline, if the total number of training iteration is T , µ T and σ 2 T are used in testing phase. These two items are called global BN statistics, where "global" refers to the full-size model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Batch Normalization</head><p>As briefly mentioned before, vanilla evaluation used in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">19]</ref> apply global BN statistics to pruned networks to fast evaluate their accuracy potential, which we think leads to the low-range accuracy results and unfair candidate selection. If the global BN statistics are out-dated to the sub-nets, we should re-calculate µ T and σ 2 T with adaptive values by conducting a few iterations of inference on part of the training set, which essentially adapts the BN statistical values to the pruned network connections. Concretely, we freeze all the network parameters while resetting the moving average statistics. Then, we update the moving statistics by a few iterations of forward-propagation, using Equation 4, but without backward propagation. We note the adaptive BN statistics asμ T andσ 2 T .  As another evidence, we compare the distance of BN statistical values between true statistics. We consider µ and σ 2 sampled from the validation data as the true statistics, noted as µ val and σ 2 val , because they are the real statistical values in the testing phase. Specially, we are not obtaining insights from the validation data, which we think is unfair, but simply showing that our evaluation results are closer to the ground truth compared to the vanilla method. Concretely, we expectμ T andσ 2 T to be as close as possible to the true BN statistics values,µ val and σ 2 val , so they could deliver close computational results. So we visualize the distance of BN statistical values gained from different evaluation methods (see <ref type="figure">Figure 5</ref>). Each pixel in the heatmaps represents a distance for a type of BN statistics, either µ val or σ 2 val , between post-evaluation results and the true statistics sampled via one filter in MobileNet V1 <ref type="bibr" target="#b8">[9]</ref>. The visual observation shows that adaptive BN provides closer statistical values to the true values while global BN is way further. A possible explanation is that the global BN statistics are out-dated and not adapted to the pruned network connections. So they mess up the inference accuracy during evaluation for the pruned networks.</p><p>Noticeably, fine-tuning also relieves such problem of mismatched BN statistics because the training process itself re-calculates the BN statistical values in the forward pass and hence fixes the mismatch. However, BN statistics are not trainable values but sampling parameters only calculated in inference time. Our adaptive BN targets on this issue by conducting re-sampling in exactly the inference step, which achieves the same goal but with way less computational cost compared to fine-tuning. This is the main reason that we claim the application of adaptive BN in pruning evaluation is more efficient than the fine-tuning-based solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Correlation Measurement</head><p>As mentioned before, a good evaluation process in the pruning pipeline should present a strong positive correlation between the evaluated pruning candidates and their corresponding converged accuracy. Here, we compare two different evaluation methods, adaptive-BN-based and vanilla evaluation, and study their correlation with the fine-tuned accuracy. So we symbolize a vector of accuracy for all pruning candidates in the searching space ( <ref type="figure">Figure 6</ref>) separately using the above two evaluation methods as X 1 and X 2 correspondingly while fine-tuned accuracy is noted as Y . We firstly use Pearson Correlation Coefficient <ref type="bibr" target="#b24">[24]</ref>(PCC) ρ X,Y , which is used to measure the linear correlation between two variables X and Y , to measure the correlation between ρ X1,Y and ρ X2,Y .</p><p>Since we particularly care about high-accuracy sub-nets in the ordered accuracy vectors, Spearman Correlation Coefficient (SCC) <ref type="bibr" target="#b1">[2]</ref> φ X,Y and Kendall rank Correlation Coefficient (KRCC) <ref type="bibr" target="#b10">[11]</ref> τ X,Y are adopted to measure the monotonic correlation. We compare the correlation between (X 1 , Y ) and (X 2 , Y ) in above three metrics with different pruning rates. All cases present a stronger correlation for the adaptive-BN-based evaluation than the vanilla strategy. See richer details about quantitative analysis in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">EagleEye pruning algorithm</head><p>Based on the discussion about the accurate evaluation process in pruning, we now present the overall workow of EagleEye in <ref type="figure">Figure 6</ref>. Our pruning pipeline  <ref type="figure">Fig. 6</ref>. Workflow of the EagleEye Pruning Algorithm contains three parts, pruning strategy generation, filter pruning, and adaptive-BN-based evaluation.</p><p>Strategy generation outputs pruning strategies in the form of layer-wise pruning rate vectors like (r 1 , r 2 , ..., r L ) for a L-layer model. The generation process follows pre-defined constraints such as inference latency, a global reduction of operations (FLOPs) or parameters and so on. Concretely, it randomly samples L real numbers from a given range [0, R] to form a pruning strategy, where r l denotes the pruning ratio for the l th layer. R is the largest pruning ratio applied to a layer. This is essentially a Monte Carlo sampling process with a uniform distribution for all legitimate layer-wise pruning rates, i.e. removed number of filters over the number of total filters. Noticeably, other strategy generation methods can be used here, such as the evolutionary algorithm, reinforcement learning etc., we found that a simple random sampling is good enough for the entire pipeline to quickly yield pruning candidates with state-of-the-art accuracy. A possible reason for this can be that the adjustment to the BN statistics leads to a much more accurate prediction to the sub-nets' potential, so the efforts of generating candidates are allowed to be massively simplified. The low computation cost of this simple component also adds the advantage of fast speed to the entire algorithm.</p><p>Filter pruning process prunes the full-size trained model according to the generated pruning strategy from the previous module. Similar to a normal filter pruning method, the filters are firstly ranked according to their L1-norm and the r l of the least important filters are trimmed off permanently. The sampled pruning candidates from the searching space are ready to be delivered to the next evaluation stage after this process.</p><p>The adaptive-BN-based candidate evaluation module provides a BN statistics adaptation and fast evaluation to the pruned candidates handed over from the previous module. Given a pruned network, it freezes all learnable parameters and traverses through a small amount of data in the training set to calculate the adaptive BN statisticsμ andσ 2 . In practice, we sampled 1/30 of the total training set for 100 iterations in our ImageNet experiments, which takes only 10-ish seconds in a single Nvidia 2080 Ti GPU. Next, this module evaluates the performance of the candidate networks on a small part of training set data, called sub-validation set, and picks the top ones in the accuracy ranking as winner candidates. The correlation analysis presented in Section 4.1 guarantees the effectiveness of this process. After a fine-tuning process, the winner candidates are finally delivered as outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantitative analysis of correlation</head><p>We use three commonly used correlation coefficient(ρ,σ and τ ) to quantitatively measure the relation between X 1 , X 2 and Y , which are defined in Section 3.3. Based on the above initial exploration, we extend the quantitative study to a larger scale applying three correlation coefficients to different pruning ratios as shown in <ref type="table" target="#tab_1">Table 1</ref>. Firstly, the adaptive-BN-based evaluation delivers stronger correlation measured in all three coefficients compared to the vanilla evaluation. In average, ρ is 0.67 higher, φ is 0.79 higher and τ is 0.46 higher. Noticeably, the correlation high in φ and τ means that the winner pruning candidates selected  from the adaptive-based evaluation module are more likely to rank high in the fine-tuned accuracy ranking as φ emphasizes the monotonic correlation. Especially, the third to fifth rows of <ref type="table" target="#tab_1">Table 1</ref> shows the correlation metrics with different pruning rates (for instance, 75% FLOPs also means 25% pruning rate to operations). The corresponding results are also visualized in <ref type="figure" target="#fig_6">Figure 7</ref>. The second row in <ref type="table" target="#tab_1">Table 1</ref> means the pruning rate follows a layer-wise Monte Carlo sampling with a uniform distribution among the legitimate pruning rate options. All the above tables and figures prove that the adaptive-BN-based evaluation shows stronger correlation, and hence a more robust prediction, between the evaluated and fine-tuned accuracy for the pruning candidates.</p><formula xml:id="formula_4">FLOPs constraints ρX 1 ,Y ρX 2 ,Y φX 1 ,Y φX 2 ,Y τX 1 ,Y τX 2 ,Y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generality of the adaptive-BN-based evaluation method</head><p>The proposed adaptive-BN-based evaluation method is general enough to plugin and improves some existing methods. As an example, we apply it to AMC <ref type="bibr" target="#b6">[7]</ref>, which is an automatic method based on Reinforcement Learning mechanism. AMC <ref type="bibr" target="#b6">[7]</ref> trains an RL-agent to decide the pruning ratio for each layer. At each training step, the agent tries applying different pruning ratios (pruning strategy) to the full-size model as an action. Then it directly evaluates the accuracy without fine-tuning, which is noted as vanilla evaluation in our paper, and takes this validation accuracy as the reward. As the RL-agent is trained with the reward based on the vanilla evaluation, which is proved to have a poor correlation to the converged accuracy of pruned networks. So we replace the vanilla evaluation process with our proposed adaptive-BN-based evaluation. Concretely, after pruning out filters at each step, we freeze all learnable parameters and do inference on the training set to fix the BN statistics and evaluate the accuracy of the model on the sub-validation set. We feed this accuracy as a reward to train the RL-agent in place of the accuracy of vanilla evaluation. The experiment about MobileNetV1 <ref type="bibr" target="#b8">[9]</ref> on ImageNet <ref type="bibr" target="#b2">[3]</ref> classification accuracy is improved from 70.5% (reported in AMC <ref type="bibr" target="#b6">[7]</ref>) to 70.7%. It shows that the RL-agent can find a better pruning strategy with the help of our adaptive-BN-based evaluation module.</p><p>Another example is the short-term fine-tune block in <ref type="bibr" target="#b26">[26]</ref>, which also can be handily replaced by our adaptiveBN-based module for a faster pruning strategy selection. On the other side, our pipeline can also be upgraded by existing methods such as the evolutionary algorithm used in <ref type="bibr" target="#b20">[20]</ref> to improve the basic Monte Carlo sampling strategy. The above experiments and discussion demonstrate the generality of our adaptive-BN-based evaluation module, but can not be analyzed in more detail due to the limited length of this paper. <ref type="table">Table 2</ref>. Comparison of computation costs of various pruning methods in the task where all pruning methods are executed to find the best pruning strategy from 1000 potential strategies (candidates).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficiency of our proposed method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Evaluation Method Candidate Selection GPU Hours</p><p>ThiNet <ref type="bibr" target="#b22">[22]</ref> finetuning 1000×10 finetune epochs ∼ 8000 NetAdapt <ref type="bibr" target="#b26">[26]</ref> finetuning 10 4 training iterations 864 Filter Pruning <ref type="bibr" target="#b12">[13]</ref> vanilla 1000×25 finetune epochs ∼ 20000</p><p>AMC <ref type="bibr" target="#b26">[26]</ref> vanilla Training an RL agent -Meta-Pruning <ref type="bibr" target="#b20">[20]</ref> PruningNet Training an auxiliary network -</p><formula xml:id="formula_5">EagleEye adaptive-BN &lt;1000×100 inference iterations 25</formula><p>Our proposed pruning evaluation based on adaptive BN turn the prediction of sub-net accuracy into a very fast and reliable process, so EagleEye is much less time-consuming to complete the entire pruning pipeline than other heavy evaluation based algorithms. In this part, we compare the execution cost for various state-of-the-art algorithms to demonstrate the efficiency of our method. <ref type="table">Table 2</ref> compares the computational costs of picking the best pruning strategy among 1000 potential pruning candidates. As ThiNet <ref type="bibr" target="#b22">[22]</ref> and Filter Pruning <ref type="bibr" target="#b12">[13]</ref> require manually assigning layer-wise pruning ratio, The final GPU hours are the estimation of completing the pruning pipeline for 1000 random strategies. In practice, the real computation cost highly depends on the expert's heuristic practice of trial-and-error. The computation time for AMC <ref type="bibr" target="#b6">[7]</ref> and Meta-pruning can be long because training either an RL network or an auxiliary network itself is time-consuming and tricky. Among all compared methods, EagleEye is the most efficient method as each evaluation takes no more than 100 iterations, which takes 10 to 20 seconds in a single Nvidia 2080 Ti GPU. So the total candidate selection is simply an evaluation comparison process, which also can be done in negligible time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effectiveness of our proposed method</head><p>To demonstrate the effectiveness of EagleEye, we compare it with several stateof-the-art pruning methods on MobileNetV1 and ResNet-50 <ref type="bibr" target="#b3">[4]</ref> models tested on the small dataset of CIFAR-10 <ref type="bibr" target="#b11">[12]</ref> and the large dataset of ImageNet.</p><p>ResNet <ref type="table" target="#tab_2">Table 3</ref> left shows EagleEye outperforms all compared methods in terms of Top-1 accuracy on CIFAR-10 dataset. To further prove the robustness of our method, we compare the top-1 accuracy of ResNet-50 on ImageNet under 84.59% FP(our-implement) <ref type="bibr" target="#b12">[13]</ref> 85.81% EagleEye 88.01% different FLOPs constraints. For each FLOPs constraint (3G, 2G, and 1G), 1000 pruning strategies are generated. Then the adaptive-BN-based evaluation method is applied to each candidate. We just fine-tune the top-2 candidates and return the best as delivered pruned model. It is shown that EagleEye achieves the best results among the compared approaches listed in <ref type="table">Table 4</ref>.</p><p>ThiNet <ref type="bibr" target="#b22">[22]</ref> prunes the channels uniformly for each layer other than finding an optimal pruning strategy, which hurts the performance significantly. Meta-Pruning <ref type="bibr" target="#b20">[20]</ref> trains an auxiliary network called "PruningNet" to predict the weights of the pruned model. But the adopted vanilla evaluation may mislead the searching of the pruning strategies. As shown in <ref type="table">Table 4</ref>, our proposed algorithm outperform all compared methods given different pruned network targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MobileNet</head><p>We conduct experiments of the compact model of MobileNetV1 and compare the pruning results with Filter Pruning <ref type="bibr" target="#b12">[13]</ref> and the directly-scaled models. Please refer to supplementary material for more details about FP implementation and training methods to get the accuracy for the directly-scaled models. <ref type="table" target="#tab_2">Table 3</ref> right shows that EagleEye gets the best results in all cases.</p><p>Pruning MobileNetV1 for ImageNet is more challenging as it is already a very compact model. We compare the top-1 ImageNet classification accuracy under the same FLOPs constraint (about 280M FLOPs) and the results are shown in <ref type="table">Table 5</ref>. 1500 pruning strategies are generated with this FLOPs constraint. Then adaptive-BN-based evaluation is applied to each candidate. After fine-tuning the top-2 candidates, the pruning candidate that returns the highest accuracy is selected as the final output. AMC <ref type="bibr" target="#b6">[7]</ref> trains their pruning strategy decision agent based on the pruned model without fine-tuning, which may lead to a problematic selection on the candidates. NetAdapt <ref type="bibr" target="#b26">[26]</ref> searches for the pruning strategy based on a greedy algorithm, which may drop into a local optimum as analysed in Section 2. It is shown that EagleEye achieves the best performance among all studied methods again in this task (see <ref type="table">Table 5</ref>).  <ref type="bibr" target="#b6">[7]</ref> 285M 70.5% -NetAdapt <ref type="bibr" target="#b26">[26]</ref> 284M 69.1% -Meta-Pruning <ref type="bibr" target="#b20">[20]</ref> 281M 70.6% -EagleEye 284M 70.9% 89.62%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusions</head><p>We presented EagleEye pruning algorithm, in which a fast and accurate evaluation process based on adaptive batch normalization is proposed. Our experiments show the efficiency and effectiveness of our proposed method by delivering higher accuracy than the studied methods in the pruning experiments on Im-ageNet dataset. An interesting work is to further explore the generality of the adaptive-BN-based module by integrating it into many other existing methods and observe the potential improvement. Another experiment that is worth a try is to replace the random generation of pruning strategy with more advanced methods such as evolutionary algorithms and so on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A typical pipeline for neural network training and pruning A typical neural network training and pruning pipeline is generalized and visualized in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Left:Histogram for accuracy collected from directly pruning MobileNet V1 and fine-tuning 15 epoches. Right:Evolution of the weight distribution of a pruned Mo-bileNetV1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Correlation between fine-tuning accuracy and inference accuracy gained from vanilla evaluation (left), adaptive-BN-based evaluation (right) based on MobileNet V1 experiments on ImageNet Top-1 classification results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>right illustrates that applying adaptive BN delivers evaluation accuracy that has a stronger correlation, compared to the vanilla evaluationFigure 4left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 . 2 T − σ 2 val 2</head><label>522</label><figDesc>Visualization of distances of BN statistics in terms of the moving mean and variance. Each pixel refers to the distance of one BN statistics of a channel in Mo-bileNetV1. (a) µT − µ val 2, distance of moving mean between global BN and the true values. (b) distance of moving mean between adaptive-BN and the true values μT − µ val 2. (c) σ 2 T − σ 2 val 2 , distance of moving variance between global BN and the true values. (d) distance of moving variance between adaptive-BN and the true values σ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Vanilla vs. adaptive-BN evaluation: Correlation between evaluation and finetuning accuracy with different pruning ratios (MobileNet V1 [9] on ImageNet [3] classification Top-1 results) Firstly, as mentioned in Section 3.1 the poor correlation, presented by Figure 4 sub-figure, is basically 10 times smaller than adaptive-BN-based results shown in Figure 4 right sub-figure. This matches with the visual observation that the adaptive-BN-based samples are more trendy while the vanilla strategy tends to give randomly distributed samples on the figure. This means the vanilla evaluation hardly present accurate prediction to the pruned networks about their fine-tuned accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Correlation analysis quantified by Pearson Correlation Coefficient ρX,Y , Spearman Correlation Coefficient φX,Y , and Kendall rank Correlation Coefficient τX,Y .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Pruning results of ResNet-56 (left) and MobileNetV1 (right) on CIFAR-10</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">FLOPs Top1-Acc</cell></row><row><cell>Method FLOPs Top1-Acc</cell><cell>0.75 × MobileNetV1</cell><cell></cell><cell>88.07%</cell></row><row><cell>ResNet-56 125.49M 93.26%</cell><cell>FP(our-implement) [13]</cell><cell>26.5M</cell><cell>91.58 %</cell></row><row><cell>FP [13] 90.90M 93.06%</cell><cell>EagleEye</cell><cell></cell><cell>91.89%</cell></row><row><cell>RFP [1] 90.70M 93.12%</cell><cell>0.5 × MobileNetV1</cell><cell></cell><cell>87.51%</cell></row><row><cell>NISP [29] 81.00M 93.01%</cell><cell>FP(our-implement) [13]</cell><cell>12.1M</cell><cell>90.4%</cell></row><row><cell>GAL [18] 78.30M 92.98%</cell><cell>EagleEye</cell><cell></cell><cell>91.44%</cell></row><row><cell>HRank [15] 88.72M 93.52%</cell><cell>0.25 × MobileNetV1</cell><cell></cell><cell></cell></row><row><cell>EagleEye 62.23M 94.66%</cell><cell></cell><cell>3.3M</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Comparisions of ResNet-50 and other pruning methods on ImageNet Comparisions of MobileNetV1 and other pruning methods on ImageNet</figDesc><table><row><cell>FLOPs after pruning</cell><cell></cell><cell>Method</cell><cell cols="3">FLOPs Top1-Acc Top5-Acc</cell></row><row><cell></cell><cell cols="2">ThiNet-70 [20]</cell><cell>2.9G</cell><cell>75.8%</cell><cell>90.67%</cell></row><row><cell>3G</cell><cell cols="3">AutoSlim [28] Meta-Pruning [20] 3.0G 3.0G</cell><cell>76.0% 76.2%</cell><cell>--</cell></row><row><cell></cell><cell></cell><cell>EagleEye</cell><cell cols="3">3.0G 77.1% 93.37%</cell></row><row><cell cols="4">0.75 × ResNet-50 [4] 2.3G</cell><cell>74.8%</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Thinet-50 [22]</cell><cell>2.1G</cell><cell>74.7%</cell><cell>90.02%</cell></row><row><cell></cell><cell></cell><cell>AutoSlim [28]</cell><cell>2.0G</cell><cell>75.6%</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>CP [8]</cell><cell>2.0G</cell><cell>73.3%</cell><cell>90.8%</cell></row><row><cell></cell><cell></cell><cell>FPGM [6]</cell><cell cols="3">2.31G 75.59% 92.63%</cell></row><row><cell>2G</cell><cell></cell><cell>SFP [5]</cell><cell cols="3">2.32G 74.61% 92.06%</cell></row><row><cell></cell><cell></cell><cell>GBN [27]</cell><cell cols="3">1.79G 75.18% 92.41%</cell></row><row><cell></cell><cell></cell><cell>GDP [16]</cell><cell cols="3">2.24G 72.61% 91.05%</cell></row><row><cell></cell><cell></cell><cell>DCP [30]</cell><cell cols="3">1.77G 74.95% 92.32%</cell></row><row><cell></cell><cell cols="3">Meta-Pruning [20] 2.0G</cell><cell>75.4%</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>EagleEye</cell><cell cols="3">2.0G 76.4% 92.89%</cell></row><row><cell cols="4">0.5 × ResNet-50 [4] 1.1G</cell><cell>72.0%</cell><cell>-</cell></row><row><cell></cell><cell cols="2">ThiNet-30 [22]</cell><cell>1.2G</cell><cell>72.1%</cell><cell>88.30%</cell></row><row><cell>1G</cell><cell></cell><cell>AutoSlim [28]</cell><cell>1.0G</cell><cell>74.0%</cell><cell>-</cell></row><row><cell></cell><cell cols="3">Meta-Pruning [20] 1.0G</cell><cell>73.4%</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>EagleEye</cell><cell>1.0G</cell><cell cols="2">74.2% 91.77%</cell></row><row><cell cols="2">Method</cell><cell cols="3">FLOPs Top1-Acc Top5-Acc</cell></row><row><cell cols="4">0.75 × MobileNetV1 [9] 325M 68.4%</cell><cell>-</cell></row><row><cell>AMC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/anonymous47823493/EagleEye arXiv:2007.02491v2 [cs.CV] 5 Aug 2020</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Jiang Su is the corresponding author of this work. This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant No.U1811463.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Building efficient convnets using redundant feature pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">O</forename><surname>Ayinde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
		<idno>abs/1802.07653</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Spherical cnns</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06866</idno>
		<title level="m">Soft filter pruning for accelerating deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pruning filter via geometric median for deep convolutional neural networks acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00250</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
		<title level="m">A new measure of rank correlation</title>
		<imprint>
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<title level="m">Pruning filters for efficient convnets</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04779</idno>
		<title level="m">Revisiting batch normalization for practical domain adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hrank: Filter pruning using high-rank feature map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno>ArXiv abs/2002.10179</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accelerating convolutional networks via global &amp; dynamic filter pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="2425" to="2432" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards optimal structured cnn pruning via generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Doermann</surname></persName>
		</author>
		<title level="m">Towards optimal structured cnn pruning via generative adversarial learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2785" to="2794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Autocompress: An automatic dnn structured pruning framework for ultra-high compression rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Metapruning: Meta learning for automatic neural network channel pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3295" to="3304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5058" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Importance estimation for neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the distribution of the correlation coefficient in small samples. appendix ii to the papers of&quot; student&quot; and ra fisher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="328" to="413" />
			<date type="published" when="1917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Netadapt: Platform-aware neural network adaptation for mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="285" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11728</idno>
		<title level="m">Network slimming by slimmable networks: Towards one-shot architecture search for channel numbers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nisp: Pruning networks using neuron importance score propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="9194" to="9203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discrimination-aware channel pruning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="875" to="886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
