<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end music source separation: is it possible in the waveform domain?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Lluís</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Music Technology Group</orgName>
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Music Technology Group</orgName>
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Music Technology Group</orgName>
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end music source separation: is it possible in the waveform domain?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms: source separation, end-to-end learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of the currently successful source separation techniques use the magnitude spectrogram as input, and are therefore by default omitting part of the signal: the phase. To avoid omitting potentially useful information, we study the viability of using end-to-end models for music source separation -which take into account all the information available in the raw audio signal, including the phase. Although during the last decades end-to-end music source separation has been considered almost unattainable, our results confirm that waveform-based models can perform similarly (if not better) than a spectrogram-based deep learning model. Namely: a Wavenet-based model we propose and Wave-U-Net can outperform DeepConvSep, a recent spectrogram-based deep learning model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When two or more sounds co-exist, they interfere with each other resulting in a novel mixture signal where sounds are superposed (and, sometimes, masked). The source separation task tackles the inverse problem of recovering each individual sound source contribution from an observed mixture signal.</p><p>With the recent advances in deep learning, source separation techniques have improved substantially <ref type="bibr" target="#b0">[1]</ref>. Interestingly, though, nearly all successful deep learning algorithms use the magnitude spectrogram as input <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> -and are therefore, by default, omitting part of the signal: the phase. Omitting the potentially useful information of the phase entails the risk of finding a sub-optimal solution. In this work, we aim to take full advantage of the acoustic modeling capabilities of deep learning to investigate whether it is possible to approach the problem of music source separation directly in an end-to-end learning fashion. Consequently, our investigation is centered on studying how to separate music sources (e.g., singing voice, bass or drums) directly from the raw waveform music mixture.</p><p>During the last two decades, matrix decomposition methods have dominated the field of audio source separation. Several algorithms have been proposed throughout the years, with independent component analysis (ICA) <ref type="bibr" target="#b3">[4]</ref>, sparse coding <ref type="bibr" target="#b4">[5]</ref>, or non-negative matrix factorization (NMF) <ref type="bibr" target="#b5">[6]</ref> being the most used ones. Given that magnitude or power spectrogram representations are always non-negative, imposing a non-negative constraint (like in NMF) is particularly useful when analyzing these spectrograms -but less appropriate for processing waveforms, which range from -1 to 1. For that reason, methods like ICA and sparse coding have historically been used to process waveforms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Waveform representations preserve all the information available in the raw signal. However, given the unpredictable behavior of the phase in real-life * Contributed equally. sounds, it is rare to find identical waveforms produced by the same sound source. As a result of this variability, a single basis 1 cannot represent a sound source and therefore, one requires i) a large amount of bases, or ii) shift-invariant bases to obtain accurate decompositions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Although several matrix decomposition methods have been used for decomposing waveform-based mixtures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, these have never worked as well as the spectrogram-based ones.</p><p>Due to the above mentioned difficulties, the phase of complex time-frequency representations is commonly discarded, assuming that magnitude spectrograms already carry meaningful information about the sound sources to be separated. Phase related problems disappear when sounds are just represented as magnitude or power spectrograms, since different realizations of the same sound are almost identical in this time-frequency plane. This allows to easily overcome the variability problem found when operating with waveforms.</p><p>Most matrix decomposition methods rely on a signal model assuming that sources add linearly in the time domain [10] 1 .</p><p>However, the addition of signals in the time and frequency domains is not equivalent if phases are discarded. Only in expectation:</p><formula xml:id="formula_0">E{|X(k)| 2 } = |Y1(k)| 2 + |Y2(k)| 2 , where X(k) = DF T {x(t)}.</formula><p>This means that we can approximate the timedomain summation in the power spectral domain. For that reason, many approaches utilize power spectrograms as inputs. Although magnitude spectrograms work well in practice <ref type="bibr" target="#b10">[11]</ref>, there is no similar theoretical justification for such an inconsistency with the signal model when the phases are discarded.</p><p>Finally, note that these methods operating on top of spectrograms still need to deliver a waveform signal. To this end, the main practice is to filter the original magnitude or power spectrogram with (predicted) time-frequency masks. Accordingly, the original noisy phase of the mixture is used when synthesizing the waveform of the estimated sources -which might introduce an additional source of error <ref type="bibr" target="#b9">[10]</ref>. Notably, many modern spectrogram-based deep learning models are also relying on this same (potentially problematic) approach <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>. To overcome this issue, some tried to consider the phase when separating the sources <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> 2 , or some others relied on a sinusoidal signal model at synthesis time <ref type="bibr" target="#b15">[16]</ref>. However, in our work, we do not want to rely on any time-frequency transform or any signal model. Instead, we aim to directly approach the problem in the waveform domain.</p><p>As seen, many issues still exist around the idea of discarding the phase: are we missing crucial information when discarding it? When using the phase of the mixture at synthesis time, are we introducing artifacts that are limiting our model's performance? Or, since magnitude spectrograms (differently from power spectrograms) are not additive, which is the effect of relying on an incorrect signal model?</p><p>Our goal is to address these historical challenges via bypassing the problem. We want to investigate the feasibility of counting on an end-to-end model instead of relying on any signal model, any time-frequency transform, or filtering any signal. However, waveforms are high-dimensional and very variable. Thus, is music source separation possible in the waveform domain? Recent literature shows that deep learning models operating on raw audio waveforms can achieve satisfactory results for several audio-based tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. And, among those, some are also recently starting to address the problem of music source separation directly in the waveform domain <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Stoller et al. <ref type="bibr" target="#b19">[20]</ref> proposed the Wave-U-Net (see Section 2.3 for more information), and Grais et al. <ref type="bibr" target="#b20">[21]</ref> proposed a multiresolution 3 CNN auto-encoder for singing-voice source separation. Unfortunately, though, these recent articles do not include any perceptual study comparing waveform-based models with spectrogram-based ones. One of our goals is to cover this literature gap to further understand which might be the impact of addressing music source separation in an end-to-end learning fashion. To this end, we set Wave-U-Net 4 as one of our baselines and run a perceptual study to get a broader picture of how end-to-end learning models can perform.</p><p>As seen, the idea of approaching the music source separation task directly in the waveform domain has not been widely explored throughout the years, possibly due to the complexity of dealing with waveforms (which are unintuitive and highdimensional). Consequently, during the last decades, music source separation in the waveform domain has been considered almost unattainable. Our work aims to keep adding knowledge on top of this rather scarce literature, to convince the reader that music source separation is possible in the waveform domain. To this end, in section 2 we first introduce a new end-to-end learning model: a Wavenet for music source separation. Later, we present two recent deep learning models that we set as baselines for our study: DeepConvSep <ref type="bibr" target="#b1">[2]</ref> and Wave-U-Net <ref type="bibr" target="#b19">[20]</ref>. In sections 3 and 4 we evaluate the above mentioned models, to conclude in section 5 that performing music source separation in the waveform domain is not only possible, but it can be a promising research direction. Hence, our main contributions can be summarized as follows:</p><p>1) We propose to use a Wavenet-based model for music source separation. Besides, we study the impact of several Wavenet hyper-parameters -a result that might also be of relevance for other application areas where Wavenet has been used.</p><p>2) We perceptually benchmark several music source separation models, including our Wavenet-based model. This first perceptual study helps to further understand which might be the contribution of end-to-end models to the field of source separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">End-to-end source separation models</head><p>We aim to discuss the feasibility of end-to-end learning models for monaural music source separation. To this end, we experiment with a new Wavenet-based model for music source separation we propose, and we compare it against two recent models: DeepConvSep <ref type="bibr" target="#b1">[2]</ref>, a spectrogram-based deep learning model for multi-instrument separation; and Wave-U-Net <ref type="bibr" target="#b19">[20]</ref>, a waveform-based model trained end-to-end for singing voice separation <ref type="bibr" target="#b3">4</ref> . We will compare the performance of these models perceptually and via assessing their BSS Eval scores <ref type="bibr" target="#b21">[22]</ref>. To allow a fair comparison, all discussed models are trained with MUSDB data down-sampled at 16kHz 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">A Wavenet-based model for source separation</head><p>We utilize an adaptation of Wavenet <ref type="bibr" target="#b23">[24]</ref> that turns the original causal Wavenet (that is generative and slow), into a non-causal model (that is discriminative and parallelizable). This idea was originally proposed by Rethage et al. <ref type="bibr" target="#b24">[25]</ref> for speech denoising, and we adapt it for monaural music source separation. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overall depiction of the model, where we can observe that every layer has residual and skip connections. Before anything else, the waveform is linearly projected to k channels by a 3x1 CNN-layer to comply with the feature map dimensions of each residual layer. Then, this projection is processed with several layers conformed by a dilated CNN passing through a tanh non-linearity controlled by a sigmoidal gate, see <ref type="figure" target="#fig_0">Figure 1</ref> (Left). The dilation factor in each layer increases in the range of 1, 2, ..., 256, 512. This ten layer pattern is repeated N times (N stacks). Later, two CNN layers (with k filters, as well) adapt the resulting feature map dimensions to be the same as the residual and skip connections. A ReLU is applied after summing all skip connections and the final two 3x1 CNNs are not dilated -they have 2048 &amp; 256 filters, respectively, and are separated by a ReLU. The output layer linearly projects this feature map into as many channels as sources we aim to separate by using 1x1 filters. For multi-instrument source separation, our model has 3 outputs; and for singing voice separation, it has 1 single output. The remaining sources are computed via substracting the estimated sources from the mixture, see <ref type="figure" target="#fig_0">Figure 1</ref> (Right). The main difference between the original Wavenet and the noncausal adaptation we use, is that some samples from the future can be used to predict the present one. As a result of removing the autoregressive causal nature of the original Wavenet, this fully convolutional model is able to predict a target field instead of one sample at a time -due to this parallelization, it is possible to run the model in real-time on a GPU <ref type="bibr" target="#b24">[25]</ref>. Another major difference with the original Wavenet is the output: we directly regress the waveform sources instead of sampling from a softmax output <ref type="bibr" target="#b24">[25]</ref>. We minimize the mean absolute error (MAE) regression loss during training. ADAM optimizer is used with a learning rate of 0.001. We set the batch size to 10, and the model is trained until the validation error does not decrease for 16 epochs. The model with the lowest validation loss is selected. The code is accessible online. <ref type="bibr" target="#b5">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DeepConvSep: a spectrogram-based model</head><p>DeepConvSep <ref type="bibr" target="#b1">[2]</ref> is a state-of-the-art spectrogram-based model that is openly available 7 . Following the common practice: mixture signals (pre-processed as magnitude spectrograms) are fed to the model to estimate time-frequency soft masks for each source <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12]</ref>. These masks are then used to filter the magnitude spectrogram of the mixture to estimate the magnitude spectrograms of the separated sources. Finally, these estimates, along with the phase of the mixture, are used to obtain the waveform signals corresponding to the separated sources. Deep-ConvSep's architecture is based on a convolutional encoderdecoder. The encoder is conformed by a first CNN layer with 50 vertical filters aiming to capture timbral representations <ref type="bibr" target="#b25">[26]</ref>, a second CNN layer with 30 horizontal filters modeling temporal cues <ref type="bibr" target="#b26">[27]</ref>, and a dense layer with 128 units acting as a bottleneck. The decoder contains two deconvolutional layers which up-sample the bottleneck feature maps up to have the same input size, which correspond to the estimated masks. The model learns via minimizing the mean squared error (MSE), together with several dissimilarity loss terms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>. We utilize the original model released by the authors, as it is, which was trained with audio at 44kHz. To allow a fair comparison among models, we downsample its predictions to 16kHz (which does not largely affect its performance, see <ref type="table" target="#tab_1">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Wave-U-Net: a waveform-based model</head><p>Wave-U-Net <ref type="bibr" target="#b19">[20]</ref> is a state-of-the-art waveform-based model that is openly available 8 . Wave-U-Net is a time-domain adaptation of the U-Net architecture for image segmentation <ref type="bibr" target="#b27">[28]</ref>. It also consists in an encoder-decoder architecture. The encoder (12 layers) successively down-samples the feature maps, and the decoder (12 additional layers) up-samples the feature maps up to have the required output-length. A fundamental aspect of U-net architectures is that each decoder layer can access to the feature maps computed by the encoder (at the same level of hierarchy). To put an example: the penultimate layer, a decoder layer, can access the second layer's feature maps, an encoder layer, since they are concatenated. As a result of allowing the decoder to make use of the encoder feature maps, the output of the model is more detailed. These details come from the encoder-decoder connections, that convey the structure of the input to the output. In order to allow a proper comparison among models, we re-train Wave-U-Net (following the best setup reported by the original authors: M3 <ref type="bibr" target="#b19">[20]</ref>) with MUSDB data at 16kHz, to fit the same train conditions as the Wavenet-based model. We minimize the MSE loss during training. ADAM optimizer is used with a learning rate of 0.0001, and we set the batch size to 10. The model is trained until the validation error does not improve for 16 epochs. We select the model with the lowest validation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-instrument source separation</head><p>The goal of this experiment is two-fold: i) compare the proposed Wavenet-based model with DeepConvSep for the task of monaural multi-instrument source separation; and ii) study several Wavenet hyper-parameter choices -that are listed below: Wavenet: wider or deeper? Provided that the GPU's memory is limited, this experiment explores the trade-off between how many filters each Wavenet layer has (the GPU's memory mostly stores learnable parameters with a wider Wavenet) and the receptive field length of the network (the GPU's memory mostly stores feature maps with a deeper Wavenet having a larger receptive field). <ref type="table" target="#tab_0">Table 1</ref> describes the setups we study. Which cost? For our basic model we consider a single-term loss: LMAE = j∈J |ŷj − yj |, whereŷj is the predicted source. However, previous work successfully reduced interferences from other sources (SIR) via adding a dissimilarity loss term <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>: L d = j∈J i∈J |ŷj − y i =j |, with the resulting cost being: L total = LMAE − α · L d . Small α's tend to perform well, and in our experiments we set α = 0.05. Perceptual tests were conducted with 15 participants to get subjective feedback. Five songs were randomly chosen from 1' to 1'10" to compose the perceptual test set. <ref type="bibr" target="#b8">9</ref> Participants were asked to "give an overall quality score, taking into consideration both: sound quality of the target source and interferences from other sources" for each of the estimated sources. The original mixture and the clean target source were presented as references. Participants provided a score between 1-5, with 1 being "very intrusive interferences from other sources and degraded audio", and 5 being "unnoticeable interferences from other sources and not degraded audio". Mean opinion score (MOS) is obtained by averaging the scores from all participants. <ref type="table" target="#tab_1">Table 2</ref> shows the results of our experiments. Wide (but less deep) architectures fail at solving the task, only models having more than 3 stacks are able to perform competently.  <ref type="table">Table 3</ref>: Multi-instrument source separation perceptual scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vocals Drums</head><p>Bass Wavenet-based 2.4 ± 0.9 2.9 ± 1.1 2.4 ± 1.0 DeepConvSep 16kHz 2.3 ± 0.9 2.5 ± 0.7 1.8 ± 0.8</p><p>Two reasons may exist for that: wide models (having &gt; 10M parameters) overfit the training set, and/or the small receptive field of wide models is not enough to solve the task. Further, we observe that the dissimilarity loss term L d does not help improving the results. Consequently, we choose the best performing model (4 stacks) for the perceptual test. <ref type="table">Table 3</ref> presents the results of the perceptual test, showing that participants preferred the separations 9 done by the Wavenet-based model, particularly for drums (t-test: p-value=0.018) and bass (t-test: p-value&lt;10 −3 ). However, participants did not show any preference for the vocals' separations (t-test: p-value=0.423). This trend is consistent with BSS Eval scores, what shows that is possible to achieve good separations with end-to-end music source separation techniques. Informal listening 9 also reveals that DeepConvSep is very conservative, possibly due to the mask-based approach used for filtering the spectrograms. Although Wavenet-based models seem to better remove the accompanying sources, they do it at the cost of introducing some artifacts that are noticeable when listening to the samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Singing voice source separation</head><p>The goal of this experiment is two-fold: i) compare the proposed Wavenet-based model with Wave-U-Net for the task of monaural singing voice source separation; and ii) study several Wavenet hyper-parameter choices. Besides running Wavenet: wider or deeper? and Which cost? experiments for this setup, we extend our study with an extra experiment: Data-sampling strategies? Our model had difficulties in producing continuous vocals. For that reason, we study training it using a higher proportion of the data containing singing voice (instead of vocal streams having silence). To study the contribution of this parameter, the percentage of forced fragments containing singing voice is set to [0, 25, 50, 75, 100] -with 0% meaning that segments are randomly selected, and 100% meaning that our sampling strategy ensures that all examples contain singing voice. <ref type="table" target="#tab_2">Table 4</ref> shows the results of our experiments. Again, architectures having 3-4 stacks tend to perform better. However, differently from our previous experiment, the model having 1 stack performs reasonably. Further, we observe that the dissimilarity loss term L d does help. And finally, note that carefully selecting the way we present our data to the model can make   <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>. That said, the remarkable performance of the proposed Wavenet-based model also indicates the potential of end-to-end music source separation models in general. Informal listening 9 also reveals that Wavenet-based models seem to better remove the accompanying sources. Although Wave-U-Net has difficulties in producing silences in parts having only accompaniment, its separations are smoother and have less artifacts -that's why these separations are preferred by the listeners. Finally, end-to-end models trained only for singing voice separation achieve much better results than their counterparts trained for multi-instrument separation (compare <ref type="table" target="#tab_1">Tables 4 and 5, against Tables 2 and 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Throughout the years, end-to-end music source separation has been considered a hard research problem. Possibly because waveforms are variable and high-dimensional, the research community has focused on processing spectrograms instead of waveforms. However, with the recent advances of deep learning, music source separation starts to be possible in the waveform domain. As seen, although end-to-end music source separation methods have only started to be explored, the encouraging results we report denote the potential of this research direction -that might, e.g., allow to bypass the inherent phase problems associated with some spectrogram-based methods, or to move beyond the current mask-based filtering paradigm. To further show the viability of this research direction, we proposed a novel end-to-end source separation model based on Wavenet, that performs comparably to Wave-U-Net. However, these two state-of-the-art waveform-based models perform ≈1.5dB (SDR) worse than the best spectrogram-based models that were published during the last SiSEC (Signal Separation Evaluation Campaign <ref type="bibr" target="#b0">[1]</ref>). Hence, although being possible and conceptually promising, end-to-end music source separation is still a challenging research topic. Finally, as an additional way to validate the direction we explored, it is worth mentioning that the speech source separation community is also starting to propose end-to-end methods with some degree of success <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>Work funded by the Maria de Maeztu Programme (MDM-2015-0502). We are grateful to NVidia for the donated GPUs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left -Residual layer. Right -Overview of the non-causal Wavenet we propose for multi-instrument source separation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Description of the models we study. Wavenet-based "k filters" stand for the number of CNN filters in each residual connection, skip connection, and dilated convolutional block.</figDesc><table><row><cell>Wavenet-based</cell><cell>k</cell><cell>#</cell><cell>receptive</cell><cell>target</cell></row><row><cell>N stacks / layers</cell><cell>filters</cell><cell>params</cell><cell>field</cell><cell>field</cell></row><row><cell>1 stack / 10</cell><cell>512</cell><cell>≈ 25.7M</cell><cell>128 ms</cell><cell>100 ms</cell></row><row><cell>2 stacks / 20</cell><cell>256</cell><cell>≈ 13.6M</cell><cell>256 ms</cell><cell>100 ms</cell></row><row><cell>3 stacks / 30</cell><cell>128</cell><cell>≈ 6.3M</cell><cell>384 ms</cell><cell>100 ms</cell></row><row><cell>4 stacks / 40</cell><cell>64</cell><cell>≈ 3.3M</cell><cell>512 ms</cell><cell>100 ms</cell></row><row><cell>5 stacks / 50</cell><cell>32</cell><cell>≈ 2.2M</cell><cell>639 ms</cell><cell>100 ms</cell></row><row><cell>DeepConvSep</cell><cell>-</cell><cell>≈ 314K</cell><cell>290 ms</cell><cell>290 ms</cell></row><row><cell>Wave-U-Net</cell><cell>-</cell><cell>≈ 10.2M</cell><cell>9.21 s</cell><cell>1.02 s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Multi-instrument source separation median scores.</figDesc><table><row><cell></cell><cell></cell><cell>Vocals</cell><cell></cell><cell></cell><cell>Drums</cell><cell></cell></row><row><cell>Wavenet-based</cell><cell>SDR</cell><cell>SIR</cell><cell cols="2">SAR SDR</cell><cell>SIR</cell><cell>SAR</cell></row><row><cell>1 stack</cell><cell>0.35</cell><cell>3.94</cell><cell>4.38</cell><cell>1.24</cell><cell>7.98</cell><cell>3.56</cell></row><row><cell>2 stacks</cell><cell>0.07</cell><cell>4.48</cell><cell>3.49</cell><cell>-0.09</cell><cell>6.87</cell><cell>2.88</cell></row><row><cell>3 stacks</cell><cell cols="3">3.46 11.26 5.18</cell><cell cols="3">4.39 13.37 5.08</cell></row><row><cell>4 stacks</cell><cell cols="3">3.35 11.25 5.24</cell><cell cols="3">4.13 13.23 5.00</cell></row><row><cell>5 stacks</cell><cell>2.84</cell><cell>9.56</cell><cell>5.20</cell><cell cols="3">4.60 12.66 6.08</cell></row><row><cell>4 stacks + Ld</cell><cell>3.05</cell><cell cols="2">10.58 4.80</cell><cell cols="3">4.09 12.85 5.31</cell></row><row><cell cols="2">DeepConvSep 16kHz 2.38</cell><cell>4.45</cell><cell>8.39</cell><cell>3.19</cell><cell>6.69</cell><cell>6.58</cell></row><row><cell cols="2">DeepConvSep 44kHz 2.37</cell><cell>4.65</cell><cell>8.04</cell><cell>3.14</cell><cell>6.73</cell><cell>6.55</cell></row><row><cell></cell><cell></cell><cell>Bass</cell><cell></cell><cell></cell><cell>Other</cell><cell></cell></row><row><cell>Wavenet-based</cell><cell>SDR</cell><cell>SIR</cell><cell cols="2">SAR SDR</cell><cell>SIR</cell><cell>SAR</cell></row><row><cell>1 stack</cell><cell>0.35</cell><cell>4.54</cell><cell>4.70</cell><cell cols="2">-2.70 -1.37</cell><cell>6.75</cell></row><row><cell>2 stacks</cell><cell>-0.55</cell><cell>0.87</cell><cell>7.80</cell><cell cols="2">-2.05 -0.97</cell><cell>8.96</cell></row><row><cell>3 stacks</cell><cell>2.24</cell><cell>6.36</cell><cell>5.94</cell><cell>0.54</cell><cell>4.07</cell><cell>4.41</cell></row><row><cell>4 stacks</cell><cell>2.49</cell><cell>6.53</cell><cell>5.77</cell><cell>0.41</cell><cell>3.83</cell><cell>4.47</cell></row><row><cell>5 stacks</cell><cell>2.48</cell><cell>6.70</cell><cell>6.27</cell><cell>0.18</cell><cell>3.26</cell><cell>4.75</cell></row><row><cell>4 stacks + Ld</cell><cell>2.23</cell><cell>5.66</cell><cell>6.37</cell><cell>-0.19</cell><cell>4.37</cell><cell>3.24</cell></row><row><cell cols="2">DeepConvSep 16kHz 0.27</cell><cell>1.92</cell><cell>7.46</cell><cell>-2.02</cell><cell>1.74</cell><cell>2.50</cell></row><row><cell cols="2">DeepConvSep 44kHz 0.17</cell><cell>1.98</cell><cell>7.06</cell><cell>-2.13</cell><cell>1.84</cell><cell>2.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Singing voice source separation median scores.</figDesc><table><row><cell></cell><cell></cell><cell>Vocals</cell><cell></cell><cell cols="3">Accompaniment</cell></row><row><cell>Wavenet-based</cell><cell>SDR</cell><cell>SIR</cell><cell>SAR</cell><cell>SDR</cell><cell>SIR</cell><cell>SAR</cell></row><row><cell>1 stack</cell><cell cols="3">2.76 10.11 4.78</cell><cell>9.73</cell><cell cols="2">12.73 13.77</cell></row><row><cell>2 stacks</cell><cell cols="3">3.05 11.13 4.50</cell><cell cols="3">10.13 13.82 12.93</cell></row><row><cell>3 stacks</cell><cell cols="3">3.62 12.33 4.96</cell><cell cols="3">10.41 13.97 13.53</cell></row><row><cell>4 stacks</cell><cell cols="3">3.67 12.14 5.24</cell><cell cols="3">10.64 14.43 13.22</cell></row><row><cell>5 stacks</cell><cell cols="3">3.02 12.44 4.44</cell><cell cols="3">10.42 13.89 13.30</cell></row><row><cell>4 stacks+Ld</cell><cell cols="3">3.78 11.76 5.44</cell><cell cols="3">10.90 14.26 13.84</cell></row><row><cell>4 stacks+Ld+25%</cell><cell cols="3">3.98 12.20 5.19</cell><cell cols="3">10.75 14.21 13.70</cell></row><row><cell>4 stacks+Ld+50%</cell><cell cols="3">4.49 13.52 6.17</cell><cell cols="3">11.39 16.37 13.49</cell></row><row><cell>4 stacks+Ld+75%</cell><cell cols="3">3.93 12.93 5.40</cell><cell cols="3">11.14 16.18 13.37</cell></row><row><cell cols="2">4 stacks+Ld+100% 2.36</cell><cell>6.25</cell><cell>5.88</cell><cell cols="3">10.44 16.73 12.15</cell></row><row><cell>Wave-U-Net</cell><cell cols="3">4.60 14.30 5.54</cell><cell cols="3">11.87 16.08 14.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Singing voice source separation perceptual scores.</figDesc><table><row><cell>MOS</cell><cell cols="2">Wavenet-based Wave-U-Net</cell></row><row><cell>Vocals</cell><cell>3.0 ± 1.0</cell><cell>3.3 ± 0.85</cell></row><row><cell cols="3">the difference. Our results greatly improve when 50% of the</cell></row><row><cell cols="3">training examples contain voice. Consequently, we choose the</cell></row><row><cell cols="3">best performing model (4 stacks+L d +50%) for the perceptual</cell></row><row><cell cols="3">test. Table 5 presents the results of the perceptual test, show-</cell></row><row><cell cols="3">ing that participants preferred Wave-U-Net separations 9 over</cell></row><row><cell cols="3">Wavenet-based ones (t-test: p-value=0.049). This trend is con-</cell></row><row><cell cols="3">sistent with BSS Eval scores, which denotes how powerful U-</cell></row><row><cell cols="3">net architectures are for source separation</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">ICA, sparse coding &amp; NMF model the mixture signal as a weighted sum of bases, which represent a source or components of a source.<ref type="bibr" target="#b1">2</ref> Using the full complex STFT number, instead of utilizing phaseless representations (either at the input or when applying the masks).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">It is multi-resolution in the sense that they use several CNN filter lengths at every layer so that short-and long-term features can be efficiently learned/encoded. For further information, see Pons et al.<ref type="bibr" target="#b17">[18]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">At the time of writing, DeepConvSep &amp; Wave-U-Net are the best performing publicly available models for monaural music source separation.<ref type="bibr" target="#b4">5</ref> DeepConvSep was trained by the original authors with DSD100 data<ref type="bibr" target="#b22">[23]</ref> at 44.1kHz. MUSDB is mostly conformed by DSD100.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/francesclluis/source-separation-wavenet/ 7 https://github.com/MTG/DeepConvSep/ 8 https://github.com/f90/Wave-U-Net</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Listen: jordipons.me/apps/end-to-end-music-source-separation/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06267</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monoaural audio source separation using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chandna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gómez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Singing voice separation with deep u-net convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting sound objects by independent subspace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AES Conference</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of sparse and shift-invariant decompositions of polyphonic music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blumensath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A maximum likelihood approach to single-channel source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1365" to="1392" />
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning methods for source separation in monaural music signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing Methods for Music Transcription</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="267" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On automatic drum transcription using non-negative matrix deconvolution and itakura saito divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liuni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrangey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Singing-voice separation from monaural recordings using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="477" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Complex nmf: A new sparse representation for acoustic signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kameoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sagayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3437" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Does phase matter for monaural source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thresher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00913</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phasebook and friends: Leveraging discrete representations for source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Separation of harmonic sound sources using sinusoidal modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klapuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10433</idno>
		<title level="m">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Randomly weighted cnns for (music) audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">End-to-end learning for music audio tagging at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prockup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Ehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Wave-u-net: A multi-scale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ISMIR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Raw multichannel audio source separation using multi-resolution convolutional auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Grais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00702</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bss eval toolbox user guide-revision 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The 2016 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rivet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fontecave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07162</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Timbre analysis of music audio signals with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Slizovskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>EUSIPCO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Designing efficient architectures for modeling temporal features with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">End-to-end source separation with adaptive front-ends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casebeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02514</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">End-to-end networks for supervised single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02568</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00541</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
