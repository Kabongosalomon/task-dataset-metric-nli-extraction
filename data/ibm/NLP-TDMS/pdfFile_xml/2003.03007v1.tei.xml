<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Centrality Graph Convolutional Networks for Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-06">6 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
							<email>dongyang3-c@my.cityu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Chu Hai College of Higher Education</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Management of Complex Systems Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><forename type="middle">Mengqi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Chu Hai College of Higher Education</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Fu</surname></persName>
							<email>hongfu@chuhai.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Chu Hai College of Higher Education</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jicong</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Chu Hai College of Higher Education</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Leung</surname></persName>
							<email>howard@cityu.edu.hk</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Operations Research and Information Engineering</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Centrality Graph Convolutional Networks for Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-06">6 Mar 2020</date>
						</imprint>
					</monogr>
					<note>5 These authors contributed equally 6 Corresponding author:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Centrality</term>
					<term>Graph convolutional network</term>
					<term>Skeleton data</term>
					<term>Hu- man action recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The topological structure of skeleton data plays a significant role in human action recognition. Combining the topological structure with graph convolutional networks has achieved remarkable performance. In existing methods, modeling the topological structure of skeleton data only considered the connections between the joints and bones, and directly use physical information. However, there exists an unknown problem to investigate the key joints, bones and body parts in every human action. In this paper, we propose the centrality graph convolutional networks to uncover the overlooked topological information, and best take advantage of the information to distinguish key joints, bones, and body parts. A novel centrality graph convolutional network firstly highlights the effects of the key joints and bones to bring a definite improvement. Besides, the topological information of the skeleton sequence is explored and combined to further enhance the performance in a fourchannel framework. Moreover, the reconstructed graph is implemented by the adaptive methods on the training process, which further yields improvements. Our model is validated by two large-scale datasets, NTU-RGB+D and Kinetics, and outperforms the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action recognition has attracted substantial attention from different research areas in recent years, since the deluge of skeleton data offers unprecedented opportunities to investigate structures, appearance, joints and bones, video sequences of human actions <ref type="bibr" target="#b57">[56,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b26">27]</ref>. The study of human action recognition provides significant insights into action surveillance, pose tracking, medical monitor and diagnosis, sports analysis, security reports, and human-computer interaction <ref type="bibr" target="#b52">[51,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7]</ref>. Compared to traditional RGB or RGB-D video data, the skeleton sequences are compact motion data, which can significantly reduce computational cost on human action recognition. Besides, the skeleton data could show better performance when the background of human action is complicated <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b55">54]</ref>.</p><p>The study of human action recognition, like walking, sitting and eating actions, has witnessed significant progress recently, like modeling the structures of human joints and analyzing their dynamic features. To accurately classify and predict human actions, many learning methods, such as recurrent neural networks (RNN) or convolutional neural networks (CNN), graph convolutional networks (GCN), and spatial-temporal graph convolutional networks (ST-GCN), have been proposed <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b57">56]</ref>. The earliest models for human action recognition often construct all coordinates of human joints from each frame for convolutional learning, like RNN or CNN <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref>. These learning models rarely investigate the endogenous relationships between joints and bones, leading to missing abundant unique and significant information. To emphasize the relationships between joints and bones, recent models build a skeleton graph, where nodes are human joints and edges are natural bones between two connected joints, and implement GCN to extract correlated features <ref type="bibr" target="#b23">[24]</ref>. Later, the spatial-temporal GCN (ST-GCN) further developed GCN to simultaneously learn spatial features of skeleton sequences <ref type="bibr" target="#b57">[56]</ref>. The ST-GCN constructed a graph to represent the connections of joints, and firstly proposed the temporal edges to link the relationships between consecutive frames. Although the ST-GCN captures the features of joints and bones, structurally key joints and bones are largely ignored, which may contain significant patterns and information of actions. For example, human hand joints and knee joints play key roles in the walking action. While the ST-GCN tries to capture all of the human joints with hierarchical GCN, the correlations between key joints and actions might weaken during the training process.</p><p>To address such issues, we need a new solution that can automatically detect the key human joints and bones, and deeply emphasize their relationships in terms of their large distance for every human action. Meanwhile, the new method could encode the highlighted features into the topological structures of the human skeleton as well as their dynamic properties, which could strongly elaborate on the strength of GCN, that is, highly clustering performance. While the skeletons are the structural graphs instead of 2D or 3D grids on different human actions, most of the previous models construct a constant graph as the input signal. Basically, to complete an action, everyone needs the cooperation of various body parts. The phenomenon indicates human actions are based on the relationships between different body parts, but some body parts may play key roles in human actions. For example, walking action strongly depends on legs, knees, and feet, while sitting action is closely related to the head and trunk. An exogenous dependency has been proposed for disconnected joints or human parts, but they could not cover wider-range human actions <ref type="bibr" target="#b60">[59]</ref>.</p><p>In this paper, we systematically study human actions, and propose a new and generic graph convolutional network to model characteristic skeletons for human action recognition, called the Centrality Graph Convolutional Networks (CGCN). In graph theory, centrality is defined to identify the most important nodes, edges or subgraphs within a graph. Centrality concepts were first developed in social network, and many of the terms used to measure centrality reflect their influential agent <ref type="bibr" target="#b39">[38]</ref>. Applications include identifying the most influential persons in a social network, network flows, walk structure, and key infrastructure subgraphs in Internet networks <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b58">57]</ref>. Considering the skeleton data, this new model answers what characterizes important joints, bones and endogenous body parts for different actions. Then, the CGCN is expected to provide a ranking that identifies the most important joints, bones and body parts.</p><p>The CGCN provides new and significant insights for skeleton-based action recognition by highlighting the key joints, bones and body parts. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, this graph model is formulated on top of a sequence of skeleton graphs, where each node reflects a joint on the human body. The basic structure learns the centrality features, like joint centrality, bone centrality and subgraph centrality, called the centrality module. Then, the extracted features are encoded into the spatial-temporal module for further training. The CGCN can model the relationships between physically connected and disconnected joints simultaneously, which could effectively capture low-level and high-order skeleton information. Besides, the centrality module via closeness, eigenvector and triplet subgraph calculations can obtain characteristic topological features from skeleton data. This module also reflects endogenous dependencies in an attention mechanism to enhance clustering performance.</p><p>Based on the intrinsic graph structures and human actions, we study new strategies to design centrality graph convolution networks, with inspirations from graph theory. The major contributions of this work are summarized in four aspects: (1) The CGCN is the first work for highlighting the centrality structures, like the key joints, bones and body parts on human actions. And it is designed to uncover the overlooked information between physically connected and disconnected parts of the human skeleton. (2) The CGCN follows several graph mechanisms in designing the centrality module to meet specific demands in human actions. It could offer a new and deep understanding of the action recognition task. (3) The motion information between consecutive frames is extracted for temporal information modeling. Both the spatial and motion information is fed into a four-channel framework for the action recognition task. (4) Our model outperforms the state-of-the-art methods on two large-scale datasets for skeleton-based action recognition. The find indicates that these centrality structures are fundamental mechanisms and factors hidden in the skeleton topology, which brings remarkable improvements for human action recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Skeleton-based action recognition. Despite the illumination change and scene variation, the reliable skeleton data could be easily and accurately extracted by pose estimation algorithms from depth sensors <ref type="bibr" target="#b47">[46]</ref> or RGB cameras <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b56">55]</ref> and temporal CNN <ref type="bibr" target="#b30">[31]</ref>. The substantial skeleton data spurs the creation of new technological or theoretical action recognition models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b53">52]</ref>. Recently, deep learning approaches have been widely applied to build the spatialtemporal frameworks of skeleton sequences in human action recognition <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b37">36]</ref>. For example, RNN models are implemented into human action recognition due to effectively learning long-term sequence data <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b46">45]</ref>. And Jain combines a spatial-temporal graph with RNN to model the human body components, like arm, leg, and trunk, which finds a good way to map joints into a graph <ref type="bibr" target="#b18">[19]</ref>. Meanwhile, compared to RNN, CNN also showed important implications for human action recognition, owing to its powerful parallelization over every element in the training process. Skeleton data are manually transferred into 2D or 3D images deployed by CNN <ref type="bibr" target="#b32">[33]</ref>, which achieves high performance in action recognition. However, these images used by CNN cannot fully reflect the topological structures of skeleton data. To address the limitations, GCN successfully applies to human action recognition, and its plausible mechanisms show more promising results <ref type="bibr" target="#b57">[56]</ref>. The traditional GCN models only consider physical connections between different joints on human actions. Since the endogenous factors, referring to physically disconnected joints, also play a preeminent role in human action recognition, the skeleton structure conveys much unrevealed information and leaves potential space to improve the performance of GCN <ref type="bibr" target="#b54">[53]</ref>. In this work, we investigate key topological properties of the skeleton data, such as closeness, eigenvalue, and triplet subgraphs. By using the centrality module, we successfully encode these characteristic features into the GCN framework and highlight the key joints, bones, and body parts for learning better human actions.</p><p>Graph convolutional networks. Mapping the relational data into graphs, the topological structures can be encoded to model the connections among nodes, and provide more promising perspectives underlying the data. Inspiring by this mechanism, GCN is successfully implemented in deep learning research. The various approaches in modeling GCNs fall into two categories, including spatial and spectral approaches. Spatial approaches use graph theory to define the nodes and edges for entities on data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b2">3]</ref>. Interestingly, spectral approaches analyze the constructed graph in the frequency domain <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref>. The spectral approach usually leverages the Laplacian eigenvector to transform a graph in the time domain to in frequency domain, potentially resulting in large computation cost <ref type="bibr" target="#b15">[16]</ref>. Considering human action recognition, most of the methods choose the spatial approaches to construct the GCN due to the large size of skeleton data. Since the human body is naturally formed as a graph, not a sequence or an image, the related features are easily extracted from skeleton data. However, their works only focus on the static graph structure and are hard to understand the dynamic information from human actions. Here, our model could adaptively extract the key information on joints, bones and body parts, which yields a dynamical GCN. Besides, the CGCN combines the high-order information to provide a new insight for learning human actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we explicate the centrality GCN for human action recognition based on skeleton data. Firstly, we introduce how to build the topological structures of a spatial graph from skeleton data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatial graph construction</head><p>A graph is a diagrammatical representation of the skeleton structure. It consists of nodes and edges and can be defined to G = (N, E), where N is the number of joints, and E is the number of bones. A is considered as its adjacency matrix with N × N dimensions. The entry A(i, j) is 1 if the edge connects node i to node j; or 0 otherwise. Let x ∈ R N be a feature vector for every node in a graph. By Fourier transformation, a spectral filter is defined to g(θ) = diag(θ), where θ ∈ R N is a parameter vector. Thus, the spectral convolution on a graph can be formulated as</p><formula xml:id="formula_0">z = g(θ) ⋆ x = U g(θ)U T x,<label>(1)</label></formula><p>where z is the extracted feature vector for each node, and U is the orthogonal matrix of eigenvectors of the Laplacian matrix L, i.e. L = U ΛU T =</p><formula xml:id="formula_1">I N ×N − D − 1 2 AD − 1 2 (D i = j A(i, j)</formula><p>is the degree vector). Through the Laplacian transform, the g(θ) can be considered as a function of the eigenvalues of L, i.e. g (θ) (Λ). Since the complexity time of U ΛU T is O(N 2 ), solving Eq. 1 is computationally expensive. In order to reduce time cost for large graphs, g (θ) (Λ) can be approximated by a truncated expansion by Chebyshev polynomials T m (x) up to M th order <ref type="bibr" target="#b57">[56]</ref> </p><formula xml:id="formula_2">g (θ ′ ) (Λ) ≈ M m=0 θ ′ m T m (Λ),<label>(2)</label></formula><p>whereΛ = 2 Λmax Λ − I N ×N , and θ ′ ∈ R M is a vector of Chebyshev coefficients. λ max is the maximum eigenvalue of L. Generally speaking, the Chebyshev polynomials are two sequences of polynomials, and defined by the recurrence relation:</p><formula xml:id="formula_3">T m (x) = 2xT m−1 (x) − T m−2 (x), with T 0 (x) = 1 and T 1 (x) = x.</formula><p>Thus, with the approximating method, the Eq. 1 can be addressed In order to obtain the linear Laplcian spectrum, we set M = 1 and λ max = 2 in Eq. 3. By this method, we could derive a linear formulation of Eq. 3 as</p><formula xml:id="formula_4">z = g(θ ′ ) ⋆ x ≈ M m=0 θ ′ m T m (L)x,<label>(3)</label></formula><formula xml:id="formula_5">whereL = 2 Λmax L − I N ×N , and (U ΛU T ) m = U Λ m U T . Note that this formula is M -localized since it is a M th -order</formula><formula xml:id="formula_6">z = g(θ ′ ) ⋆ x ≈ θ ′ 0 x + θ ′ 1 (L − I N ×N )x = θ ′ 0 x − θ ′ 1 D − 1 2 AD − 1 2 x,<label>(4)</label></formula><p>with two parameters θ ′ 0 and θ ′ 1 . Here, Eq. 4 allows us to construct deeper models to improve their capacity on large graphs. Besides, the two parameters could adapt to the change in scale during training.</p><p>In practice, minimizing the number of operations per layer, and Eq. 4 can be simplified to</p><formula xml:id="formula_7">z = g(θ) ⋆ x ≈ θ I N ×N + D − 1 2 AD − 1 2 x,<label>(5)</label></formula><p>where θ = θ ′ 0 = −θ ′ 1 , and the eigenvalues of</p><formula xml:id="formula_8">I N ×N + D − 1 2 AD − 1 2 range from 0 to 2.</formula><p>For Eq. 5, the normalized formulation can be written as</p><formula xml:id="formula_9">Z =D − 1 2ÂD − 1 2 XΘ = AXΘ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_10">A =D − 1 2ÂD − 1 2 = I N ×N + D − 1 2 AD − 1 2</formula><p>,D i = jÂ ij , X ∈ R N ×C and Θ ∈ R C×F (C is the dimensions of feature vector per node and F is the filter channels). Note that Z is the convolved or extracted feature matrix, and Θ is the filtering-parameters matrix. Thus, the complexity of this normalized formulation is O(EF C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Centrality graph convolutional networks</head><p>Considering human actions, the skeleton graphs are the sequence graphs or temporal graphs, that is, G = G 1 , G 2 , G 3 , ..., G t , where t is the time steps. Besides, human action recognition can be defined as a supervised-learning classification problem. Based on GCN, the proposed model, centrality graph convolutional networks, consists of three modules, such as skeleton-data input module, centrality module, Softmax module. The schematic of CGCN is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. In the first module, the joints and bones of the skeleton are extracted from the input images, i. e. X in . The last module obtain the trained features and model (i. e. X out ) to calculate the action scores, and then make a prediction <ref type="bibr" target="#b57">[56]</ref>. The function of the centrality module plays a significant role in highlighting the key joints, bones and body parts during training. It has three components, namely, joint centrality, bone centrality, subgraph centrality. Next, the components of the centrality module will be explicated below.</p><p>Joint centrality. In the directed skeleton graph, the joint can be considered as a node in the graph. Meanwhile, every node has the corresponding coordinates. And different human action has different coordinates in the spatial domain, that is, the spatial distance between two nodes is changed on human action. In graph theory, the normalized closeness centrality is a reasonable solution to measure the relevance between two nodes in the spatial distance. In a connected graph, the closeness centrality of a node is the average length of the shortest path between the node and all other nodes <ref type="bibr" target="#b3">[4]</ref>. It can be defined as the reciprocal of the distance</p><formula xml:id="formula_11">N c (i) = N − 1 j d(i, j) ,<label>(7)</label></formula><p>where d(i, j) is the spatial distance between node i and j, N is the number of nodes in the graph (normalizing the closeness value). Basically, the more central a node is, the closer it is to all other nodes. Measuring distances between a node and other nodes are irrelevant in undirected graphs, whereas it can yield highly correlate results in directed graphs. Indeed, our skeleton graph is a directed graph. Inspired by the mechanism, the skeleton graph can easily capture a high closeness centrality from outgoing edges, but low closeness centrality from incoming edges. This method can efficiently highlight the key joints (nodes) in the skeleton graphs for various human actions. To encode the significant and potential features into training, the closeness matrix of the skeleton graph is set as J ∈ R N ×N . Note that J is a symmetric matrix, has same size with the adjacency matrix.</p><p>Bone centrality. In the skeleton graph, the bone can be served as an edge. Due to the different coordinates of each node, the distances of bones have various changes. Considering graph theory, the edge betweenness centrality is a feasible method to measure the importance degree of an edge within a graph. The edge betweenness centrality is defined as the number of the shortest paths that pass through a specific edge in a graph or network <ref type="bibr" target="#b13">[14]</ref>. Each edge in the skeleton graph can be associated with an edge betweenness centrality value. It can be written as</p><formula xml:id="formula_12">B c (e i,j ) = (l,q) =(i,j) S l,q (e i,j )d l,q (e i,j ) S l,q d l,q ,<label>(8)</label></formula><p>where e i,j is the edge between node i and node j, S l,q is the number of all existing shortest paths from node l to node q, S l,q (e i,j ) is the number of all shortest paths from node l to node q that pass through edge e i,j , d l,q is the total distance length from node l to node q, and d l,q (e i,j ) is the total distance length from node l to node q that pass through edge e i,j . An edge with a high edge betweenness centrality score represents a bridge-like connection between two parts of a graph. If this edge is removed, it will directly affect the connectivity between many pairs of nodes through this edge. Maybe the removal of this edge leads to a partition of the skeleton graph into two densely connected subgraphs. Thus, the edge betweenness can directly highlight the importance of edges in the skeleton graph. The edge betweenness scores are also encoded into the betweenness matrix, i.e. B ∈ R N ×N . Subgraph centrality. Different body parts are associated with complete human actions. However, there exists a problem of how to evaluate the influence of every body part in human actions and measure the correlations between two body parts. In graph theory, the body part could be seen as a subgraph. A subgraph S is a graph whose node set N (S) is a subset of the node set N (G), i. e. N (S) ⊆ N (G). Similarly, the subgraph edge set E(S) is a subset of the edge set E(G), that is E(S) ⊆ E(G). In 2016, Benson et al. <ref type="bibr" target="#b0">[1]</ref> investigated the triplet subgraph in directed graphs, and proposed a feasible and efficient method to encode subgraph features into every corresponding node. Here, let S 1 , S 2 , S 3 , ..., S k denote a set of subgraphs (these subgraphs could have different node sets; in this work, we only consider 3-node subgraphs. For every node belonging to a subgraph S k , its weight will be counted by 1. Thus the subgraphweighted matrix, named W ∈ R N ×N , is built by this method. The associations with many pairs of subgraphs are measured by every node weights, that is, a weighted adjacency matrix. Through the subgraph centrality, strong connections are built through disconnected body parts, which could highlight the key body parts of human actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementing CGCN</head><p>To implement the CGCN in human action recognition, let X = X in be the input feature matrix on total frames per one video sample. Then, Eq. 6 can be rewritten as</p><formula xml:id="formula_13">Z = AXΘ = AX in Θ,<label>(9)</label></formula><p>Considering the proposed centrality matrixes J, B, W , A is replaced by J + B + W + A. Solving Eq. 9 yields</p><formula xml:id="formula_14">Z = (J + B + W + A)X in Θ.<label>(10)</label></formula><p>Using Softmax classifier, one has</p><formula xml:id="formula_15">Z = Sof tmax (J + B + W + A)ReLU ((J + B + W + A)XΘ 0 )Θ 1 = Sof tmax Ĉ ReLU (ĈXΘ 0 )Θ 1 ,<label>(11)</label></formula><p>whereĈ = J + B + W + A, Θ 0 ∈ R C×N is a weight matrix with N feature maps, and Θ 0 ∈ R N ×F is a hidden-to-output weight matrix. The softmax operation normalizes the combinations of four matrixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network architecture</head><p>One convolutional block for the CGCN is based on ST-GCN, i.e., implementing spatial and temporal convolution on three dimensions of feature maps, including the joint features (C), the frame number (T ), the number of joints (N ). In <ref type="figure" target="#fig_2">Fig. 2</ref>, one basic block of the CGCN is composed of seven layers, namely a spatial convolution (S-conv) layer, two batch normalization (BN) layers, two ReLU layers, a temporal convolution (T-conv) layer and a dropout layer. Note that the drop rate is set to 0.5. Note that the spatial convolution layer contains four different streams, such as joint centrality stream, bone centrality stream, subgraph centrality stream, and adjacency stream. The centrality graph convolutional network (CGCN) has nine basic convolution blocks. The numbers of output channels are 64, 64, 64, 128, 128, 128, 256, 256 and 256 on every block. At the beginning of nine basic blocks, there exists a data BN layer, which normalizes the input skeleton data. Through nine basic blocks, it is followed by a global average pooling layer, adjusting different feature dimensions of video samples into the same dimensions. Finally, the final results are calculated by a Softmax classifier layer to yield the classification results.</p><p>According to the architecture of CGCN in <ref type="figure" target="#fig_0">Fig. 1</ref>, the extracted topology information, i.e., the joint centrality, bone centrality and subgraph centrality, plays a significant role for human action recognition but is ignored in previous methods. Thus, we develop a new method to extract topology information with a four-channel framework to improve recognition.</p><p>In the directed skeleton graph, each bone is linked by two joints, such as the source node and the target node, respectively. The source node is close to the gravity point of the skeleton, while the target node is far away from that. A directed edge is defined as a bone between the source node and the target node. Note that every directed edge has the length of the corresponding bone and the direction information. For example, given two joints coordinates, that is, the source node n i = (x i , y i , z i ) and the target node n j = (x j , y j , z j ), the length of the bone can be calculated as d(n i , n j ) = (</p><formula xml:id="formula_16">x j − x i ) 2 + (y j − y i ) 2 + (z j − z i ) 2 .</formula><p>In this directed skeleton graph, there is no cycle, which indicates every directed edge can only contain a source node and a target node. In this way, the skeleton adjacency, joint centrality, bone centrality and subgraph centrality are calculated and implemented into graph convolution blocks, respectively. Finally, the Softmax classifier calculates the results of four streams to yield the score ranking and predict the action classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>To evaluate the proposed CGCN model, we conduct the experiments on two large-scale action recognition datasets: NTU-RGB+D <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b34">35]</ref> and Kinetics <ref type="bibr" target="#b20">[21]</ref>. First, since the size of the NTU-RGB+D dataset is smaller than that of the Kinetics dataset, we perform the ablation studies of our model on the NTU-RGB+D dataset, which examines every component of the proposed model on the action recognition performance. Then, the performance of the CGCN model is verified and compared with that of the other state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>NTU-RGB+D: NTU-RGB+D is currently one of the largest dataset widely used in the skeleton-based action recognition, which consists of 56,880 video samples categorized into 60 action classes. All of video samples are acted by 40 volunteers, who come from different age groups from 10 to 35. Each action is recorded by three Kinect V2 cameras concurrently from different horizontal angles, like −45 • , 0 • , 45 • . This dataset contains 3D skeletal data for each video sample. Here, it records 25 joints for each frame on each subject sample, while each video sample has one or two subjects. The dataset recommends two benchmarks. The first one is the cross-subject (X-sub), which is composed of a training set (40,320 video samples) and a testing set (16,560 video samples). The second one is the cross-view (X-view), where the training set contains 37,920 video samples captured by cameras No. 2 and No. 3, and the testing set contains 18,960 video samples captured by camera No. 1. In the comparison, we report the top-1 accuracy on two benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kinetics:</head><p>Kinetics is a large-scale dataset for human action recognition, containing 300,000 video samples. All of video samples covers 400 human action classes. The video samples are recorded by YouTube and have various subjects. It only contains RGB video samples without raw skeleton data. We obtain 18 joints' positions of every frame using the publicly available OpenPose toolbox <ref type="bibr" target="#b4">[5]</ref>. The captured skeleton data contains two dimensions of coordinates, i.e., (x, y), and confidencd score (c) for each joint. The dataset consists of a training set (240,000 video samples) and a testing set (20,000 video samples). Following the evaluation method, we report the top-1 and top-5 accuracies on the benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training details</head><p>The CGCN model is implemented on the PyTorch deep learning framework <ref type="bibr" target="#b42">[41]</ref>. The kernel size of this model is set to 4. And it contains four channels to train the skeleton data. The optimization method uses the stochastic gradient descent with Nesterov momentum (0.9). And the loss function chooses the cross-entropy for backpropagating gradients. The batch size is 32 for 65 epochs. The CGCN model is trained on 2 GTX-1080Ti GPUs. For different datasets, we set the specific configurations of the CGCN model.</p><p>For the NTU-RGB+D dataset, each video sample contains no more than two persons. The feature dimensions are respectively 64, 128 and 256. The number of frames in each video sample is set to 300. If the frame number is less than 300 frames, we repeat the video sample until it reaches 300 frames. In Eq. 10, X in should be the total features from 300 frames. The learning rate is set to 0.1 and the decay rate is set to 0.0001. The training process ends at the 49th epoch. I</p><p>For the Kinetics dataset, the input configuration of Kinetics is set to 150 frames with 2 persons in each video sample. We select the same data-augmentation methods as ST-GCN. For example, we randomly select 150 frames from the input skeleton data, and disorder the joint coordinates with randomly chosen translations and rotations. Here, X in should be the total features from 150 frames in Eq. 10. The learning rate is also set as 0.1 and the decay rate is set to 0.0001. The training process converges to the 57th epoch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>We examine the influence of each centrality components in the centrality graph convolutional network (CGCN) with the X-view benchmark on the NTU-RGB+D dataset. The performance of ST-GCN on the NTU-RGB+D dataset is 88.3%. By highlighting the centrality information and the specially four-channel training framework, the result is improved to 96.4%, which brings a definite improvement. The detail is introduced in the below parts.</p><p>Centrality graph convolutional block. As mentioned in Section 3.2, there are four types of centrality structures in the centrality graph convolutional block, i.e., A, J, B, and W . Note that A is the original skeleton graph, namely, adjacency matrix. Note that A is the same as the adjacency matrix in ST-GCN. Here, we only investigate the effects of the other three centrality structures. The different combinations of the centrality structures are tested on CGCN, including CGCN without J, CGCN without B and CGCN without W , and the results are shown in <ref type="table" target="#tab_0">Table 1</ref>. The results show that each centrality structure extracted from the graph is significant for human action recognition. Besides, deleting any one of centrality structures will dramatically reduce the performance. After combining all centrality structures added together, we can utilize all in information in each centrality structure and achieve better outcomes. We also test the influence of the adaptive skeleton graph used in the original ST-GCN. The result indicates that the weight and dynamic adjacency matrix is important, which also demonstrates the significance of the adaptive skeleton graph.</p><p>Visualization of the centrality graphs. <ref type="figure">Fig. 3</ref> is a visualization of the three centrality graph for one layer on the same action (from left to right is the joint centrality, bone centrality, and subgraph centrality, respectively). The skeleton graphs are plotted from the physical connections of the human body. The red joints, bones, body parts represent the highlighted parts by the centrality graphs. In <ref type="figure">Fig. 3</ref>, it indicates that the joints and bones of the human trunk play significant roles in this action. Besides, it also suggests that a traditional skeleton graph is not the best choice for the action recognition task, and different actions need <ref type="figure">Fig. 3</ref>. Illustration of the three centrality graphs for one layer on the same action. Red joints, bones and body parts are highlighted by the centrality graphs, namely, key joints, bones and body parts.</p><p>graphs with different centrality structures. The joint centrality graph pays more attention to the adjacent joints in the physical skeleton graph. For the subgraph centrality structures, the neck part and the hipbone part are detected to have a stronger connection, although they are far away from each other in the physical skeleton. Compared to previous works, the centrality graphs further capture the low-level features, like key joints and bones, and high-order structures, namely body parts. This information could uncover endogenous factors on different human actions and give new perspectives on human action recognition and graph convolutional networks. Thus, the centrality structures are more relevant to the action classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the state-of-the-art</head><p>We compare the performance of our model with the state-of-the-art action recognition methods on the NTU-RGB+D dataset <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5</ref> and Kinetics dataset <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">44]</ref>. The results of these two comparisons are shown in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref>, respectively. The methods used for comparison are categorized as RNN-based methods, CNN-based methods, and GCN-based methods. Importantly, our model outperforms the state-of-the-art methods on both datasets, which validate the superiority of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose a novel centrality graph convolutional network (CGCN) for human action recognition. It captures the key joints, bones and body parts from human actions and embeds them into the graph convolution networks to adaptively learn and update. Besides, the existing methods always ignore or overlook the importance of centrality information on skeleton data, i.e., joint centrality, bone centrality, and subgraph centrality. However, these centrality structures could reflect the low-level features and high-order information of the skeleton data. Moreover, a four-channel framework explicitly employs these centrality  <ref type="bibr" target="#b44">[43]</ref> 60.7 67.3 2s-3DCNN <ref type="bibr" target="#b33">[34]</ref> 66.8 72.6 ST-LSTM <ref type="bibr" target="#b37">[36]</ref> 69.2 77.7 STA-LSTM <ref type="bibr" target="#b48">[47]</ref> 73.4 81.2 VA-LSTM <ref type="bibr" target="#b59">[58]</ref> 79.2 87.7 ARRN-LSTM <ref type="bibr" target="#b27">[28]</ref> 80.7 88.8 Ind-RNN <ref type="bibr" target="#b29">[30]</ref> 81.8 88.0 TCN <ref type="bibr" target="#b22">[23]</ref> 74.3 83.1 C-CNN+MTLN <ref type="bibr" target="#b21">[22]</ref> 79.6 84.8 Synthesized CNN <ref type="bibr" target="#b38">[37]</ref> 80.0 87.2 ST-GCN <ref type="bibr" target="#b57">[56]</ref> 81. structures, which further improves performance. Finally, the CGCN model is validated on two large-scale datasets, namely NTU-RGB+D and Kinetics, and it outperforms the state-of-the-art methods on two datasets. The finding indicates that these centrality structures are fundamental mechanisms and factors hidden in the skeleton topology. This novel model sheds some new lights on future studies of skeleton-based action recognition. For example, how to incorporate the correlations between adjacent frames, such as similarities, distances, and structures into CGCN becomes a natural question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of the Centrality Graph Convolution Networks (CGCN). The "Diff" module calculates three orders of differences, namely joint positions, velocities and accelerations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>polynomial in the Laplacian matrix, that is, it is determined by nodes that are at maximum M steps away from the central node (M th -order neighborhood). The complexity time of Eq. 3 is O(E), where E is the number of edges. Defferrard et al. (2016) use this M -localized convolution to successfully implement a convolutional network on graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The schematic of the centrality graph convolutional blocks. S-conv is the spatial convolution layer, and T-conv is the temporal convolution layer. Both of them are connected to a BN layer and a ReLU layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of the top-1 accuracy when adding centrality graph convolutional block with or without J, B and W .</figDesc><table><row><cell>Methods</cell><cell>Accuracy(%)</cell></row><row><cell cols="2">ST-GCN without adaptive 88.3</cell></row><row><cell>ST-GCN with adaptive</cell><cell>90.8</cell></row><row><cell>CGCN without J</cell><cell>95.1</cell></row><row><cell>CGCN without B</cell><cell>95.9</cell></row><row><cell>CGCN without W</cell><cell>95.9</cell></row><row><cell>CGCN</cell><cell>96.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparisons of the top-1 accuracy with state-of-the-art methods on the NTU-RGB+D dataset</figDesc><table><row><cell>Methods</cell><cell cols="2">X-sub (%) X-view (%)</cell></row><row><cell>H-RNN [11]</cell><cell>59.1</cell><cell>64.0</cell></row><row><cell>Deep LSTM</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons of the top-1 and top-5 accuracies with state-of-the-art methods on the Kinetics-Skeleton dataset.</figDesc><table><row><cell></cell><cell>5</cell><cell>88.3</cell></row><row><cell cols="2">CNN-Motion+Trans [10] 83.2</cell><cell>89.3</cell></row><row><cell>ResNet152 [26]</cell><cell>85.0</cell><cell>92.3</cell></row><row><cell>DPRL+GCNN [50]</cell><cell>83.5</cell><cell>89.8</cell></row><row><cell>2s-AGCN [44]</cell><cell>88.5</cell><cell>95.1</cell></row><row><cell>CGCN (ours)</cell><cell>90.3</cell><cell>96.4</cell></row><row><cell>Methods</cell><cell cols="2">Top-1 (%) Top-5 (%)</cell></row><row><cell cols="2">Feature Encoding [13] 14.9</cell><cell>25.8</cell></row><row><cell>Deep LSTM [43]</cell><cell>16.4</cell><cell>35.3</cell></row><row><cell>TCN [23]</cell><cell>20.3</cell><cell>40.0</cell></row><row><cell>ST-GCN [56]</cell><cell>30.7</cell><cell>52.8</cell></row><row><cell>AS-GCN [29]</cell><cell>34.8</cell><cell>56.3</cell></row><row><cell>2s-AGCN [44]</cell><cell>36.1</cell><cell>58.7</cell></row><row><cell>CGCN (ours)</cell><cell>37.5</cell><cell>60.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5167" to="5176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03286</idno>
		<title level="m">On the robustness of the cvpr 2018 white-box adversarial example defenses</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Communication patterns in task-oriented groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bavelas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of the acoustical society of America</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="725" to="730" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Review of constraints on vision-based gesture recognition for human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Bhuyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Macdorman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Computer Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sports camera calibration via synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Community structure in social and biological networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences 99</title>
		<meeting>the national academy of sciences 99</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="7821" to="7826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Artificial intelligence in radiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hosny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quackenbush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Aerts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Cancer</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="500" to="510" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Broadband absorbing semiconducting polymer nanoparticles for photoacoustic imaging in second near-infrared window</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Upputuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pramanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nano letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4964" to="4969" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE conference on computer vision and pattern recognition workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06055</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Skeleton-based relational modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02556</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Person re-identification by deep joint learning of multiloss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04724</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptive rnn tree for largescale human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1444" to="1452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01036</idno>
		<title level="m">Demystifying neural style transfer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Two-stream 3d convolutional neural network for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<title level="m">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TPAMI.2019.2916873</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2916873" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Networks. Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A measure of betweenness centrality based on random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="54" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ranking of closeness centrality for large-scale social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on frontiers in algorithmics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning a deep model for human action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="667" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning and refining of privileged information-based rnns for action recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3461" to="3470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning class regularized features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02651</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Natural history and tumor volume kinetics of papillary thyroid cancers during active surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Minkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Untch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ganly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Shaha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Otolaryngology-Head &amp; Neck Surgery</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1015" to="1020" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatio-temporal naive-bayes nearest-neighbor (stnbnn) for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4171" to="4180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08254</idno>
		<title level="m">Memory attention networks for skeleton-based action recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00977</idno>
		<title level="m">Pose flow: Efficient online pose tracking</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Distributed algorithms for computation of centrality measures in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tempo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2080" to="2094" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01189</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
