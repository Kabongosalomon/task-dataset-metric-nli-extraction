<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyang</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuu</forename><surname>Jinnai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present AlphaX, a fully automated agent that designs complex neural architectures from scratch. AlphaX explores the search space with a distributed Monte Carlo Tree Search (MCTS) and a Meta-Deep Neural Network (DNN). MCTS guides transfer learning and intrinsically improves the search efficiency by dynamically balancing the exploration and exploitation at fine-grained states, while Meta-DNN predicts the network accuracy to guide the search, and to provide an estimated reward to speed up the rollout. As the search progresses, AlphaX also generates the training data for Meta-DNN. So, the learning of Meta-DNN is endto-end. In 8 GPU days, AlphaX found an architecture that reaches 97.88% top-1 accuracy on CIFAR-10, and 75.5% top-1 accuracy on ImageNet. We also evaluate AlphaX on a large scale NAS dataset for reproducibility. On NASBench-101, AlphaX also demonstrates 3x and 2.8x speedup over Random Search and Regularized Evolution in finding the global optimum. Finally, we show the searched architecture improves a variety of vision applications from Neural Style Transfer, to Image Captioning and Object Detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Designing efficient neural architectures is extremely laborious. A typical design iteration starts with a heuristic design hypothesis from domain experts, followed by the design validation with hours of GPU training. The entire design process renders many of such iterations before finding a satisfying architecture. Neural Architecture Search (NAS) has emerged as a promising tool to alleviate human effort in this trial and error design process, but the tremendous computing resources required by current NAS methods motivates us to investigate the search efficiency.</p><p>AlphaGo/AlphaGoZero <ref type="bibr" target="#b32">[33]</ref> recently show super-human performance in playing the game of Go, by using a specific search algorithm called Monte-Carlo Tree Search (MCTS) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4]</ref>. Given the current game state, MCTS gradually builds an online model for its subsequent game states to evaluate the winning chance at that state, based on search experiences in the current and prior games, and makes a decision. Search experience is from the previous search tra- ). An online model is to evaluate how promising the current search branch based on prior rollouts, and Random Search has no online model. (b) Search methods guided by online performance models built from previous rollouts. With static, coarse-grained exploration strategy (e.g., -greedy in Q-learning), they may quickly be stuck in a sub-optimal solution; and the chance to escape is exponentially decreasing along the trajectory.</p><p>(c) AlphaX builds online models of both performance and visitation counts for adaptive exploration. (d) Performance of different search algorithms on NASBench-101 <ref type="bibr" target="#b42">[43]</ref>. Al-phaX is 3x, 1.5x more sample-efficient than random search and -greedy based Q learning.</p><p>jectories (called rollouts) that have been tried, and their consequences (whether the player wins or not). Different from traditional MCTS approach that evaluates the consequence of a trajectory by random self-play to the end of a game, Al-phaGo uses a predictive model (or value network) to predict the consequence, which enjoys much lower variance. Furthermore, due to its built-in exploration mechanism using Upper Confidence bound applied to Trees (UCT) <ref type="bibr" target="#b1">[2]</ref>, based MetaQNN (QL) <ref type="bibr" target="#b2">[3]</ref> × --greedy × × Zoph (PG) <ref type="bibr" target="#b43">[44]</ref> × RNN -√ × PNAS (HC) <ref type="bibr" target="#b16">[17]</ref> × RNN -√ × Regularized Evolution <ref type="bibr" target="#b29">[30]</ref> √ -top-k mutation √ × Random Search <ref type="bibr" target="#b31">[32]</ref> √ -random √ × DeepArchitect (MCTS) <ref type="bibr" target="#b27">[28]</ref> √ -UCT × × Wistuba (MCTS) <ref type="bibr" target="#b39">[40]</ref> √ Gaussian UCT × × AlphaX (MCTS) √ meta-DNN UCT √ √ AlphaX builds an online model which guides the future search, compared to greedy methods, e.g. Q-learning, Regularized Evolution or Top-K methods, AlphaX dynamically trades off exploration and exploitation and can escape from locally optimal solutions with fewer number of search trials. <ref type="figure" target="#fig_0">Fig. 1</ref> summarizes the trade-offs. Furthermore, while prior works on MCTS applied to Architecture Search <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b27">28]</ref> only report performance on CIFAR-10, our distributed Al-phaX system can scale up to 32 machines (with 32 acceleration ratio), reports better accuracies on CIFAR-10 and achieves performance on large-scale dataset like ImageNet that is on par with the SOTA. Particularly, AlphaX is up to 3x faster than Random Search and Regularized Evolution in finding the best performing network on NASBench-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Monte Carlo Tree Search: DeepArchitect <ref type="bibr" target="#b27">[28]</ref> implemented vanilla MCTS for NAS without a predictive model, and Wistuba <ref type="bibr" target="#b39">[40]</ref> uses statistics from the current search (e.g., RAVE and Contextual Reward Prediction) to predict performance of a state. In comparison, our performance estimate is from both searched rollouts so far and a model (meta-DNN) learned from performances of known architectures and can generalize to unseen architectures. Both previous works report performance on CIFAR-10, while Al-phaX reports performance on CIFAR-10, ImageNet, and NASBench-101. Most importantly, AlphaX is the first scalable MCTS based design agent.</p><p>Bayesian Optimization (BO) is a popular method for the hyper-parameter search <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b12">13]</ref>; It is proven to be an effective black-box optimization technique for small scale problems, e.g. finding good hyper-parameters for Stochas- tic Gradient Descent (SGD). In a large-scale problem, it demands a good, sophisticated high-dimensional representation kernel to work, requires calculating the inverse of a covariance matrix O(n 2.376 ) that quadratically increases with samples (n), so BO is not a practical algorithm for NAS.</p><p>Reinforcement Learning (RL): Several RL techniques have been investigated for NAS <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">44]</ref>. Baker et al. proposed a Q-learning agent to design network architectures <ref type="bibr" target="#b2">[3]</ref>. The agent takes a -greedy policy: with probability 1 − , it chooses the action that leads to the best expected return (i.e. accuracy) estimated by the current model, otherwise uniformly chooses an action. Zoph et al. built an RNN agent trained with Policy Gradient to design CNN and LSTM <ref type="bibr" target="#b43">[44]</ref>. However, directly maximizing the expected reward in vanilla Policy Gradient could lead to local optimal solution <ref type="bibr" target="#b26">[27]</ref>. In comparison, MCTS records both expected return and visitation counts of states to balance between exploration and exploitation at the state level.</p><p>Hill Climbing (HC): Elsken et al. proposed a simple hill climbing for NAS <ref type="bibr" target="#b6">[7]</ref>. Starting from an architecture, they train every descendent network before moving to the best performing child. Liu et al. deployed a beam search which follows a similar procedure to hill climbing but selects the top-K architectures instead of only the best <ref type="bibr" target="#b16">[17]</ref>. HC is akin to the vanilla Policy Gradient tending to trap into a local optimum from which it can never escape, while MCTS demonstrates provable convergence toward the global optimal given enough time <ref type="bibr" target="#b13">[14]</ref>.</p><p>Evolutionary Algorithm (EA): Evolutionary algorithms represent each neural network as a string of genes and search the architecture space by mutation and recombinations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30]</ref>. Strings which represent a neural network with top performance are selected to generate child models. The selection process is in lieu of exploitation, and the mutation is to encourage exploration. Still, GA algorithms do not consider the visiting statistics at individual states, and its performance is on par with random search for lacking an online model to inform decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell Input1</head><p>Cell Input2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AlphaX: A Scalable MCTS Design Agent</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Design, State and Action Space</head><p>Design Space: the neural architectures for different domain tasks, e.g. the object detection and the image classification, follow fundamentally different designs. This renders different design spaces for the design agent. AlphaX is flexible to support various search spaces with an intuitive state and action abstraction. Here we provide a brief description of two search spaces used in our experiments.</p><p>• NASNet Search Space: <ref type="bibr" target="#b44">[45]</ref> proposes searching a hierarchical Cell structure as shown in <ref type="figure" target="#fig_3">Fig.3a</ref>. There are two types of Cells, Normal Cell (N Cell) and Reduction Cell (RCell). Normal Cell maintains the input and output dimensions with the padding, while Reduction Cell reduces the height and width by half with the striding. Then, the network is constituted by stacking multiple cells.</p><p>• NASBench Search Space: <ref type="bibr" target="#b42">[43]</ref> proposes searching a small Direct Acyclic Graph (DAG) with each node representing a layer and each edge representing the interlayer dependency. Similarly, the network is constituted by stacking multiple such DAGs.</p><p>State Space: a state represents a network architecture, and AlphaX utilizes states (or nodes) to keep track of past trails to inform future decisions. We implement a state as a map that defines all the hyper-parameters for each network layers and their dependencies. We also introduce a special terminal state to allow for multiple actions. All the other states can transit to the terminal state by taking the terminal action, and the agent only trains the network, from which it reaches the terminal. With the terminal state, the agent freely modifies the architecture before reaching the terminal. This enables multiple actions for the design agent to bypass shallow architectures.</p><p>Action Space: an action morphs the current network architecture, i.e. current state, to transit to the next state. It not only explicitly specifies the inter-layer connectivity, but also all the necessary hyper-parameters for each layer. Unlike games, actions in NAS are dynamically changing w.r.t the current state and design spaces. For example, AlphaX needs to leverage the current DAG (state) in enumerating all the feasible actions of 'adding an edge'. In our experiments, the actions for the NASNet search domain are adding a new layer in the left or right branch of a Block i in a Cell, creating a new Block with different input combinations, and the terminating. The actions for the NASBench search domain are either adding a node or an edge, and the terminating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Search Procedure</head><p>This section elaborates the integration of MCTS and metaDNN in AlphaX. The MCTS is to analyze the most promising move at a state, while the meta-DNN is to learn the sampled architecture performance and to generalize to unexplored architectures so that MCTS can simulate many rollouts with only an actual network training in evaluating a new node. The superior search efficiency of AlphaX is due to balancing the exploration and exploitation at the finest granularity, i.e. state level, by leveraging the visiting statistics. Each node tracks these two statistics: 1) N (s, a) counts the selection of action a at state s; 2) Q(s, a) is the expected reward after taking action a at state s, and intuitively Q(s, a) is an estimate of how promising this search direction. <ref type="figure" target="#fig_2">Fig.2</ref> demonstrates a typical searching iteration in AlphaX, which consists of Selection, Expansion, Meta-DNN assisted Simulation, and Backpropagation. We elucidate each step as follows.</p><p>Selection traverses down the search tree to trace the current most promising search path. It starts from the root and stops till reaching a leaf. At a node, the agent selects actions based on UCB1 <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_0">π tree (s) = arg max a∈A Q(s, a) N (s, a) + 2c 2 log N (s) N (s, a) ,<label>(1)</label></formula><p>where N (s) is the number of visits to the state s (i.e. N (s) = a∈A N (s, a)), and c is a constant. The first term ( Q(s,a) N (s,a) ) is the exploitation term estimating the expected accuracy of its descendants. The second term (2c 2 log N (s) N (s,a) ) is the exploration term encouraging less visited nodes. The exploration term dominates π tree (s) if N (s, a) is small, and the exploitation term otherwise. As a result, the agent favors the exploration in the beginning until building proper confidences to exploit. c controls the weight of exploration, and it is empirically set to 0.5. We iterate the tree policy to reach a new node.</p><p>Expansion adds a new node into the tree. Q(s, a) and N (s, a) are initialized to zeros. Q(s, a) will be updated in the simulation step. Meta-DNN assisted Simulation randomly samples the descendants of a new node to approximate Q(s, a) of the node with their accuracies. The process is to estimate how promising the search direction rendered by the new node and its descendants. The simulation starts at the new node. The agent traverses down the tree by taking the uniformrandom action until reaching a terminal state, then it dispatches the architecture for training.</p><p>The more simulation we roll, the more accurate estimate of this search direction we get. However, we cannot conduct many simulations as network training is extremely timeconsuming. AlphaX adopts a novel hybrid strategy to solve this issue by incorporating a meta-DNN to predict the network accuracy in addition to the actual training. We delay the introduction of meta-DNN to sec.3.3. Specifically, we estimate q = Q(s, a) with</p><formula xml:id="formula_1">Q(s, a) ← Acc(sim 0 (s )) + 1 k i=1..k P red(sim i (s )) /2</formula><p>(2) where s = s + a, and sim(s ) represents a simulation starting from state s . Acc is the actually trained accuracy in the first simulation, and P red is the predicted accuracy from Meta-DNN in subsequent k simulations. If a search branch renders architectures similar to previously trained good ones, Meta-DNN updates the exploitation term in Eq.1 to increase the likelihood of going this branch.</p><p>Backpropagation back-tracks the search path from the new node to the root to update visiting statistics. Please note we discuss the sequential case here, and the backpropagation will be split into two parts in the distributed setting (sec.3.5). With the estimated q for the new node, we iteratively back-propagate the information to its ancestral as:</p><formula xml:id="formula_2">Q(s, a) ← Q(s, a) + q, N (s, a) ← N (s, a) + 1 s ← parent(s), a ← π tree (s)<label>(3)</label></formula><p>until it reaches the root node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The design of Meta-DNN and its related issues</head><p>Meta-DNN intends to generalize the performance of unseen architectures based on previously sampled networks. It provides a practical solution to accurately estimate a search branch with many simulations without involving the actual training (see the metaDNN assisted simulation for details). New training data is generated as AlphaX advances in the search. So, the learning of Meta-DNN is end-to-end. The input of Meta-DNN is a vector representation of architecture, while the output is the prediction of architecture performance, i.e. test accuracy.</p><p>The coding scheme for NASNet architectures is as follows: we use 6-digits vector to code a Block; the first two digits represent up to two layers in the left branch, and the 3rd and 4th digits for the right branch. Each layer is represented by a number in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref> to represent 12 different layers, and the specific layer code is available in Appendix TABLE.4. We use 0 to pad the vector if a layer is absent. The last two digits represent the input for the left and right branch, respectively. For the coding of block inputs, 0 corresponds to the output of the previous Cell, 1 is the previous, previous Cell, and i + 2 is the output of Block i . If a block is absent, it is [0,0,0,0,0,0]. The left part of <ref type="figure" target="#fig_4">Fig.4</ref> demonstrates an example of NASNet encoding scheme. A Cell has up to 5 blocks, so a vector of 60 digits is sufficient to represent a state that fully specifies both RCell and N Cell. The coding scheme for NASBench architectures is a vector of flat adjacency matrix, plus the nodelist. Similarly, we pad 0 if a layer or an edge is absent. The right part of <ref type="figure" target="#fig_4">Fig.4</ref> demonstrates an example of NASBench encoding scheme. Since NASBench limits nodes ≤ 7, 7×7 (adjacency matrix)+ 7 (nodelist) = 56 digits can fully specify a NASBench architecture. Now we cast the prediction of architecture performance as a regression problem. Finding a good metaDNN is heuristically oriented and it should vary from tasks to tasks. We calculate the correlation between predicted accuracies and true accuracies from the sampled architectures in evaluating the design of metaDNN. Ideally, the metaDNN is expected to rank an unseen architecture in roughly similar to its true test accuracy, i.e. corr = 1. Various ML models, such as Gaussian Process, Neural Networks, or Decision Tree, are candidates for this regression task. We choose Neural Networks as the backbone model for its powerful generalization on the high-dimensional data and the online training capability. More ablations studies for the specific choices of metaDNN is available in sec.4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Transfer Learning</head><p>As MCTS incrementally builds a network with primitive actions, networks falling on the same search path render similar structures. This motivates us to incorporate Transfer Learning in AlphaX to speedup network evaluations. In simulation <ref type="figure" target="#fig_2">(Fig. 2)</ref>, AlphaX recursively traverses up the tree to find a previously trained network with the minimal edit distance to the newly sampled network. Then we transfer  <ref type="figure">Figure 5</ref>: Distributed AlphaX: we decouple the original back-propagation into two parts: one uses predicted accuracy (green arrow), while the other uses the true accuracy (blue arrow). The pseudocode for the whole system is available in Appendix Sec.6 the weights of overlapping layers, and randomly initialize new layers. In the pre-training, we train every samples for 70 epochs, while we train an architecture for 20 epochs if the parent weights are available. We provide an ablation study in <ref type="figure" target="#fig_0">Fig. 12</ref> to justify the design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Distributed AlphaX</head><p>It is imperative to parallelize AlphaX to work on a large scale distributed systems to tackle the computation challenges rendered by NAS. <ref type="figure">Fig.5</ref> demonstrates the distributed AlphaX. There is a master node exclusively for scheduling the search, while there are multiple clients (GPU) exclusively for training networks. The general procedures on the server side are as follows: 1) The agent follows the selection and expansion steps described in <ref type="figure" target="#fig_2">Fig.2.</ref> 2) The simulation in MCTS picks a network arch n for the actual training, and the agent traverses back to find the weights of parent architecture having the minimal edit distance to arch n for transfer learning; then we push both arch n and parent weights into a job queue. We define arch n as the selected network architecture at iteration n, and rollout − f rom(arch n ) as the node which it started the rollout from to reach arch n . 3) The agent preemptively backpropagatesq ← 1 </p><p>4) The server checks the receive buffer to retrieve a finished job from clients that includes arch z , acc z . Then the agent starts the second backpropagation to propagate q ← accz+q 2 (Eq. 2) from the node the rollout started </p><p>The client constantly tries to retrieve a job from the master job queue if it is free. It starts training once it gets the job, then it transmits the finished job back to the server. So, each client is a dedicated trainer. We also consider the faulttolerance by taking a snapshot of the server's states every few iterations, and AlphaX can resume the searching from the breakpoint using the latest snapshot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluations of Architecture Search</head><p>Experiment setup: An anonymized implementation of AlphaX to search on NASBench-101 for the reviewers are at <ref type="bibr" target="#b0">[1]</ref>. Appendix Sec.9 provides the details of experiment setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Finding the Global Optimal on NASBench-101</head><p>Searching NASNet requires training thousands of networks, so it is not computationally feasible to conduct many trials to fairly evaluate a search algorithm. Current literatures mainly evaluate NAS algorithms with the final test accuracy, while <ref type="bibr" target="#b31">[32]</ref> has shown many state-of-the-art NAS algorithms, e.g. DARTS <ref type="bibr" target="#b20">[21]</ref>, NAO <ref type="bibr" target="#b23">[24]</ref>, ENAS <ref type="bibr" target="#b28">[29]</ref>, cannot even outperform Random Search in the same setting. To truly evaluate a search algorithm, and to bypass the computational challenges, Christ et al collected NASBench <ref type="bibr" target="#b42">[43]</ref> that enumerates all the possible DAGs of nodes ≤ 7, constituting of (420k+) networks and their final test accuracies.</p><p>In our experiments, we limit the maximal nodes in a DAG ≤ 6 for repeating each algorithm for 200 trails, and the rest follows the same NASBench setting, i.e. taking a subset of NASBench-101 with 64521 valid networks. The search target is the network with the highest mean test accuracy (global optimal) at 108th epochs, which can be known ahead by querying the dataset. Our evaluation metric is the number of samples to reach the best performance architecture in testing. We choose Random Search (RS) <ref type="bibr" target="#b31">[32]</ref> and Regularized Evolution (RE) <ref type="bibr" target="#b29">[30]</ref> as the baseline, and experimental results are in <ref type="figure">Fig.7</ref>. We have run each algorithm for 200 independent trials, and each trail is a new search fed with a different random seed. The search terminates once it reaches the target (the global optimal). <ref type="figure">Fig.7</ref> demonstrates AlphaX is 2.8x and 3x faster than RS and RE, respectively. As we analyzed in <ref type="figure" target="#fig_0">Fig.1</ref>, Random Search lacks an online model. Regularized Evolution only mutates on top-k performing models, while MCTS explicitly builds a search tree to dynamically trade off the exploration and exploitation at individual states. Please note that the slight difference in <ref type="figure">Fig.7a</ref> actually reflects a huge gap in speed as indicated by <ref type="figure">Fig.7b</ref>. This is due to the minor difference (within 0.5%) between the near optimal architectures and the global optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Open domain search [45]</head><p>we perform the search on CIFAR-10 using 10 NVIDIA 1080 TI. One GPU works as a server, while the rest work as clients. The client-server communications are through python sockets. To sample more architectures within limited resources, we early terminate the training at the 70th on each search clients. Then we rank architectures by their preliminary accuracies to perform an additional 530 epochs on good candidates. In acquiring the final accuracy, cutout is applied [23] using 1 random crop of size 16 × 16. For the subsequent ImageNet training, we construct the network for ImageNet with searched RCell and N Cell according to <ref type="figure" target="#fig_2">Fig.2</ref> in <ref type="bibr" target="#b44">[45]</ref>. We set up the ImageNet training using the standard mobile configuration with the input image size of (224 × 224) <ref type="bibr" target="#b44">[45]</ref>. More details are available in the ap-    pendix. AlphaX sampled 1000 networks, and we selected the top 20 networks in the pre-training to fine-tune another 530 epochs. <ref type="figure">Fig.6</ref> demonstrates the architecture that yields the highest accuracy after fine-tuning.</p><p>Comparisons to State-of-the-Art Results: <ref type="table" target="#tab_2">Table. 2 and  Table.</ref> 3 summarize state-of-the-art results on CIFAR10 and ImageNet, and AlphaX consistently exceeds state-of-the-art methods in both samples (M) and accuracies. Our end-toend search cost, i.e. GPU days, is on par with SToA methods thanks to the early terminating and transfer learning. Notably, AlphaX achieves the similar accuracy to Amoe-baNet with 27x less samples for the case with cutout and filters = 32. Without cutout and filters = 32, AlphaX outperforms NAONet by 0.14% in the test error with 13.3x less GPU days. Proxyless-G used a different search space, and our result is on par with it after increasing the filters to 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Qualitative Evaluations of AlphaX</head><p>Several interesting insights are observable in <ref type="figure" target="#fig_0">Fig.9. 1</ref>) MCTS explicitly builds a search tree to better inform future decisions based on prior rollouts. The choice of actions in MCTS is fine-grained, leveraging both the visiting statistics and value at individual states; while the random or evolutionary search is stateless, utilizing a coarse estimation of search direction (e.g. mutation). Therefore, MCTS is a lot faster than RS and RE in <ref type="figure">Fig.7</ref>. 2) MCTS invests more on the promising directions (high value path), and less otherwise (low value path). Unlike greedy based algorithms, e.g. hill climbing, MCTS consistently explores the search space as the search tree is similar to a balanced tree in <ref type="figure">Fig.9</ref>. All of  these manifest that MCTS automatically balances the exploration and exploitation in the progress of the search. 3) the best performing network is not necessarily located on the most promising branch, and there are many local optimal branches (dark blue nodes). MCTS is theoretically guaranteed to find the global optimal in this non-convex search space, while PNAS or DARTs can easily trap into a local optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Meta-DNN Design and its Impact</head><p>The metric in evaluating metaDNN is the correlation between the predicted v.s. true accuracy. We used 80% NASBench for training, and 20% for testing. Since DNNs have shown great success in modeling complex data, we start with Multilayer Perceptron (MLP) and Recurrent Neural Network (RNN) on building the regression model. Specific architecture details are available in appendix.10. <ref type="figure" target="#fig_0">Fig. 11d</ref> and <ref type="figure" target="#fig_0">Fig .11e</ref> demonstrate the performance of MLP (corr=0.784) is 4% better than RNN (corr=0.743), as the MLP <ref type="figure" target="#fig_0">(Fig. 11b</ref>) performs much better than RNN <ref type="figure" target="#fig_0">(Fig. 11a</ref>) in the training set. However, MLP still mispredicts many networks around 0.1, 0.4 and 0.6 and 0.8 (x-axis) as shown in <ref type="figure" target="#fig_0">Fig. 11e</ref>. This clustering effect is consistent with the architecture distribution in <ref type="figure" target="#fig_8">Fig. 8</ref> for having many networks around these accuracies. To alleviate this issue, we propose a mult-stage model, the core idea of which is to have several dedicated MLPs to predict different ranges of accuracies, e.g. [0, 25%], along with another MLP to predict which MLP to use in predicting the final accuracy. <ref type="figure" target="#fig_0">Fig. 11f</ref> shows multi-stage model successfully improves the correlation by 1.2% from MLP, and the mispredictions have been greatly reduced. Since the multi-stage model has achieved corr =  1 on the training set, we choose it as the backbone regression model for AlphaX. <ref type="figure">Fig. 7</ref> demonstrates our meta-DNN successfully improves the search efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer Learning</head><p>The transfer learning significantly speed network evaluations up, and <ref type="figure" target="#fig_0">Fig. 12</ref> empirically validates the effectiveness of transfer learning. We randomly sampled an architecture as the parent network. On the parent network, we added a block with two new 5x5 separable conv layers on the left and right branch as the child network. We trained the parent network toward 70 epochs and saved its weights. In training the child network, we used weights from the parent network in initializing the child network except for two new conv layers that are randomly initialized. <ref type="figure" target="#fig_0">Fig. 12</ref> shows the accuracy progress of transferred child network at different training epochs. The transferred child network retains the same accuracy as training from scratch (random initialization) with much less epochs, but insufficient epochs loses the accuracy. Therefore, we chose 20 epochs in pre-training an architecture if transfer learning applied. <ref type="figure" target="#fig_0">Fig. 13</ref> evaluates MCTS against Q-Learning (QL), Hill Climbing (HC) and Random Search (RS) on a simplified search space. Setup details and the introduction of the design domain are available in appendix.8. These algorithms are widely used in NAS <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref>. We conduct 10 trials for each algorithm. <ref type="figure" target="#fig_0">Fig. 13b</ref> demonstrates AlphaX is 2.3x faster than QL and RS. Though HC is the fastest, <ref type="figure" target="#fig_0">Fig. 13a</ref> indicates HC traps into a local optimal. Interestingly, <ref type="figure" target="#fig_0">Fig. 13b</ref> indicates the inter-quartile range of QL is longer than RS. This is because QL quickly converges to a suboptimal, spending a huge time to escape. This is consistent with <ref type="figure" target="#fig_0">Fig. 13a</ref> that QL converges faster than RS before the 50th samples, but random can easily escape from the local optimal afterward. <ref type="figure" target="#fig_0">Fig. 13b</ref> (MCTS v.s. AlphaX) fur- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Algorithm Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Improved Features for Applications</head><p>CNN is a common component for Computer Vision (CV) models. Here, we demonstrate the searched architecture can improve a variety of downstream Computer Vision (CV) applications. Please check the Appendix Sec.11 for the experiment setup. 1) Object Detection: We replace MobileNet-v1 with AlphaX-1 in SSD <ref type="bibr" target="#b21">[22]</ref> object detection model, and the mAP (mini-val) increases from 20.1% to 23.7% at the 300 × 300 resolution. <ref type="figure" target="#fig_0">(Fig.18</ref>) 2) Neural Style Transfer: AlphaX-1 is better than a shallow network (VGG) in capturing the rich details and textures of a sophisticated style image <ref type="figure" target="#fig_0">(Fig.19)</ref>.</p><p>3) Image Captioning: we replace the VGG with AlphaX-1 in show attend and tell <ref type="bibr" target="#b41">[42]</ref>. On the 2014 MSCOCO-val dataset, AlphaX-1 outperforms VGG by 2.4 (RELU-2), 4.4 (RELU-3), 3.7 (RELU-4), respectively <ref type="figure" target="#fig_2">(Fig.20)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present AlphaX, the first scalable MCTS based design agent for NAS. AlphaX demonstrates superior search efficiency over mainstream algorithms on 3 different search domains, highlighting MCTS as a very promising search algorithm for NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Pseudocode for AlphaX</head><p>In this section, we describe the pseudocode of the Distributed AlphaX. Algorithm 3 describes the search engine of AlphaX. Algorithm 2 is the server procedure to send the architecture to train chosen by the MCTS to the client and collect the architectures trained and their scores. Algorithm 1 is the client which trains and tests the architecture provided by the server. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Details of the State and Action Space</head><p>This section contains the description of the state and action space for NASNet design space.</p><p>We constrain the state space to make the design problem manageable. The state space exponentially grows with the depth: a k layers linear network has n k architecture variations, where n is the number of layer types. We leverage the GPU DRAM size, our current computing resources and the design heuristics from leading DNNs, to propose the following constraints on the state space: 1) a branch has at most 1 layer; 2) a cell has at most 5 blocks; 3) the depth of blocks is limited to 2; 5) we use the layers in TABLE.4:</p><p>Actions also preserve the constraints imposed on the state space. If the next state reaches out of the design boundary, the agent automatically removes the action from the action set. For example, we exclude the "adding a new layer" action for a branch if it already has 1 layer. So, the action set is dynamically changing w.r.t states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experimental Setup for Section 4.4</head><p>The state space we consider consists of small simple CNNs. We consider a convolution layer and a softmax layer. For a convolutional layer, we allow a range of 1 or 2 for stride, 32 or 64 for filters, and 2 or 4 for kernels. We set the maximum depth to 3. We constraint that the final layer is always a dense layer with output size set to the number of classes.</p><p>Actions consist of (1) add conv layer, (2) add softmax layer, (3) increment or decrement one of the parameters for a convolution layer in the current CNN. For MCTS, random, and Q-learning agents have a terminal action to terminate the episode.</p><p>MCTS with meta-DNN: We implemented MCTS search algorithm followed the procedure with 3.2. c from Eq.1 is set to 200. The design of meta-DNN is consistent with 3.3. The meta-DNN model uses SGD optimizer with 0.9 momentum rate. All parameters in fully connected layers are initialized with the random distribution. The learning rate is set to 0.0001. MCTS without meta-DNN: we also present the results without meta-DNN, the experiment setup is consistent with above but without meta-DNN assisted simulation.</p><p>Random: agent selects action uniformly at random. Q-learning: We implemented a tabular Q-learning agent with -greedy strategy. The learning rate is set to 0.2. We set the discount factor to be 1 in order not to prioritize shortterm rewards. We fix to 0.2. We initialize the Q-value with 0.5.</p><p>Hill Climbing: For a hill climbing, an agent starts from a randomly chosen initial state. It trains every architecture in the child nodes and moves to the child node of which architecture performed the best, and repeat this procedure. Unlike MCTS and Q-learning which trains a NN only when it is a terminal state, hill climbing considers every state (and its child nodes) it visits to train. As such, we do not have a terminal action for hill climbing. As we observed that the hill climbing tends to stick to a local optimum, we restart from a randomly chosen initial state if it visits the same state twice in the trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Experiment Setup for Searching and Training 9.1. Setup for searching networks on NASBench</head><p>Regularized Evolution: We implemented Regularized Evolution <ref type="bibr" target="#b29">[30]</ref> search algorithm for searching the best architecture on NASBench. The population size is set to 500. The tournament size is set to 50. We only mutate the best architecture in tournament set and replace the oldest individual in the population. Once we find the best architecture on NASBench set, we terminate the search process.</p><p>AlphaX: We implemented MCTS search algorithm followed the procedure with 3.2 on NASBench. c from Eq.1 is set to 2. The simulation times k from Eq.2 is set to 10. The design of meta-DNN is consistent with 4.2. The setup for meta-DNN is consistent with 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">Setup for searching networks on CIFAR</head><p>We use 16 NV-1080ti gpus for searching procedure. One of them is the server running the searching program and remaining 15 gpus are clients for training the searched architectures. We use dictionary data structure in Python to save the searched architectures and convert to json file to store them in the disk. All of our training procedures are implemented in MXNET framework. The setup for training CIFAR-10 during the search are as follows: 1) We early terminate the training at the 70th epoch(3 periods of cosine restart learning rate schedule <ref type="bibr" target="#b22">[23]</ref>) due to the limited computing resources; then we rank networks to filter out top ones to perform additional 560 epochs to acquire the final accuracy. 2) Cutout is applied <ref type="bibr" target="#b44">[45]</ref> by using 1 crop of size 16 × 16. 3) Our models use cosine restart learning rate schedule <ref type="bibr" target="#b22">[23]</ref> with 3 periods, the base learning rate is 0.05 and the batch size is 144. 4) We use the momentum optimizer with momentum rate set to 0.9 and L2 weight decay. 5) We also use dropout ratio schedule in the training. The droppath ratio is set to 0.3 and dense dropout ratio is set to 0.2 for searching procedure and applied Schedule-DropPath <ref type="bibr" target="#b44">[45]</ref> for the final training. 6) We use an auxiliary classifier located at 2/3 of depth of the network. The loss of the auxiliary classifier is weighted by 0.4 <ref type="bibr" target="#b37">[38]</ref>. 7) The weights of our models are initialized with Gaussian distribution subjected to 0.01 standard deviation. 8) we randomly crop 32x32 patches from upsampled images of size 40 × 40 and apply random horizontal flips consistent with <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3.">Setup for ImageNet</head><p>The setup for training ImageNet are as follows: 1) We construct the network for ImageNet with searched RCell and N Cell according to <ref type="figure">Fig.6</ref> in <ref type="bibr" target="#b44">[45]</ref>. 2) The input image size is 224 × 224 (the mobile setting across literature).</p><p>3) Our models for ImageNet use polynomial learning rate schedule, starting with 0.05 and decay through 200 epochs. 4) We use the momentum optimizer with momentum rate set to 0.9 and L2 weight decay. 5) Our model uses an auxiliary classifier located at 2/3 of depth of the network. The loss of the auxiliary classifier is weighted by 0.4 <ref type="bibr" target="#b37">[38]</ref>. 6) Dense dropout is applied to the final softmax layer with probability 0.5. 7) We set the batch size as 256. 8) The weights of our models are initialized with Gaussian distribution subjected to 0.01 standard deviation. We use an LSTM model as our predictor which is consistent with <ref type="bibr" target="#b16">[17]</ref>. The hidden state size is set to 100. The embedding size is set to 100 as well. The final LSTM hid- den state goes through a fully-connected layer and sigmoid to regress the validation accuracy. <ref type="figure" target="#fig_0">Fig.16a</ref> shows the architecture of the RNN predictor.</p><p>The setup for training RNN predictor are as follows: 1) There are total of 20 epochs for each training. 2) The base learning late is set to 0.00002 and the batch size is 128. 3) We use the Adam optimizer for training. 4) The embeddings use uniform initialization in range [-0.1, 0.1]. the weights of fully connected layers are initialized with Uniform distribution with initial bias 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2.">Architecture and setup for multilayer perceptron (MLP)</head><p>We implement both multi-stage MLP model and single MLP to predict the network accuracy. Both of multi-stage model and single MLP model share the same architecture. We use 5 fully connected layers with 512, 2048, 2048, 512 and 1 nodes. The final fully-connected layer uses the sigmoid function to regress the validation accuracy. <ref type="figure" target="#fig_0">Fig.16b</ref> shows the architecture of the MLP predictor.</p><p>The setup for training MLP predictor are as follows: 1) There are total of 20 epochs for each training. 2) The base learning late is set to 0.00002 and the batch size is 128. 3) We use the Adam optimizer for training. 4) The weights of our models are initialized with Uniform distribution with initial bias 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Setup for Vision Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1.">Object detection</head><p>We use AlphaX-1 model pre-trained on ImageNet dataset. The training dataset is MSCOCO for object detection <ref type="bibr" target="#b14">[15]</ref> which contains 90 classes of objects. Each image is scaled to 300 × 300 in RGB channels. We trained the model with 200k iterations with 0.04 initial learning rate and the batch size is set to 24. We applied the exponential  <ref type="figure" target="#fig_0">Figure 17</ref>: Cell based architecture of AlphaX model learning rate decay schedule with the 0.95 decay factor. Our model uses momentum optimizer with momentum rate set to 0.9. We also use the L2 weight decay for training. We process each image with random horizontal flip and random crop <ref type="bibr" target="#b21">[22]</ref>. We set the matched threshold to 0.5, which means only the probability of an object over 0.5 is effective to appear on the image. We use 8000 subsets of validation images in MSCOCO validation set and report the mean average precision (mAP) as computed with the standard COCO metric library <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2.">Neural style</head><p>We implement the neural style transfer application by replacing the VGG model to AlplaX-1 model <ref type="bibr" target="#b8">[9]</ref>. AlplaX-1 model is pre-trained on ImageNet dataset. In order to produce a nice result, we set the total 1000 iterations with 0.1 learning rate. We set 10 as the style weight which represents the extent of style reconstruction and 0.025 as the content weight which represents the extent of content reconstruction. We test different kinds of combinations of the outputs of different layers. <ref type="figure" target="#fig_0">Fig.17</ref> shows the structure of AlphaX model, we found that for AlphaX-1 model, the best result can be generated by the concat layer of 13th normal cell as the feature for content reconstruction and the concat layer in first reduction cell as the feature for style reconstruction, the types of layers in each cell are shown in <ref type="figure">Fig.6</ref> and <ref type="figure" target="#fig_0">Fig.15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3.">Image captioning</head><p>The training dataset of image captioning is MSCOCO <ref type="bibr" target="#b14">[15]</ref>, a large-scale dataset for the object detection, segmentation, and captioning. Each image is scaled to 224 × 224 in RGB channels and subtract the channel means as the input to a AlphaX-1 model. For training AlphaX-1 model, We use the SGD optimizer with the 16 batch size and the initial learning rate is 2.0. We applied  the exponential learning rate decay schedule with the 0.5 decay factor in every 8 epochs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparisons of NAS algorithms: (a) random search makes independent decision without using prior rollouts (previous search trajectories</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview of AlphaX search procedures: explanations of four steps are in sec.3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Design space: (a) the cell structure of NASNet and (b) the DAG structure of NASBench-101. Then the network is constructed by stacking multiple Cells or DAGs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Encoding scheme of NASBench and NASNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>k i=1..k P red(sim i (s )) based only on predicted accuracies from the Meta-DNN at iteration n. Q(s, a) ← Q(s, a) +q, N (s, a) ← N (s, a) + 1, s ← parent(s), a ← π tree (s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>the RCell and N Cell that yield the highest accuracy in the search. Finding the global optimum on NASBench-101: AlphaX is 3x, 2.8x faster than Random Search and Regularized Evolution on NASBench-101 (nodes ≤ 6). The results are from 200 trails with different random seeds. (s ← rollout − f rom(arch z )) to replace the backpropagatedq with q: Q(s, a) ← Q(s, a) + q −q, s ← parent(s), a ← π tree (s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Accuracy distribution of networks in NASBench.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>AlphaX search visualization:each nodes represents a MCTS state; the node color reflects its value, i.e. accuracy, indicating how promising a search branch. Meta-DNN accelerates the search: the performance of AlphaX in cases of with/without meta-DNN on NASBench-101. The data is collected from 40 trails.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>meta-DNN design ablations: True v.s. predicted accuracies of MLP, RNN and multi-stage MLP on architectures from NASBench. The scatter density is highlighted by color to reflect the data distribution; Red means high density, and blue otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Validation of transfer learning: transferring weights significantly reduces the number of epochs in reaching the same accuracy of random initializations (Transfer 17 → 70 epochs v.s. random initialization), but insufficient epochs loses accuracy (Transfer, 9 epochs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 : 1 Figure 14 :</head><label>13114</label><figDesc>Algorithmic comparisions: AlphaX is consistently the fastest algorithm to reach the global optimal on another simplified search domain (appexdix.8), while Hill Climbing can easily trap into a local optimal.(a) content (b) style (c) VGG (d) AlphaX-Neural Style Transfer: AlphaX-1 v.s. VGG. ther corroborates the effectiveness of meta-DNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>the RCell and N Cell of AlphaX-2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>10 .</head><label>10</label><figDesc>MetaDNN Architecture and Setup 10.1. Architecture and setup for recurrent neural network (RNN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 :</head><label>16</label><figDesc>Architecture of metaDNN: (a) the architecture of recurrent neural network style metaDNN (b) the architecture of multilayer perceptron style metaDNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 18 :Figure 19 :</head><label>1819</label><figDesc>Object Detection: the object detection system is more precise with AlphaX-1 than MobileNet.(a) content (b) style (c) VGG synthesized picture (d) AlphaX-1 synthesized picture Neural Style Transfer: AlphaX-1 is better than VGG in capturing the style with sophisticated details and textures. (a) VGG: a tennis player is playing tennis on the court. AlphaX-1: a couple of people playing tennis on a tennis court. (b) VGG: a bus is parked on the side of the road. AlphaX-1: a blue bus is driving down a street. (c) VGG: a cup of coffee and a cup of coffee. AlphaX-1: a book and a cup of coffee on a table. (d) VGG: a fire hydrant on the side of a road. AlphaX-1: a red fire hydrant on the side of a road.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 20 :</head><label>20</label><figDesc>Image Captioning: AlphaX-1 captures more details than VGG in the captions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Highlights of NAS search algorithms: compared with DeepArchitect and Wistuba, AlphaX features a scalable online model, and is ready for distributed system. on its online model, MCTS dynamically adapts itself to the most promising search regions, where good consequences are likely to happen. Inspired by this idea, we present AlphaX that uses MCTS for efficient model architecture search with Meta-DNN as a predictive model to estimate the accuracy of a sampled architecture. Compared with Random Search,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The comparisons of our NASNet search results to other state-of-the-art results on CIFAR-10. M is the number of sampled architectures in the search. The cell structure of AlphaX is in Fig. 6.</figDesc><table><row><cell>model</cell><cell cols="4">multi-adds params latency top1/top5 err</cell></row><row><cell>NASNet-A [45]</cell><cell>564M</cell><cell>5.3M</cell><cell cols="2">39.68ms 26.0/8.4</cell></row><row><cell cols="2">AmoebaNet-B [30] 555M</cell><cell>5.3M</cell><cell cols="2">32.15ms 26.0/8.5</cell></row><row><cell>DARTS [21]</cell><cell>574M</cell><cell>4.7M</cell><cell cols="2">39.32ms 26.7/8.7</cell></row><row><cell>RENASNet [6]</cell><cell>574M</cell><cell>4.7M</cell><cell cols="2">41.21ms 24.3/7.4</cell></row><row><cell>PNAS [17]</cell><cell>588M</cell><cell>5.1M</cell><cell cols="2">40.34ms 25.8/8.1</cell></row><row><cell>Proxyless-G [5]</cell><cell>-</cell><cell>-</cell><cell>83ms</cell><cell>25.4/7.8</cell></row><row><cell>AlphaX-1</cell><cell>579M</cell><cell>5.4M</cell><cell cols="2">38.56ms 24.5/7.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">The error rate (%) comparisons of our best-</cell></row><row><cell cols="3">performing architecture to other state-of-the-art results on</cell></row><row><cell cols="3">ImageNet. The network setup follows the mobile setting</cell></row><row><cell>defined in [45].</cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell>0.4 accuracies in NASBench 0.6</cell><cell>0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The code of different types of layers</figDesc><table><row><cell>layers</cell><cell cols="2">code layer</cell><cell cols="2">code layer</cell><cell>code layer</cell><cell>code</cell></row><row><cell cols="2">3x3 avg pool 1</cell><cell cols="2">3x3 max pool 4</cell><cell cols="2">3x3 conv 7</cell><cell>3x3 depth-separable conv 10</cell></row><row><cell cols="2">5x5 avg pool 2</cell><cell cols="2">5x5 max pool 5</cell><cell cols="2">5x5 conv 8</cell><cell>5x5 depth-separable conv 11</cell></row><row><cell cols="2">7x7 avg pool 3</cell><cell cols="2">7x7 max pool 6</cell><cell>identity</cell><cell>9</cell><cell>7x7 depth-separable conv 12</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1 Client 1: Require: Start working once building connection to the server <ref type="bibr">2:</ref> while True do <ref type="bibr">3:</ref> if The client is connected to server then <ref type="bibr">4:</ref> network ← Receive() <ref type="bibr">5:</ref> accuracy ← Train(network) <ref type="bibr">6:</ref> Send (network, accuracy) to the Server 7: while no idle client do 3:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continue</head><p>Wait for dispatching jobs until there are idle clients Send network to a Client <ref type="bibr">8:</ref> if Received − Signal() then <ref type="bibr">9:</ref> network, accuracy ← Receive − Result() <ref type="bibr">10:</ref> acc(network) ← accuracy <ref type="bibr">11:</ref> state ← rollout − f rom(network) <ref type="bibr">12:</ref> Backpropagation(state, (accuracy−q(state))/2, 0) Server() <ref type="bibr">27:</ref> cur − state ← root − node <ref type="bibr">28:</ref> i ← 0 <ref type="bibr">29:</ref> while i &lt; M AX − tree − depth do <ref type="bibr">30:</ref> i ← i + 1 <ref type="bibr">31:</ref> next − action ← Selection(cur − state) Select an action based on Eq. 1 <ref type="bibr">32:</ref> if next − state not in tree then <ref type="bibr">33:</ref> next − state ← Expansion(next − action) <ref type="bibr">34:</ref> T t ← Simulation t (next − state) for t = 0...k k is the number of simulations we run using the Meta-DNN <ref type="bibr">35:</ref> T ASK − QU EU E.push(T 0 ) <ref type="bibr" target="#b35">36</ref>:  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Alphax for searching nasbench-101</title>
		<ptr target="https://drive.google.com/open?id=" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Designing Neural Network Architectures using Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of monte carlo tree search methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rohlfshagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tavener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in games</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Renas: Reinforced evolutionary neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4787" to="4796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Simple and efficient architecture search for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04528</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolution by evolution: Differentiable pattern producing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno>abs/1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lstm: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequential model-based optimization for general algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning and Intelligent Optimization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">High dimensional bayesian optimisation and bandits via additive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bandit based monte-carlo planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00559</idno>
		<title level="m">Progressive neural architecture search</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Progressive Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hierarchical Representations for Efficient Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00436</idno>
		<title level="m">Hierarchical representations for efficient architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7816" to="7827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Francon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shahrzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Navruzyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00548</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Evolving deep neural networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Designing neural networks using genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Hegde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICGA</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="379" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving policy gradient by exploring under-appreciated rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09321</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08792</idno>
		<title level="m">Deeparchitect: Automatically designing and training deep architectures</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Large-Scale Evolution of Image Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Compositional pattern producing networks: A novel abstraction of development. Genetic programming and evolvable machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="131" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="99" to="127" />
		</imprint>
	</monogr>
	<note>Evolutionary computation</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A Genetic Programming Approach to Designing Convolutional Neural Network Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno>abs/1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Auto-weka: Combined selection and hyperparameter optimization of classification algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="847" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07420</idno>
		<title level="m">Finding competitive network architectures within a day using uct</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Genetic cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01513</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09635</idno>
		<title level="m">Nas-bench-101: Towards reproducible neural architecture search</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Neural Architecture Search with Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
