<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Extreme Inception Network: Towards a Robust CNN Model for Edge Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Soria</surname></persName>
							<email>xsoria@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Universitat Autonoma de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Riba</surname></persName>
							<email>eriba@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Universitat Autonoma de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Sappa</surname></persName>
							<email>asappa@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Universitat Autonoma de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Escuela Superior Politécnica del Litoral</orgName>
								<address>
									<settlement>Guayaquil</settlement>
									<country key="EC">Ecuador</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dense Extreme Inception Network: Towards a Robust CNN Model for Edge Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a Deep Learning based edge detector, which is inspired on both HED (Holistically-Nested Edge Detection) and Xception networks. The proposed approach generates thin edge-maps that are plausible for human eyes; it can be used in any edge detection task without previous training or fine tuning process. As a second contribution, a large dataset with carefully annotated edges, has been generated. This dataset has been used for training the proposed approach as well the state-of-the-art algorithms for comparisons. Quantitative and qualitative evaluations have been performed on different benchmarks showing improvements with the proposed method when F-measure of ODS and OIS are considered.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Edge detection is a recurrent task required for several classical computer vision processes (e.g., segmentation <ref type="bibr" target="#b38">[39]</ref>, image recognition <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30]</ref>), or even in the modern tasks such as image-to-image translation <ref type="bibr" target="#b40">[41]</ref>, photo sketching <ref type="bibr" target="#b17">[18]</ref> and so on. Moreover, in fields such as medical image analysis <ref type="bibr" target="#b26">[27]</ref> or remote sensing <ref type="bibr" target="#b15">[16]</ref> most of their heart activities require edge detectors. In spite of the large amount of work on edge detection, it still remains as an open problem with space for new contributions.</p><p>Since the Sobel operator <ref type="bibr" target="#b32">[33]</ref>, many edge detectors have been proposed <ref type="bibr" target="#b24">[25]</ref> and most of the techniques like Canny <ref type="bibr" target="#b4">[5]</ref> are still being used nowadays. Recently, in the era of Deep Learning (DL), Convolutional Neural Netwoks (CNN) based edge detectors like DeepEdge <ref type="bibr" target="#b3">[4]</ref>, HED <ref type="bibr" target="#b35">[36]</ref>, RCF <ref type="bibr" target="#b19">[20]</ref>, BDCN <ref type="bibr" target="#b13">[14]</ref> among others, have been proposed. These models are capable of predicting an edge-map from a given image just like the low level based methods <ref type="bibr" target="#b41">[42]</ref>, with better performance. The success of these methods is mainly by the CCNs applied at different scales to a large set of images together with the training regularization techniques. Most of the aforementioned DL based approaches are trained on already existing boundary detection or object segmentation datasets <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24]</ref> to detect edges. Even though most of the images on those datasets are well annotated, there are a few of them that contain missing edges, which difficult the training, thus the predicted edge-maps lost some edges in the images (see <ref type="figure" target="#fig_0">Fig. 1</ref>). In the current work, those datasets are used just for qualitative comparisons due to the objective of the current work is edge detection (not objects' boundary/contour detection). The boundary/contour detection tasks, although related and some times assumed as a synonym task, are different since just objects' boundary/contour need to be detected, but not all edges present in the given image.</p><p>This manuscript aims to demonstrate the edge detection generalization from a DL model. In other words, the model is capable of being evaluated in other datasets for edge detection without being trained on those sets. To the best of our knowledge, the unique dataset for edge detection shared to the community is Multicue Dataset for Boundary Detection (MDBD-2016) <ref type="bibr" target="#b22">[23]</ref>, which although mainly generated for the boundary detection study, it contains a subset of Image (BSDS500) GT RCF HED CED BDCN DexiNed (Ours) <ref type="figure">Figure 2</ref>. Edge-maps predicted from the state-of-the-art models and DexiNed on three BSDS500 <ref type="bibr" target="#b2">[3]</ref> images. Note that DexiNed was just trained with BIPED, while all the others were trained on BSDS500.</p><p>images devoted for edge detection. Therefore, a new dataset has been collected to train the proposed edge detector. The main contributions in the paper are summarized as follow:</p><p>• A dataset with carefully annotated edges has been generated and released to the community-BIPED: Barcelona Images for Perceptual Edge Detection. <ref type="bibr" target="#b0">1</ref> • A robust CNN architecture for edge detection is proposed, referred to as DexiNed: Dense Extreme Inception Network for Edge Detection. The model has been trained from the scratch, without pretrained weights.</p><p>The rest of the paper is organized as follow. Section 2 summarizes the most relevant and recent work on edge detection. Then, the proposed approach is described in Section 3. The experimental setup is presented in Section 4. Experimental results are then summarized in Section 5; finally, conclusions and future work are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are a large number of work on the edge detection literature, for a detailed review see <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b10">11]</ref>. According to the technique the given image is processed, proposed approaches can be categorized as: i) Low level feature; ii) Brain-biologically inspiration; iii) Classical learning algorithms; iv) Deep learning algorithms.</p><p>Low-level feature: Most of the algorithms in this category generally follow a smooth process, which could be performed convolving the image with a Gaussian filter or manually performed kernels. A sample of such methods are <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26]</ref>. Since Canny <ref type="bibr" target="#b4">[5]</ref>, most of the nowadays methods use non-maximum suppression <ref type="bibr" target="#b5">[6]</ref> as the last process of edge detection.</p><p>Brain-biologically inspiration: This kind of method started their research in the 60s of the last century analyzing the edge and contour formation in the vision systems <ref type="bibr" target="#b0">1</ref> Code + dataset: https://github.com/xavysp/DexiNed of monkeys and cats <ref type="bibr" target="#b7">[8]</ref>. inspired on such a work, in <ref type="bibr" target="#b11">[12]</ref> the authors proposed a method based on simple cells and Gabor filters. Another study focused on boundary detection is presented in <ref type="bibr" target="#b22">[23]</ref>. This work proposes to use Gabor and derivative of Gaussian filters, considering three different filter sizes and machine learning classifiers. More recently, in <ref type="bibr" target="#b36">[37]</ref>, an orientation selective neuron is presented, by using first derivative of a Gaussian function. This work has been recently extended in <ref type="bibr" target="#b1">[2]</ref> by modeling retina, simple cells even the cells from V2.</p><p>Classical learning algorithms: These techniques are usually based on sparse representation learning <ref type="bibr" target="#b20">[21]</ref>, dictionary learning <ref type="bibr" target="#b34">[35]</ref>, gPb (gradient descent) <ref type="bibr" target="#b2">[3]</ref> and structured forest <ref type="bibr" target="#b8">[9]</ref> (decision trees). At the time these approaches have been proposed, they outperformed state-ofthe-art techniques based on low level processes reaching the best F-measure values in BSDS segmentation dataset <ref type="bibr" target="#b2">[3]</ref>. Although obtained results were acceptable in most of the cases, these techniques still have limitations in challenging scenarios.</p><p>Deep learning algorithms: With the success of CNN, principally because of its result in <ref type="bibr" target="#b16">[17]</ref>, many methods have been proposed <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>. In HED <ref type="bibr" target="#b35">[36]</ref> for example, an architecture based on VGG16 <ref type="bibr" target="#b31">[32]</ref> and pre-trained with ImageNet dataset is proposed. The network generates edges from each convolutional block constructing a multi-scale learning architecture. The training process uses a modified cross entropy loss function for each predicted edge-maps. Using the same architecture as their backbone, <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b33">[34]</ref> have proposed improvements. While in <ref type="bibr" target="#b19">[20]</ref> every output is feed from each convolution from every block, in <ref type="bibr" target="#b33">[34]</ref> a set of fusion backward process, with the data of each outputs, is performed. In general, most of the current DL based models use as their backbone the convolutional blocks of VGG16 architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dense Extreme Inception Network for Edge Detection</head><p>This section presents the architecture proposed for edge detection, termed DexiNed, which consists of a stack of learned filters that receive as input an image then predict an edge-map with the same resolution. DexiNed can be seen as two sub networks (see Figs. 3 and 4): Dense extreme inception network (Dexi) and the up-sampling block (UB). While Dexi is fed with the RGB image, UB is fed with feature maps from each block of Dexi. The resulting network (Dex-iNed) generates thin edge-maps, avoiding missed edges in the deep layers. Note that even though without pre-trained data, the edges predicted from DexiNed are in most of the cases better than state-of-the-art results, see <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DexiNed Architecture</head><p>The architecture is depicted in <ref type="figure" target="#fig_1">Fig. 3</ref>, it consists of an encoder with 6 main blocks inspired in the xception network <ref type="bibr" target="#b6">[7]</ref>. The network outputs feature maps at each of the main blocks to produce intermediate edge-maps using an upsampling block defined in Section 3.2. All the edge-maps resulting from the upsampling blocks are concatenated to feed the stack of learned filters at the very end of the network and produce a fused edge-map. All six upsampling blocks do not share weights.</p><p>The blocks in blue consists of a stack of two convolutional layers with kernel size 3 × 3, followed by batch normalization and ReLU as the activation function (just the last convs in the last sub-blocks does not have such activation). The max-pool is set by 3 × 3 kernel and stride 2. As the architecture follows the multi-scale learning, like in HED, an upsampling process (horizontal blocks in gray, <ref type="figure" target="#fig_1">Fig. 3</ref>) is followed (see details in Section 3.2).</p><p>Even though DexiNed is inspired in xception, the similarity is just in the structure of the main blocks and connections. Major differences are detailed below:</p><p>• While in xception separable convolutions are used, DexiNed uses standard convolutions.</p><p>• As the output is a 2D edge-map, there is "not exit flow", instead, another block at the end of block five has been added. This block has 256 filters and as in block 5 there is not maxpooling operator.</p><p>• In block 4 and block 5, instead of 728 filters, 512 filters have been set. The separations of the main blocks are done with the blocks connections (rectangles in green) drawn on the top side of <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>• Concerning to skip connections, in xception there is one kind of connection, while in DexiNed there are two type of connections, see rectangles in green on the top and bottom of <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>Since many convolutions are performed, every deep block losses important edge features and just one mainconnection is not sufficient, as highlighted in DeepEdge <ref type="bibr" target="#b3">[4]</ref>, from the forth convolutional layer the edge feature loss is more chaotic. Therefore, since block 3, the output of each sub-block is averaged with edge-connection (orange squares in <ref type="figure" target="#fig_1">Fig. 3</ref>). These processes are inspired in ResNet <ref type="figure">Figure 5</ref>. Edge-maps from DexiNed in BIPED test dataset. The six outputs are delivered from the upsampling blocks, the fused is the concatenation and fusion of those outputs and the averaged is the average of all previous predictions.</p><p>[ <ref type="bibr" target="#b14">15]</ref> and RDN <ref type="bibr" target="#b39">[40]</ref> with the following notes: i) as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, after the max-pooling operation and before summation with the main-connection, the edge-connection is set to average each sub-blocks output (see rectangles in green, bottom side); ii) from the max-pool, block 2, edgeconnections feed sub-blocks in block 3, 4 and 5, however, the sub-blocks in 6 are feed just from block 5 output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Upsampling Block</head><p>DexiNed has been designed to produce thin edges in order to enhance the visualization of predicted edge-maps. One of the key component of DexiNed for the edge thinning is the upsampling block, as appreciated in <ref type="figure" target="#fig_1">Fig. 3</ref>, each output from the Dexi blocks feeds the UB. The UB consists of the conditional stacked sub-blocks. Each sub-block has 2 layers, one convolutional and the other deconvolutional; there are two types of sub-blocks. The first subblock (sub-block1) is feed from Dexi or sub-block2; it is only used when the scale difference between the feature map and the ground truth is equal to 2. The other sub-block (sub-block2), is considered when the difference is greater than 2. This sub-block is iterated till the feature map scale reaches 2 with respect to the GT. The sub-block1 is set as follow: kernel size of the conv layer 1 × 1; followed by a ReLU activation function; kernel size of the deconv layer or transpose convolution s × s, where s is the input feature map scale level; both layers return one filter and the last one gives a feature map with the same size as the GT. The last conv layer does not have activation function. The sub-block2 is set similar to sub-block1 with just one difference in the number of filters, which is 16 instead of 1 in sub-block1. For example, the output feature maps from block 6 in Dexi has the scale of 16, there will be three iterations in the sub-block2 before fed the sub-block1. The upsampling process of the second layer from the sub-blocks can be performed by bi-linear interpolation, sub-pixel convolution and transpose convolution, see Sec. 5 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Functions</head><p>DexiNed could be summarized as a regression function ð, that is,Ŷ = ð(X, Y ), where X is an input image, Y is its respective ground truth, andŶ is a set of predicted edge maps.Ŷ = [ŷ 1 ,ŷ 2 , ...,ŷ N ], whereŷ i has the same size as Y , and N is the number of outputs from each upsampling block (horizontal rectangles in gray, <ref type="figure" target="#fig_1">Fig. 3</ref>);ŷ N is the result from the last fusion layer f (ŷ N =ŷ f ). Then, as the model is deep supervised, it uses the same loss as <ref type="bibr" target="#b35">[36]</ref> (weighted cross-entropy), which is tackled as follow:</p><formula xml:id="formula_0">n (W, w n ) = −β j∈Y + log σ(y j = 1|X; W, w n ) − (1 − β) j∈Y − log σ(y j = 0|X; W, w n ),<label>(1)</label></formula><p>then,</p><formula xml:id="formula_1">L(W, w) = N n=1 δ n × n (W, w n ),<label>(2)</label></formula><p>where W is the collection of all network parameters and w is the n corresponding parameter, δ is a weight for each scale level. </p><formula xml:id="formula_2">β = |Y − |/|Y + +Y − | and (1−β)=|Y + |/|Y + + Y − | (|Y − |, |Y + |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>This section presents details on the datasets used for evaluating the proposed model, in particular the dataset and annotations (BIPED) generated for an accurate training of the proposed DexiNed. Additionally, details on the evaluation metrics and network's parameters are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Barcelona Images for Perceptual Edge Detection (BIPED)</head><p>The other contributions of the paper is a carefully annotated edge dataset. It contains 250 outdoor images of 1280×720 pixels each. These images have been carefully annotated by experts on the computer vision field, hence no redundancy has been considered. In spite of that, all results have been cross-checked in order to correct possible mistakes or wrong edges. This dataset is publicly available as a benchmark for evaluating edge detection algorithms. The generation of this dataset is motivated by the lack of edge detection datasets, actually, there is just one dataset publicly available for the edge detection task (MDBD <ref type="bibr" target="#b22">[23]</ref>). Edges in MDBM dataset have been generated by different subjects, but have not been validated, hence, in some cases, the edges correspond to wrong annotations. Some examples of these missed or wrong edges can be appreciated in the ground truths presented in <ref type="figure">Fig. 8</ref>; hence, edge detector algorithms that obtain these missed edges are penalized during the evaluation. The level of details of the dataset annotated in the current work can be appreciated looking at the GT, see Figs. 5 and 7. In order to do a fair comparison between the different state-of-the-art approaches proposed in the literature, BIPED dataset has been used for training those approaches, which have been later on evaluated in ODS, OIS, and AP. From the BIPED dataset, 50 images have been randomly selected for testing and the remainders 200 for training and validation. In order to increase the number of training images a data augmentation process has been performed as follow: i) as BIPED data are in high resolution they are split up in the half of image width size; ii) similarly to HED, each of the resulting images is rotated by 15 different angles and crop by the inner oriented rectangle; iii) the images are horizontally flip; and finally iv) two gamma corrections have been applied (0.3030, 0.6060). This augmentation process resulted in 288 images per each 200 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Test Datasets</head><p>The datasets used to evaluate the performance of DexiNed are summarized bellow. There is just one dataset intended for edged detection MDBD <ref type="bibr" target="#b22">[23]</ref>, while the remainders are for objects' contour/boundary extraction/segmentation: CID <ref type="bibr" target="#b11">[12]</ref>, BSDS <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3]</ref>, NYUD <ref type="bibr" target="#b30">[31]</ref> and PASCAL <ref type="bibr" target="#b23">[24]</ref>.</p><p>MDBD: The Multicue Dataset for Boundary Detection has been intended for the purpose of psychophysical studies on object boundary detection in natural scenes, from the early vision system. The dataset is composed of short binocular video sequences of natural scenes <ref type="bibr" target="#b22">[23]</ref>, containing 100 scenes in high definition (1280 × 720). Each scene has 5 boundary annotations and 6 edge annotations. From the given dataset 80 images are used for training and the re-mainders 20 for testing <ref type="bibr" target="#b22">[23]</ref>. In the current work, DexiNed has been evaluated using the first 20 images (the sub set for edge detection).</p><p>CID: This dataset has been presented in <ref type="bibr" target="#b11">[12]</ref>, a brainbiologically inspired edge detector technique. The main limitation of this dataset is that it just contains a set of 40 images with their respective ground truth edges. This dataset highlight that in addition to the edges the ground truth map contains contours of object. In this case the DexiNed has been evaluated with the whole CID data.</p><p>BSDS: Berkeley Segmentation Dataset, consists of 200 new test images <ref type="bibr" target="#b2">[3]</ref> additional to the 300 images contained in BSDS300 <ref type="bibr" target="#b21">[22]</ref>. In previous publications, the BSDS300 is split up into 200 images for training and 100 images for testing. Currently, the 300 images from BSDS300 are used for training and validation, while the remainders 200 images are used for testing. Every image in BSDS is annotated at least by 6 annotators; this dataset is mainly intended for image segmentation and boundary detection. In the current work both datasets are evaluated BSDS500 (200 test images) and BSDS300 (100 test images).</p><p>NYUD: New York University Dataset is a set of 1449 RGBD images that contains 464 indoor scenarios, intended for segmentation purposes. This dataset is split up by <ref type="bibr" target="#b12">[13]</ref> into three subsets-i.e., training, validation and testing sets. The testing set contains 654 images, while the remainders images are used for training and validation purposes. In the current work, although the proposed model was not trained with this dataset, the testing set has been selected for evaluating the proposed DexiNed.</p><p>PASCAL: The Pascal-Context <ref type="bibr" target="#b23">[24]</ref> is a popular dataset in segmentation; currently most of major DL methods for edge detection use this dataset for training and testing, both for edge and boundary detection purposes. This dataset contains 11530 annotated images, about 5% of them (505 images) have been considered for testing DexiNed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>The evaluation of an edge detector has been well defined since the pioneer work presented in <ref type="bibr" target="#b41">[42]</ref>. Since BIPED has annotated edge-maps as GT, three evaluation metrics widely used in the community have been considered: fixed contour threshold (ODS), per-image best threshold (OIS), and average precision (AP). The F-measure (F) <ref type="bibr" target="#b2">[3]</ref> of ODS and OIS, will be considered, where F = 2×P recision×Recall P recision+Recall .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation Notes</head><p>The implementation is performed in TensorFlow <ref type="bibr" target="#b0">[1]</ref>. The model converges after 150k iterations with a batch size of 8 using Adam optimizer and learning rate of 10 −4 . The training process takes around 2 days in a TITAN X GPU with color images of size 400x400 as input. The weights for fusion layer are initialized as: N ). After a hyperparameter search to reduce the number of parameters, best performance was obtained using kernel sizes of 3 × 3, 1 × 1 and s × s on the different convolutional layers of Dixe and UB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>This section presents quantitative and qualitative evaluations conducted by the metrics presented in Sec. 4. Since the proposed DL architecture demands several experiments to be validated, DexiNed has been carefully tuned till reach its final version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Results</head><p>Firstly, in order to select the upsampling process that achieves the best result, an empiric evaluation has been performed, see <ref type="figure" target="#fig_4">Fig. 6(a)</ref>. The evaluation consists in conducting the same experiments by using the three upsampling methods; DexiNed-bdc refers to upsampling performed by a transpose convolution initialized with a bi-linear kernel; DexiNed-dc uses transpose convolution with trainable kernels; and DexiNed-sp uses subpixel convolution. According to F-measure, the three versions of DexiNed get the similar results, however, when analyzing the curves in <ref type="figure" target="#fig_4">Fig.  6</ref>(a), a small difference in the performance of DexiNed-dc appears. As a conclusion, the DexiNed-dc upsampling strategy is selected; from now on, all the evaluations performed on this section are obtained using a DexiNed-dc upsampling; for simplicity of notation just the term DexiNed is used instead of DexiNed-dc. <ref type="figure" target="#fig_4">Figure 6</ref>(b) and <ref type="table">Table 1</ref>(a) present the quantitative results reached from each DexiNed edge-map prediction. The results from the eight predicted edge-maps are depicted, the best quantitative results, corresponding to the fused (DexiNed-f) and averaged (DexiNed-a) edge-maps are selected for the comparisons. Similarly to <ref type="bibr" target="#b35">[36]</ref> the averaged of all predictions (DexiNed-a) gets the best results in the three evaluation metrics, followed by the prediction generated in the fusion layer. Note that the edge-maps predicted from the block 2 till the 6 get similar results to DexiNed-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Methods ODS OIS AP Edge detection dataset MDBD <ref type="bibr" target="#b22">[23]</ref> HED <ref type="bibr" target="#b35">[36]</ref> .851 .864 .890 RCF <ref type="bibr" target="#b19">[20]</ref> .857 .862 -DexiNed-f .837 .837 .751 DexiNed-a .859 .864 .917 Contour/boundary detection/segmentation datasets CID <ref type="bibr" target="#b11">[12]</ref> SCO <ref type="bibr" target="#b36">[37]</ref> .58 .64 .61 SED <ref type="bibr" target="#b1">[2]</ref> .65 .69</p><p>.68 DexiNed-f .65 .67 .59 DexiNed-a .65 .69 .71 BSDS300 <ref type="bibr" target="#b21">[22]</ref> gPb <ref type="bibr" target="#b2">[3]</ref> .700 .720 .660 SED <ref type="bibr" target="#b1">[2]</ref> .69 .71</p><p>.71 DexiNed-f .707 .723 .52 DexiNed-a .709 .726 .738 BSDS500 <ref type="bibr" target="#b2">[3]</ref> HED <ref type="bibr" target="#b35">[36]</ref> .790 .808 .811 RCF <ref type="bibr" target="#b19">[20]</ref> .806 .823 -CED <ref type="bibr" target="#b33">[34]</ref> .803 .820 .871 SED <ref type="bibr" target="#b1">[2]</ref> .710 .740 .740 DexiNed-f .729 .745 .583 DexiNed-a .728 .745 .689 NYUD <ref type="bibr" target="#b30">[31]</ref> gPb <ref type="bibr" target="#b2">[3]</ref> .632 .661 .562 HED <ref type="bibr" target="#b35">[36]</ref> .720 .761 .786 RCF <ref type="bibr" target="#b19">[20]</ref> .743 .757 -DexiNed-f .658 .674 .556 DexiNed-a .602 .615 .490 PASCAL <ref type="bibr" target="#b23">[24]</ref> CED <ref type="bibr" target="#b33">[34]</ref> .726 .750 .778 HED <ref type="bibr" target="#b35">[36]</ref> .584 .592 .443 DexiNed-f .431 .458 .274 DexiNed-a .475 .497 .329 <ref type="table">Table 2</ref>. Quantitative results of DexiNed trained on BIPED and the state-o-the-art methods trained with the corresponding datasets (values from other approaches come from the corresponding publications). f, this is due to the fact of the proposed skip-connections. For a qualitative illustration, <ref type="figure">Fig. 5</ref> presents all edge-maps predicted from the proposed architecture. Qualitatively, the result from DexiNed-f is considerably better than the one from DexiNed-a (see illustration in <ref type="figure">Fig. 5</ref>). However, according to <ref type="table">Table 1</ref>(a), DexiNed-a produces slightly better quantitative results than DexiNed-f. As a conclusion both approaches (fused and averaged) reach similar results; through this manuscript whenever the term DexiNed is used it corresponds to DexiNed-f. <ref type="table">Table 1</ref>(b) presents a comparison between the DexiNed and the state-of-the-art techniques on edge and boundary detection. In all the cases BIPED dataset has been considered, both for training and evaluating the DL based models (i.e., HED <ref type="bibr" target="#b28">[29]</ref>, RCF <ref type="bibr" target="#b19">[20]</ref>, CED <ref type="bibr" target="#b33">[34]</ref>) and BDCN <ref type="bibr" target="#b13">[14]</ref>, the training process for each model took about two days. As can Image GT CED <ref type="bibr" target="#b33">[34]</ref> HED <ref type="bibr" target="#b35">[36]</ref> RCF <ref type="bibr" target="#b19">[20]</ref> BDCN <ref type="bibr" target="#b13">[14]</ref> DexiNed be appreciated from <ref type="table">Table 1</ref>(b), DexiNed-a reaches the best results in all evaluation metrics. Actually both, DexiNed-a and DexiNed-f obtain the best results in almost all evaluation metrics. The F-measure obtained by comparing these approaches is presented in <ref type="figure" target="#fig_4">Fig. 6(c)</ref>; it can be appreciated how for Recall above 75% DexiNed gets the best results. Illustrations of the edges obtained with DexiNed and the state-of-the-art techniques are depicted in <ref type="figure" target="#fig_5">Figure 7</ref>, just for four images from the BIPED dataset. As it can be appreciated, although RCF and BDCN obtain similar quantitative results than DexiNed, which were the second best ranked algorithms in <ref type="table">Table 1</ref>(b), DexiNed predicts qualitative better results. Note that the proposed approach was trained from scratch without pre-trained weights.</p><p>The main objective of DexiNed is to get a precise edgemap from every dataset (RGB or Grayscale). Therefore, all the datasets presented in Sec. 4.2 have been considered, split up into two categories for a fair analysis; one for edge detection and the others for contour/boundary detection/segmentation. Results of edge-maps obtained with state-of-the-art methods are presented in <ref type="table">Table 2</ref>. It should be noted that for each dataset the methods compared with DexiNed have been trained using images from that dataset, while DexiNed is trained just once with BIPED. It can be appreciated that DexiNed obtains the best performance in the MDBD dataset. It should be noted that DexiNed is evaluated in CID and BSDS300, even though these datasets contain a few images, which are not enough for training other approaches (e.g., HED, RCF, CED). Regarding BSDS500, NYUD and PASCAL, DexiNed does not reach the best results since these datasets have not been intended for edge detection, hence the evaluation metrics penalize edges detected by DexiNed. To highlight this situation, <ref type="figure">Fig. 8</ref> depicts results from <ref type="table">Table 2</ref>. Two samples from each dataset are considered. They are selected according to the best and worst F measure. Therefore, as shown in <ref type="figure">Fig. 8</ref>, when the image is fully annotated the score reaches around 100%, otherwise it reaches less than 50%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>GT DexiNed Image GT DexiNed <ref type="figure">Figure 8</ref>. Results from the proposed approach using different datasets (note that DexiNed has been trained just with BIPED).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative Results</head><p>As highlighted in previous section, when the deep learning based edge detection approaches are evaluated in datasets intended for objects' boundary detection or objects segmentation, the results will be penalized. To support this claim, we present in <ref type="figure">Fig. 8</ref> two predictions (the best and the worst results according to F-measure) from all datasets used for evaluating the proposed approach (except BIPED that has been used for training). The F-measure obtained in the three most used datasets (i.e., BSDS500, BSDS300 and NYUD) reaches over 80% in those cases where images are fully annotated; otherwise, the F-measure reaches about 30%. However, when the edge dataset (MDBD <ref type="bibr" target="#b22">[23]</ref>) is considered the worst F-measure reaches over 75%. As a conclusion, it should be stated that edge detection and contour/boundary detection are different problems that need to be tackled separately when a DL based model is considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>A deep structured model (DexiNed) for image's edge detection is proposed. Up to our knowledge, it is the first DL based approach able to generate thin edge-maps. A large experimental results and comparisons with state-of-the-art approaches is provided showing the validity of DexiNed. Even though DexiNed is trained just one time (with BIPED) it outperforms the state-of-the-art approaches when evalu-ated in other edge oriented datasets. A carefully annotated dataset for edge detection has been generated and is shared to the community. Future work will be focused on tackling the contour and boundary detection problems by using the proposed architecture and approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The edge-maps predictions from the proposed model in images acquired from internet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FusedFigure 3 .</head><label>3</label><figDesc>Proposed architecture: Dense Extreme Inception Network, consists of an encoder composed by six main blocks (showed in light gray). The main blocks are connected between them through 1x1 convolutional blocks. Each of the main blocks is composed by sub-blocks that are densely interconnected by the output of the previous main block. The output from each of the main blocks is fed to an upsampling block that produces an intermediate edge-map in order to build a Scale Space Volume, which is used to compose a final fused edge-map. More details are given in Sec. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Detail of the upsampling block that receives as input the learned features extracted from each of the main blocks. The features are fed into a stack of learned convolutional and transposed convolutional filters in order to extract an intermediate edge-map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>denote the edge and non-edge in the ground truth). See Section 4.4 for hyper-parameters and optimizer details for the regularization in the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Precision/recall curves on BIPED dataset. (a) DexiNed upsampling versions. (b) The outputs of DexiNed in testing stage, the 8 outputs are considered. (c) DexiNed comparison with other DL based edge detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Results from different edge detection algorithms trained and evaluated in BIPED dataset.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work has been partially supported by: the Spanish Government under Project TIN2017-89723-P; the "CERCA Programme / Generalitat de Catalunya" and the ESPOL project PRAIM (FIEC-09-2015). The authors gratefully acknowledge the support of the CYTED Network: "Ibero-American Thematic Network on ICT Applications for Smart Cities" (REF-518RT0559) and the NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. Xavier Soria has been supported by Ecuador government institution SENESCYT under a scholarship contract 2015-AR3R7694.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Feedback and surround modulated boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Parraga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepedge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in computer vision</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1987" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="184" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding edges and lines in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Massachusetts Inst. of Tech. Cambridge Artificial Intelligence Lab</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by twodimensional visual cortical filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1160" to="1169" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast edge eetection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1558" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">n 4 fields: Neural network nearest neighbor fields for image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An overview of contour detection approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contour detection based on nonclassical receptive field inhibition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grigorescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Westenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bidirectional cascade network for perceptual edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10903</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual rearning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rivamap: An automated river analysis and mapping engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Isikdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Passalacqua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Photosketching: Inferring contour drawings from images. WACV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mˇech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5872" to="5881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative sparse image models for class-specific edge detection and image interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="43" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A systematic comparison between visual cues for boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mély</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A survey on edge detection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Oskoei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">1</biblScope>
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Essex</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting and localizing edges composed of steps, peaks and roofs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Unknown</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation in mri scans using deeply-supervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="320" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Edge detection with gaussian filters at multiple scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of a Workshop on Computer Vision</title>
		<meeting>a Workshop on Computer Vision<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="208" to="210" />
		</imprint>
	</monogr>
	<note>Published by</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3982" to="3991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiscale categorical object recognition using contour fragments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1270" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Camera models and machine perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Department</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
	<note type="report_type">Technion</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep crisp boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminatively trained sparse code gradients for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiaofeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="584" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Boundary detection using double-opponency and spatial sparseness constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detecting faces in images: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="58" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A level set approach to image segmentation with intensity inhomogeneity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="546" to="557" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Edge detection techniques-an overview. Pattern Recognition and Image Analysis C/C of Raspoznavaniye Obrazov I Analiz Izobrazhenii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tabbone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="537" to="559" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
