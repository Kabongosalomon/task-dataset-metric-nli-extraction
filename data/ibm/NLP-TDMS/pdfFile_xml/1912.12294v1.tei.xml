<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning by cheating</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
								<address>
									<settlement>Austin</settlement>
									<country>UT</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
							<affiliation key="aff3">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning by cheating</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Autonomous driving</term>
					<term>imitation learning</term>
					<term>sensorimotor control</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-based urban driving is hard. The autonomous system needs to learn to perceive the world and act in it. We show that this challenging learning problem can be simplified by decomposing it into two stages. We first train an agent that has access to privileged information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. This two-stage training procedure is counter-intuitive at first, but has a number of important advantages that we analyze and empirically demonstrate. We use the presented approach to train a vision-based autonomous driving system that substantially outperforms the state of the art on the CARLA benchmark and the recent NoCrash benchmark. Our approach achieves, for the first time, 100% success rate on all tasks in the original CARLA benchmark, sets a new record on the NoCrash benchmark, and reduces the frequency of infractions by an order of magnitude compared to the prior state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>How should we teach autonomous systems to drive based on visual input? One family of approaches that has demonstrated promising results is imitation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>. The agent is given trajectories generated by an expert driver, along with the expert's sensory input. The goal of learning is to produce a policy that will mimic the expert's actions given corresponding input <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Despite impressive progress, learning vision-based urban driving by imitation remains hard. The agent is tasked with organizing the "blooming, buzzing confusion" of the visual world <ref type="bibr" target="#b10">[11]</ref> by correlating it with a set of actions shown in the demonstration. A recent study argues that even with tens of millions of examples, direct imitation learning does not yield satisfactory driving policies <ref type="bibr" target="#b1">[2]</ref>.</p><p>In this paper, we show that imitation learning for vision-based urban driving can be made much more effective by decomposing the learning process into two stages. First, we train an agent that has access to privileged information: it can directly observe the layout of the environment and the positions of other traffic participants. This privileged agent is trained to imitate the expert trajectories. In the second stage, a sensorimotor agent that has no privileged information is trained to imitate the privileged agent. The privileged agent cheats by accessing the ground-truth state of the environment for both training and deployment. The final sensorimotor agent doesn't: it only uses visual input from legitimate sensors (a single forward-facing camera in our experiments) and does not use any privileged information.</p><p>The effectiveness of this decomposition is counter-intuitive. If direct imitation learning -from expert trajectories to vision-based driving -is hard, why is the decomposition of the learning process into two stages, both of which perform imitation, any better?</p><p>Conceptually, direct sensorimotor learning conflates two difficult tasks: learning to see and learning to act. Our procedure tackles these in turn. For the privileged agent, in the first stage, perception is solved by providing direct access to the environment's state, and the agent can thus focus on learning to act. In the second stage, the privileged agent acts as a teacher, and provides abundant supervision (a) An agent with access to privileged information learns to imitate expert demonstrations. This agent learns a robust policy by cheating. It does not need to learn to see because it gets direct access to the environment's state. (b) A sensorimotor agent without access to privileged information then learns to imitate the privileged agent. The privileged agent is a "white box" and can provide high-capacity on-policy supervision. The resulting sensorimotor agent does not cheat.</p><p>to the sensorimotor student, whose primary responsibility is learning to see. This is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Concretely, the decomposition provides three advantages. First, the privileged agent operates on a compact intermediate representation of the environment, and can thus learn faster and generalize better <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. In particular, the representation we use (a bird's-eye view) enables simple and effective data augmentation that facilitates generalization.</p><p>Second, the trained privileged agent can provide much stronger supervision than the original expert trajectories. It can be queried from any state of the environment, not only states that were visited in the original trajectories. This enables automatic DAgger-like training in which supervision from the privileged agent is gathered adaptively via online rollouts of the sensorimotor agent <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>. It turns passive expert trajectories into an online agent that can provide adaptive on-policy supervision.</p><p>The third advantage is that the privileged agent produced in the first stage is a "white box", in the sense that its internal state can be examined at will. In particular, if the privileged agent is trained via conditional imitation learning <ref type="bibr" target="#b5">[6]</ref>, it can provide an action for each possible command (e.g., "turn left", "turn right") in the second stage, all at once, in any state of the environment. Thus all conditional branches of the privileged agent can train all branches of the sensorimotor agent in parallel. In every state visited during training, the sensorimotor student can in effect ask the privileged teacher "What would you do if you had to turn left here?", "What would you do if you had to turn right here?", etc. This is both a powerful form of data augmentation and a high-capacity learning signal.</p><p>While our training is conducted in simulation -and indeed relies on simulation in order to access privileged information during the first stage -the final sensorimotor policy does not rely on any privileged information and is not restricted to simulation. It can be transferred to the physical world using any approach from sim-to-real transfer <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>We validate the presented approach via extensive experiments on the CARLA benchmark <ref type="bibr" target="#b7">[8]</ref> and the recent NoCrash benchmark <ref type="bibr" target="#b6">[7]</ref>. Our approach achieves, for the first time, 100% success rate on all tasks in the original CARLA benchmark. We also set a new record on the NoCrash benchmark, advancing the state of the art by 18 percentage points (absolute) in the hardest, dense-traffic condition. Compared to the recent state-of-the-art CILRS architecture <ref type="bibr" target="#b6">[7]</ref>, our approach reduces the frequency of infractions by at least an order of magnitude in most conditions.</p><p>Background. Imitation is one of the earliest approaches to learning to drive. It was pioneered by Pomerleau <ref type="bibr" target="#b18">[19]</ref> and developed further in subsequent work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref>. While these earlier investigations focus on lane following and obstacle avoidance, more recent work pushes into urban driving, with nontrivial road layouts and traffic <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. Our work fits into this line and advances it. In particular, we substantially improve upon the state-of-the-art in recent urban driving benchmarks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. Our work builds on online (or "on-policy") imitation learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Sun et al. <ref type="bibr" target="#b23">[24]</ref> summarize this line of work and argue that the availability of an optimal oracle can substantially accelerate training. These ideas were applied to off-road racing by Pan et al. <ref type="bibr" target="#b16">[17]</ref>, who trained a neural network policy to imitate a classic MPC expert that had access to expensive sensors. Our work develops related ideas in the context of urban driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The goal of our sensorimotor agent is to control an autonomous vehicle by generating steering s, throttle t, and braking signals b in each time step. The agent's input is a monocular RGB image I from a forward-facing camera, the speed v of the vehicle, and a high-level command c ("followlane", "turn left", "turn right", "go straight") <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. Command input guides the agent along reproducible routes and reduces ambiguity at intersections <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our approach first trains a privileged agent. This privileged agent gets access to a map M that contains ground-truth lane information, location and status of traffic lights, and vehicles and pedestrians in its vicinity. This map M is not available to the final sensorimotor agent, and is only accessible by the privileged agent. Both agents predict a series of waypoints for the vehicle to steer towards. A low-level PID controller then translates these waypoints into control commands (steering s, throttle t, brake b). The privileged and sensorimotor agents are illustrated schematically in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Training proceeds in two stages. In the first stage, we train the privileged agent from a set of expert demonstrations. Next, we train the sensorimotor agent off-policy by imitating the privileged agent on the same set of states as in the first stage, using offline behavior cloning on all commandconditioned branches. Finally, we train the sensorimotor agent on-policy, using the privileged agent as an oracle that provides adaptive on-demand supervision in any state reached by the sensorimotor student <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>In the following subsections, we explain each aspect of the approach in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Privileged agent</head><p>The privileged agent sees the world through a ground-truth map M ∈ {0, 1} W ×H×7 , anchored at the agent's current position. This map contains binary indicators for objects, road features, and traffic lights. See <ref type="figure" target="#fig_3">Figure 3</ref>(a) for an illustration. The task of the privileged agent is to predict K waypoints w = {w 1 , . . . , w K } that the vehicle should travel to. The agent additionally observes the speed of the vehicle v and the high-level command c. We parameterize this agent f * θ : M, v →ŵ c as a convolutional network that outputs a series of heatmaps h c,k ∈ [0, 1] W ×H , one for each waypoint k and high-level command c. We convert the heatmaps to waypoints using a soft-argmax w c k = x,y [x, y] h c,k x,y / x,y h c,k x,y . The convolutional network and soft-argmax are end-to-end differentiable. This representation has the advantage that the input M and the intermediate output h c,k are in perfect alignment, exploiting the spatial structure of the CNN.</p><p>The privileged agent is trained using behavior cloning from a set of expert driving trajectories</p><formula xml:id="formula_0">{τ 0 , τ 1 , . . .}. For each trajectory τ i = {(M 0 , c 0 , v 0 , x 0 , R 0 ), (M 1 , c 1 , v 1 , x 1 , R 1 ), .</formula><p>. .}, we store the ground-truth road map M t , high-level navigation command c t , and the agent's velocity v t , position x t , and orientation R t in world coordinates. We generate the ground-truth waypoints from future locations of the agent's vehicle</p><formula xml:id="formula_1">w t = {R −1 t (x t+1 − x t ), . . . , R −1 t (x t+K − x t )}, where the inverse rotation matrix R −1</formula><p>t rotates the relative offset into the agent's reference frame. Given a set of ground-truth trajectories and waypoints, our training objective is to imitate the training trajectories as well as possible, by minimizing the L 1 distance between the future waypoints and the agent's predictions:</p><p>minimize</p><formula xml:id="formula_2">θ E (M,v,c,w)∼τ w − f * θ (M, v) c 1 .</formula><p>The training data M , v, c, and w are sampled from an offline dataset of expert driving trajectories τ .  In prior imitation learning approaches, data augmentation, namely multiple camera angles <ref type="bibr" target="#b4">[5]</ref> or trajectory noise injection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>, was a crucial ingredient. In our setup, the driving trajectories τ are noise-free. We simulate trajectory noise by shifting and rotating the ground-truth map M , and propagating the same geometric transformations to the waypointŝ w. This is illustrated in <ref type="figure" target="#fig_3">Figure 3</ref></p><formula xml:id="formula_3">(b).</formula><p>The agent is thus placed in a variety of perturbed configurations (e.g., facing the sidewalk or the opposite lane) and learns to find its way back onto the road by predicting waypoints that lie in the correct lane. Random rotation and shifting of the map mimic both the multi-camera augmentations and the trajectory perturbations used in other imitation learning setups. The augmentation is completely offline and does not require any modifications to the data collection procedure or the expert trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sensorimotor agent</head><p>The structure of the sensorimotor agent f θ : I, v →w c closely mimics the privileged agent. We use a similar network architecture, and the same heatmap and waypoint prediction. However, the sensorimotor agents only sees the world through an RGB image I, and predicts waypointsw c in the reference frame of the RGB camera. This ensures that the input and output representations are aligned. Waypoints in the map viewŵ and camera vieww are related by a fixed perspective transformation T P that depends only on the intrinsic parameters and position of the RGB camera, and the size and resolution of the overhead map:ŵ = T P (w). We compute this transformation in closed form, as described in the supplement.</p><p>The sensorimotor agent is trained to imitate the privileged agent using an L 1 loss:</p><formula xml:id="formula_4">minimize θ E (M,I,v) ∼D T p (f (I, v)) − f * θ (M, v) 1 ,</formula><p>where D is a dataset of corresponding road maps M , images I, and velocities v.</p><p>This stage has two major advantages. First, sampling is no longer restricted to the offline trajectories provided by the original expert. In particular, the learning algorithm can sample states adaptively by rolling out the sensorimotor agent during training <ref type="bibr" target="#b20">[21]</ref>. The second advantage is that the sensorimotor agent can be supervised on all its waypoints and across all commands c at once. Both of these capabilities provide a significant boost to the driving performance of the resulting sensorimotor agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Low-level controller</head><p>The privileged and sensorimotor agents both rely on a low-level controller to translate waypoints into driving commands. Given a set of waypointsŵ = {ŵ 1 , . . . ,ŵ K } predicted or projected into the vehicle's coordinate frame, the goal of this controller is to produce steering, throttle, and braking commands (s, t, and b, respectively). We use two independent PID controllers for this purpose.</p><p>A longitudinal PID controller tries to match a target velocity v * as closely as possible. This target velocity is the average velocity that the vehicle needs to pass through all waypoints: <ref type="figure">Figure 4</ref>: Lateral PID controller.</p><formula xml:id="formula_5">v * t = 1 K K k=1 ŵ i −ŵ i−1 2 δt , s* p</formula><p>Here the agent aims at the projection of the second waypoint onto the fitted arc. s * denotes the angle between the vehicle and the target point p.</p><p>where δt is the temporal spacing between waypoints and w 0 = [0, 0]. The longitudinal PID controller then computes the throttle t to minimize the error v * − v, where v is the current speed of the vehicle. We ignore negative throttle commands, and only brake if the predicted velocity is below some threshold v * &lt; ε. We use ε = 2.0 km/h.</p><p>A lateral PID controller tries to match a target steering angle s * . Instead of directly steering toward one of the predicted points, we first fit an arc to all waypoints and steer towards a point on the arc, as shown in <ref type="figure">Figure 4</ref>. This averages out prediction error in individual waypoints. Specifically, we fit a parametrized circular arc to all waypoints using least-squares fitting. We then steer towards a point p on the arc. The target steering angle is s * = tan −1 (p y /p x ). The point p is a projection of one of the predicted waypoints onto the arc. We use w 2 for the straight and follow-the-road commands, w 3 for right turn, and w 4 for left turn. Later waypoints allow for a larger turning radius. These hyperparameters and all parameters of the PID controllers were tuned using a subset of the training routes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation details</head><p>We implemented the presented approach in PyTorch <ref type="bibr" target="#b17">[18]</ref> and both CARLA 0.9.5 and 0.9.6 <ref type="bibr" target="#b7">[8]</ref>. We render the map M using a slightly modified version of the internal CARLA road map. We render vehicles, traffic lights, pedestrians, lanes, and road footprint onto separate channels. We use circles to represent traffic lights. The map has resolution 320 × 320 and corresponds to a 64m × 64m square in the simulated world.</p><p>Network architecture. We use a lightweight keypoint detection architecture <ref type="bibr" target="#b26">[27]</ref> to predict waypoints. The privileged agent uses a randomly initialized ResNet-18 backbone, while the sensorimotor agent uses a ResNet-34 backbone pretrained on ImageNet <ref type="bibr" target="#b8">[9]</ref>. Both architectures use three up-convolutional layers to produce an output feature map. Each up-convolutional layer additionally sees the vehicle velocity v as an input. The network predicts each waypoint heatmap in a separate output channel using a 1 × 1 convolution and a linear classifier from a shared feature map. Following prior work <ref type="bibr" target="#b5">[6]</ref>, the network branches into four heads, where each head produces a K-channel heatmap. The branches represent one of the four high-level commands ("follow-lane", "turn left", "turn right", "go straight"). A differentiable soft-argmax then converts the heatmaps into spatial coordinates.</p><p>The input resolution of the privileged agent is 192 × 192 and the resolution of its output heatmap is 48 × 48. The input image is cropped such that the center of the agent's vehicle is at the bottom of the map. During training we apply random rotation and shift augmentation to the map. We first rotate the input image by an angle of [−5, 5] degrees uniformly at random, then shift the image left or right by [−5, 5] pixels uniformly at random. This shift corresponds to a 1m offset in the simulated world.</p><p>The sensorimotor agent sees a 384 × 160 RGB image as input, and produces 96 × 40 heatmaps. We use the same image augmentations as CIL <ref type="bibr" target="#b5">[6]</ref>, including pixel dropout, blurring, Gaussian noise, and color perturbations. The sensorimotor agent predicts waypoints in camera coordinates, which are then projected into the vehicle's coordinate frame.</p><p>Training. We first train the sensorimotor agent on the same trajectories used to train the privileged agent. Next, we train the sensorimotor agent online via DAgger <ref type="bibr" target="#b20">[21]</ref>, using the privileged agent as an oracle. The second stage alone -without pre-training on the original trajectories -works equally well in the final accuracy. However, the online training with DAgger is slower than training on pre-existing trajectories, so we use the first stage to accelerate the overall training process.</p><p>We resample the data following Bastani et al. <ref type="bibr" target="#b2">[3]</ref>. Critical states with higher loss are sampled more frequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We perform all experiments in the open-source CARLA simulator <ref type="bibr" target="#b7">[8]</ref>. We train the privileged agent from trajectories of a handcrafted expert autopilot that leverages the internal state of the simulator to navigate through fine-grained hand-designed waypoints. We collect 100 training trajectories at 10 fps, which amount to about 157K frames (174K in our CARLA 0.9.6 implementation) and 4 hours of driving. Each frame contains world position x, rotation R, velocity v, monocular RGB camera image I, high-level command c, and privileged information in the form of a bird's-eye view map M . High-level commands c are computed using a topological graph and simulate a simple navigation system.</p><p>Training and validation frames are collected in Town1, under the four training weathers specified by the CARLA benchmark <ref type="bibr" target="#b7">[8]</ref>. We use Town2 for the test town evaluation, and do not train or tune any parameters on it.</p><p>Experimental setup. We evaluate the presented approach on the original CARLA benchmark <ref type="bibr" target="#b7">[8]</ref> (subsequently referred to as CoRL2017) and on the recent NoCrash benchmark <ref type="bibr" target="#b6">[7]</ref>. At each frame, agents receive a monocular RGB image I, velocity v, and a high-level command c to compute steering s, throttle t, and brake b, in order to navigate to the specified goals. Agents are evaluated in an urban driving setting, with intersections and traffic lights. The CoRL2017 benchmark consists of four driving conditions, each with 25 predefined navigation routes. The four driving conditions are: driving straight, driving with one turn, full navigation with multiple turns, and the same full navigation routes but with traffic. A trial on a given route is considered successful if the agent reaches the goal within a certain time limit. The time limit corresponds to the amount of time needed to drive the route at a cruising speed of 10 km/h. In the CoRL2017 benchmark, collisions and red light violations do not count as failures. We thus conduct a separate infraction analysis to examine the behavior of the agents in more detail.</p><p>The NoCrash benchmark <ref type="bibr" target="#b6">[7]</ref> consists of three driving conditions, each on a shared set of 25 predefined routes with comparable difficulty to the full navigation condition in the CoRL2017 benchmark. The three conditions differ in the presence of traffic: no traffic, regular traffic, and dense traffic, respectively. As in CoRL2017, a trial is considered successful if the agent reaches the goal within a given time limit. The time limit corresponds to the amount of time needed to drive the route at a cruising speed of 5 km/h. In addition, in NoCrash, a trial is considered a failure if a collision above a preset threshold occurs.</p><p>Both benchmarks are evaluated under six weather conditions, four of which were seen during training and the other two only used at test time. The training weathers are "Clear noon", "Clear noon after rain", "Heavy raining noon", and "Clear sunset". For CoRL2017, the test weathers are "Cloudy noon after rain" and "Soft raining sunset" <ref type="bibr" target="#b5">[6]</ref>. For NoCrash, the test weathers are "After rain sunset" and "Soft raining sunset". We train a single agent for all conditions.</p><p>The CARLA simulator underwent a significant revision in version 0.9.6, including an update of the rendering engine and pedestrian logic. This makes CARLA 0.9.5 and prior versions not comparable to the current CARLA versions. We thus compare to all prior work on the older CARLA 0.9.5, but also provide numbers on the newer 0.9.6 version for future reference. For a fair comparison, we reran the current state-of-the-art CILRS model <ref type="bibr" target="#b6">[7]</ref> on CARLA 0.9.5, but were unable to run other methods due to the lack of open implementations or support for newer versions of CARLA. See the supplement for more detail on the effect of the simulator version on the benchmark.</p><p>Ablation study. <ref type="table">Table 1</ref> compares our full learning-by-cheating (LBC) approach to simpler baselines on the CoRL2017 benchmark ("navigation" condition).  <ref type="table">Table 1</ref>: Ablation study on the CoRL2017 benchmark (CARLA 0.9.5, "navigation" condition, test town, test weather). Two key advantages of the presented decomposition -white-box supervision and on-policy trajectories -each substantially improve performance and together achieve 100% success rate on the benchmark.</p><p>"Direct" one-stage training of the sensorimotor policy by imitation of the autopilot expert does not perform well in our experiments. This is in part due to the lack of trajectory augmentations in our training data. Prior direct imitation approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> heavily relied on trajectory noise injection during training.</p><p>Vanilla two-stage training suffers from the same issues and does not perform better than a direct supervision. However, simply supervising all conditional branches during training ("white-box") significantly increases the performance of the model. White-box supervision appears to function as powerful data augmentation that is extremely effective even in the offpolicy setting. Consider an intersection with left and right turns. In traditional imitation learning, the student only receives gradients for the branch taken by the supervising agent, and only on the supervising agent's near-perfect trajectory.</p><p>With white-box supervision, the sensorimotor student receives supervision on what it would need to do even if it suddenly had to turn right in the middle of a left turn. Multi-branch training also helps the sensorimotor model decorrelate its outputs across branches, as it gets to see multiple different signals for each training example.   PV denotes the performance of the privileged agent, AT is the performance of the built-in CARLA autopilot. Since the graphics and simulator behavior changed significantly with CARLA 0.9.6, we evaluate and compare our method on CARLA 0.9.5. CILRS was also run on this version of CARLA. Our approach outperforms prior work by significant factors, achieving 100% success rate in the "Empty" condition and reaching 85% success rate or higher in other conditions. "On-policy" refers to the sensorimotor agent rolling out its own policy during training. Training with both white-box multi-branch supervision and student rollouts (bottom row in <ref type="table">Table 1</ref>) yields the best results and achieves 100% success rate in all conditions. We use this setting in all experiments that follow.</p><p>Comparison to the state of the art. <ref type="table" target="#tab_2">Tables 2 and 6</ref> compare the performance of our final sensorimotor agent to the state of the art on the CoRL2017 <ref type="bibr" target="#b7">[8]</ref> and NoCrash <ref type="bibr" target="#b6">[7]</ref> benchmarks, respectively. We substantially outperform the prior state of the art on both benchmarks. On CoRL2017, we achieve 100% success rate on all routes in the full-generalization setting (new town, new weather).</p><p>On NoCrash, we outperform the recent CILRS model <ref type="bibr" target="#b6">[7]</ref> by significant factors, achieving 100% success rate without traffic and reaching 85% success rate or higher in all conditions. T1 T1* T2 T2* Infraction analysis. To examine the driving behavior of the agents in further detail, we conduct an infraction analysis on all routes from the NoCrash benchmark in CARLA 0.9.5. We compare the presented approach (LBC) with the previous state of the art (CILRS <ref type="bibr" target="#b6">[7]</ref>). We measure the average number of traffic light violations (i.e., running a red light) and collisions per 10 km. The results are summarized in <ref type="figure" target="#fig_4">Figure 5</ref>. Our approach cuts the frequency of infractions by at least an order of magnitude in most conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We showed that imitation learning for vision-based urban driving can be made much more effective by decomposing the learning process into two stages: first training a privileged ("cheating") agent and then using this privileged agent as a teacher to train a purely vision-based system. This decomposition partially decouples learning to act from learning to see and has a number of advantages. We have validated these advantages experimentally and have used the presented approach to train a vision-based urban driving system that substantially outperforms the state of the art on standard benchmarks.</p><p>Our training procedure leverages simulation, and indeed highlights certain benefits of simulation. ("Cheating" by accessing the ground-truth state of the environment is difficult in the physical world.) However, the procedure yields genuine vision-based driving systems that are not tied to simulation in any way. They can be transferred to the physical world using any procedure for sim-to-real transfer <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>. We leave such demonstration to future work and hope that the presented ideas will serve as a powerful shortcut en route to safe and robust autonomous driving systems.</p><p>Another exciting opportunity for future work is to combine the presented ideas with reinforcement learning and train systems that exceed the capabilities of the expert that provides the initial demonstrations <ref type="bibr" target="#b23">[24]</ref>.</p><p>Our implementation and benchmark results are available at https://github.com/dianchen96/ LearningByCheating.</p><p>ResNet34 backbone <ref type="bibr" target="#b8">[9]</ref>, followed by five layers of bilinear-upsampling + convolution + ReLU to produce a map of the original resolution of 192 × 192. The perception network is trained using an L1 loss between the network's output, and the ground truth privileged representation. The second network is used for action and predicts waypoints from the output of the first network. We use the same architecture and training procedure as the privileged agent in our main experiments, and we freeze the weights of the perception network during training.</p><p>We use a dataset of 150K frames collected from an expert in training conditions, with no trajectory noise. The offline map predictions on the training and validation sets are quite good, but we notice that during evaluation, even slightly out-of-distribution observations produce erroneous map predictions, causing the waypoint network to fail. To address this, we collect another dataset of the same size and employ trajectory noise <ref type="bibr" target="#b5">[6]</ref> in 20% of the frames, to broaden the states seen by the perception network. <ref type="table">Table 4</ref> shows the results. Both map prediction agents performs significantly worse than our two-stage agent.  <ref type="table">Table 4</ref>: Map prediction baseline with and without trajectory noise compared to our two-stage LBC, evaluated on the Navigation task of the CoRL2017 benchmark on CARLA 0.9.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Benchmark results</head><p>For completeness, <ref type="table" target="#tab_6">Table 5</ref> and <ref type="table">Table 6</ref> show the training town performance in the CARLA CoRL 2017 and NoCrash benchmarks, respectively. We again compare to MP <ref type="bibr" target="#b7">[8]</ref>, CIL <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>, CAL <ref type="bibr" target="#b21">[22]</ref>, CIRL <ref type="bibr" target="#b13">[14]</ref>, and CILRS <ref type="bibr" target="#b6">[7]</ref>. <ref type="table" target="#tab_8">Table 7</ref> compares different CARLA versions, 0.8 and 0.9.5. We compare the performance of CILRS on the Navigation Dynamic task with and without a working pedestrian autopilot (versions 0.8 and 0.9.5, respectively). The CILRS performance in 0.9.5 matches the older CARLA version in test weathers and is slightly lower in the training weathers. This indicates that CARLA 0.9.5 does not make the task easier. We report the higher numbers from the CILRS paper <ref type="bibr" target="#b6">[7]</ref>.  Empty test 83 ± 2 85 ± 2 87 ± 1 100 ± 0 87 ± 4 100 ± 0 100 ± 0 Regular 55 ± 5 68 ± 5 88 ± 2 99 ± 1 87 ± 3 97 ± 3 99 ± 1 Dense 13 ± 4 33 ± 2 70 ± 3 97 ± 2 63 ± 1 81 ± 6 83 ± 6 <ref type="table">Table 6</ref>: Quantitative results on the training town in the NoCrash benchmark. The methods were run on CARLA &lt;=0.9.5. LBC † denotes our agent trained and evaluated on our customized CARLA based on 0.9.6, the most up-to-date CARLA version. Note that CARLA 0.9.6 has different graphics compared to 0.8 and 0.9.5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Town</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3rdFigure 1 :</head><label>1</label><figDesc>Conference on Robot Learning (CoRL 2019), Osaka, Japan. arXiv:1912.12294v1 [cs.RO] 27 Dec 2019 Overview of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Agent architectures. (a) The privileged agent receives a bird's-eye view image of the environment and produces a set of heatmaps that go through a soft-argmax layer (SA), yielding waypoints for all commands. The input command selects one conditional branch. The corresponding waypoints are given to a low-level controller that outputs the steering, throttle, and brake. (b) The sensorimotor agent receives genuine sensory input (image from a forward-facing camera). It produces waypoints in the egocentric camera frame. Waypoints are selected based on the command, projected into the vehicle's coordinate frame, and passed to the low-level controller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Road map (b) Rotation and shift aug.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) Map M provided to the privileged agent. One channel each for road (light grey), lane boundaries (grey), vehicles (blue), pedestrians (orange), and traffic lights (green, yellow, and red). The agent is centered at the bottom of the map. The agent's vehicle (dark red) and predicted waypoints (purple) are shown for visualization only and are not provided to the network. (b) The map representation affords simple and effective data augmentation via rotation and shifting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Infraction analysis. Number of traffic light violations and crashes per 10 km in Town 1 (T1), Town 1 with new weather (T1*), Town 2 (T2), and Town 2 with new weather (T2*).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the success rate of the presented approach (LBC) to the state of the art on the original CARLA benchmark (CoRL2017) in the test town. (The supplement provides results on the training town.) LBC † denotes our agent trained and evaluated on CARLA 0.9.6. All other agents were evaluated on CARLA 0.8 and 0.9.5. Our approach outperforms all prior work and achieves 100% success rate on all routes in the full-generalization setting (test town, test weather).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">CARLA ≤0.9.5</cell><cell></cell><cell></cell><cell>CARLA 0.9.6</cell></row><row><cell>Task</cell><cell cols="5">Weather CIL [6] CAL [22] CILRS [7] LBC</cell><cell>LBC</cell><cell>PV</cell><cell>AT</cell></row><row><cell>Empty</cell><cell></cell><cell>48 ± 3</cell><cell>36 ± 6</cell><cell cols="4">51 ± 1 100 ± 0 100 ± 0 100 ± 0 100 ± 0</cell></row><row><cell>Regular</cell><cell>train</cell><cell>27 ± 1</cell><cell>26 ± 2</cell><cell>44 ± 5</cell><cell>96 ± 5</cell><cell cols="2">94 ± 3 95 ± 1 99 ± 1</cell></row><row><cell>Dense</cell><cell></cell><cell>10 ± 2</cell><cell>9 ± 1</cell><cell>38 ± 2</cell><cell>89 ± 1</cell><cell cols="2">51 ± 3 46 ± 8 60 ± 3</cell></row><row><cell>Empty</cell><cell></cell><cell>24 ± 1</cell><cell>25 ± 3</cell><cell cols="4">90 ± 2 100 ± 2 70 ± 0 100 ± 0 100 ± 0</cell></row><row><cell>Regular</cell><cell>test</cell><cell>13 ± 2</cell><cell>14 ± 2</cell><cell>87 ± 5</cell><cell>94 ± 4</cell><cell cols="2">62 ± 2 93 ± 2 99 ± 1</cell></row><row><cell>Dense</cell><cell></cell><cell>2 ± 0</cell><cell>10 ± 0</cell><cell>67 ± 2</cell><cell>85 ± 1</cell><cell cols="2">39 ± 8 45 ± 10 59 ± 6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the success rate of the presented approach (LBC) to the previous approaches on the NoCrash benchmark in the test town. (The supplement provides results on the training town.)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Quantitative results on the training town in the CoRL2017 CARLA benchmark. LBC † denotes our agent trained and evaluated on our customized CARLA based on 0.9.6, the most up-todate CARLA version. Note that CARLA 0.9.6 has different graphics compared to 0.8 and 0.9.5.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">CARLA ≤0.9.5</cell><cell></cell><cell>CARLA 0.9.6</cell></row><row><cell>Task</cell><cell cols="3">Weather CIL[6] CAL[22] CILRS[7] LBC</cell><cell>LBC</cell><cell>PV</cell><cell>AT</cell></row><row><cell>Empty</cell><cell></cell><cell>79 ± 1 81 ± 1</cell><cell cols="3">87 ± 1 100 ± 0 97 ± 1 100 ± 1 100 ± 0</cell></row><row><cell>Regular</cell><cell>train</cell><cell>60 ± 1 73 ± 2</cell><cell cols="3">83 ± 0 99 ± 1 93 ± 1 96 ± 3 99 ± 1</cell></row><row><cell>Dense</cell><cell></cell><cell>21 ± 2 42 ± 1</cell><cell cols="3">42 ± 2 95 ± 2 71 ± 5 80 ± 5 86 ± 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison of CILRS<ref type="bibr" target="#b6">[7]</ref> on different versions of CARLA on the Navigation Dynamic task of the CoRL2017 benchmark.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge the Texas Advanced Computing Center (TACC) for providing computing resources. This work has been supported in part by the National Science Foundation under grants IIS-1845485 and CNS-1414082. We thank Xingyi Zhou for constructive feedback on the network architecture, and Felipe Codevilla for image augmentation code.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Additional details</head><p>Map-view perspective transformation. Given a predicted waypointw = (w x ,w y ) in camera coordinates, we compute its projection onto the ground plane (ŵ x ,ŵ y , 0) using the camera's horizontal field of view (fov) f = w 2 tan(f ov/2) , height p y , and canvas center c x = w 2 , c y = h 2 . We assume the camera always faces forward, as the map is anchored at the agent's position and local coordinate frame. We always project points onto a constant ground plane z = 0 to avoid depth estimation: w y = f cy−wy p y ,ŵ x =w x −cx cy−wy p y . To make the projection a one-to-one mapping, we additionally move the projected points back by 4 meters, to prevent points closer to the ego-vehicle getting clipped at the bottom of the image. This transformation is differentiable and the sensorimotor agent can be trained end-to-end. The camera is placed at p = (2, 0, 1.4) in the vehicle's reference frame, at the hood position. The camera faces forward and has a resolution of 384 × 160 with horizontal fov 90 • .</p><p>Data collection. To train the privileged agent, we use 157K training frames and 39K validation frames collected at 10 fps by a hand-crafted autopilot. We use 174K training frames in our 0.9.6 implementation. For both our offline dataset collection and privileged rollouts, we collect the frames using four training weather conditions uniformly sampled in the training town. We add 100 other vehicles to share the traffic with the ego-vehicle. We add 250 pedestrians in our 0.9.6 implementation.</p><p>Hyperparameters. We use the Adam optimizer with initial learning rate 10 −4 and no weight decay to train all our models. We use batch size 32 to train all of our models in 0.9.5, batch size 128 to train the privileged model in 0.9.6, and batch size 96 to train the sensorimotor model in 0.9.6. We used the batch augmentation trick <ref type="bibr" target="#b9">[10]</ref> with m = 4 for our image model training in 0.9.6. For the spatial argmax layers in both privileged and sensorimotor agent, we fix temperature β = 1 instead of a learnable parameter. We use PyTorch 1.0 to train and evaluate our models.</p><p>Image model warm-up. Since a randomly initialized network returns the canvas center at the end of the spatial argmax layer in the image coordinate, it corresponds to infinite distance when projected. This causes gradients to explode in the backward pass. To address this issue, we warm up our image model by first supervising it with loss in the projected image coordinate space for 1K iterations before the two-stage training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Additional experiments</head><p>If an agent can perform near perfectly using a map representation, why not simply try to predict the map representation from raw pixels, then act on that? This approach resembles that of Müller et al. <ref type="bibr" target="#b14">[15]</ref>, where perception and control are explicitly decoupled and trained separately.</p><p>We train two networks. The first network is used for perception and directly predicts the privileged representation from an RGB image. We resize the RGB image to 192 × 192 and feed this into a</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of robot learning from demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Argall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Browning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ChauffeurNet: Learning to drive by imitating the best and synthesizing the worst</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Verifiable reinforcement learning via policy extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to drive from simulation without real world labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rigley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hawke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-D</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zieba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end driving via conditional imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring the limitations of behavior cloning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09335</idno>
		<title level="m">Augment your batch: better training with larger batches</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The Principles of Psychology. Henry Holt and Company</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1890</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">DART: Noise injection for robust imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Off-road obstacle avoidance through end-to-end learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CIRL: Controllable imitative reinforcement learning for vision-based self-driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Driving policy transfer via modularity and abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An algorithmic perspective on imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Osa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pajarinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Robotics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2)</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Agile autonomous driving using end-to-end deep imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saigol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ALVINN: An autonomous land vehicle in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Reinforcement and imitation learning via interactive no-regret learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5979</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional affordance learning for driving in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning from demonstration for autonomous navigation in complex unstructured terrain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeply AggreVaTeD: Differentiable imitation learning for sequential prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monocular plan view networks for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Does computer vision matter for action?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">30</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
