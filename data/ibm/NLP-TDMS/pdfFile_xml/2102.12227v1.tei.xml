<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Attentive Residual Networks for Argument Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Galassi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lippi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Torroni</surname></persName>
						</author>
						<title level="a" type="main">Multi-Task Attentive Residual Networks for Argument Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Argument mining</term>
					<term>residual networks</term>
					<term>neural at- tention</term>
					<term>multi-task learning</term>
					<term>ensemble learning</term>
					<term>natural language processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications. On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size.</p><p>Index Terms-Argument mining, residual networks, neural attention, multi-task learning, ensemble learning, natural language processing.</p><p>With respect to our previous work <ref type="bibr" target="#b6">[7]</ref>, this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections <ref type="bibr" target="#b0">[1]</ref>. AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others <ref type="bibr" target="#b1">[2]</ref>. In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.</p><p>While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton <ref type="bibr" target="#b2">[3]</ref>, who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links.</p><p>AM approaches are very often tailored to specific corpora or genres <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</p><p>This work was supported by the European Union's Justice programme, project "Analytics for DEcision of LEgal Cases", under Grant 101007420. (Corresponding author: Andrea Galassi.)</p><p>A. <ref type="bibr">Galassi</ref>  upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest. These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph.</p><p>We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets. They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance.</p><p>Our main contributions are:</p><p>• A novel approach to AM, which extends our previous work <ref type="bibr" target="#b6">[7]</ref> by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address. • A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The adoption of deep learning approaches in AM is relatively recent, compared to other areas of NLP. That is probably a consequence of a lack of large AM corpora, considering the complexity and peculiarities of the tasks at hand. Indeed, the annotation of large corpora for AM system evaluation and training proved to be challenging, as demonstrated by relatively low IAA indicators and several unsatisfactory attempts at crowdsourcing annotations. That is especially true for some genres like user-generated content <ref type="bibr" target="#b8">[8]</ref>. Reasons for that are the nature of the task, which is intellectually demanding, and the lack of a unified argument model, as "arguments" may take very different shapes in different genres, also leading to a trade-off between the expressiveness of the argument model and the complexity of the annotation process and availability of relevant data points, often resolved in favor or simple argument models <ref type="bibr" target="#b0">[1]</ref>. Earlier research mainly focused on the definition of features for specific genres or even for specific corpora. The differences between corpora, both regarding the domain and the theoretical framework followed during the annotation process, force researchers to test a model on the same corpora on which it was trained, and to the best of our knowledge, transfer learning approaches have not seen wide experimentation. These two elements lead to the common practice to define a method or a model and validate it only on a single corpus or on a few corpora <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-task Learning and Joint Learning for AM</head><p>Since AM includes many subtasks that are strongly interrelated, a recent trend of this research field is to address many of them at the same time using multi-task or joint learning techniques. The aim of such approaches is to transfer knowledge from the auxiliary tasks to the main one, or to obtain coherent results on multiple tasks performed at once.</p><p>Stab and Gurevych <ref type="bibr" target="#b3">[4]</ref> jointly address component classification and link prediction on persuasive essays, using Integer Linear Programming and a rich set of specific features, such as lexical, structural, and contextual information. Various neural architectures are tested in <ref type="bibr" target="#b9">[9]</ref>, including the deep biLSTM multi-task learning (MTL) setting of <ref type="bibr" target="#b10">[10]</ref>, using sub-tasks as auxiliary tasks. They conclude that neural networks can outperform feature-based techniques in argument mining tasks. Schulz et al. <ref type="bibr" target="#b11">[11]</ref> investigate MTL settings addressing component detection on five datasets as five different tasks. Their architecture is composed of a CRF layer on top of a biLSTM, whose recurrent layers are shared across the tasks. They obtain positive results, and the MTL setting shows to be beneficial especially for small datasets, even if the auxiliary AM tasks involve different domains and even different component classes. Lauscher et al. <ref type="bibr" target="#b12">[12]</ref> analyze an MTL setting where rhetorical classification tasks are performed along with component detection. They use a hierarchical attention-based model so to perform both word-level and sentence-level tasks with the same neural architecture. The results show improvements in the rhetorical tasks, but not in AM.</p><p>In <ref type="bibr" target="#b13">[13]</ref>, a structured learning framework based on factor graphs is used to jointly classify all the propositions in a document and determine which ones are linked together. The models heavily rely on a priori knowledge, encoded as factors and constraints, designed so to to enforce adherence to the desired argumentation structure, according to the argument model and domain characteristics. The authors discuss experiments with six different models, which differ by complexity and by how they model the factors, using RNNs and SVMs. Their best result is obtained by using the same set of features used in <ref type="bibr" target="#b3">[4]</ref>, resulting in a total feature size of around 7,000 for propositions and 2,100 for links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Attention for AM</head><p>Neural attention is a mechanism widely used in NLP to improve performance and interpretability of neural networks, and it is the core of many NLP architectures like RNNsearch <ref type="bibr" target="#b14">[14]</ref>, Pointer Networks <ref type="bibr" target="#b15">[15]</ref>, and Transformer <ref type="bibr" target="#b16">[16]</ref>. Given an input sequence, and possibly a query element, attention consists in the computation of a set of weights that represent the importance of each element of the sequence, which can be further used to create a compact representation of such an input. There are many different ways to compute such weights. A taxonomy of attention models is proposed in our survey <ref type="bibr" target="#b17">[17]</ref>.</p><p>Among the AM systems that use neural attention, the one used in <ref type="bibr" target="#b18">[18]</ref> integrate hierarchical attention and biGRU for the analysis of the quality of the argument, the one in <ref type="bibr" target="#b19">[19]</ref> use attention to integrate sentiment lexicon, while in other works <ref type="bibr" target="#b20">[20]</ref>- <ref type="bibr" target="#b22">[22]</ref> attention modules are stacked on top of recurrent layers. The use of Pointer Networks for AM has also been investigated <ref type="bibr" target="#b23">[23]</ref>. Transformer-based approaches in AM use language representation models such as BERT <ref type="bibr" target="#b24">[24]</ref> and ELMO <ref type="bibr" target="#b25">[25]</ref> to create contextualized word embeddings. Specifically, Reimers et al. <ref type="bibr" target="#b26">[26]</ref> address component classification and argument clustering, a related task whose aim is to identify similar arguments. Similarly, Lugini and Litman <ref type="bibr" target="#b27">[27]</ref> use BERT embeddings alongside other contextual information to perform component classification, and Wang et al. <ref type="bibr" target="#b28">[28]</ref> use them to train a different model for each type of component. Trautmann et al. <ref type="bibr" target="#b29">[29]</ref> use pre-trained BERT models to perform word-level classification of the stance of components regarding a given topic, while Poudyal et al. <ref type="bibr" target="#b30">[30]</ref> use RoBERTa <ref type="bibr" target="#b31">[31]</ref>, an improved version of the original BERT.</p><p>Mayer et al. <ref type="bibr" target="#b32">[32]</ref> present and conduct extensive experimentation on the AbstRCT corpus, addressing four AM subtasks with a pipeline scheme. They analyze the impact of various BERT models, which are pre-trained on other corpora and then fine-tuned on the corpus at hand. Segmentation and component classification are performed as sequence tagging with BIO scheme. Link prediction and relation classification follow, taking into account all the pairs of components obtained in the first step and classifying their relations as attack, support, or non-existing. Their architecture is based on bi-directional transformers followed by a softmax layer and various encoders. Their approach is completely distanceindependent, but since they compare every possible pair of components, the size of the dataset grows quadratically with the number of components in the document, which makes it hardly scalable to large documents. Another approach, consisting of predicting at most one related component for each component, and then classifying their relation, has been tested but yields worse results. The architectures that yield the best results are BioBERT <ref type="bibr" target="#b33">[33]</ref>, which is pre-trained on a largescale biomedical corpus, SciBERT <ref type="bibr" target="#b34">[34]</ref>, which is pre-trained on scientific articles of various nature, and RoBERTa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL</head><p>The design of our model is inspired by the great success that residual networks <ref type="bibr" target="#b35">[35]</ref> have obtained across many different tasks related to NLP <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref>. The core idea behind residual networks is to create shortcuts that link neurons belonging to distant layers, whereas standard feed-forward networks typically link neurons belonging to subsequent layers only. This kind of architecture usually results in a more efficient training phase, allowing to train networks with considerably more layers, reducing the overall computational footprint.</p><p>The architecture we propose makes use of the dense residual network model, along with a Long Short-Term Memory (LSTM) network <ref type="bibr" target="#b38">[38]</ref>, and an attention module <ref type="bibr" target="#b17">[17]</ref>. The network is trained to jointly perform three argument mining sub-tasks: argument component classification, link prediction, and relation classification.</p><p>More specifically, our approach operates on sentence pairs, does not rely on document-level global optimization, and does not enforce model constraints induced, for example, by domain-or genre-specific background knowledge. This makes our approach amenable to a possible integration within more complex and sophisticated systems.</p><p>We performed model selection and hyper-parameter tuning on a single corpus (CDCP, see Section IV) and we collected results on validation data in order to tune the whole architecture. There are two reasons for this choice: one the one hand, we aim to show the robustness of the approach across different corpora, while on the other we believe it is important to limit the footprint of these experiments -an issue that is receiving a growing attention in the community <ref type="bibr" target="#b39">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model description</head><p>In order to achieve a general method which may be applicable in any domain, our method does not rely on a specific argument model, but rather it reasons in terms of abstract entities, such as argumentative components and links among them. We instantiate such abstract entities into concrete categories given by annotations, such as claims and premises, supports and attacks, as soon as we apply the method to a specific corpus whose annotations follow a concrete argument model.</p><p>The detection of argumentative content in text is one typical stage of AM systems <ref type="bibr" target="#b0">[1]</ref>. Other works only focus on AM tasks that assume that argumentative components and their boundaries are already identified in the data. Such is the case with Niculae et al. <ref type="bibr" target="#b13">[13]</ref>, whose CDCP dataset only consists of argumentative elements, and with others <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b40">[40]</ref> who simply ignore the non-argumentative elements of the input text. Accordingly, we define a document D as a sequence of argumentative components and disregard the rest of the input text. An argumentative component in turn is a sequence of tokens, i.e., words and punctuation marks, representing an argument, or part thereof. The labeling of components is induced by the chosen argument model. Such a labeling associates each component with the corresponding category P of the argument component it contains. For this reason, we will use the terms component, sentence, and proposition as equivalent, and implying them as being argumentative by assumption.</p><p>Given two argumentative components a and b belonging to the same document, we represent a directed relation from the former (source) to the latter (target) as a → b. Reflexive relations (a → a) are not allowed. <ref type="bibr" target="#b1">2</ref> Any pair of components is characterized by four labels: the types of the two components (P a and P b ), the Boolean link label L a→b , and relation (type) label (R a→b ). The link label indicates the presence of a link, and is therefore true if there exists a directed link from a to b, and false otherwise. The relation label instead contains information on the nature of the link connecting a and b. It represents the relationship between the two components, according to the links that connect a to b or b to a. Its domain is composed, according to the underlying argument model, not only by all the possible link types, but also by their opposite types (e.g., attack and attackedBy), as well as by a special category, None, meaning no link in either direction. One reason to introduce opposite relation types is to mitigate the unbalance caused by limited amount of instances each relation type typically has, if compared with the number of instances belonging to the None class. Likewise, we speculate that the introduction of additional labels may contribute positively to the optimization process. We shall remark that opposite relation lables are exploited during training, but they are discarded in the test phase, where they are simply substituted with the None label, consistently with previous work.</p><p>We use a multi-objective learning setting where multiple tasks are performed jointly for each possible input pair of components (a, b) belonging to the same document D. Our main focus is the identification of the link label L a→b for each possible input pair of propositions (a, b) belonging to the same document D. Our first objective is thus a link prediction task, which can be considered as a sub-task of argument structure prediction. A second objective is the classification of the two components, <ref type="bibr" target="#b2">3</ref> and our final objective is the classification of the relationship between such components, i.e., the prediction of labels P a , P b , R a→b . A common issue in the classification of pairs of document components is the fact that pairs grow quadratically with the number of components, causing a large imbalance against the negative class <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b32">[32]</ref>. One way of dealing with that issue is to limit the possible pairs by setting a maximum distance, thus obtaining a number of pairs proportional to the number of components. Such a distance is a hyper-parameter, and as such it may be empirically determined <ref type="bibr" target="#b30">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Embeddings and features</head><p>Faithful to the main purpose of this work, of evaluating the effectiveness of deep residual networks and attention for AM without resorting to domain-or genre-specific information, our system relies on a minimal set of widely applicable features.</p><p>Words are encoded using pre-trained GloVe embeddings <ref type="bibr" target="#b41">[41]</ref> of size 300. Input sequences are zero-padded to the length of the longest sequence in the datasets (henceforth T ). Out-of-vocabulary terms are handled by creating random embeddings.</p><p>In our previous work, we empirically assessed how the distance between two components may be a relevant feature for argument mining in the CDCP corpus <ref type="bibr" target="#b6">[7]</ref>. The same observation has been recently made also with reference to other corpora <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b32">[32]</ref>. Similarly to what has been done in <ref type="bibr" target="#b9">[9]</ref>, we define the number of argumentative components separating source and target as argumentative distance, using the positive sign when the source precedes the target, and the negative sign otherwise. Inspired by works in other domains <ref type="bibr" target="#b42">[42]</ref>- <ref type="bibr" target="#b45">[44]</ref>, we encode such a scalar number in a 10-bit array, using the first 5 bits for those cases where the source precedes the target, and the other 5 bits for the opposite case. The number of consecutive "1" values encodes the value of the distance, with a maximum value of 5. For example, if the argumentative distance is −3, the encoding is 00111 00000; if the argumentative distance is 2, the encoding is 00000 11000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The RESARG architecture</head><p>We use our own previous system <ref type="bibr" target="#b6">[7]</ref> as a baseline. We refer to it as RESARG. Its architecture, depicted in <ref type="figure" target="#fig_0">Figure 1a</ref>, is based on residual networks <ref type="bibr" target="#b35">[35]</ref> and comprises the following macro blocks:</p><p>• two deep embedders, one for sources and one for targets, that manipulate token embeddings; • a dense encoding layer that reduces the dimensionality of the features; • a biLSTM that processes the sequences; • a residual network; • the final-stage classifiers. The purpose of the deep embedders is to fine-tune the pretrained embeddings, a common procedure in deep learningbased NLP solutions <ref type="bibr" target="#b46">[45]</ref> whose usefulness was confirmed by preliminary experiments. Each embedder is composed of a single residual block consisting of four pre-activated timedistributed dense layers. Accordingly, each layer applies the same transformation to each embedding, regardless of their position inside the sentence. All the layers have 50 neurons, except for the last one, which has 300 neurons.</p><p>The dense encoding layer is necessary in order to obtain an LSTM with fewer parameters, thus reducing the time needed for training, and limiting overfitting. It applies a timedistributed dense layer, which reduces the embedding size to 50, and a time average-pooling layer <ref type="bibr" target="#b47">[46]</ref>, which reduces the sequence size by a factor of 10.</p><p>The resulting sequences are then given as input to the same bidirectional LSTM, producing a single representation of size 50 for each component. Thus, for each proposition, T embeddings of size 300 are transformed first into T embeddings of size 50, then into T /10 embeddings of size 50, and finally in a single feature of size 50.</p><p>Source and target are processed in parallel in the first three blocks, then concatenated together, along with the encoding of the distance, and given as input to the final residual network. The first level of the final residual network is a dense encoding layer with 20 neurons, while the residual block is composed of a layer with 5 neurons and one with 20 neurons. The outputs of the first and the last layers of the residual networks are summed up and provided as input to the classifiers.</p><p>The final stage of RESARG are three independent softmax classifiers used to predict the source, the target, and the relation labels. Each classifier, which predicts a label for a dedicated task, contributes simultaneously to our learning model. The link classifier is obtained by summing the relevant scores produced by the relation classifier, aggregating the probability assigned to the relation labels into a single link label.</p><p>All the dense layers use the rectifier activation function <ref type="bibr" target="#b48">[47]</ref>, and they randomly initialize weights with He initialization <ref type="bibr" target="#b49">[48]</ref>. The application of all non-linearity functions is preceded by batch-normalization layers <ref type="bibr" target="#b50">[49]</ref> and by dropout layers <ref type="bibr" target="#b51">[50]</ref>, with probability p = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The RESATTARG architecture</head><p>Motivated by the remarkable results obtained by attentionbased architectures in NLP tasks, we have extended RESARG by including a neural attention block after the bi-LSTM module. To better exploit the new attention module, we removed the time pooling layer from the dense encoding block, so as to avoid loss of information along the temporal axis, and to maintain the whole output sequence from the LSTM. Therefore, in this new model, the input and the output of the LSTM module have size (T , 50). The resulting architecture, named RESATTARG, is depicted in <ref type="figure" target="#fig_0">Figure 1b</ref>.</p><p>The attention module is implemented as coarse-grained parallel co-attention <ref type="bibr" target="#b17">[17]</ref>, so as to consider both components at the same time while computing attention on each of them. Our method consists of exploiting the average embedding of one proposition as a query element while computing attention on the other, similarly to what has been done in <ref type="bibr" target="#b52">[51]</ref>. Specifically, calling K s and K t the outputs of the bi-LSTM obtained from, respectively, the processing of the source and the target propositions, we compute the (masked) average of K t , obtaining a single embedding g t of size 50 (Eq. 1). This embedding is used as query element to compute additive soft attention <ref type="bibr" target="#b17">[17]</ref> on K s , obtaining a single source context vector c s of size 50. The details of this process are described in Equations 2, 3, and 4, where the matrices W 1 , W 2 and the vectors b, w 3 are learnable parameters. An equivalent symmetric procedure is used to compute attention on K t so as to obtain c t . The output of this block are two embeddings of size 50, as in our previous architecture.</p><formula xml:id="formula_0">g t = masked avg(K t )<label>(1)</label></formula><formula xml:id="formula_1">e s = w 3 T relu(W 1 K s + W 2 g t + b)<label>(2)</label></formula><p>(a) RESARG architecture <ref type="bibr" target="#b6">[7]</ref>.</p><p>(b) RESATTARG architecture. </p><formula xml:id="formula_2">c s = T i=1 k si a si<label>(3)</label></formula><p>The resulting architecture has close to 5.5M parameters, 140,000 of which are trainable. If compared with other stateof-the-art neural architectures, such as BERT BASE and its 110M parameters, RESATTARG is considerably smaller, and accordingly it is less computationally demanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Repeated trainings and ensemble learning</head><p>Since the training of neural models is non-deterministic, the results of a single training procedure are influenced by the random seed that is used, thus they may not be reliable or reproducible <ref type="bibr" target="#b53">[52]</ref>, <ref type="bibr" target="#b54">[53]</ref>. Such problem also affects our previous results <ref type="bibr" target="#b6">[7]</ref>, since they were obtained from a single training experiment.</p><p>We have decided to replicate that experiment by repeating the training procedure 10 times, with different seeds, obtaining 10 trained neural networks for each configuration. We will evaluate our models in two different ways. At first, we will consider the average of the scores obtained by every single network for each metric. Then, we evaluate the predictions obtained using all the 10 models in ensemble voting.</p><p>In our ensemble setting the class of each entity is assigned as the class voted by the majority of the networks. This technique is similar to the concept of bootstrap aggregating, also known as bagging <ref type="bibr" target="#b55">[54]</ref>. However, while in standard bagging each model is trained on a random sample of the training set, here we train all the models on the same training set, since stochastic elements are already present in the training procedure itself. We have chosen this ensemble method for the sake of simplicity, but more advanced techniques do exist and may yield better results <ref type="bibr" target="#b56">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CORPORA</head><p>We validate our approach on 4 corpora differing from each other in various dimensions: the domain of the documents, their average length, the formatting, and the argumentative model followed for the annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cornell eRulemaking Corpus (CDCP)</head><p>The Cornell eRulemaking Corpus (CDCP) <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b57">[56]</ref> consists of user-generated documents in which specific regulations are discussed. The authors have collected user comments from an eRulemaking website on the topic of Consumer Debt Collection Practices rule. The corpus contains 731 user comments, for a total of about 4,700 components, all considered to be argumentative.</p><p>As typical of user-generated data, the comments are not structured, and often present grammatical errors, typos, and do not follow usual writing conventions (such as the blank space after the period mark). This complicates pre-processing, since most of the off-the-shelf tools turn out to be inaccurate even in simple tasks such as tokenization.</p><p>Annotations follow the argument model proposed in <ref type="bibr" target="#b58">[57]</ref>, where links are constrained to form directed graphs. The corpus is suitable both for component and relation classification, since it presents 5 classes of propositions and two types of links. We will use the version of CDCP without nested proposition and guaranteed transitive closure <ref type="bibr" target="#b13">[13]</ref>.</p><p>The components are addressed as propositions, and they consist of a sentence or a clause. Propositions are divided into POLICY (17%), VALUE (45%), FACT (16%), TESTIMONY (21%) and REFERENCE (1%). Only 3% of more than 43,000 possible proposition pairs are linked; almost all links are labeled as REASON (97%), whereas only a few are labeled as EVIDENCE (3%).</p><p>The unstructured nature of documents, the strong unbalance between the classes, and the presence of noise make the corpus particularly challenging for all the subtasks of argument mining, especially those that involve the relationships between components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. AbstRCT</head><p>The AbstRCT Corpus <ref type="bibr" target="#b32">[32]</ref> extends previous work <ref type="bibr" target="#b59">[58]</ref>, and consists of abstracts of scientific papers regarding randomized control trials for the treatment of specific diseases (i.e., neoplasm, glaucoma, hypertension, hepatitis b, diabetes). The final corpus contains 659 abstracts, for a total of about 4,000 argumentative components. AbstRCT is divided into three parts: neoplasm, glaucoma, and mixed. The first one contains 500 abstracts about neoplasm, divided into train (350), test (100), and validation (50) splits. The remaining two are designed to be test sets. One contains 100 abstracts for glaucoma, the other 20 abstracts for each disease <ref type="bibr" target="#b3">4</ref> .</p><p>Components are labeled as EVIDENCE (2,808) and CLAIM <ref type="bibr" target="#b0">(1,</ref><ref type="bibr">390)</ref>, while relations are labeled as SUPPORT (2,259) and ATTACK (342). <ref type="bibr" target="#b4">5</ref> About 10% of about 25,000 possible component pairs have a labeled relationship. The argumentative model chosen for annotation enforces only one constraint: claims can have an outgoing link only to other claims.</p><p>With respect of CDCP, this corpus is less noisy and the distribution of the classes is more balanced. We have chosen this as a benchmark to demonstrate that our approach is independent of the domain and of the argument model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Doctor Inventor Argumentative Corpus (DrInventor)</head><p>The Doctor Inventor Argumentative Corpus (DrInventor) <ref type="bibr" target="#b60">[59]</ref> is the result of an extension of the Doctor Inventor corpus <ref type="bibr" target="#b61">[60]</ref>, which includes an annotation layer containing argumentative components and relations. DrInventor consists of 40 scientific publications from computer graphics, which contain about 12,000 argumentative component labels, as well as annotations for other tasks.</p><p>The classes of argumentative components are DATA (4,093), OWN CLAIM (5,445), and BACKGROUND CLAIM (2,751). The former two are related to the concepts of premises and claims, while the latter is something in between, since it is a claim related to some background knowledge, such as that made by another author in a previous work. The relation classes are SUPPORTS (5,790), CONTRADICTS (696), and SEMANTICALLY SAME (44), since it is common practice in scientific publications to re-iterate the same claim (or more rarely the same data) multiple times.</p><p>Since DrInventor includes documents where the structure of the discourse is complex, and data are often presented along with claims, it makes argument mining more challenging: in more than 1,000 cases some components are split into multiple text sequences, located in non-contiguous parts of the documents. This phenomenon mostly concerns claims, but data are affected too, in fewer cases. This introduces the difficulty of recognizing different segments of the documents as part of a single component and makes link prediction more difficult to address through non-pipeline approaches.</p><p>The unbalanced distribution between the three classes and the presence of split components makes this corpus quite challenging for link prediction, a difficulty which is highlighted also by the low inter-annotator agreement reported in the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Persuasive Essays Corpus (UKP)</head><p>The Persuasive Essays Corpus (UKP) <ref type="bibr" target="#b3">[4]</ref> consist of 402 documents coming from an online community where users post essays and other material, provide feedback, and advise each other. The dataset is divided into a test split of 80 essays and a training split with the remaining documents.</p><p>UKP defines three classes of argumentative components: MAJOR CLAIM (751), CLAIM (1,506), and PREMISE <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">832)</ref>. Premises may be linked to CLAIMS through relations of SUPPORT <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">613)</ref> or ATTACK (219). MAJOR CLAIMS are not linked to other components. <ref type="bibr" target="#b5">6</ref> While these classes of components are not unlike those used in other datasets, the argumentation model instead is tailored on this specific corpus. It amounts to an argument graph consisting of trees, each rooted on a claim. Each tree can only include components belonging to the same paragraph. Claims do not have outgoing relationships because only premises can descend from claims. Each premises has exactly one outgoing relation. Finally, the structure of the argumentation follows domain-specific conventions. For example, in most cases, the MAJOR CLAIM is in the first or the last paragraph of the document, and more often than not it is the only argumentative component in its paragraph.</p><p>Thanks to the highly constrained nature of the UKP data, we expect methods based on the document structure to have an edge over general, structure-agnostic methods such as the one we propose. However, we believe it is important to include also this type of data in the present study, in order to evaluate our approach in as many and diverse scenarios as possible, and better understand its advantages as well as its limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL SETTING</head><p>We consider a multi-task formulation for our learning problem. The loss function is given by the weighted sum of four different components: the categorical cross-entropy on three labels (source and target categories, link relation category) and an L 2 regularization on the network parameters.</p><p>We initially evaluate our new architecture against our previous model and the structured learning approach of <ref type="bibr" target="#b13">[13]</ref> on CDCP, presenting an ablation study of the new components we have introduced. Then, we extend the evaluation to other three data sets, for which we compare our approach against the state-of-the-art.</p><p>In our approach each component is involved in many pairs, both as a source and as a target, and accordingly it is classified multiple times by the same network. The label will be assigned by the model by considering the average probability computed by the ensemble for each class, and by thus choosing the class with the highest score. Alternative approaches could be to assign the class that results to be the most probable in most of the cases, thus relying on a majority vote. A further option could be to simply consider the label with the highest confidence. However, the latter procedure might be more sensitive to outliers, because the misclassification of a component in just one pair would lead to the final misclassification of the component, regardless of all the other pairs. A deeper analysis of different techniques to address these issues is left to future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Preparation</head><p>Our architecture allows us to use the CDCP and AbstRCT datasets directly, without need for further pre-processing.</p><p>For what concerns DrInventor, instead, specific data preprocessing is needed to address two aspects of this dataset: the presence of lengthy documents and split components. Lengthy documents make it inconvenient to consider all the possible pairs of argumentative units. Doing so would not only be infeasibile with regular computational resources, but it would also yield an extremely unbalanced dataset for link prediction, with less than 1% of pairs linked. We thus filtered out all the pairs that did not appear in the same section of the document, and whose argumentative distance is included between -10 and +10. A second peculiarity of this dataset is the presence of components that include non-argumentative material. These "split components" are made of two sequences x and y separated by a third, non-argumentative sequence z. In those cases, we split x and y into two unrelated components, and attributed them the same label, the same links, and the same argumentative relations with the other components. The resulting dataset consists of about 8,700 links out of 240,000 possible pairs, which amount roughly to 3.6%. Among these links, SUPPORTS amount to 89%, CONTRADICTS to 10%, and the remaining 1% are SEMANTICALLY SAME relations.</p><p>Regarding UKP, like others did before us <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[13]</ref>, we also consider exclusively pairs of components that belong to the same paragraph. However, many paragraphs contain only a single component. That is the case, for instance, with about 400 paragraphs containing a single major claim. In order to include also them in our pair-based classification method, we decided to introduce "self pairs" into our dataset, which are instances where the same component acts both as source and target. This significantly increases the number of pairs (from 22,000 to 28,000). So, to improve optimization and enable a comparison with previous approaches, we did not consider these pairs for link prediction and relation classification in validation and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with other methods</head><p>Not all the approaches to AM are easily compared against one another. This is the case, for example, of approaches that perform only few tasks versus end-to-end systems, or pipeline versus joint learning approaches. Since we perform component classification on propositions or sentences, to make our results comparable with architectures that perform it token-wise, we split each classified component into tokens that share the same label, and compute the evaluation of token-wise classification. Since the tokenization method may not be the same one used by other approaches, the final results may not be perfectly comparable, but we believe that this minor difference will not introduce appreciable errors.</p><p>We shall also remark that in our approach we consider argumentative components as already selected and perfectly bounded, therefore we perform component classification only between argumentative classes and we do not consider the "non-argumentative" class as a possibility. This makes our figures incomparable against those obtained by architectures that address both component identification and classification at once, such as <ref type="bibr" target="#b32">[32]</ref>, since they include "non-argumentative" among the possible classes and thus address a harder problem. A similar consideration holds regarding the pipeline approaches that perform evaluation of each step based on the result of the previous one instead of using the gold standard. In this case, the errors introduced by early steps introduce noise which may affect the evaluation of subsequent steps. It is once again the case of <ref type="bibr" target="#b32">[32]</ref>, where errors obtained during the first step may introduce noise in the link prediction/relation classification tasks. We could not find a solution to this problem, but we argue that, nevertheless, a qualitative evaluation of our method can still benefit from a comparison with these other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization</head><p>We shall remark that the hyper-parameters of the architecture and of the learning model have been tuned on the validation set of CDCP. It is also important to highlight that we use the same set of hyper-parameters in all the experiments. Our purpose is to test whether our approach can yield satisfactory results across different and heterogeneous corpora without the need of re-tuning, and therefore limiting its cost and its environmental impact <ref type="bibr" target="#b39">[39]</ref>. Nonetheless, we are aware that performing a specific calibration for each corpus would probably improve our results. We use the Adam optimizer <ref type="bibr" target="#b62">[61]</ref> with parameters b 1 = 0.9 and b 2 = 0.9999, applying proportional decay of the initial learning rate α 0 = 5 × 10 −3 . The weights of the four components of the loss function are set to 1 for the cross entropy of source and target, 10 for the cross entropy of relation, and 10 −4 for the regularization. The training was early-stopped after 100 epochs with no improvements on the F 1 score of the Link class computed over validation data, except for DrInventor, where we earlystopped after 20 epochs of patience due to the dataset's size and much heavier computational footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS AND DISCUSSION</head><p>In this section we will present the experimental results obtained for each corpus. We will compare the RESATTARG and RESARG architectures, thus assessing the impact of the attention module, and we will also analyze the performance gain introduced by the ensemble approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CDCP</head><p>We used the same validation set as in our earlier work <ref type="bibr" target="#b6">[7]</ref>, which was created by randomly selecting documents from the original training split with 10% probability. We used the remaining documents as training data and the original test split as is. To provide a summary evaluation, following <ref type="bibr" target="#b13">[13]</ref>, we measured the performance of the models by computing the F 1 score for links, propositions, and the average between the two. More specifically, for the links we measured the F 1 of the positive classes, whereas for the propositions we used the score of each class and then we computed the macro-average. We also reported the F 1 score for each relation class, alongside their macro-average. The NONE class of relation classification corresponds to the negative class of link prediction.</p><p>A first question we address was whether our results with a single model were solid or to what extent influenced by the non-deterministic nature of the training procedure. We compared our baseline model with the average scores obtained by 10 networks, with our ensemble setting, and against the structured approach used in <ref type="bibr" target="#b13">[13]</ref>. The results are shown in <ref type="table" target="#tab_1">Table I</ref>. The average computed over the 10 networks leads to a worse performance on Link prediction with respect to our previous results. This difference suggests that our previous results were the result of a "lucky" training. Nonetheless, the average score on the two tasks remains similar (between 47 and 48), just a few points below the state of the art. The ensemble approach substantially improves the results, outperforming the structured learning approach on both tasks. The results on link prediction are still below those obtained in the first experiment, if only by less than 1%.</p><p>Introducing the attention module in the architecture leads to appreciable improvements for both the average and the ensemble approach. In particular, the the latter's performance marks a new state-of-the-art for this corpus, even for the relation classification task. As far as relation label prediction, our approaches fail to predict the EVIDENCE relation. it is a negative result, but hardly surprising, since EVIDENCE is a rather rare class in this dataset (less than 1% of all relations).</p><p>To estimate the agreement among the networks in the ensemble architecture, and have a measure of the architecture's robustness against the implicit randomness of the training procedure, we have computed Krippendorff's alpha <ref type="bibr" target="#b63">[62]</ref> for the three tasks. We obtained α = 0.68 for component classification, and α = 0.50 for both link prediction and relation classification. These values are similar to the IAA obtained by the authors of the corpus, and confirm the difficulty of the link prediction task. <ref type="figure">Figure 2</ref> shows confusion matrices for component classification on CDCP. Unsurprisingly, the most common mistake regards the prediction of facts as values -VALUE being by far the largest class in the corpus, and so affected by many false positives. Such an ambiguity between the two classes has also been reported during the annotation process.</p><p>Interestingly, the confusion matrices of the structured approach and of our methods are quite similar. We speculate that our networks may have learned a behavior similar to that produced by the structured approach, with no need to receive any of the constraints or information regarding the argumentative structure that are instead injected in the structured approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. AbstRCT</head><p>For what concerns AbstRCT, we compare our architectures against the best methods presented by its authors <ref type="bibr" target="#b32">[32]</ref>, whose results are reported in the first rows of <ref type="table" target="#tab_1">Tables II and III.</ref> We trained and validated our model on the respective splits of the Neoplasm dataset, using the remainder of the dataset for testing. For reasons we already explained, the approach presented by Mayer et al. <ref type="bibr" target="#b32">[32]</ref> is not directly comparable with ours, therefore the comparison can only be qualitative. To ease comparison with future approaches, we report in <ref type="table" target="#tab_1">Table IV</ref> some additional details on our results.</p><p>As for component classification, RESATTARG with ensemble yields the best result, performing comparably with the state of the art. Our approaches obtain better scores for EVIDENCE than CLAIM on all datasets. Similarly to the Transformerbased approaches, our architectures perform better on the mixed test set than on the neoplasm one. We yield better results on all datasets for what concerns the micro f 1 score. However, for what concerns macro F 1 , although our architecture improves the previous approaches on Neoplasm, it is outperfomed by BioBERT on Glaucoma and Mixed. In relation classification, RESATTARG with ensemble outperforms all the other models on Neoplasm and Glaucoma, and it performs about 1% worse than SciBERT on Mixed. It is interesting to notice that in this task BioBERT is largely outperformed by our approach. Almost all the metrics confirm that the introduction of attention and ensemble improve our architectures. The agreement between the networks RESATTARG is very high for token-wise component classification in each dataset (α between 0.81 and 0.83), and lower but still acceptable for the other two tasks (α = 0.67 on neoplasm and α = 0.62 for the other two).</p><p>These good results indicate that our method may be a valuable approach with well-structured corpora. Moreover, such results are attained without resorting to contextual embeddings or pre-training on domain-related corpora, but by only relying on non-contextual, general-purpose embeddings.  <ref type="bibr" target="#b6">[7]</ref> obtained with a single training of RESARG, the average scores of the same architecture trained 10 times, the scores of the ensemble learning setting of the same model, the average and the ensemble scores of the new attention-based architecture RESATTARG, and the best results of structured approaches based on SVM and RNN. For each class, the number of instances is reported in parenthesis. F 1 and macro F 1 percentage scores are reported.  <ref type="bibr" target="#b6">[7]</ref>, RESARG used in ensemble fashion, and RESATTARG used in ensemble fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DrInventor</head><p>To the best of our knowledge, the only approach tested on this corpus is the architecture for token-wise component classification used by Lauscher et al. <ref type="bibr" target="#b12">[12]</ref>, which makes use of GloVe embeddings and a Bi-LSTM followed by a feed-forward neural network with a single hidden layer as classifier. We thus consider such an approach as a baseline. Like Lauscher et al., we reserved 30% of the documents of the DrInventor corpus as test set, and 20% of the remaining part as validation set. It is worth remarking that for the tasks of link prediction and relation classification we are considering a limited number of pairs. Tables V and VI includes a detailed report of our performance on the dataset. We outperform the baseline by a wide margin. Moreover, we address two additional tasks, link prediction and relation classification, thus offering a benchmark for future work. These results confirm once more that attention and ensemble together give a crucial contribution to the classifier.</p><p>Differently from previous experiments, the agreement between the networks RESATTARG is similar for all the tasks, with only α = 0.56 for component classification and α = 0.60 for the remaining tasks. The agreement for Component Classification is lower than on the previous datasets and may suggest that this dataset is more challenging.</p><p>Our model is incapable of classifying the SEMANTI-CALLY SAME relation and has difficulties also with CON-TRADICTS. That is hardly surprising, if we consider that these are the two least represented classes in this dataset. It is less straightforward to understand why the model is better at classifying BACKGROUND CLAIM rather than DATA, even if the latter are more represented than the former. We speculate it may be related to the fact that in some instances data may amount to citations or text other than proper sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. UKP</head><p>UKP comes with two strong baselines: the ILP joint model proposed by the authors of the dataset <ref type="bibr" target="#b3">[4]</ref> and Niculae et al.'s structured learning approach <ref type="bibr" target="#b13">[13]</ref>. We compared based on the original test split of the dataset, using about 10% of the documents of the training split as validation split. Apparently our approach is largely below the baselines, with a difference in F 1 scores between 20% and 30%. The agreement between the networks is also low, with α = 0.57 for component classification and α = 0.38 for link prediction, assessing them as nearly acceptable for the first task but unreliable for the others. We interpret these results as an indication that a general-purpose and domain-agnostic architecture such as ours struggles with datasets characterized by strong regularities that can be best exploited by domain-oriented approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>In this paper we presented RESATTARG, a new neural architecture for argument mining based on residual networks, II: Results of component classification on AbstRCT. We report the F 1 score related to the micro average, the macro average, the EVIDENCE class, and the CLAIM class obtained on the 3 test sets. Our approach is not directly comparable with component classification of <ref type="bibr" target="#b32">[32]</ref> and the comparison must be considered qualitative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neoplasm</head><p>Glaucoma Mixed Level</p><p>Approach   multi-task learning, neural attention, and ensemble learning. Our approach does not rely on domain-tailored features or encodings. On the contrary, it only uses general-purpose embeddings and a broadly applicable distance feature, making it suitable for any domain and argumentative model. Moreover, RESATTARG is considerably smaller than other state-of-theart approaches, making it less expensive to train, and more sustainable from an environmental perspective <ref type="bibr" target="#b39">[39]</ref>. RESATTARG outperforms state-of-the-art architectures on a variety of tasks and datasets. We conducted ablation studies to demonstrate that the attention module and the ensemble learning addition give a positive contribution, improving on our previous architecture <ref type="bibr" target="#b6">[7]</ref>. The use of ensemble also in-  creases the robustness of this approach against the intrinsic randomness of neural architecture training. The main limitations of RESATTARG are its seemingly poor performance on datasets characterized by strong regularities and its limited scalability to large documents. To fill this gap, we believe that neural-symbolic approaches <ref type="bibr" target="#b64">[63]</ref> may enable a systematic and modular integration of background knowledge. Such a knowledge would contribute during the optimization process, so as to influence and improve the training, without compromising the generality of the neural architecture. For what concerns scalability, we have addressed this problem by limiting the range of argumentative relationships using a fixed-size window, with the drawback of imposing a constraint on the model of the argument. Alternatively to our pair-based approach, other authors have proposed multiple-choice classifiers <ref type="bibr" target="#b32">[32]</ref>, pointer networks <ref type="bibr" target="#b23">[23]</ref>, and sequence labelling <ref type="bibr" target="#b9">[9]</ref>. Such methods should scale better, but they enforce a constraint on the argument model as well, imposing that any component can have only one outgoing relationship, which makes them unsuitable to some corpora. While successful approaches to argument retrieval have been recently published <ref type="bibr" target="#b66">[64]</ref>, scalability is still an open challenge for AM systems aiming to reconstruct the argumentation structure.</p><formula xml:id="formula_4">f 1 F 1 E C f 1 F 1 E C f 1 F 1 E C</formula><p>Andrea Galassi holds a Master Degree in Computer Engineering, and is a PhD student and Research Fellow at the Department of Computer Science and Engineering at the University of Bologna. His research activity concerns artificial intelligence and machine learning, focusing on argumentation mining and related NLP tasks. Other research interests involve deep learning applications to games and CSPs.</p><p>Marco Lippi is associate professor at the Department of Sciences and Methods for Engineering, University of Modena and Reggio Emilia. He previously held positions at the Universities of Florence, Siena and Bologna, and he was visiting scholar at UPMC, Paris. His work focuses on machine learning and artificial intelligence, with applications to several areas, including argumentation mining, legal informatics, and medicine. In 2012 he was awarded the "E. Caianiello" prize for the best Italian PhD thesis in the field of neural networks.</p><p>Paolo Torroni is an associate professor with the University of Bologna since 2015. His main research focus is artificial intelligence. He edited over 20 books and special issues and authored over 150 articles in computational logics, multi-agent systems, argumentation, and natural language processing. He serves as an associate editor for Fundamenta Informaticae and Intelligenza Artificiale.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>A block diagram of the proposed architectures. The figure shows, next to each arrow, the dimensionality of the data involved (using the CDCP temporal size T = 153), so as to clarify the size of the inputs and the outputs of each block. a s = softmax(e s )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and P. Torroni are with the Department of Computer Science and Engineering (DISI), University of Bologna, Bologna 40126, Italy (e-mail: a.galassi@unibo.it; paolo.torroni@unibo.it).</figDesc><table /><note>M. Lippi is with the Department of Sciences and Methods for Engineering (DISMI), University of Modena and Reggio Emilia, Modena 42122, Italy (e-mail: marco.lippi@unimore.it).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Results of the experiments involving multiple trainings of the same models and use of attention on CDCP. From left to right: our previous result</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.20 93.58 82.81 91.84 87.49 94.86 80.11 91.25 87.79 94.29 81.29 RESARG+Ensemble 90.75 88.10 93.72 82.47 92.50 88.48 95.28 81.68 91.61 88.21 94.54 91.61 RESATTARG (avg) 90.80 88.60 93.59 83.61 92.02 88.02 94.93 81.11 91.58 88.72 94.40 83.04 RESATTARG+Ensemble 92.12 90.14</figDesc><table><row><cell></cell><cell>Transformer-based [32]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>BioBERT+GRU+CRF</cell><cell>90</cell><cell>84</cell><cell>90</cell><cell>87</cell><cell>92</cell><cell>91</cell><cell>91</cell><cell>93</cell><cell>92</cell><cell>91</cell><cell>92</cell><cell>91</cell></row><row><cell></cell><cell>SciBERT+GRU+CRF</cell><cell>90</cell><cell>87</cell><cell>92</cell><cell>88</cell><cell>91</cell><cell>89</cell><cell>91</cell><cell>93</cell><cell>91</cell><cell>88</cell><cell>93</cell><cell>90</cell></row><row><cell>Token</cell><cell>Our approach</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>RESARG (avg)</cell><cell cols="4">90.66 8894.56 85.72</cell><cell>92.92</cell><cell cols="3">89.35 95.52 83.19</cell><cell>92.79</cell><cell cols="3">90.26 95.23 85.30</cell></row><row><cell></cell><cell>Our approach</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>RESARG (avg)</cell><cell cols="12">87.42 86.18 90.31 82.04 88.08 85.53 91.59 79.48 88.20 86.74 91.13 82.35</cell></row><row><cell>Component</cell><cell>RESARG+Ensemble</cell><cell cols="12">87.76 86.38 90.71 82.05 89.39 87.13 92.53 81.74 89.00 87.59 91.77 83.42</cell></row><row><cell></cell><cell>RESATTARG (avg)</cell><cell cols="12">87.32 86.19 90.11 82.27 88.50 86.26 91.79 80.72 88.65 87.51 91.27 83.74</cell></row><row><cell></cell><cell cols="3">RESATTARG+Ensemble 88.92 87.87</cell><cell cols="2">91.44 84.30</cell><cell cols="2">89.73 87.71</cell><cell cols="2">92.69 86.54</cell><cell cols="2">90.67 89.70</cell><cell cols="2">92.86 82.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Results of relation classification on AbstRCT. We report the macro averaged F 1 score obtained on the 3 test sets.</figDesc><table><row><cell></cell><cell cols="3">Neoplasm Glaucoma Mixed</cell></row><row><cell>Approach</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer-based [32]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BioBERT</cell><cell>64</cell><cell>58</cell><cell>61</cell></row><row><cell>SciBERT</cell><cell>68</cell><cell>62</cell><cell>69</cell></row><row><cell>RoBERTa</cell><cell>67</cell><cell>66</cell><cell>67</cell></row><row><cell>Our approach</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RESARG (avg)</cell><cell>59.15</cell><cell>57.23</cell><cell>60.31</cell></row><row><cell>RESARG+Ensemble</cell><cell>63.16</cell><cell>61.86</cell><cell>68.35</cell></row><row><cell>RESATTARG (avg)</cell><cell>66.49</cell><cell>62.68</cell><cell>63.47</cell></row><row><cell>RESATTARG+Ensemble</cell><cell>70.92</cell><cell>68.40</cell><cell>67.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Results of RESATTARG with Ensemble forLink Prediction and Relation Classification on AbstRCT. The "Link" column refers to the F 1 score of the positive class. The "Relation" column reports the result of the macro F 1 score. The other columns report the F 1 score of the respective classes.</figDesc><table><row><cell></cell><cell>Link</cell><cell>Relation</cell><cell cols="3">SUPPORT ATTACK NONE</cell></row><row><cell>Test set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Neoplasm</cell><cell>54.43</cell><cell>70.92</cell><cell>52.77</cell><cell>65.38</cell><cell>94.54</cell></row><row><cell cols="2">Glaucoma 55.23</cell><cell>68.40</cell><cell>54.73</cell><cell>56.00</cell><cell>94.36</cell></row><row><cell>Mixed</cell><cell>51.20</cell><cell>67.66</cell><cell>49.62</cell><cell>59.09</cell><cell>94.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>Results of component classification on DrInventor."Average" is the macro F 1 score, the remaining columns report the F 1 score of the classes OWN CLAIM, BACKGROUND CLAIM, DATA.</figDesc><table><row><cell>Level</cell><cell>Approach</cell><cell>Average</cell><cell>O C</cell><cell>B C</cell><cell>D</cell></row><row><cell></cell><cell>Bi-LSTM [12]</cell><cell>44.70</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Token</cell><cell>RESARG (avg) RESARG+Ens</cell><cell>58.27 61.16</cell><cell cols="3">72.31 51.36 51.14 75.77 54.24 53.48</cell></row><row><cell></cell><cell>RESATTARG (avg)</cell><cell>61.77</cell><cell cols="3">73.66 57.70 53.95</cell></row><row><cell></cell><cell>RESATTARG+Ens</cell><cell>65.71</cell><cell cols="3">78.03 61.58 57.53</cell></row><row><cell></cell><cell>RESARG (avg)</cell><cell>60.62</cell><cell cols="3">65.65 45.63 70.56</cell></row><row><cell>Component</cell><cell>RESARG+Ens RESATTARG (avg)</cell><cell>62.97 66.19</cell><cell cols="3">68.97 48.03 71.90 68.61 56.07 73.89</cell></row><row><cell></cell><cell>RESATTARG+Ens</cell><cell>69.64</cell><cell cols="3">73.13 59.63 76.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI :</head><label>VI</label><figDesc>Results of RESATTARG with Ensemble for Link Prediction, and Relation Classification on DrInventor. The "Link" column refers to the F 1 score of the positive class. The "Relation" column reports the result of the macro F 1 score. The other columns report the F 1 score of the respective classes.</figDesc><table><row><cell>Link</cell><cell cols="3">Relation SUPP CON</cell><cell>SEM</cell><cell>NONE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SAME</cell><cell></cell></row><row><cell>43.66</cell><cell>37.72</cell><cell>45.90</cell><cell>6.61</cell><cell>0</cell><cell>98.37</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We will partially consider reflexive relations for the UKP dataset for a specific reason explained in Section V.<ref type="bibr" target="#b2">3</ref> Since we examine only argumentative propositions, we do not consider the non-argumentative class for component classification.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Glaucoma and neoplasm documents of the mixed set are present also in the respective test set.<ref type="bibr" target="#b4">5</ref> The corpus allows also the distinction between CLAIM/MAJOR CLAIM and ATTACK/PARTIAL ATTACK. For the sake of consistency with previous works, this detail will not be considered.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In fact, each component is linked to a MAJOR CLAIM via an attribute called stance. Therefore, one could use this dataset for stance detection, by creating explicit relationships toward the major claims. However, that would be outside the scope of this work.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Argumentation mining: State of the art and emerging trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2850417</idno>
		<ptr target="http://doi.acm.org/10.1145/2850417" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Internet Technol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Argument mining: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="765" to="818" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Argumentation theory: A very short introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Argumentation in Artificial Intelligence</title>
		<editor>G. Simari and I. Rahwan</editor>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parsing argumentation structures in persuasive essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="619" to="659" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end argumentation mining in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL. The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1384" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Claims on demand -An initial demonstration of a system for automatic detection and polarity identification of context dependent claims in massive corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alzate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dankin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Eiron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hershcovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polnarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zwerdling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Konopnicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfreund</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C14-2002" />
	</analytic>
	<monogr>
		<title level="m">COLING 2014</title>
		<editor>L. Tounsi and R. Rak</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Argumentative link prediction using residual networks and multi-objective learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Argument Mining</title>
		<meeting>the 5th Workshop on Argument Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.18653/v1/w18-5201</idno>
		<ptr target="https://doi.org/10.18653/v1/w18-5201" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Argumentation mining in user-generated web discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/J17-1004" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="179" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural end-to-end learning for computational argumentation mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multitask learning for argumentation mining in low-resource settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kahse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Investigating the role of argumentation in the rhetorical analysis of scientific publications with neural multi-task learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Glavas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="3326" to="3338" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Argument mining with structured svms and rnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="985" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.3019893</idno>
		<ptr target="https://doi.org/10.1109/TNNLS.2020.3019893" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Argument annotation and analysis using deep learning with attention mechanism in bahasa indonesia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suhartono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Gema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Fanany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Arymurthy</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-020-00364-z</idno>
		<ptr target="https://doi.org/10.1186/s40537-020-00364-z" />
	</analytic>
	<monogr>
		<title level="j">J. Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lexicon guided attentive neural network model for argument mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ArgMining@ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="67" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-topic argument mining from heterogeneous sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP. ACL</title>
		<imprint>
			<biblScope unit="page" from="3664" to="3674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Is it worth the attention? a comparative evaluation of attention layers for argument unit segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spliethöver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heuer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-08" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="74" to="82" />
			<pubPlace>ArgMining@ACL. Florence, Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What makes a convincing argument? empirical analysis and detecting attributes of convincingness in web argumentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Here&apos;s my point: Joint pointer architecture for argument mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Potash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1364" to="1373" />
			<date type="published" when="2017" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1). ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classification and clustering of arguments with contextualized word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="567" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Contextual argument component classification for class discussions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING. International Committee on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1475" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Argumentation mining on essays at multi scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING. International Committee on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5480" to="5493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fine-grained argument unit recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trautmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI. AAAI Press</publisher>
			<biblScope unit="page" from="9048" to="9056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">ECHR: Legal corpus for argument mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poudyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Savelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ieven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Quaresma</surname></persName>
		</author>
		<editor>ArgMining. Online</editor>
		<imprint>
			<date type="published" when="2020-12" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="67" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.11692" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transformer-based argument mining for healthcare applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Villata</surname></persName>
		</author>
		<idno type="DOI">10.3233/FAIA200334</idno>
		<ptr target="https://doi.org/10.3233/FAIA200334" />
	</analytic>
	<monogr>
		<title level="m">ECAI 2020, ser. Frontiers in Artificial Intelligence and Applications</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">325</biblScope>
			<biblScope unit="page" from="2108" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btz682" />
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1371" />
		<editor>EMNLP, K. Inui, J. Jiang, V. Ng, and X. Wan</editor>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="3613" to="3618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep residual learning for weaklysupervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1803" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An annotated corpus of argumentative microtexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peldszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Argumentation and Reasoned Action: Proceedings of the 1st European Conference on Argumentation</title>
		<meeting><address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="801" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Can deep networks learn to play by the rules? A case study on nine men&apos;s morris</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chesani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Games</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="344" to="353" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TG.2018.2804039</idno>
		<ptr target="https://doi.org/10.1109/TG.2018.2804039" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Model agnostic solution of CSPs via deep learning: A preliminary study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milano</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93031-2_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-93031-218" />
	</analytic>
	<monogr>
		<title level="m">CPAIOR, ser</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">10848</biblScope>
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<ptr target="https://web.stanford.edu/∼jurafsky/slp3/" />
		<title level="m">Speech and Language Processing</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>3rd Ed. draft</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS, ser. JMLR Proceedings</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI. IJCAI.org</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="338" to="348" />
			<date type="published" when="2017" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00058655</idno>
		<ptr target="https://doi.org/10.1007/BF00058655" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A survey on ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="258" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A corpus of erulemaking user comments for measuring evaluability of arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC. European Language Resources Association (ELRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Toward machine-assisted participation in erulemaking: an argumentation model of evaluability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAIL</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="206" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Argument mining on clinical trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COMMA, ser. Frontiers in Artificial Intelligence and Applications</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">305</biblScope>
			<biblScope unit="page" from="137" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An argument-annotated corpus of scientific publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Glavas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArgMining@EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="40" to="46" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A multi-layered annotated corpus of scientific papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fisas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ronzano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC. European Language Resources Association (ELRA)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Content analysis: An introduction to its methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krippendorff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sage publications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Neuralsymbolic argumentation mining: An argument in favor of deep learning and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<idno type="DOI">10.3389/fdata.2019.00052</idno>
		<ptr target="https://doi.org/10.3389/fdata.2019.00052" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Corpus wide argument mining -A working solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ein-Dor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shnarch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dankin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halfon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sznajder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alzate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleize</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7683" to="7691" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
