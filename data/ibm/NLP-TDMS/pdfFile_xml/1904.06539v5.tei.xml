<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HAKE: Human Activity Knowledge Engine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
							<email>li@sjtu.edu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
							<email>liangxu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
							<email>xinpengliu0907@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
							<email>huangxijie1108@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HAKE: Human Activity Knowledge Engine</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human activity understanding is crucial for building automatic intelligent system. With the help of deep learning, activity understanding has made huge progress recently. But some challenges such as imbalanced data distribution, action ambiguity, complex visual patterns still remain. To address these and promote the activity understanding, we build a large-scale Human Activity Knowledge Engine (HAKE) based on the human body part states. Upon existing activity datasets, we annotate the part states of all the active persons in all images, thus establish the relationship between instance activity and body part states. Furthermore, we propose a HAKE based part state recognition model with a knowledge extractor named Activity2Vec and a corresponding part state based reasoning network. With HAKE, our method can alleviate the learning difficulty brought by the long-tail data distribution, and bring in interpretability. Now our HAKE has more than 7 M+ part state annotations and is still under construction. We first validate our approach on a part of HAKE in this preliminary paper, where we show 7.2 mAP performance improvement on Human-Object Interaction recognition, and 12.38 mAP improvement on the one-shot subsets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human activity understanding is an active topic in computer vision and has a large number of potential applications and business prospects. Facilitated by the growth of image and video data and the renaissance of Deep Neural Networks (DNNs), lots of works have been proposed to forward this direction. Activity recognition has strong rela-Draft, work in progress. *Cewu Lu is the corresponding author. tions with other research contents of computer vision, such as object detection <ref type="bibr" target="#b19">[20]</ref>, pose estimation <ref type="bibr" target="#b14">[15]</ref>, video analysis <ref type="bibr" target="#b10">[11]</ref>, visual relationship <ref type="bibr" target="#b17">[18]</ref>. Recent works on activity and action recognition almost rely on the end-to-end supervised paradigm to address this high-level cognition task, i.e. perception from raw pixels directly to the activity classes in one stage. This paradigm shows poor performance on large-scale activity benchmarks, such as HICO <ref type="bibr" target="#b12">[13]</ref>, HICO-DET <ref type="bibr" target="#b11">[12]</ref>, AVA <ref type="bibr" target="#b9">[10]</ref>.</p><p>The limited performance of present one-stage paradigm on these large-scale and exceedingly difficult datasets are possibly due to the own difficulties of activity understanding. For instance, activity recognition has many challenges such as long-tail data distribution, variability and complexity of action visual patterns, crowd background in daily scenes, various camera viewpoints and motions, occlusion and self-occlusion, crowd-sourced annotations and data. In the absence of data, one-stage paradigm which needs to bridge the huge gap is powerless. To this end, we propose a new Human Activity Knowledge Engine (HAKE) based on body part states <ref type="bibr" target="#b8">[9]</ref>. Based on HAKE, a new corresponding hierarchical two-stage paradigm for activity recognition is also presented.</p><p>Different from the one-stage paradigm, we divide the activity understanding into two phases: 1. Part states <ref type="bibr" target="#b8">[9]</ref> recognition from the visual patterns, and the activity representation by combining visual and linguistic knowledge; 2. Reasoning the activities from part states, as seen in <ref type="figure" target="#fig_1">Fig. 2</ref>. Part states mean the finer level atomic body part actions <ref type="bibr" target="#b8">[9]</ref> which compose the action of human instance. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an example of instance activity and its corresponding part states. Based on the reductionism <ref type="bibr" target="#b7">[8]</ref>, our assumption is that: the human instance action consists of the atomic actions or states of all the body parts. Thus we can divide the instance action recognition into two sub-tasks: body part state recognition and the recombination of part states.</p><p>The most obvious advantages of our hierarchical paradigm are three-fords. First, part states are the basic components of instance actions, their relationship can be in analogy with the amino acid and protein, letter and word. Different instance actions like "person hold an apple" and "person eats an apple" share the same part states "hand holds something" and "head looks at something". Thus the imbalanced data problem will be greatly alleviated, for that the samples per category largely increases on the same data scale. For supervised learning, it will effectively reduce the learning difficulty. Furthermore, part state recognition is much easier than the instance action recognition because of fewer categories and simpler visual patterns. In our experiment, a simple model consists of shallow CNNs and fully connected layers can achieve acceptable perfor-mance on part state recognition, which is generally relative 50% higher than the instance action recognition. Second, with part state recognition as the midpoint, the gap between the image space and the semantic space would be greatly narrowed. Third, we can obtain a more powerful representation of action patterns based on part states. In our experiment, the combinative visual-linguistic part state embeddings present obvious semantic meaning and better interpretation. When the model predicts what he/she is doing, we can easily know the reasons: what his/her body parts are doing.</p><p>The main contributions of this work are: 1. We construct a large-scale Human Activity Knowledge Engine named HAKE that bridges the relationship between instance activity and body part states. We will keep on enlarging and enriching it, and call on the community to help us make it more powerful to promote activity understanding. 2. A new hierarchical paradigm is proposed based on HAKE, which outperforms state-of-the-art methods on several activity recognition benchmarks. In particular, the performance of rare action categories on several benchmarks are significantly boosted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Construction of HAKE</head><p>In this section, we will illustrate the construction of HAKE. Considering the complexity, we first construct part states annotations on still images, and then expand to the consecutive frames of videos. The key characteristic of our HAKE are: the definition of part states are based on atomic and composite actions, crowd-sourced images from several widely-used activity datasets, realistic visual contexts, di-  versity and variability of activities. Part States Definition. HAKE is based on the existing well-designed datasets, for example, HICO-DET <ref type="bibr" target="#b11">[12]</ref>, V-COCO <ref type="bibr" target="#b15">[16]</ref>, OpenImage <ref type="bibr" target="#b16">[17]</ref>, HCVRD <ref type="bibr" target="#b21">[22]</ref>, HICO <ref type="bibr" target="#b12">[13]</ref>, MPII <ref type="bibr" target="#b0">[1]</ref>, AVA <ref type="bibr" target="#b9">[10]</ref>, which are structured around a rich semantic ontology. The activity categories contained in HAKE are chosen according to the most common human daily actions/activities, social interactions with daily objects and person. We first select 154 instance activity categories from above datasets in the case of hierarchical activity structure <ref type="bibr" target="#b10">[11]</ref>. All the part states will be annotated upon instance level activities. Then we decompose the human body into ten body parts following <ref type="bibr" target="#b13">[14]</ref>, namely head, arms, hands, hip, legs, feet. Third, we select about 200 part states based on the verbs from WordNet <ref type="bibr" target="#b1">[2]</ref> as the candidates to build a part state pool, e.g. "hold", "push", "pick" for hands, "listen to", "eat", "talk to" for head and so on.</p><p>To ensure the quality of part state selection, we invite several experts to use their own understandings to depict the selected 154 instance actions in the body part level. For example, when we show an image with activity "person drive a car" to them, they may describe it as "hip sit on something", "hands hold something", "head look at something". Based on their choices, we use the Normalized Point-wise Mutual Information (NPMI) <ref type="bibr" target="#b2">[3]</ref> to calculate the co-occurrence between the instance action categories and part state candidates. Finally, we choose 92 candidates with the highest NPMI values as the final part states.</p><p>Based on 154 instance activities and 92 part states, we can construct a hierarchical graph structure shown in <ref type="figure" target="#fig_3">Fig.4</ref>  Actions and part states are represented as the edges between the subject and object nodes (for the non-object actions, the edge is a loop). Part State Annotation. We annotate all part states belonging to all the actions of all the active persons in all the collected images exhaustively. To be specific, existing datasets already have the human and object bounding box annotations and the relationship links between them. We then use pose estimation <ref type="bibr" target="#b14">[15]</ref> to obtain the pose keypoints of all the annotated persons, and their part bounding boxes following <ref type="bibr" target="#b13">[14]</ref>. We adopt a semiautomatic method to build HAKE. First, we invite nine experts to annotate 10 thousands of images with all the 154 instance actions as the basis, and generate the initial part states for all the rest of images based on their annotation distribution. Thus the other annotators will use our tool to amend and refine the initial annotations according to their understanding of these actions. To ensure the quality, one instance with multiple actions would be annotated multiple times for each activity. Furthermore, each image will be checked at least twice by the automatic procedure and experts. We cluster these labels and discard the obvious outliers to obtain the robust label agreements. It is worth noting that, action recognition is a multi-label classification problem, an active person may have more than one actions. For each instance action, we annotate its corresponding part states respectively and then combine all sets of part states in the final round. In other words, a body part can also have multiple states at the same time, e.g., activity "person cuts an apple" would have part states "right-hand holds a knife" and "right-hand uses something to cut something" at the same time.</p><p>At present, we have finished the annotations of 104 K+ images, which include 677 k+ human instances, 278 K+ interacted objects, 733 K+ instance actions, 7 M+ human body part states. In addition, our labeling is still in progress, and we have build a project page (http://hake-mvig. cn) and an online annotation tool. We hope the volunteers from all over the world to help us enlarge and enrich HAKE to advance the activity understanding. With these densely annotated part states, we believe in that we can deepen and promote the activity understanding significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hierarchical Paradigm</head><p>On the basis of our human activity knowledge engine, we can address the activity recognition in a hierarchical way: 1. Part State Recognition with knowledge extraction via Activ-ity2Vec; 2. Reasoning from Part States to Instance Activity. This hierarchy would bring in more interpretability and a new possibility for the following-up researches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Part State Recognition and Activity2Vec</head><p>In the first phase, we utilize the canonical pipeline to address the part state recognition. With the object detection of images, we can obtain the bounding boxes of all the detected person and objects. Second, we extract the ROI pooling features of all body parts as the input of the Part States Classification Network (PSC), as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. Within HAKE, we have annotated all the part states of all the human instances, thus we can construct part state classification loss for each part. As mentioned before, part state recognition is much easier in the supervised learning paradigm, which is also proven by our experiments. Besides, we also use part-level interactiveness predictors <ref type="bibr" target="#b20">[21]</ref> for all parts, which can infer the relationship between each body part and the object. If a body part has interactions with the object, thus its interactiveness label will be one. These interactiveness labels are also converted from the part states labels. More details can be found in <ref type="bibr" target="#b20">[21]</ref>. With the part-level in-  <ref type="figure">Figure 5</ref>. Samples of activity reasoning via part states. By combining the classified part states, we can reason out the activity from co-occurrence and prior knowledge. Inner relations between part states can also be helpful. For instance, holding an apple by hand and eating an apple with mouth often appear together. These two part states can derive the activity human, eat, apple . teractiveness predictions, we can obtain the body part attentions, which indicate whether a part is important for the action recognition. For example, in the action "person eat an apple", only the hands and head are essential for the action classification. All the visual body part features will be multiplied with interactiveness attentions first.</p><p>To enhance the representation ability and promote the subsequent activity reasoning, we additionally utilize the uncased BERT-Base pre-trained model <ref type="bibr" target="#b3">[4]</ref> as the language feature extractor. Bert <ref type="bibr" target="#b3">[4]</ref> is a language understanding model trained on large-scale word corpus, it can generate contextual embeddings for many types of downstream NLP tasks. Bert has considered the surrounding (left and right) of a word, and use a deep bidirectional Transformer to extract the general embeddings of words. Thus its pre-trained representations carry the contextual information from the enormous text data, e.g. Wikipedia. These features usually contain implicit semantic knowledge about the activity and part states, which is clearly different from visual information. In specific, we divide a part state into tokens, e.g. "head"(body part), "eat"(verb), "apple"(object). Each part state will be converted to a 2304 vector (three 768 vectors for the part, verb, object respectively). Second, we multiply the fixed language features with predicted part state proba-  bilities from the part state classifiers, which can also be seen as an attention mechanism. A more possible part state will get a larger attention.</p><p>Our goal is to bridge the gap between part states and instance activity. The combination of the visual and linguistic knowledge thus can be a powerful clue for establishing this mapping. We align the visual and linguistic features by using triplet loss <ref type="bibr" target="#b4">[5]</ref>, and concatenate them as the output, this process is called as Activity2Vec. (Seen in <ref type="figure" target="#fig_5">Fig. 6</ref>) The output embedding is 3584 sized and as the input of the downstream tasks, e.g. activity recognition, Visual Question Answering, action retrieval. Especially, before utilizing this embedding, we will first use the activity recognition task to pre-train it to capture the activity knowledge. From the experiment results, we can find that the embedding generated by our Activity2Vec can significantly improve the performance of multiple activity related benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Reasoning from Part States to Instance Activity</head><p>With the embeddings from the Activity2Vec, we can better infer the relationship between part states and activities. If all the activities can be seen as the nodes at a higher level within a hierarchical graph, the part states will be the nodes in the lower level. Inner relationships between part state nodes can be seen as their co-occurrence, so do the activity nodes. The edges linked the instance activity and part states are the key elements in activity understanding, as seen in <ref type="figure">Fig. 7</ref>.</p><p>We propose a Part States Reasoning Network (PSR) to estimate these cross-level edges between activity nodes and part state nodes. In our vanilla version, we directly use the fully-connected layers to infer the actions from the combined part-level features, which obtains surprising improvements. More details of the proposed PSC and PSR models will be illustrated in our official version paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">An analogy: simplified Action Recognition</head><p>In this section, we design a simplified experiment to give a better intuition about our approach. We build a dataset derived from MNIST which consists of handwritten digits from 0 to 9 and with size 28 × 28 × 1. To test our assumption, we generate a set of new 128 × 128 × 1 images which is a combination of 3 to 5 handwritten digits randomly selected from MNIST and randomly distributed in images. And the corresponding label of each new image is the sum of the largest and second largest value of the digits within this image. Thus the total number of categories is 19 (0 to 18). This problem is a simplified analogy of human activity recognition, as shown in <ref type="figure" target="#fig_7">Fig. 8</ref>. We argue that actions can be decomposed into a set of part states, which highly resembles the relationship between the digits and the sum function in the above case. The random amount of digits and their distribution aim to simulate the randomness of part states. Taking the influence of background into consideration, we also add Gaussian noise on the whole generated image samples.</p><p>To compare the one-stage (instance based) paradigm and two-stage (part based) paradigm, we adopt a simple network to conduct a test. The network is composed of shallow sequential convolution layers and fully connected layers (shown in <ref type="figure" target="#fig_9">Fig.9</ref>).</p><p>Two paradigms are trained with the same optimizer, learning rate and epochs. The results are shown in <ref type="figure" target="#fig_0">Fig.10</ref> and Tab. 1, which show the significant superiority of part based paradigm (174% relative increases on accuracy) over instance based paradigm. To some extent, this result sup-  <ref type="figure">Figure 7</ref>. Reasoning from part states to instance activities. ports our assumption about the decomposability of human instance activity and the effectiveness of part state knowledge representation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activity Knowledge based Reasoning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implicit relationship between activity and part states</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning via Part States</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analogy: A Toy Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison---Function Fitting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Human-Object Interaction Recognition</head><p>To verify the effectiveness of our HAKE and hierarchical paradigm, we perform experiments on Human-Object Interaction (HOI) recognition task <ref type="bibr" target="#b12">[13]</ref> in still images. HOI <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b20">21]</ref> usually accounts for a large proportion of daily life activities. And it is more complex than the non-HOI activity, e.g. "dance", "swim". In the initial version, we just report the results on HICO <ref type="bibr" target="#b12">[13]</ref> to evaluate the im-provements brought by HAKE, especially on one-shot and few-shot learning problems. More results on other activity understanding tasks will be reported in the official paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Accuracy Instance Based Paradigm 15.2 Part Based Paradigm 41.7   <ref type="bibr" target="#b12">[13]</ref> contains 38,116 images in train set and 9,658 images in test set. To be fair, we follow the experiment settings of <ref type="bibr" target="#b13">[14]</ref> to compare the recognition performance. Considering that our HAKE-based part-level representation is complementary to the instance-level representation, we use the late fusion strategy to generate the final prediction. Method mAP AlexNet+SVM <ref type="bibr" target="#b12">[13]</ref> 19.4 R*CNN <ref type="bibr" target="#b6">[7]</ref> 28.5 Girdhar &amp; Ramanan <ref type="bibr" target="#b5">[6]</ref> 34.6 Mallya &amp; Lazebnik <ref type="bibr" target="#b18">[19]</ref> 36.1 Pairwise <ref type="bibr" target="#b13">[14]</ref> 39  <ref type="table">Table 3</ref>. Effectiveness on few-shot problems. Few@i represent the average mAP on few-shot activity sets. @i means the number of training images is less than i, if i is 1 then it means one-shot problem. On HICO <ref type="bibr" target="#b12">[13]</ref>, there is obvious positive correlation between performance and the quantity of training samples. Our approach can obviously improve the recognition effect on few-shot problem, for the reason of reusability and composability of part states.</p><p>From Tab. 2 we can find that our method achieve 7.2 mAP gain over the state-of-the-art result on HICO <ref type="bibr" target="#b12">[13]</ref>. And when the part state recognition is perfect, we can achieve surprising 62.5 mAP on HICO dataset. That is, if we input the ground truth part states into the Activity2Vec module and use the corresponding output activity representation vector for the classification, the upper bound of HAKE-based method is very high. This is a powerful proof of the representation ability and effectiveness of part states knowledge. Thus what remains to do for the activity understanding is to refine the part states recognition, then we can obtain more powerful activity representations and achieve better performance.</p><p>Moreover, on the one-shot and few-shot sets (training images are less than 5 and 10) of HICO, our method can achieve more than 11 mAP improvement. Specific results can be seen in Tab. 3. These results show that our HAKE and HAKE-based hierarchical paradigm can significantly enhance the learning ability of model under few-shot circumstances.</p><p>Some qualitative results on HOI recognition are shown in <ref type="figure" target="#fig_0">Fig.11</ref>. The body part, part verb, object part with the highest scores are visualized in blue, green and red bounding boxes, and their corresponding labels are demonstrated under each image with colors consisted with boxes. The final predictions with the highest scores are represented too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel body part state knowledge base named Human Activity Knowledge Engine (HAKE) for human activity understanding, and a corresponding hierarchical paradigm. Our two-stage method consists of two components: Part State Recognition with Activity2Vec, and Part State based Reasoning. With HAKE, we can obtain a new embedding combined both visual and linguistic semantic knowledge, which brings the interpretability in the human activity recognition. Part states can significantly heighten the activity reasoning to alleviate the learning difficulties brought by the imbalanced data, and utilize the co-occurrence relation to bridge the semantic interspace between part states and activity. Our experiment results on HOI recognition show that HAKE can significantly improve the performance, especially under the fewshot circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future Work</head><p>Considering that our HAKE is still under construction, we will keep on enriching and enlarging it to promote the research in human activity understanding. We will also use HAKE to promote other tasks related to human activity understanding, e.g. video-based activity understanding, action retrieval, Visual Question Answering and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>Some samples from the proposed Part State Library, each part state consists of a state description and a corresponding cropped part region:     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Body part state samples. A driving scene contains activity person, drive, car . It can be decomposed into various body part states like head, inspect, rearview , right hand, hold, wheel , lef t hand, hold, wheel .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Previous one-stage paradigm and our hierarchical two-stage paradigm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The construction of our HAKE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The graphic model of instance activity and part states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>head-drinks_with-bottleneck right_hand-hold-bottle_body human-drink_with-bottle head-talk_on-cellphone right_hand-hold-cellphone human-talk_on-cellphone hip-sit_on-seat right_hand-hold-handle left_hand-hold-handle right_foot-step_on-pedal left_foot-step_on-pedal human-ride-bike</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>The overview of Part state recognition and Activity2Vec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>•</head><label></label><figDesc>Even the simplest Linear Combination of Part States !!! • Reasoning from Part States to Instance Activities ( , ,..., ) = • Taking GNNs, RNNs, FCs as all show prominent performance improvement (•)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>AAnalogyFigure 8 .</head><label>8</label><figDesc>analogy to human activity recognition:Part State Labels: head-look_at-sth right_arm-swing left_arm-swing right_foot-kick-sth An analogy to activity recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Instance based model and part based model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Comparison of loss and accuracy HICO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>left_hand-hold-skateboard right_foot-jump_from-skateboard left_foot-jump_from-skateboard left_hand-hold-baseball_bat right_hand-swing-baseball_bat left_hand-swing-baseball_bat left_hand-wear-baseball_glove hip-sit_on-seat right_hand-hold-handle left_hand-hold-handle right_foot-step_on-pedal left_foot-step_on-pedal right_foot-dribble-sports_ball right_foot-kick-sports_ball left_foot-kick-sports_ball left_hand-hold-book right_hand-hold-book head-read-book left_hand-hold-kite right_hand-hold-kite left_hand-launch-kite right_hand-launch-kite left_knee-lean_on-bench right_knee-lean_on-bench hip-sit_on-bench sit_on-bench lie_on-bench left_hand-hold-hot_dog right_hand-hold-hot_dog head-eat-hot_dog ride-bicycle straddle-bicycle jump-bicycle hold-baseball_bat swing-baseball_bat wear-baseball_glove kick-sports_ball dribble-sports_ball blockSome predictions of our method. Triplets under images are predicted activities. Body part, part verb and object part are represented in blue, green and red, so are the activity results. Green tick means right prediction and red cross is the opposite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Some "hand" states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 .</head><label>13</label><figDesc>Some "hip" and "arm" states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Some "head" states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .</head><label>15</label><figDesc>Some "leg" states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 .</head><label>16</label><figDesc>Some "foot" states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>How to build HAKE</head><label></label><figDesc></figDesc><table><row><cell>Existing Activity Datasets with</cell><cell></cell></row><row><cell>Instance-level Annotations</cell><cell>Part States Definition:</cell></row><row><cell></cell><cell>WordNet</cell></row><row><cell></cell><cell>Expert Annotation</cell></row><row><cell></cell><cell>Automatic</cell></row><row><cell>Expert</cell><cell>Generation</cell></row><row><cell>Check</cell><cell>Initial Annotations</cell></row><row><cell></cell><cell>Part States Refinement:</cell></row><row><cell></cell><cell>Annotators</cell></row><row><cell></cell><cell>Repetitive Labelling</cell></row><row><cell></cell><cell>Automatic Check</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell>Verb</cell><cell></cell></row><row><cell>Human</cell><cell>Edge</cell><cell>Object</cell></row><row><cell>Node</cell><cell></cell><cell>Node</cell></row><row><cell></cell><cell></cell><cell>Object Parts</cell></row><row><cell>Body Parts</cell><cell></cell><cell>part1</cell></row><row><cell>head</cell><cell></cell><cell>part2</cell></row><row><cell>arms</cell><cell></cell><cell>part3</cell></row><row><cell>hands</cell><cell></cell><cell>part4</cell></row><row><cell>hip</cell><cell></cell><cell></cell></row><row><cell>legs</cell><cell></cell><cell></cell></row><row><cell>feet</cell><cell></cell><cell></cell></row><row><cell cols="3">&lt;body_part, part_verb, object_part&gt;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Converting action to embedding via body part states Activity2Vec</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Part State Classification</cell></row><row><cell></cell><cell></cell><cell>head-no_action right_hand-hold-sth</cell><cell>Various Tasks</cell></row><row><cell>CNN</cell><cell>FC</cell><cell>left_hand-hold-sth</cell></row><row><cell></cell><cell></cell><cell>hip-sit_on-sth</cell><cell>Activity Recognition</cell></row><row><cell></cell><cell></cell><cell>right_foot-step_on-sth</cell></row><row><cell></cell><cell></cell><cell>left_foot-step_on-sth …</cell><cell>Bert</cell><cell>VQA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Action Retrieval</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Visual Reasoning</cell></row><row><cell></cell><cell>Part State</cell><cell></cell></row><row><cell>Part ROI</cell><cell>Losses</cell><cell></cell></row><row><cell>Feature</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Comparison of accuracy on our dataset</figDesc><table><row><cell></cell><cell>2.6 2.8 3.0</cell><cell></cell><cell></cell><cell cols="3">Model Loss Comparison Instance-based Part-based</cell></row><row><cell></cell><cell>2.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model Loss</cell><cell>2.0 2.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.2</cell><cell>0</cell><cell>200</cell><cell>400 Epoch</cell><cell>600</cell><cell>800</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Comparison with previous methods on HICO. "Pairwise<ref type="bibr" target="#b13">[14]</ref>+HAKE" means the late fusion of the results from Pairwise<ref type="bibr" target="#b13">[14]</ref> and our HAKE.</figDesc><table><row><cell></cell><cell></cell><cell>.9</cell><cell></cell></row><row><cell cols="2">Pairwise [14]+HAKE-GT</cell><cell>62.5</cell><cell></cell></row><row><cell cols="2">Pairwise [14]+HAKE</cell><cell>47.1</cell><cell></cell></row><row><cell>Gain</cell><cell></cell><cell>7.2</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Few@1 Few@5 Few@10</cell></row><row><cell>Pairwise [14]</cell><cell>13.02</cell><cell>19.79</cell><cell>22.28</cell></row><row><cell>Pairwise [14]+HAKE</cell><cell>25.40</cell><cell>32.48</cell><cell>33.71</cell></row><row><cell>Gain</cell><cell>12.38</cell><cell>12.69</cell><cell>11.43</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<title level="m">2D Human Pose Estimation: New Benchmark and State of the Art Analysis In CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno>1995. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanks</surname></persName>
		</author>
		<idno>1990. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contextual action recognition with r* cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Oxford companion to philosophy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Honderich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OUP Oxford</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond holistic object recognition: Enriching image understanding with part states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sukthankar and others. Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pairwise bodypart attention for recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<title level="m">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning models for actions and person-object interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08264</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Transferable Interactiveness Prior for Human-Object Interaction Detection. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09892</idno>
		<title level="m">Care about you: towards large-scale human-centric visual relationship detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10437</idno>
		<title level="m">iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
