<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Strongly Incremental Constituency Parsing with Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
							<email>kaiyuy@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
							<email>jiadeng@cs.princeton.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Strongly Incremental Constituency Parsing with Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Parsing sentences into syntax trees can benefit downstream applications in NLP. Transition-based parsers build trees by executing actions in a state transition system. They are computationally efficient, and can leverage machine learning to predict actions based on partial trees. However, existing transition-based parsers are predominantly based on the shift-reduce transition system, which does not align with how humans are known to parse sentences. Psycholinguistic research suggests that human parsing is strongly incremental-humans grow a single parse tree by adding exactly one token at each step. In this paper, we propose a novel transition system called attach-juxtapose. It is strongly incremental; it represents a partial sentence using a single tree; each action adds exactly one token into the partial tree. Based on our transition system, we develop a strongly incremental parser. At each step, it encodes the partial tree using a graph neural network and predicts an action. We evaluate our parser on Penn Treebank (PTB) and Chinese Treebank (CTB). On PTB, it outperforms existing parsers trained with only constituency trees; and it performs on par with state-of-the-art parsers that use dependency trees as additional training data. On CTB, our parser establishes a new state of the art. Code is available at https://github.com/princeton-vl/ attach-juxtapose-parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Constituency parsing is a core task in natural language processing. It recovers the syntactic structures of sentences as trees ( <ref type="figure">Fig. 1</ref>). State-of-the-art parsers are based on deep neural networks and typically consist of an encoder and a decoder. The encoder embeds input tokens into vectors, from which the decoder generates a parse tree. A main class of decoders builds trees by executing a sequence of actions in a state transition system <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>. These transition-based parsers achieve linear runtime in sentence length. More importantly, they construct partial trees during decoding, enabling the parser to leverage explicit structural information for predicting the next action.</p><p>Most existing transition-based parsers adopt the shift-reduce transition system <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>. They represent the partial sentence as a stack of subtrees. At each step, the parser either pushes a new token onto the stack (shift) or combines two existing subtrees in the stack (reduce).</p><p>Despite their empirical success, shift-reduce parsers appear to differ from how humans are known to perform parsing. Psycholinguistic research <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref> has suggested that human parsing is strongly incremental: at each step, humans process exactly one token-no more, no less-and integrate it into a single parse tree for the partial sentence. In a shift-reduce system, however, only shift actions process new tokens, and the partial sentence is represented as a stack of disconnected subtrees rather than a single connected tree. <ref type="figure">Figure 1</ref>: The constituency tree for "Arthur is King of the Britons." Leaves are labeled with tokens, and internal nodes are labeled with syntactic categories, e.g., S for sentence, NP for noun phrase, VP for verb phrase, and PP for prepositional phrase.</p><p>This observation puts forward an intriguing question: can a strongly incremental transition system lead to a better parser? Intuitively, a strongly incremental system is more aligned with human processing; as a result, the sequence of actions may be easier to learn.</p><p>Attach-juxtapose transition system We propose a novel transition system named attach-juxtapose, which enables strongly incremental constituency parsing. For a sentence of length n, we start with an empty tree and execute n actions; each action integrates exactly one token into the current partial tree, deciding on where and how to integrate the new token. There are two types of actions: attach, which attaches the new token as a child to an existing node, and juxtapose, which juxtaposes the new token as a sibling to an existing node while also creating a shared parent node <ref type="figure">(Fig. 2)</ref>. We can prove that any parse tree without unary chains can be constructed by a unique sequence of actions in this attach-juxtapose system.</p><p>Being strongly incremental, our system represents the state as a single tree rather than a stack of multiple subtrees. Not only is the single-tree representation more aligned with humans, but it also allows us to tap into a large inventory of model architectures for learning from graph data, such as TreeLSTMs <ref type="bibr" target="#b39">[40]</ref> and graph neural networks (GNNs) <ref type="bibr" target="#b18">[19]</ref>. Further, the single-tree representation provides valid syntax trees for partial sentences, which is impossible in bottom-up shift-reduce systems <ref type="bibr" target="#b32">[33]</ref>. Taking "Arthur is King of the Britons" as an example, we can produce a valid tree for the prefix "Arthur is King" <ref type="figure">(Fig. 2 Bottom)</ref>. Whereas in bottom-up shift-reduce systems, you must complete the subtree for "is King of the Britons" before connecting it to "Arthur." Therefore, our representation captures the complete syntactic structure of the partial sentence, and thus provides stronger guidance for action generation.</p><p>Our transition system can be understood as a refactorization of In-order Shift-reduce System (ISR) proposed by Liu and Zhang <ref type="bibr" target="#b22">[23]</ref>. We prove that a sequence of actions in our system can be translated into a sequence of actions in ISR, but our sequence is shorter (Theorem 4). Specifically, to generate a parse tree with n leaves and m internal nodes (assuming no unary chains), our sequence has length n, whereas ISR has length n + 2m. On the other hand, each of our actions has a larger number of choices, resulting in a different trade-off between the sequence length and the number of choices per action. We hypothesize that our system achieves a trade-off more amenable to machine learning due to closer alignment with human processing.</p><p>Action generation with graph neural networks Based on the attach-juxtapose system, we develop a strongly incremental parser by training a deep neural network to generate actions. Specifically, we adopt the encoder in prior work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50]</ref> and propose a novel graph-based decoder. It uses GNNs to learn node features in the partial tree, and uses attention to predict where and how to integrate the new token. To our knowledge, this is the first time GNNs are applied to constituency parsing.</p><p>We evaluate our method on two standard benchmarks for constituency parsing: Penn Treebank (PTB) <ref type="bibr" target="#b24">[25]</ref> and Chinese Treebank (CTB) <ref type="bibr" target="#b46">[47]</ref>. On PTB, our method outperforms existing parsers trained with only constituency trees. And it performs competitively with state-of-the-art parsers that use dependency trees as additional training data. On CTB, we achieve an F1 score of 93.59-a significant improvement of 0.95 upon previous best results. These results demonstrate the effectiveness of our strongly incremental parser.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contributions Our contributions are threefold. First, we propose attach-juxtapose, a novel transition system for constituency parsing. It is strongly incremental and motivated by psycholinguistics. Second, we provide theoretical results characterizing its capability and its connections with an existing shift-reduce system <ref type="bibr" target="#b22">[23]</ref>. Third, we develop a parser by generating actions in the attach-juxtapose system. Our parser achieves state-of-the-art performance on two standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Constituency parsing Significant progress in constituency parsing has been made by powerful token representations. Stern et al. <ref type="bibr" target="#b36">[37]</ref> fed tokens into an LSTM <ref type="bibr" target="#b16">[17]</ref> to obtain contextualized embeddings. Gaddy et al. <ref type="bibr" target="#b13">[14]</ref> demonstrated the value of character-level features. Kitaev and Klein <ref type="bibr" target="#b20">[21]</ref> replaced LSTMs with self-attention layers <ref type="bibr" target="#b41">[42]</ref>. They also show that pre-trained contextualized embeddings such as ELMo <ref type="bibr" target="#b30">[31]</ref> significantly improve parsing performance. Further improvements <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50]</ref> came with more powerful pre-trained embeddings, including BERT <ref type="bibr" target="#b9">[10]</ref> and XL-Net <ref type="bibr" target="#b47">[48]</ref>. Mrini et al. <ref type="bibr" target="#b27">[28]</ref> proposed label attention layers that improved upon self-attention layers. All these works use an existing decoder (chart-based) and focus on designing a new encoder. In contrast, we use an existing encoder by Kitaev and Klein <ref type="bibr" target="#b20">[21]</ref> and focus on designing a new decoder.</p><p>There are several types of decoders in prior work: chart-based decoders search for a tree maximizing the sum of span scores via dynamic programming (e.g., the CKY algorithm) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b27">28]</ref>; transition-based decoders build trees through a sequence of actions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref>; sequence-based decoders generate a linearized sequence of the tree using seq2seq models <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref>. Our method is transition-based; but unlike the conventional shift-reduce methods, we propose a novel transition system, which is strongly incremental.</p><p>State-of-the-art parsers perform joint constituency parsing and dependency parsing, e.g., through head-driven phrase structure grammar (HPSG) <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b27">28]</ref>. They use dependency trees as additional training data, which are converted from constituency trees by a set of hand-crafted rules <ref type="bibr" target="#b8">[9]</ref>. In contrast, our parser is trained with only constituency trees and achieves competitive performance.</p><p>Transition-based constituency parsers Most transition-based parsers adopt the shift-reduce transition system: a state consists of a buffer B holding unprocessed tokens and a stack S holding processed subtrees. Initially, S is empty, and B contains the entire input sentence. In a successful final state, B is empty, and S contains a single subtree-the complete parse tree. Below are actions for a standard bottom-up shift-reduce system such as Sagae and Lavie <ref type="bibr" target="#b32">[33]</ref>, which corresponds to post-order traversal of the complete parse tree.</p><p>• shift: Remove the first element from B and push it onto S.</p><p>• unary_reduce(X): Pop a subtree from S; add a label X as its parent; and push it back onto S.</p><p>• binary_reduce(X): Pop two subtrees; add a label X as their shared parent; and push back.</p><p>Dyer et al. <ref type="bibr" target="#b11">[12]</ref> proposed a top-down (pre-order) variant. Liu and Zhang <ref type="bibr" target="#b22">[23]</ref> proposed an in-order shift-reduce system, outperforming bottom-up <ref type="bibr" target="#b32">[33]</ref> and top-down <ref type="bibr" target="#b11">[12]</ref> baselines. Compared to our transition system, none of these shift-reduce systems is strongly incremental, because they represent the partial sentence as a stack of disconnected subtrees and they do not process exactly one token per action-only the shift action consumes a token.</p><p>Among the shift-reduce systems, our approach is most related to the in-order system by Liu and Zhang <ref type="bibr" target="#b22">[23]</ref> in that each action in our system can be mapped to a combination of actions in their system (see Sec. 3 for details). An analogy is that their actions resemble a set of microinstructions for CPUs, where each instruction is simple but it takes many instructions to complete a task; our actions resemble a set of complex instructions, where each instruction is more complex but it takes fewer instructions to complete the same task.</p><p>Transition-based parsers use machine learning to make local decisions-determining the action to take at each step. This poses the question of how to represent a stack of subtrees in shift-reduce systems. Earlier works such as Sagae and Lavie <ref type="bibr" target="#b32">[33]</ref> and Zhu et al. <ref type="bibr" target="#b50">[51]</ref> use hand-crafted features. More recent works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref> have switched to recurrent neural networks and LSTMs. In particular, Dyer et al. <ref type="bibr" target="#b10">[11]</ref> propose an LSTM-based model named Stack LSTM for representing the stack. It is designed for dependency parsing but applies to constituency parsing as well <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>. However, we do not have to represent stacks thanks to the single-tree state representation. Instead, we use graph neural networks (GNNs) <ref type="bibr" target="#b18">[19]</ref> to represent partial trees.</p><p>Incremental parsing Prior work has built parsers inspired by the incremental syntax processing of humans. Earlier works focused on psycholinguistic modeling of humans and evaluated on a handful of carefully curated sentences <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b0">1]</ref>. More recent methods switched to developing efficient parsers with a wide coverage of real-world texts. Roark <ref type="bibr" target="#b31">[32]</ref> proposed a top-down incremental parser that expands nodes in the partial tree using a probabilistic context-free grammar (PCFG). In contrast, our system is more flexible by not restricted to any predefined grammar. Also, we predict actions leveraging not only top-down information but also bottom-up information.</p><p>Costa et al. <ref type="bibr" target="#b6">[7]</ref> proposed a transition system for incremental parsing. Similar to ours, it integrates exactly one token per step into the partial tree. However, at each step, they have to predict an unbounded number of labels, whereas we have to predict no more than two. Therefore, our action space is smaller than theirs and thus easier to navigate by learning-based parsers. In fact, this limitation may have prevented Costa et al. <ref type="bibr" target="#b6">[7]</ref> from building a fully functional parser, and they only evaluated on action generation. Collins and Roark <ref type="bibr" target="#b5">[6]</ref> developed a parser based on Costa et al. <ref type="bibr" target="#b6">[7]</ref> by using grammar rules and heuristics to prune the action space. In contrast, our action space is more flexible without grammar rules but still tractable for machine learning models.</p><p>Graph neural networks for syntactic processing GNNs have been used to process syntactic information. These methods obtain syntax trees using external parsers and apply GNNs to the trees for downstream tasks such as pronoun resolution <ref type="bibr" target="#b45">[46]</ref>, relation extraction <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b33">34]</ref>, and machine translation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In contrast, we apply GNNs to partial trees for the task of parsing. Ji et al. <ref type="bibr" target="#b17">[18]</ref> used GNNs for graph-based dependency parsing. However, their method is not transition-based. They apply GNNs to complete graphs formed by all tokens, whereas we apply GNNs to partial trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Attach-juxtapose Transition System</head><p>Overview We introduce a novel transition system named attach-juxtapose for strongly incremental constituency parsing. Our system is inspired by psycholinguistic research <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref>; it maintains a single parse tree and adds one token to it at each step. Our system can produce valid syntax trees for partial sentences and can handle trees with arbitrary branching factors.</p><p>For parsing a sentence of length n, we start with an empty tree and sequentially execute n actions; each action integrates the next token into the current partial tree. Formally speaking, for the sentence [w 0 , w 1 , . . . , w n−1 ], the state at ith step is s i = (T i , w i ), where T i is the partial tree for the prefix [w 0 , . . . , w i−1 ]. The state transition rules are T 0 = empty_tree and T i+1 = T i (a i ), where T i (a i ) denotes the result of executing action a i on tree T i . After n steps, we end up with a complete tree T n . The actions are designed to capture where and how to integrate a new token into the partial tree.</p><p>Where to integrate the new token Since the new token is to the right of existing tokens, it must appear on the rightmost chain-the chain of nodes starting from the root and iteratively descending to the rightmost child (A similar observation was also made by Costa et al. <ref type="bibr" target="#b6">[7]</ref>). Formally speaking, at the ith step, we have a partial tree T i and a new token w i . Let rightmost_chain(T i ) denote the set of internal nodes on the rightmost chain of T i . We pick target_node ∈ rightmost_chain(T i ) as where the new token should be integrated.</p><p>How to integrate the new token <ref type="figure" target="#fig_10">Fig. 2</ref> Top shows the rightmost chain and target_node (orange), we design two types of actions specifying how to integrate the new token (blue):</p><p>• attach(target_node, parent_label): Attach the token as a descendant of target_node.</p><p>The parameter parent_label is optional; when provided, we create an internal node labeled parent_label (green) as the parent of the new token. Parent_label then becomes the rightmost child of target_node (as in <ref type="figure" target="#fig_10">Fig. 2</ref> Top). When parent_label is not provided, the new token itself becomes the rightmost child of target_node.</p><p>• juxtapose(target_node, parent_label, new_label): Create an internal node labeled new_label (gray) as the shared parent of target_node and the new token. It then replaces target_node in the tree. Similar to attach, we can optionally create a parent for the new token via the parent_label parameter. We represent target_node using its index on the rightmost chain (starting from 0). The complete action sequence to parse the sentence correctly ( <ref type="figure" target="#fig_10">Fig. 1</ref>) would be: attach(0, NP), juxtapose(0, VP, S), attach(1, NP), juxtapose(2, PP, NP), attach(3, NP), attach(4, None). Note that the first action attach(0, NP) is a degenerated case. Since T 0 = empty_tree, it is impossible to pick target_node ∈ rightmost_chain(T 0 ). In this case, imagine a dummy root node for T 0 ; then we can execute attach(0, parent_label), making parent_label the new root.</p><p>Oracle actions Having defined the attach-juxtapose transition system, we are yet to show its capability for constituency parsing: Given a constituency tree, is it always possible to find a sequence of oracle actions to produce the tree? This question is important because if the oracle actions did not exist, it would not be possible to parse the sentence correctly. If the oracle actions do exist, a further question is: For a given tree, is the sequence of oracle actions unique? Uniqueness is desirable because it guarantees an unambiguous supervision signal at each step when training the parser. We prove that the answers to both questions are positive under mild conditions: Theorem 1 (Existence of oracle actions). Let T be a constituency tree for a sentence of length n. If T does not contain unary chains, there exists a sequence of actions a 0 , a 1 , . . . , a n−1 such that empty_tree(a 0 )(a 1 ) . . . (a n−1 ) = T . Theorem 2 (Uniqueness of oracle actions). Let T be a constituency tree for a sentence of length n, and T does not contain unary chains. If a 0 , a 1 , . . . , a n−1 is a sequence of oracle actions, it is the only action sequence that satisfies empty_tree(a 0 )(a 1 ) . . . (a n−1 ) = T .</p><p>The condition regarding unary chains is not a restriction in practice, as we can remove unary chains using the preprocessing technique in prior work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b27">28]</ref>. The theorems above can be proved by constructing an algorithm to compute the oracle actions. We present the algorithm and detailed proofs in Appendix A. Intuitively, given a tree T , we recursively find and undo the last action until T becomes empty_tree.</p><p>Connections with In-order Shift-reduce System Our attach-juxtapose transition system is closely related to In-order Shift-reduce System (ISR) proposed by Liu and Zhang <ref type="bibr" target="#b22">[23]</ref>. ISR's state space is strictly larger than ours; we prove it to be equivalent to an augmented version of our state space. Given a sentence [w 0 , w 1 , . . . , w n−1 ], the space of partial trees is U = {t | ∃ 0 ≤ m ≤ n, s.t. t is a constituency tree for [w 0 , w 1 , . . . , w m−1 ]}. By Theorem 1, our state space (for the given sentence) is a subset of U, i.e., U AJ = {t | t ∈ U, t does not contain unary chains}. To bridge U AJ and U ISR (the state space of IRS), we define the augmented space of partial trees to be</p><formula xml:id="formula_0">U = {(t, i) | t ∈ U, i ∈ Z, −1 ≤ i &lt; L(t)},</formula><p>where L(t) denotes the number of internal nodes on the rightmost chain of t. We assert that U is equivalent to U ISR .  <ref type="figure" target="#fig_10">Figure 3</ref>: The architecture of our model for action generation. We use the self-attention encoder in prior work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50]</ref> and generate actions using a GCN-based <ref type="bibr" target="#b18">[19]</ref> decoder.</p><p>Theorem 3 (Connection in state spaces). There is a bijective mapping ϕ : U ISR → U between the set of legal states in In-order Shift-reduce System and the augmented space of partial trees.</p><p>We prove the theorem in Appendix B. Intuitively, a state in ISR is a stack configuration; it corresponds to an element of U , which can be understood as a partial tree t ∈ U with a special node on the rightmost chain marked by an integer i.</p><p>Not only is ϕ bijective, but it also preserves actions. In other words, each action in our system can be mapped to a combination of actions in ISR. To see this, we define an injective mapping ξ : U AJ → U such that ξ(t) = (t, L(t) − 1). Then by Theorem 3, ϕ −1 • ξ : U AJ → U ISR is an injective mapping from our state space to ISR's state space. And we have the following connection between actions:</p><p>Theorem 4 (Connection in actions). Let t 1 and t 2 be two partial trees without unary chains, i.e., t 1 , t 2 ∈ U AJ . If a is an attach-juxtapose action that brings t 1 to t 2 , there must exist a sequence of actions in In-order Shift-reduce System that brings</p><formula xml:id="formula_1">ϕ −1 • ξ(t 1 ) to ϕ −1 • ξ(t 2 ).</formula><p>We present a constructive proof in Appendix B, making it possible to translate any action sequence on our system to a longer sequence in ISR. Theorem 4 also implies any reachable parse tree in our system can also be reached in ISR. And by Theorem 1, both our system and ISR can generate any tree without unary chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Action Generation with Graph Neural Networks</head><p>Given the attach-juxtapose system, we develop a model for constituency parsing by generating actions based on the partial tree and the new token ( <ref type="figure" target="#fig_10">Fig. 3</ref>). First, it encodes input tokens as vectors using the self-attention encoder in prior work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50]</ref>. These vectors are used to initialize node features in the partial tree, which is then fed into a graph convolutional network (GCN) <ref type="bibr" target="#b18">[19]</ref>. The GCN produces features for each node, and in particular, the features on the rightmost chain. Finally, an attention-based action decoder generates the action based on the new token and the rightmost chain.</p><p>Token encoder We use the same encoder as Kitaev and Klein <ref type="bibr" target="#b20">[21]</ref> and Zhou and Zhao <ref type="bibr" target="#b49">[50]</ref>. It consists of a pre-trained contextualized embedding such as BERT <ref type="bibr" target="#b9">[10]</ref> or XLNet <ref type="bibr" target="#b47">[48]</ref>, followed by a few additional self-attention layers. Like in prior work, we separate content and position information; the resulting token features are the concatenation of content features and position features. The encoder is not incremental due to how self-attention works; it is applied once to the entire sentence. Then we apply the decoder to generate actions in a strongly incremental manner.</p><p>Graph convolutional neural network We use GCN on the partial tree to produce features for nodes on the rightmost chain. Initially, leaf features are provided by the encoder, whereas for an internal node labeled l that spans from position i to j (endpoints included), the initial feature x = [W l , x p ] ∈ R D is a concatenation of label and position embeddings: W l is a D 2 -dimensional learned embedding for label l. And x p = (P i + P j )/2 is the position embedding averaging two endpoints, where P is the same position embedding matrix in the self-attention encoder.</p><p>The initial node features go through several GCN layers with residual connections. We use a variant of the original GCN layer <ref type="bibr" target="#b18">[19]</ref> to separate content features and position features (details in Appendix C). The GCN produces features for all nodes. However, we are only interested in nodes on the rightmost chain, as they are candidates for target_node in actions.</p><p>Action decoder Given the structure of our actions (Sec. 3), the action decoder has to (1) choose a target_node on the rightmost chain; (2) decide between attach and juxtapose; and (3) generate the parameters parent_label and new_label.</p><p>We choose target_node using attention on the rightmost chain. Let L be the size of the chain,</p><formula xml:id="formula_2">Y = [Y c , Y p ] ∈ R L×D be the features on the chain produced by the GCN, z = [z c , z p ] ∈ R 1×D</formula><p>be the feature of the new token given by the encoder. They are both concatenation of content and position features. We generate attention weights for nodes on the rightmost chain as</p><formula xml:id="formula_3">w = f c ([Y c , 1 L×1 z c ]) + f p ([Y p , 1 L×1 z p ]), where 1 L×1 is a L×1 matrix of ones. [·, ·]</formula><p>concatenates two matrices horizontally, and f c , f p are two-layer fully-connected networks with ReLU <ref type="bibr" target="#b28">[29]</ref> and layer normalization <ref type="bibr" target="#b1">[2]</ref>. w ∈ R L×1 and we pick the node with maximum attention as target_node.</p><p>Since the parameter new_label is only for juxtapose, we can interpret new_label = None as attach. So we only need to generate parent_label, new_label ∈ V ∪ {None}, where V is the vocabulary of labels. We generate them using the new token and a weighted average of the rightmost chain:</p><formula xml:id="formula_4">[u, v] = g([z, σ(w) T Y ]), where σ is the sigmoid function, u, v ∈ R |V |+1</formula><p>are predicted log probabilities of parent_label and new_label, and g is a two-layer layer-normalized network.</p><p>Training We train the model to predict oracle actions.</p><p>At any step, let a = (target_node, parent_label, new_label) be the oracle action; recall that new_label = None implies attach, and new_label = None implies juxtapose. The loss function is a sum of cross-entropy losses for each component: </p><formula xml:id="formula_5">L = CE(w, target_node) + CE(u, parent_label) + CE(v, new_label).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Setup We evaluate our model for constituency parsing on two standard benchmarks: Chinese Treebank 5.1 (CTB) <ref type="bibr" target="#b46">[47]</ref> and the Wall Street Journal part of Penn Treebank (PTB) <ref type="bibr" target="#b24">[25]</ref>. PTB consists of <ref type="bibr" target="#b38">39</ref> For both datasets, we follow the standard data splits and preprocessing in prior work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50]</ref>. In evaluation, we report four metrics-exact match (EM), F1 score, labeled precision (LP), and labeled recall (LR)-all computed by the standard Evalb 1 tool. The testing numbers are produced by models trained on training data alone (not including validation data).</p><p>We use the same technique in prior work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b27">28]</ref> to remove unary chains by collapsing multiple labels in a unary chain into a single label. It does not affect evaluation, as we revert this process before computing evaluation metrics.</p><p>Training details We train the model to predict oracle actions through teacher forcing <ref type="bibr" target="#b44">[45]</ref>-the model takes actions according to the oracle rather than the predictions. Model parameters are optimized using RMSProp <ref type="bibr" target="#b40">[41]</ref> with a batch size of 32. We decrease the learning rate by a factor of 2 when the best validation F1 score plateaus. The model is implemented in PyTorch <ref type="bibr" target="#b29">[30]</ref> and takes 2 ∼ 3 days to train on a single Nvidia GeForce GTX 2080 Ti GPU. For fair comparisons with prior work, we use the same pre-trained BERT and XLNet models 2 : xlnet-large-cased and bert-large-uncased for PTB; bert-base-chinese for CTB. <ref type="table" target="#tab_6">Table 1</ref> summarizes our PTB results compared to state-of-the-art parsers, including both chart-based parsers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b27">28]</ref> and transition-based parsers <ref type="bibr" target="#b22">[23]</ref>. Methods with <ref type="table" target="#tab_6">Table 1</ref>: Constituency parsing performance on Penn Treebank (PTB). Methods with are trained with extra supervision from dependency parsing data. Methods with † are reported in the re-implementation by Fried et al. <ref type="bibr" target="#b12">[13]</ref>. Liu and Zhang <ref type="bibr" target="#b22">[23]</ref> is transition-based, whereas other baselines are chart-based. We run each experiment 5 times to report the mean and standard error (SEM) of four metricsexact match (EM), F1 score, labeled precision (LP), and labeled recall (LR). Our method performs competitively with state of the art and achieves the highest EM. are trained with extra supervision from dependency parsing data. Methods with † are reported in not their original papers but the re-implementation by Fried et al. <ref type="bibr" target="#b12">[13]</ref>, since the original versions did not use BERT. Liu and Zhang <ref type="bibr" target="#b22">[23]</ref> (BERT) † performs beam search during testing with a beam size of 10. We do the same for fair comparisons, which improves the performance marginally (0.05 in EM and 0.02 in F1 for our model with XLNet). Some metrics for prior work are missing because they are neither reported in the original papers nor available using the released model and code. We run each experiment 5 times with different random seeds to report the mean and its standard error (SEM). Overall, our method performs competitively with state-of-the-art parsers on PTB. It achieves higher EM using the same pre-trained embedding (BERT or XLNet). Also, our method has a comparable number of parameters with existing methods. Parsing speed We measure parsing speed empirically using the wall time for parsing the 2,416 PTB testing sentences. Results are shown in <ref type="table" target="#tab_3">Table 3</ref>. It takes 33.9 seconds for our method (with XLNet, without beam search), 37.3 seconds for Zhou and Zhao <ref type="bibr" target="#b48">[49]</ref>, and 40.8 seconds for Mrini et al. <ref type="bibr" target="#b26">[27]</ref>. Our method is slightly faster, but the gap is small. About 50% of the time is spent on the XLNet encoder, which is shared among all three methods and explains their similar run time. These experiments were run on machines with 2 CPUs, 16GB memory, and one GTX 2080 Ti GPU. Effect of the transition system Our method differs from Liu and Zhang <ref type="bibr" target="#b22">[23]</ref> in not only the transition system but also the overall model architecture. To more closely compare our attachjuxtapose transition system with their In-order Shift-reduce System (ISR), we perform an ablation that only changes the transition system while keeping the other part of the model as close as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parsing performance</head><p>We implement a baseline that generates ISR actions on top of our encoder and GCNs. To that end, we rely on Theorem 3 to interpret ISR states (stacks) as augmented partial trees. Specifically, given an ISR state s ∈ U ISR , we have ϕ(s) ∈ U from Theorem 3. We know ϕ(s) = (t, i), where t is a partial tree and i is an integer ranging from −1 to L(t) − 1. First, we encode t in the same way as before, using XLNet and GCNs. Then, we take the GCN feature of the ith node on the rightmost chain and use it to generate actions in ISR. We add a special node to the rightmost chain to handle i = −1.</p><p>Results are shown in <ref type="table" target="#tab_4">Table 4</ref>. the ISR baseline achieves an average F1 score of 96.23 on PTB, which is lower than our method (96.34). This ablation demonstrates that the attach-juxtapose transition system contributes to the performance. Effect of graph neural networks A key ingredient of our model is using GNNs to effectively leverage structural information in partial trees. We conduct an ablation study to demonstrate its importance. Specifically, we keep the encoder fixed and replace the graph-based decoder with a simple two-layer network. For each new token, it predicts an action from the token feature alone-no partial tree is built. It predicts target_node as an integer in [0, 249]. We increase the feature dimensions so that both models have the same number of parameters. Results are in <ref type="table" target="#tab_7">Table 5</ref>; the graph-based decoder achieves better performance in all settings, which demonstrates the value explicit structural information. <ref type="table" target="#tab_7">Table 5</ref>: Ablation study comparing our graph-based action decoder with a sequence-based decoder that cannot leverage structural information in partial trees. The graph-based decoder leads to better performance in all settings, which demonstrates the importance of explicit structural information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed the attach-juxtapose transition system for constituency parsing. It is inspired by the strong incrementality of human parsing discovered by psycholinguistics. We presented theoretical results characterizing its capability and its connections with existing shift-reduce systems. Further, we developed a parser based on it and achieved state-of-the-art performance on two standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>We evaluated our method on constituency parsing for English and Chinese. They are the two most spoken languages, with more than two billion speakers across the globe. However, there are more than 7,000 languages in the world. And it is important to deliver parsing and other NLP technology to benefit speakers of diverse languages. Fortunately, our method can be applied to many languages with little additional effort. We developed the system using PTB, and when adding CTB, we only had to make a few minor changes in language-specific preprocessing.</p><p>However, a potential barrier is the lack of training data for low-resource languages. Our method relies on supervised learning with a large number of annotated parse trees, which are available only for some languages. A potential solution is to do joint multilingual training as in Kitaev et al. <ref type="bibr" target="#b19">[20]</ref>.</p><p>Definition 1 (Constituency tree). Given a sentence s = [w 0 , w 1 , . . . , w n−1 ] of length n, we define a constituency tree for s as a rooted tree with arbitrary branching factors. It has n leaves labeled with tokens w 0 , w 1 , . . . w n−1 from left to the right, whereas its internal nodes are labeled with syntactic categories. The root node must be an internal node. In the degenerated case of n = 0, we define a special constant empty_tree to be the constituency tree for s = [].</p><p>Definition 2 (Unary chain). Let T be a constituency tree. We say T contains unary chains if there exist two internal nodes x and y such that y is the only child of x. Conversely, if such x and y do not exist, we say T does not contain unary chains.</p><p>Then we present Algorithm 1 for computing oracle actions. Given a constituency tree T without unary chains, it recursively finds and undoes the last action until T becomes empty_tree. The algorithm has a time complexity of O(n log n), where n is the sentence length. It calls last_action n times, and each call needs O(log n) for locating the last leaf of the tree. Now we are ready to state and prove Theorem 1 and Theorem 2 in the main paper.</p><p>Theorem 1 (Existence of oracle actions). Let T be a constituency tree for a sentence of length n. If T does not contain unary chains, there exists a sequence of actions a 0 , a 1 , . . . , a n−1 such that empty_tree(a 0 )(a 1 ) . . . (a n−1 ) = T . And this sequence of actions can be computed via Algorithm 1.</p><p>Proof. We prove the correctness of Algorithm 1 by induction on the sentence length n.</p><p>When n = 0, we have T = empty_tree (Definition 1), and oracle_action_sequence(T ) returns an empty action sequence []. The conclusion holds straightforwardly.</p><p>When n &gt; 0, it is sufficient to prove T is a valid constituency tree without unary chains for a sentence of length n − 1. We proceed by enumerating all possible execution traces in last_action.</p><p>The function contains two conditional statements and therfore 2 × 3 = 6 execute traces. We use "Case i-j" to denote the execution trace taking the ith branch in the first conditional statement and the jth branch in the second conditional statement.</p><p>• Case 1-1-last_leaf has siblings, and last_subtree is the root node. We have last_subtree = last_leaf (the first conditional statement). So last_leaf is the root node while being a leaf, which contradicts with the assumption that T is a constituency tree (Definition 1).  if last_subtree is the root node: <ref type="bibr" target="#b18">19</ref> return attach(0, parent_label) <ref type="bibr" target="#b19">20</ref> elif last_subtree has exactly one sibling and its sibling is an internal node: • Case 1-2-last_leaf has siblings; last_subtree is not the root node; last_subtree has exactly one sibling, and its sibling is an internal node. We have last_subtree = last_leaf (the first conditional statement). The local configuration of T looks like <ref type="figure" target="#fig_10">Fig. A Right,</ref> on its left is T obtained from T by undoing action juxtapose(target_node, parent_label, new_label). T is still a valid constituency tree without unary chains.</p><p>• Case 1-3-last_leaf has siblings; last_subtree is not the root node; last_subtree has either no sibling, one leaf node as its sibling, or more than one siblings. We have last_subtree = last_leaf (the first conditional statement). So last_subtree has either one leaf node or more than one nodes as its siblings. These two cases are shown separately in <ref type="figure" target="#fig_10">Fig. B</ref>. In both cases, T is still a valid constituency tree without unary chains.</p><p>• Case 2-1-last_leaf has no sibling, and last_subtree is the root node. We have last_subtree = last_leaf 's parent (the first conditional statement). As shown in <ref type="figure" target="#fig_10">Fig. C</ref>, T is empty_tree in this case, which is also a valid constituency tree without unary chains.</p><p>• Case 2-2-last_leaf has no sibling; last_subtree is not the root node; last_subtree has exactly one sibling, and its sibling is an internal node.   We have last_subtree = last_leaf 's parent (the first conditional statement). As <ref type="figure" target="#fig_10">Fig. D</ref> shows, T is still a valid constituency tree without unary chains.</p><p>• Case 2-3-last_leaf has no sibling; last_subtree is not the root node; last_subtree has either no sibling, one leaf node as its sibling, or more than one siblings. We have last_subtree = last_leaf 's parent (the first conditional statement). So last_subtree is an internal node. Since T does not contain unary chains, any non-root internal node must have siblings. As a result, last_subtree has either one leaf node or more than one nodes as its sibling. These two cases are shown separately in <ref type="figure" target="#fig_10">Fig. E</ref>. In both cases, T is still a valid constituency tree without unary chains.</p><p>We have proved T to be a valid constituency tree for a sentence of length n−1 no matter which execution trace last_action takes. Applying the induction hypothesis, we know oracle_action_sequence(T ) outputs a sequence of actions a 0 , a 1 , . . . , a n−2 such that empty_tree(a 0 ) . . . (a n−2 ) = T . Since T (a n−1 ) = T , we have finally derived empty_tree(a 0 ) . . . (a n−1 ) = T .</p><p>Theorem 2 (Uniqueness of oracle actions). Let T be a constituency tree for a sentence of length n, and T does not contain unary chains. If oracle_action_sequence(T ) = a 0 , a 1 , . . . , a n−1 , it is the only action sequence that satisfies empty_tree(a 0 )(a 1 ) . . . (a n−1 ) = T .</p><p>Proof. We prove by contradiction. Assume there is different action sequence a 0 , a 1 , . . . , a n−1 that satisfies empty_tree(a 0 )(a 1 ) . . . (a n−1 ) = T . We first prove a n−1 = a n−1 , in other words, a n−1 is the only possible last action. Similar to Theorem 1, we prove by enumerating all execution traces.</p><p>• Case 1-1-last_leaf has siblings, and last_subtree is the root node. Similarly, it contradicts with T being a constituency tree.</p><p>• Case 1-2-last_leaf has siblings; last_subtree is not the root node; last_subtree has exactly one sibling, and its sibling is an internal node.  In both cases, the last action is an attach.</p><p>In <ref type="figure" target="#fig_10">Fig. A Right,</ref> it is apparent that parent_label = None. Also, a n−1 must be a juxtapose action since otherwise the gray node will introduce an unary chain in T . Therefore, a n−1 = a n−1 .</p><p>• Case 1-3-last_leaf has siblings; last_subtree is not the root node; last_subtree has either no sibling, one leaf node as its sibling, or more than one siblings. <ref type="figure" target="#fig_10">In Fig. B</ref>, parent_label = None. No matter how many siblings last_subtree has, a n−1 must be an attach action. Therefore, a n−1 = a n−1 .</p><p>The remaining three cases are similar, and we omit the details. Now that we have proved a n−1 = a n−1 , it is straightforward to apply the same reasoning to derive a n−2 = a n−2 , a n−3 = a n−3 and all the way until a 0 = a 0 . This contradicts with the assumption that a 0 , a 1 , . . . , a n−1 is different from a 0 , a 1 , . . . , a n−1 . Therefore, we have an unique sequence of oracle actions a 0 , a 1 , . . . , a n−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Proofs about Connections with In-order Shift-reduce System</head><p>We prove Theorem 3 and Theorem 4 in the main paper; they reveal the connections between our system with In-order Shift-reduce System (ISR) proposed by Liu and Zhang <ref type="bibr" target="#b22">[23]</ref>. Before any formal derivation, we first illuminate the connections through an example to gain some intuition. our state space-the set of partial trees without unary chains. Whereas U ISR denotes ISR's state space-the set of legal stack configurations. We represent a stack from left (stack bottom) to the right (stack top). "(X" denotes a projected nonterminal X, while an S-expression such as "(NP Arthur)" denotes a subtree in the stack. U denotes the augmented space of partial trees; each element in U is a constituency tree that may have a node on the rightmost chain marked as special (orange). We observe a one-to-one correspondence between U and U ISR , which is denoted by the mapping ϕ.</p><p>We proceed to generalize this example to arbitrary state transitions in our system, which involves formulating and proving Theorem 3 and Theorem 4 in the main paper. First, we assume a fixed sentence and define the state spaces of our system and ISR: Definition 3 (Space of partial trees). Given the sentence [w 0 , w 1 , . . . , w n−1 ], we define the space of partial trees to be:</p><formula xml:id="formula_6">U = {t | ∃ 0 ≤ m ≤ n, s.t. t is a constituency tree for [w 0 , w 1 , . . . , w m−1 ]}.<label>(1)</label></formula><p>From theorem 1, we know that the state space of our system is a subset of U that does not contain unary chains: U AJ = {t | t ∈ U, t does not contain unary chains}.</p><p>(2) Definition 4 (Augmented space of partial trees). Given the sentence [w 0 , w 1 , . . . , w n−1 ]. We define the augmented space of partial trees to be:</p><formula xml:id="formula_7">U = {(t, i) | t ∈ U, i ∈ Z, −1 ≤ i &lt; L(t)},<label>(3)</label></formula><p>where L(t) denotes the number of internal nodes on the rightmost chain of t. We can define an injective mapping ξ : U AJ → U :</p><formula xml:id="formula_8">ξ(t) = (t, L(t) − 1).<label>(4)</label></formula><p>In ISR, the parser can be trapped in a state that will never lead to a complete tree no matter what actions it takes, e.g., the states where two tokens are at the bottom of the stack. We call such states illegal states and prove a lemma characterizing the set of legal states. Lemma 1 (Legal states in ISR). Let s be a stack configuration in In-order Shift-reduce System, it is a legal state if and only if you can end up with a single partial tree in the stack by repeatedly executing reduce.</p><p>Proof. For the "if" part, we fist keep executing reduce until there is only a single partial tree in the stack. Then we can arrive at a complete tree by executing one PJ-X, several shift to consume all remaining tokens, and one final reduce. Therefore, the stack s is legal.</p><p>For the "only if" part, s is legal. We prove by contradiction, assuming it is impossible to get a single partial tree by executing multiple reduce. Then we must be stuck somewhere. Referring to the definition of the reduce action <ref type="bibr" target="#b22">[23]</ref>, there could be several reasons for being stuck: (1) The stack has more than one element but no projected nonterminal; (2) the only projected nonterminal is at the bottom of the stack; (3) there are two consecutive projected nonterminals. For all three cases, the offending pattern must also exist in the original s, and no action sequence can remove them from s. Therefore, s must be illegal, which contradicts the assumption.</p><p>Although there are illegal states in ISR, it is possible to avoid them using heuristics in practice. So we do not consider them a problem for ISR. In the following derivations, we safely ignore illegal states and assume ISR's state space to consist of only legal states: Definition 5 (Space of legal stack configurations). Given the sentence [w 0 , w 1 , . . . , w n−1 ], we define U ISR as the set of legal stack configurations in In-order Shift-reduce System.</p><p>As one of our main theoretical conclusions, ISR's state space is equivalent to the augmented space of partial trees; therefore, it is strictly larger than our state space: Theorem 3 (Connection in state spaces). There is a bijective mapping ϕ : U ISR → U between the legal states in In-order Shift-reduce System and the augmented space of partial trees.</p><p>Proof. For a legal stack s ∈ U ISR , let L(s) be the number of projected nonterminals in s (introduced by PJ-X actions in Liu and Zhang <ref type="bibr" target="#b22">[23]</ref>). We are abusing the notation a little bit as we have used L(t) to denote the number of internal nodes on the rightmost chain of a tree t. But as we will show, they are actually the same. If s is an empty stack, let t = empty_tree. Otherwise, let t be the tree produced by repeatedly executing reduce until there is only one partial tree remaining in the stack. This is always possible since s is a legal state (Lemma 1). Then we can define ϕ(s) = (t, L(s) − 1). We prove ϕ is bijective by constructing its inverse mapping ϕ −1 : U → U IRS .</p><p>Given any (t, i) ∈ U , t is a partial tree. We define the depth of a node in t as its distance to the root node. Further, we extend the definition of in-order traversal from binary trees to trees with arbitrary branching factors: the first subtree → root node → the second subtree → the third subtree → . . .</p><p>We define a mapping γ : U → U IRS . Let γ(t, i) be the stack obtained by starting with an empty stack and traversing the tree t in-order: At any subtree rooted at node x, (1) if node x is not on the rightmost chain or depth(x) &gt; i, we push the entire subtree x onto the stack and skip traversing the nodes in it. <ref type="formula">(2)</ref> If node x is on the rightmost chain and depth(x) ≤ i, we push node x's label as a projected nonterminal and keep traversing the nodes in subtree x.</p><p>We now prove γ to be the inverse of ϕ, i.e. ϕ • γ(t, i) = (t, i). It is straightforward that the stack γ(t, i) has i + 1 projected nonterminals, corresponding to nodes on the rightmost chain with depth 0, 1, . . . , i. So, L(γ(t, i)) − 1 = i, and we only have to prove t to be the tree obtained by repeatedly executing reduce on γ(t, i).</p><p>In the trivial case of t = empty_tree, we have L(t) = 0, and i must be −1. γ(t, i) is an empty stack, and executing reduce on it will give empty_tree, which equals to t.</p><p>In the non-trivial case of t = empty_tree, we prove by induction on the number of reduce actions (k) executed on the stack γ(t, i) to get a single tree.</p><p>When k = 0, γ(t, i) contains a single tree, which must be t itself.</p><p>When k &gt; 0, let γ 1 (t, i) be the stack after executing one reduce on γ(t, i). We assert that γ 1 (t, i) = γ(t, i − 1). We can see this by comparing the in-order traversal of (t, i) and (t, i − 1). The only difference is how we process the subtree rooted at the ith node on the rightmost chain. When traversing (t, i), we push a projected nonterminal and proceed to nodes in the subtree. When traversing (t, i − 1), we push the entire subtree and skip the nodes in it, which corresponds exactly to executing one reduce on γ(t, i). Therefore, γ 1 (t, i) = γ(t, i − 1).</p><p>We only need k − 1 reduce actions to get a single tree from the stack γ 1 (t, i), or equivalently, γ(t, i − 1). Applying the induction hypothesis, we will get the tree t by repeatedly applying reduce on γ 1 (t, i). Therefore, we will also get the same t by repeatedly applying reduce on γ(t, i), i.e. ϕ • γ(t, i) = (t, i).</p><p>Since (t, i) is arbitrary, we have proved γ to be the inverse mapping of ϕ, and ϕ is thus bijective.</p><p>Corollary 1 (Connections in state spaces). ϕ −1 • ξ : U AJ → U ISR is an injective mapping from our state space to ISR's state space.</p><p>Proof. It is straightforward given that ξ : U AJ → U is injective (Definition 4) and ϕ : U ISR → U is bijective (Theorem 3).</p><p>The mapping ϕ −1 • ξ bridges our state space and ISR's state space. Not only is it injective, but it also preserves actions-each action in our system can be mapped to a combination of actions in ISR. We prove this for attach actions (Lemma 2) and juxtapose actions (Lemma 3) separately.</p><p>Lemma 2 (Translating attach actions to ISR). Let t 1 and t 2 be two partial trees without unary chains, i.e., t 1 , t 2 ∈ U aj . If attach(i, X) brings t 1 to t 2 , The following action sequence in In-order Shift-reduce System will bring ϕ −1 • ξ(t 1 ) to ϕ −1 • ξ(t 2 ):</p><p>reduce, . . . , reduce</p><formula xml:id="formula_9">L(t1)−i−1 , shift, PJ-X if X =None ,<label>(5)</label></formula><p>where L(t 1 ) is the number of internal nodes on the rightmost chain of t 1 . X = None means the optional argument parent_label is not provided; in this case, we exclude PJ-X.</p><p>Proof. We know ϕ −1 • ξ(t 1 ) = ϕ −1 (t 1 , L(t 1 ) − 1) (Definition 4). Recall that in Theorem 3 we have proved that executing one reduce on ϕ −1 (t, i) gives us ϕ −1 (t, i − 1). Therefore, executing reduce L(t 1 ) − i − 1 times on ϕ −1 (t 1 , L(t 1 ) − 1) gives us ϕ −1 (t 1 , i). We still have to execute one shift and one optional PJ-X.</p><p>When X = None, we only have to execute a shift. The resulting stack is ϕ −1 (t 1 , i) plus a new token at the top. We prove that the new stack equals to ϕ −1 (t 2 , L(t 2 ) − 1). Since t 2 is a result of executing attach(i, None) on t 1 , we know L(t 2 ) − 1 = i from the definition of the attach action. So, we only have to prove that the new stack equals to ϕ −1 (t 2 , i). We unfold the definition of ϕ −1 (in the proof of Theorem 3) and compare the in-order traversal of (t 2 , i) and (t 1 , i). When visiting the subtree rooted at target_node i in t 2 , we have the new token as the rightmost child; it corresponds to the new token at the top of the stack. Therefore, ϕ −1 (t 2 , i) equals to ϕ −1 (t 1 , i) plus the new token. We have proved the new stack to be ϕ −1 (t 2 , L(t 2 ) − 1) and therefore it equals to ϕ −1 • ξ(t 2 ) (Definition 4).</p><p>When X = None, we have to execute a shift and a PJ-X. The resulting stack is ϕ −1 (t 1 , i) plus a new token and a projected nonterminal X at the top. Similarly, we want to prove the new stack to equal to ϕ −1 (t 2 , L(t 2 ) − 1). The reasoning is similar, by comparing the in-order traversal of (t 1 , i) and (t 2 , i). We thus omit the details.</p><p>Therefore, in ISR, we can arrive at the state ϕ −1 • ξ(t 2 ) from the state ϕ −1 • ξ(t 1 ) by executing L(t 1 ) − i − 1 reduce actions, one shift action and one optional PJ-X action.</p><p>Lemma 3 (Translating juxtapose actions to ISR). Let t 1 and t 2 be two partial trees without unary chains, i.e., t 1 , t 2 ∈ U aj . If juxtapose(i, X, Y) brings t 1 to t 2 , The following action sequence in In-order Shift-reduce System will bring ϕ −1 • ξ(t 1 ) to ϕ −1 • ξ(t 2 ):</p><p>reduce, . . . , reduce</p><formula xml:id="formula_10">L(t1)−i , PJ-Y, shift, PJ-X if X =None .<label>(6)</label></formula><p>Proof. Similar to Lemma 2, we first execute L(t 1 ) − i reduce actions on ϕ −1 • ξ(t 1 ) = ϕ −1 (t 1 , L(t 1 ) − 1) to get ϕ −1 (t 1 , i − 1). We still have to execute one PJ-Y, one shift and one optional PJ-X.</p><p>When X = None, we only have to execute a PJ-Y and a shift. The resulting stack is ϕ −1 (t 1 , i − 1) plus a projected nonterminal Y and a new token. We prove that the new stack equals to ϕ −1 (t 2 , L(t 2 ) − 1). Since t 2 is a result of executing juxtapose(i, None, Y) on t 1 , we know L(t 2 ) − 1 = i from the definition of the juxtapose action. So, we only need the new stack to equal to ϕ −1 (t 2 , i), which can be proved by comparing the in-order traversal of (t 2 , i) and (t 1 , i − 1): In t 2 , the subtree rooted at Y has 2 children; the left child corresponds to the ith subtree on the rightmost chain of t 1 , whereas the right child is a single leaf. When we reach Y in the in-order traversal of t 2 , we first push its left subtree onto the stack. Now the stack equals to ϕ −1 (t 1 , i − 1). Then, we visit the node Y and its right child, which push the additional projected nonterminal Y and a new token onto the stack. Therefore, ϕ −1 (t 2 , i) is the new stack.</p><p>When X = None, we have to execute a PJ-Y, a shift, and a PJ-X. The derivation is similar, so we omit the details. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Fig. 2</head><label>22</label><figDesc>Actions in the attach-juxtapose transition system. Top-left: Given a target node (orange) on the rightmost chain, the attach action attaches the new token (blue) as its descendant. Top-right:The juxtapose action juxtaposes the new token and the target node in different branches of a shared ancestor (gray). Both actions can optionally create a parent (green) for the new token. Bottom: Two example actions when parsing the sentence "Arthur is King of the Britons." Bottom shows two example actions when parsing "Arthur is King of the Britons."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For a batch of training examples, the losses are summed across steps and averaged across different examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>6 T 7 return</head><label>67</label><figDesc>= Undo the last action a n−1 on T label of last_leaf 's parent 16 last_subtree = last_leaf 's parent17   18    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>21 new_label=Figure A: Case 1 - 2 .</head><label>2112</label><figDesc>The label of last_subtree's parent 22 target_node = The index of last_subtree's sibling 23 return juxtapose(target_node, parent_label, new_label) The last action is a juxtapose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure B: Case 1 - 3 .</head><label>13</label><figDesc>There are two possible cases depending on the number of siblings of last_subtree. In both cases, the last action is an attach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Case 2-1. T = empty_tree and last_subtree is the root of T . The last action is an attach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure D: Case 2 - 2 .</head><label>22</label><figDesc>The last action is an juxtapose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure E: Case 2 - 3 .</head><label>23</label><figDesc>There are two possible cases depending on the number of siblings of last_subtree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig.FFigure F :</head><label>F</label><figDesc>shows a juxtapose action when parsing "Arthur is King of the Britons." in our system, which can be translated into 4 actions in ISR: reduce, PJ-NP, shift, PJ-PP. In the figure, U AJ denotes ), (S, is, (VP, King, (NP (NP Arthur), (S, is, (VP, (NP King) (NP Arthur), (S, is, (VP, (NP King), (NP (NP Arthur), (S, is, (VP, (NP King), (NP, of (NP Arthur), (S, is, (VP, (NP King), (NP, of, An juxtapose action when parsing "Arthur is King of the Britons." in our attach-juxtapose system. It can be translated into 4 actions in In-order Shift-reduce System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>,832 training examples; 1,700 validation examples; and 2,416 testing examples. Whereas CTB consists of 17,544/352/348 examples for training/validation/testing respectively. Each example is a constituency tree with words and POS tags.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>± 0.83 93.59 ± 0.26 93.80 ± 0.26 93.40 ± 0.28Table 2summarizes our results on CTB. Our method outperforms existing parsers by a large margin (0.95 in F1). Compared to PTB, the CTB results have a larger SEM. The reason could be that CTB has a small testing set of only 348 examples, leading to less stable evaluation metrics. However, the SEM is still much smaller than our performance margin with existing parsers.</figDesc><table><row><cell>Model</cell><cell>EM</cell><cell>F1</cell><cell>LP</cell><cell>LR</cell></row><row><cell>Kitaev et al. [20]</cell><cell>-</cell><cell>91.75</cell><cell>91.96</cell><cell>91.55</cell></row><row><cell>Kitaev et al. [20] (BERT)  †</cell><cell>44.42</cell><cell>92.14</cell><cell>-</cell><cell>-</cell></row><row><cell>Zhou and Zhao [50]</cell><cell>-</cell><cell>92.18</cell><cell>92.33</cell><cell>92.03</cell></row><row><cell>Mrini et al. [28] (BERT)</cell><cell>-</cell><cell>92.64</cell><cell>93.45</cell><cell>91.85</cell></row><row><cell>Liu and Zhang [23]</cell><cell>-</cell><cell>86.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Liu and Zhang [23] (BERT)  † 44.94</cell><cell>91.81</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (BERT)</cell><cell>49.72</cell><cell></cell><cell></cell><cell></cell></row></table><note>Constituency parsing performance on Chinese Treebank (CTB). and † bear the same meaning as in Table 1. Our method outperforms state-of-the-art parsers by a large margin.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The wall time for parsing the PTB testing set. We run each experiment 5 times.Ours (XLNet) Zhou and Zhao<ref type="bibr" target="#b49">[50]</ref> (XLNet) Mrini et al.<ref type="bibr" target="#b27">[28]</ref> (XLNet) Time (seconds) 33.9 ± 0.3 37.3 ± 0.2 40.8 ± 0.9</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>± 0.11 96.23 ± 0.04 Attach-juxtapose 59.17 ± 0.33 96.34 ± 0.03</figDesc><table><row><cell cols="2">Transition system EM</cell><cell>F1</cell></row><row><cell>ISR [23]</cell><cell>58.99</cell></row></table><note>Comparison on PTB between different transition systems. Both models use XLNet for encoding tokens and GCNs for learning graph features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>53.26 ± 0.45 94.89 ± 0.03 55.84 ± 0.53 95.54 ± 0.07 44.14 ± 0.88 90.98 ± 0.35 Graph-based 57.29 ± 0.57 95.79 ± 0.05 59.17 ± 0.33 96.34 ± 0.03 49.72 ± 0.83 93.59 ± 0.26</figDesc><table><row><cell>Decode</cell><cell>BERT (PTB)</cell><cell></cell><cell>XLNet (PTB)</cell><cell></cell><cell>BERT (CTB)</cell></row><row><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>Sequence-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Algorithm 1 :</head><label>1</label><figDesc>Computing the oracle actions for a constituency tree without unary chains</figDesc><table><row><cell>1 def oracle_action_sequence(T ):</cell></row></table><note>2 if T == empty_tree:3 return []</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table B :</head><label>B</label><figDesc>Categorization of parsing errors made by Mrini et al.<ref type="bibr" target="#b27">[28]</ref> and our model (with XLNet). For each category, we show its occurrence and the number of brackets attributed to it.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://nlp.cs.nyu.edu/evalb/ 2 https://huggingface.co/transformers/pretrained_models.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Proofs about Oracle Actions</head><p>Given a constituency tree without unary chains, we prove the existence of oracle actions (Theorem 1) by proving the correctness of an algorithm (Algorithm 1) for computing oracle actions. Further, we prove that the oracle action sequence is unique. Before diving into the theorems and proofs, we first define the relevant terms:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C: Separating Content and Position Features in GCN Layers</head><p>We use the encoder in Kitaev and Klein <ref type="bibr" target="#b20">[21]</ref>. They showed that separating content and position features improves parsing performance. Specifically, the input token features to the encoder are the concatenation of content features (e.g., from BERT <ref type="bibr" target="#b9">[10]</ref> or XLNet <ref type="bibr" target="#b47">[48]</ref>) and position features from a learnable position embedding matrix P . Kitaev and Klein <ref type="bibr" target="#b20">[21]</ref> proposed a variant of self-attention that processes two types of features independently. As a result, the output token features are also the concatenation of content and position.</p><p>We extend this idea to GCN layers. Vanilla GCN layers perform a linear transformation on the input node feature (i.e. y = Θx + b) before normalizing and aggregating the neighbors 3 . We assume the node features to be concatenation of content and position: x = [x c , x p ], and perform linear transformations for them separately:</p><p>As a result, the node features at each GCN layer are also concatenation of content and position. As stated in the main paper, we merge the two types of features when generating attention weights w for nodes on the rightmost chain.</p><p>To study the effect of the separation, we present an ablation experiment. We compare our model with vanilla GCN layers and with GCN layers that separate content and position. We also scale the feature dimensions so that both models have approximately the same number of parameters. Results are summarized in <ref type="table">Table A</ref>. We run each experiment 5 times and report the mean and its standard error (SEM). For PTB, we use XLNet as the pre-trained embeddings. Results show that separating content and position improves performance in all settings, which is consistent with prior work <ref type="bibr" target="#b20">[21]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D: Error Categorization using Berkeley Parser Analyzer</head><p>We use Berkeley Parser Analyzer <ref type="bibr" target="#b21">[22]</ref> to categorize the errors of Mrini et al. <ref type="bibr" target="#b27">[28]</ref> and our model (with XLNet) on PTB. Results are shown in <ref type="table">Table B</ref>. Two methods have the same relative ordering of error categories. The 3 most frequent categories are "PP Attachment", "Single Word Phrase", and "Unary". Compared to Mrini et al., our method has more "PP Attachment" (342 vs. 320) and "UNSET move" (33 vs. 23), but fewer "Clause Attachment" (110 vs. 122) and "XoverX Unary" (48 vs. 56).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error category</head><p>Mrini et al. <ref type="bibr">[</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incremental finite-state parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><surname>Ait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Mokhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>Chanod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth conference on Applied natural language processing</title>
		<meeting>the fifth conference on Applied natural language processing</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
	<note>Annual Meeting of the Association for Computational Linguistics (ACL)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards incremental parsing of natural language using recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Soda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="9" to="25" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lrec</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-domain generalization of neural constituency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What&apos;s going on in neural constituency parsers? an analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Constituent parsing as sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Rodríguez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention guided graph convolutional networks for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multilingual constituency parsing with self-attention and pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constituency parsing with a self-attentive encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parser showdown at the wall street corral: An empirical investigation of error types in parser output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>James R Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1048" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">In-order transition-based constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="413" to="424" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving sequence-to-sequence constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linguistic structure and speech shadowing at very short latencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Marslen-Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">244</biblScope>
			<biblScope unit="issue">5417</biblScope>
			<biblScope unit="page" from="522" to="523" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incremental interpretation of categorial grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Chapter of the Association for Computational Linguistics (EACL)</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking selfattention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Mrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapa</forename><surname>Nakashole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03875</idno>
	</analytic>
	<monogr>
		<title level="m">Towards interpretability in neural parsing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probabilistic top-down parsing and language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="276" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A classifier-based parser with linear run-time complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Parsing Technology</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inter-sentence relation extraction with document-level graph convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Sunil Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Straight to the tree: Constituency parsing with neural syntactic distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The finite connectivity of linguistic structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stabler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Perspectives on Sentence Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A minimal span-based neural constituency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Processing coordinated structures: Incrementality and connectedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Sturt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Lombardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="305" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An empirical study of building a strong baseline for constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transition-based neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Look again at the syntax: Relational graph convolutional network for gendered ambiguous pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinchuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Gender Bias in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Head-driven phrase structure grammar parsing on penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Theorem 4 (Connection in actions). Let t 1 and t 2 be two partial trees without unary chains, i.e., t 1 , t 2 ∈ U aj . If a is an attach-juxtapose action that brings t 1 to t 2 , there must exist a sequence of actions in In-order Shift-without unary chains, i.e., t 1 , t 2 ∈ U aj</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Fast and accurate shift-reduce constituent parsing. If there exists a sequence of attach-juxtapose actions that brings t 1 to t 2 , there must exist a sequence of actions in In-order Shift</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
