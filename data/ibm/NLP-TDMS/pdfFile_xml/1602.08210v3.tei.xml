<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Architectural Complexity Measures of Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institut des Hautes Études Scientifiques</orgName>
								<address>
									<postCode>5 CIFAR</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Architectural Complexity Measures of Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the "depth" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. We rigorously prove each measure's existence and computability. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems <ref type="bibr">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, gradient vanishing/exploding related problems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> introduced different forms of "stacked RNNs", researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures. Some examples include <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b16">17]</ref> who explored the use of skip connections; <ref type="bibr" target="#b17">[18]</ref> who pointed out the distinction of constructing a "deep" RNN from the view of the recurrent paths and the view of the input-to-hidden and hidden-to-output maps. However, they did not rigorously formalize the notion of "depth" and its implications in "deep" RNNs. Besides "deep" RNNs, there still remains a vastly unexplored field of connecting architectures. We argue that one barrier for better understanding the architectural complexity is the lack of a general definition of the connecting architecture. This forced previous researchers to mostly consider the simple cases while neglecting other possible connecting variations. Another barrier is the lack of quantitative measurements of the complexity of different RNN connecting architectures: even the concept of "depth" is not clear with current RNNs.</p><p>In this paper, we try to address these two barriers. We first introduce a general formulation of RNN connecting architectures, using a well-defined graph representation. Observing that the RNN undergoes multiple transformations not only feedforwardly (from input to output within a time step) but also recurrently (across multiple time steps), we carry out a quantitative analysis of the number of transformations in these two orthogonal directions, which results in the definitions of recurrent depth and feedforward depth. These two depths can be viewed as general extensions of the work of <ref type="bibr" target="#b17">[18]</ref>. We also explore a quantity called the recurrent skip coefficient which measures how quickly information propagates over time. This quantity is strongly related to vanishing/exploding gradient issues, and helps deal with long term dependency problems. Skip connections crossing different timescales have also been studied by <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Instead of specific architecture design, we focus on analyzing the graph-theoretic properties of recurrent skip coefficients, revealing the fundamental difference between the regular skip connections and the ones which truly increase the recurrent skip coefficients. We rigorously prove each measure's existence and computability under the general framework.</p><p>We empirically evaluate models with different recurrent/feedforward depths and recurrent skip coefficients on various sequential modelling tasks. We also show that our experimental results further validate the usefulness of the proposed definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">General Formulations of RNN Connecting Architectures</head><p>RNNs are learning machines that recursively compute new states by applying transition functions to previous states and inputs. Its connecting architecture describes how information flows between different nodes. In this section, we formalize the concept of the connecting architecture by extending the traditional graph-based illustration to a more general definition with a finite directed multigraph and its unfolded version. Let us first define the notion of the RNN cyclic graph G c that can be viewed as a cyclic graphical representation of RNNs. We attach "weights" to the edges in the cyclic graph G c that represent time delay differences between the source and destination node in the unfolded graph.</p><formula xml:id="formula_0">Definition 2.1. Let G c = (V c , E c ) be a weighted directed multigraph 2 , in which V c = V in ∪ V out ∪ V hid is a finite nonempty set of nodes, E c ⊂ V c × V c × Z is a finite set of directed edges. Each e = (u, v, σ) ∈ E c</formula><p>denotes a directed weighted edge pointing from node u to node v with an integer weight σ. Each node v ∈ V c is labelled by an integer tuple (i, p). i ∈ {0, 2, · · · m − 1} denotes the time index of the given node, where m is the period number of the RNN, and p ∈ S, where S is a finite set of node labels. We call the weighted directed multigraph G c = (V c , E c ) an RNN cyclic graph, if (1) For every edge e = (u, v, σ) ∈ E c , let i u and i v denote the time index of node u and v,</p><formula xml:id="formula_1">then σ = i v − i u + k · m for some k ∈ Z. (2) There exists at least one directed cycle 3 in G c . (3)</formula><p>For any closed walk ω, the sum of all the σ along ω is not zero.</p><p>Condition (1) assures that we can get a periodic graph (repeating pattern) when unfolding the RNN through time. Condition (2) excludes feedforward neural networks in the definition by forcing to have at least one cycle in the cyclic graph. Condition (3) simply avoids cycles after unfolding. The cyclic representation can be seen as a time folded representation of RNNs, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a). Given an RNN cyclic graph G c , we unfold G c over time t ∈ Z by the following procedure: Definition 2.2 (Unfolding). Given an RNN cyclic graph G c = (V c , E c , σ), we define a new infinite set of nodes V un = {(i + km, p)|(i, p) ∈ V, k ∈ Z}. The new set of edges E un ∈ V un × V un is constructed as follows: ((t, p), (t , p )) ∈ E un if and only if there is an edge e = ((i, p), (i , p ), σ) ∈ E such that t − t = σ, and t ≡ i(mod m). The new directed graph G un = (V un , E un ) is called the unfolding of G c . Any infinite directed graph that can be constructed from an RNN cyclic graph through unfolding is called an RNN unfolded graph. Lemma 2.1. The unfolding G un of any RNN cyclic graph G c is a directed acyclic graph (DAG).</p><p>Figure 1(a) shows an example of two graph representations G un and G c of a given RNN. Consider the edge from node (1, 7) going to node (0, 3) in G c . The fact that it has weight 1 indicates that the corresponding edge in G un travels one time step, ((t + 1, 7), (t + 2, 3)). Note that node (0, 3) also has a loop with weight 2. This loop corresponds to the edge ((t, 3), (t + 2, 3)). The two kinds of graph representations we presented above have a one-to-one correspondence. Also, any graph structure θ on G un is naturally mapped into a graph structureθ on G c . Given an edge tupleē = (u, v, σ) in G c , σ stands for the number of time steps crossed byē's covering edges in E un , i.e., for every corresponding edge e ∈ G un , e must start from some time index t to t + σ. Hence σ corresponds to the "time delay" associated with e. In addition, the period number m in Definition 2.1 can be interpreted as the time length of the entire non-repeated recurrent structure in its unfolded RNN graph G un . In other words, shifting the G un through time by km time steps will result in a DAG which is Vin is denoted by square, V hid is denoted by circle and Vout is denoted by diamond. In Gc, the number on each edge is its corresponding σ. The longest path is colored in red. The longest input-output path is colored in yellow and the shortest path is colored blue. The value of three measures are dr = identical to G un , and m is the smallest number that has such property for G un . Most traditional RNNs have m = 1, while some special structures like hierarchical or clockwork RNN <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref> have m &gt; 1.</p><p>For example, <ref type="figure" target="#fig_0">Figure 1</ref>(a) shows that the period number of this specific RNN is 2.</p><p>The connecting architecture describes how information flows among RNN units. Assumev ∈ V c is a node in G c , let In(v) denotes the set of incoming nodes ofv,</p><formula xml:id="formula_2">In(v) = {ū|(ū,v) ∈ E c }.</formula><p>In the forward pass of the RNN, the transition function Fv takes outputs of nodes In(v) as inputs and computes a new output. For example, vanilla RNNs units with different activation functions, LSTMs and GRUs can all be viewed as units with specific transition functions. We now give the general definition of an RNN:</p><formula xml:id="formula_3">Definition 2.3. An RNN is a tuple (G c , G un , {Fv}v ∈Vc ), in which G un = (V un , E un )</formula><p>is the unfolding of RNN cyclic graph G c , and {Fv}v ∈Vc is the set of transition functions. In the forward pass, for each hidden and output node v ∈ V un , the transition function Fv takes all incoming nodes of v as the input to compute the output.</p><p>An RNN is homogeneous if all the hidden nodes share the same form of the transition function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Measures of Architectural Complexity</head><p>In this section, we develop different measures of RNNs' architectural complexity, focusing mostly on the graph-theoretic properties of RNNs. To analyze an RNN solely from its architectural aspect, we make the mild assumption that the RNN is homogeneous. We further assume the RNN to be unidirectional. For a bidirectional RNN, it is more natural to measure the complexities of its unidirectional components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recurrent Depth</head><p>Unlike feedforward models where computations are done within one time frame, RNNs map inputs to outputs over multiple time steps. In some sense, an RNN undergoes transformations along both feedforward and recurrent dimensions. This fact suggests that we should investigate its architectural complexity from these two different perspectives. We first consider the recurrent perspective.</p><p>The conventional definition of depth is the maximum number of nonlinear transformations from inputs to outputs. Observe that a directed path in an unfolded graph representation G un corresponds to a sequence of nonlinear transformations. Given an unfolded RNN graph G un , ∀i, n ∈ Z, let D i (n) be the length of the longest path from any node at starting time i to any node at time i + n. From the recurrent perspective, it is natural to investigate how D i (n) changes over time. Generally speaking, D i (n) increases as n increases for all i. Such increase is caused by the recurrent structure of the RNN which keeps adding new nonlinearities over time. Since D i (n) approaches ∞ as n approaches ∞, <ref type="bibr" target="#b3">4</ref> to measure the complexity of D i (n), we consider its asymptotic behaviour, i.e., the limit of Di(n) n as n → ∞. Under a mild assumption, this limit exists. The following theorem prove such limit's computability and well-definedness:</p><formula xml:id="formula_4">Theorem 3.2 (Recurrent Depth).</formula><p>Given an RNN and its two graph representation G un and G c , we denote C(G c ) to be the set of directed cycles in G c . For ϑ ∈ C(G c ), let l(ϑ) denote the length of ϑ and σ s (ϑ) denote the sum of edge weights σ along ϑ. Under a mild assumption 5 ,</p><formula xml:id="formula_5">d r = lim n→+∞ D i (n) n = max ϑ∈C(Gc) l(ϑ) σ s (ϑ) .<label>(1)</label></formula><p>More intuitively, d r is a measure of the average maximum number of nonlinear transformations per time step as n gets large. Thus, we call it recurrent depth: Definition 3.1 (Recurrent Depth). Given an RNN and its two graph representations G un and G c , we call d r , defined in Eq.(1), the recurrent depth of the RNN.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>(a), one can easily verify that</p><formula xml:id="formula_6">D t (1) = 5, D t (2) = 6, D t (3) = 8, D t (4) = 9 . . . Thus Dt(1) 1 = 5, Dt(2) 2 = 3, Dt(3) 3 = 8 3 , Dt(4) 4 = 9 4 . . .</formula><p>., which eventually converges to 3 2 as n → ∞. As n increases, most parts of the longest path coincides with the path colored in red. As a result, d r coincides with the number of nodes the red path goes through per time step. Similarly in G c , observe that the red cycle achieves the maximum ( 3 2 ) in Eq.(1). Usually, one can directly calculate d r from G un . It is easy to verify that simple RNNs and stacked RNNs share the same recurrent depth which is equal to 1. This reveals the fact that their nonlinearities increase at the same rate, which suggests that they will behave similarly in the long run. This fact is often neglected, since one would typically consider the number of layers as a measure of depth, and think of stacked RNNs as "deep" and simple RNNs as "shallow", even though their discrepancies are not due to recurrent depth (which regards time) but due to feedforward depth, defined next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feedforward Depth</head><p>Recurrent depth does not fully characterize the nature of nonlinearity of an RNN. As previous work suggests <ref type="bibr" target="#b2">[3]</ref>, stacked RNNs do outperform shallow ones with the same hidden size on problems where a more immediate input and output process is modeled. This is not surprising, since the growth rate of D i (n) only captures the number of nonlinear transformations in the time direction, not in the feedforward direction. The perspective of feedforward computation puts more emphasis on the specific paths connecting inputs to outputs. Given an RNN unfolded graph G un , let D * i (n) be the length of the longest path from any input node at time step i to any output node at time step i + n. Clearly, when n is small, the recurrent depth cannot serve as a good description for D * i (n). In fact. it heavily depends on another quantity which we call feedforward depth. The following proposition guarantees the existence of such a quantity and demonstrates the role of both measures in quantifying the nonlinearity of an RNN. Proposition 3.3.1 (Input-Output Length Least Upper Bound). Given an RNN with recurrent depth d r , we denote d f = sup i,n∈Z D * i (n) − n · d r , the supremum d f exists and thus we have the following upper bound for D * i (n):</p><formula xml:id="formula_7">D * i (n) ≤ n · d r + d f .</formula><p>The above upper bound explicitly shows the interplay between recurrent depth and feedforward depth: when n is small, D * i (n) is largely bounded by d f ; when n is large, d r captures the nature of the bound (≈ n · d r ). These two measures are equally important, as they separately capture the maximum number of nonlinear transformations of an RNN in the long run and in the short run. Definition 3.2. (Feedforward Depth) Given an RNN with recurrent depth d r and its two graph representations G un and G c , we call d f , defined in Proposition 3.3.1, the feedforward depth 6 of the RNN.</p><p>The following theorem proves d f 's computability: Theorem 3.4 (Feedforward Depth). Given an RNN and its two graph representations G un and G c , we denote ξ(G c ) the set of directed paths that start at an input node and end at an output node in G c . For γ ∈ ξ(G c ), denote l(γ) the length and σ s (γ) the sum of σ along γ. Then we have:</p><formula xml:id="formula_8">d f = sup i,n∈Z D * i (n) − n · d r = max γ∈ξ(Gc) l(γ) − σ s (γ) · d r ,</formula><p>where m is the period number and d r is the recurrent depth of the RNN.</p><p>For example, in <ref type="figure" target="#fig_0">Figure 1</ref>(a), one can easily verify that d f = D * t (0) = 3. Most commonly, d f is the same as D * t (0), i.e., the maximum length from an input to its current output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Recurrent Skip Coefficient</head><p>Depth provides a measure of the complexity of the model. But such a measure is not sufficient to characterize behavior on long-term dependency tasks. In particular, since models with large recurrent depths have more nonlinearities through time, gradients can explode or vanish more easily. On the other hand, it is known that adding skip connections across multiple time steps may help improve the performance on long-term dependency problems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. To measure such a "skipping" effect, we should instead pay attention to the length of the shortest path from time i to time i + n. In G un , ∀i, n ∈ Z, let d i (n) be the length of the shortest path. Similar to the recurrent depth, we consider the growth rate of d i (n). Theorem 3.6 (Recurrent Skip Coefficient). Given an RNN and its two graph representations G un and G c , under mild assumptions 7 j = lim</p><formula xml:id="formula_9">n→+∞ d i (n) n = min ϑ∈C(Gc) l(ϑ) σ s (ϑ) .<label>(2)</label></formula><p>Since it is often the case that j is smaller or equal to 1, it is more intuitive to consider its reciprocal. Definition 3.3. (Recurrent Skip Coefficient) <ref type="bibr" target="#b7">8</ref> . Given an RNN and corresponding G un and G c , we define s = 1 j , whose reciprocal is defined in Eq. <ref type="formula" target="#formula_9">(2)</ref>, as the recurrent skip coefficient of the RNN. With a larger recurrent skip coefficient, the number of transformations per time step is smaller. As a result, the nodes in the RNN are more capable of "skipping" across the network, allowing unimpeded information flow across multiple time steps, thus alleviating the problem of learning long term dependencies. In particular, such effect is more prominent in the long run, due to the network's recurrent structure. Also note that not all types of skip connections can increase the recurrent skip coefficient. We will consider specific examples in our experimental results section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>In this section we conduct a series of experiments to investigate the following questions: (1) Is recurrent depth a trivial measure? (2) Can increasing depth yield performance improvements? (3) Can increasing the recurrent skip coefficient improve the performance on long term dependency tasks? (4) Does the recurrent skip coefficient suggest something more compared to simply adding skip connections? We show our evaluations on both tanh RNNs and LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks and Training Settings</head><p>PennTreebank dataset: We evaluate our models on character level language modelling using the PennTreebank dataset <ref type="bibr" target="#b21">[22]</ref>. It contains 5059k characters for training, 396k for validation and 446k for test, and has a alphabet size of 50. We set each training sequence to have the length of 50. Quality of fit is evaluated by the bits-per-character (BPC) metric, which is log 2 of perplexity.</p><p>text8 dataset: Another dataset used for character level language modelling is the text8 dataset 9 , which contains 100M characters from Wikipedia with an alphabet size of 27. We follow the setting from <ref type="bibr" target="#b22">[23]</ref> and each training sequence has length of 180.</p><p>adding problem: The adding problem (and the following copying memory problem) was introduced in <ref type="bibr" target="#b9">[10]</ref>. For the adding problem, each input has two sequences with length of T where the first sequence are numbers sampled from uniform[0, 1] and the second sequence are all zeros except two elements which indicates the position of the two elements in the first sequence that should be summed together. The output is the sum. We follow the most recent results and experimental settings in <ref type="bibr" target="#b23">[24]</ref> (same for copying memory). copying memory problem: Each input sequence has length of T + 20, where the first 10 values are random integers between 1 to 8. The model should remember them after T steps. The rest of the sequence are all zeros, except for the last 11 entries in the sequence, which starts with 9 as a marker indicating that the model should begin to output its memorized values. The model is expected to give zero outputs at every time step except the last 10 entries, where it should generate (copy) the 10 values in the same order as it has seen at the beginning of the sequence. The goal is to minimize the average cross entropy of category predictions at each time step.   sequential MNIST dataset: Each MNIST image data is reshaped into a 784 × 1 sequence, turning the digit classification task into a sequence classification one with long-term dependencies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>. A slight modification of the dataset is to permute the image sequences by a fixed random order beforehand (permuted MNIST). Results in <ref type="bibr" target="#b24">[25]</ref> have shown that both tanh RNNs and LSTMs did not achieve satisfying performance, which also highlights the difficulty of this task.</p><p>For all of our experiments we use Adam <ref type="bibr" target="#b25">[26]</ref> for optimization, and conduct a grid search on the learning rate in {10 −2 , 10 −3 , 10 −4 , 10 −5 }. For tanh RNNs, the parameters are initialized with samples from a uniform distribution. For LSTM networks we adopt a similar initialization scheme, while the forget gate biases are chosen by the grid search on {−5, −3, −1, 0, 1, 3, 5}. We employ early stopping and the batch size was set to 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Recurrent Depth is Non-trivial</head><p>To investigate the first question, we compare 4 similar connecting architectures: 1-layer (shallow) "sh", 2-layers stacked "st", 2-layers stacked with an extra bottom-up connection "bu", and 2-layers stacked with an extra top-down connection "td", as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), left panel. Although the four architectures look quite similar, they have different recurrent depths: sh, st and bu have d r = 1, while td has d r = 2. Note that the specific construction of the extra nonlinear transformations in td is not conventional. Instead of simply adding intermediate layers in hidden-to-hidden connection, as reported in <ref type="bibr" target="#b17">[18]</ref>, more nonlinearities are gained by a recurrent flow from the first layer to the second layer and then back to the first layer at each time step (see the red path in <ref type="figure" target="#fig_1">Figure 2a</ref>, left panel).</p><p>We first evaluate our architectures using tanh RNN on PennTreebank, where sh has hidden-layer size of 1600. Next, we evaluate four different models for text8 which are tanh RNN-small, tanh RNN-large, LSTM-small, LSTM large, where the model's sh architecture has hidden-layer size of 512, 2048, 512, 1024 respectively. Given the architecture of the sh model, we set the remaining three architectures to have the same number of parameters.  <ref type="bibr" target="#b22">[23]</ref> with Multiplicative RNN (MRNN). It is also interesting to note the improvement we obtain when switching from bu to td. The only difference between these two architectures lies in changing the direction of one connection (see <ref type="figure" target="#fig_1">Figure 2</ref>(a)), which also increases the recurrent depth. Such a fundamental difference is by no means self-evident, but this result highlights the necessity of the concept of recurrent depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparing Depths</head><p>From the previous experiment, we found some evidence that with larger recurrent depth, the performance might improve. To further investigate various implications of depths, we carry out a systematic analysis for both recurrent depth d r and feedforward depth d f on text8 and sequential MNIST datasets. We build 9 models in total with d r = 1, 2, 3 and d f = 2, 3, 4, respectively (as shown in <ref type="figure" target="#fig_1">Figure 2(b)</ref>). We ensure that all the models have roughly the same number of parameters (e.g., the model with d r = 1 and d f = 2 has a hidden-layer size of 360).  <ref type="bibr" target="#b17">[18]</ref>, which suggests that d r should be neither too small nor too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Recurrent Skip Coefficients</head><p>To investigate whether increasing a recurrent skip coefficient s improves model performance on long term dependency tasks, we compare models with increasing s on the adding problem, the copying memory problem and the sequential MNIST problem (without/with permutation, denoted as MNIST and pMNIST). Our baseline model is the shallow architecture proposed in <ref type="bibr" target="#b24">[25]</ref>. To increase the recurrent skip coefficient s, we add connections from time step t to time step t + k for some fixed integer k, shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), right panel. By using this specific construction, the recurrent skip coefficient increases from 1 (i.e., baseline) to k and the new model with extra connection has 2 hidden matrices (one from t to t + 1 and the other from t to t + k).</p><p>For the adding problem, we follow the same setting as in <ref type="bibr" target="#b23">[24]</ref>. We evaluate the baseline LSTM with 128 hidden units and an LSTM with s = 30 and 90 hidden units (roughly the same number of parameters as the baseline). The results are quite encouraging: as suggested in <ref type="bibr" target="#b23">[24]</ref> baseline LSTM works well for input sequence lengths T = 100, 200, 400 but fails when T = 750. On the other hand, we observe that the LSTM with s = 30 learns perfectly when T = 750, and even if we increase T to 1000, LSTM with s = 30 still works well and the loss reaches to zero.</p><p>For the copying memory problem, we use a single layer RNN with 724 hidden units as our basic model, and 512 hidden units with skip connections. So they have roughly the same number of parameters. Models with a higher recurrent skip coefficient outperform those without skip connections by a large margin. When T = 200, test set cross entropy (CE) of a basic model only yields 0.2409, but with s = 40 it is able to reach a test set cross entropy of 0.0975. When T = 300, a model with s = 30 yields a test set CE of 0.1328, while its baseline could only reach 0.2025. We varied the sequence length (T ) and recurrent skip coefficient (s) in a wide range (where T varies from 100 up to 300, and s from 10 up to 50), and found that this kind of improvement persists.</p><p>For the sequential MNIST problem, the hidden-layer size of the baseline model is set to 90 and models with s &gt; 1 have hidden-layer sizes of 64. The results in <ref type="table" target="#tab_5">Table 2</ref>, top-left panel, show that tanh RNNs with recurrent skip coefficient s larger than 1 could improve the model performance dramatically. Within a reasonable range of s, test accuracy increases quickly as s becomes larger.</p><p>We note that our model is the first tanh RNN model that achieves good performance on this task, even improving upon the method proposed in <ref type="bibr" target="#b24">[25]</ref>. In addition, we also formally compare with the previous results reported in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>, where our model (referred to as stanh) has a hidden-layer size of 95, which is about the same number of parameters as in the tanh model of <ref type="bibr" target="#b23">[24]</ref>. <ref type="table" target="#tab_5">Table 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>MNIST pMNIST iRNN <ref type="bibr" target="#b24">[25]</ref> 97.0 ≈82.0 uRNN <ref type="bibr" target="#b23">[24]</ref> 95.1 91.4 LSTM <ref type="bibr" target="#b23">[24]</ref> 98.2 88.0 RNN(tanh) <ref type="bibr" target="#b24">[25]</ref> ≈35.0 ≈35.0 stanh(s = <ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11)</ref> 98  and achieves almost the same performance as LSTM on the MNIST dataset with only 25% number of parameters <ref type="bibr" target="#b23">[24]</ref>. Note that obtaining good performance on sequential MNIST requires a larger s than that for pMNIST (see Appendix B.4 for more details). LSTMs also showed performance boost and much faster convergence speed when using larger s, as displayed in <ref type="table" target="#tab_5">Table 2</ref>, top-right panel. LSTM with s = 3 already performs quite well and increasing s did not result in any significant improvement, while in pMNIST, the performance gradually improves as s increases from 4 to 6. We also observed that the LSTM network performed worse on permuted MNIST compared to a tanh RNN. Similar result was also reported in <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Recurrent Skip Coefficients vs. Skip Connections</head><p>We also investigated whether the recurrent skip coefficient can suggest something more than simply adding skip connections. We design 4 specific architectures shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), right panel. <ref type="formula" target="#formula_5">(1)</ref> is the baseline model with a 2-layer stacked architecture, while the other three models add extra skip connections in different ways. Note that these extra skip connections all cross the same time length k. In particular, <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_10">(3)</ref> share quite similar architectures. However, ways in which the skip connections are allocated makes big differences on their recurrent skip coefficients: (2) has s = 1, (3) has s = k 2 and (4) has s = k. Therefore, even though <ref type="formula" target="#formula_9">(2)</ref>, <ref type="formula" target="#formula_10">(3)</ref> and <ref type="formula" target="#formula_23">(4)</ref> all add extra skip connections, the fact that their recurrent skip coefficients are different might result in different performance.</p><p>We evaluated these architectures on the sequential MNIST and pMNIST datasets. The results show that differences in s indeed cause big performance gaps regardless of the fact that they all have skip connections (see <ref type="table" target="#tab_5">Table 2</ref>, bottom-right panel). Given the same k, the model with a larger s performs better. In particular, model (3) is better than model (2) even though they only differ in the direction of the skip connections. It is interesting to see that for MNIST (unpermuted), the extra skip connection in model (2) (which does not really increase the recurrent skip coefficient) brings almost no benefits, as model (2) and model (1) have almost the same results. This observation highlights the following point: when addressing the long term dependency problems using skip connections, instead of only considering the time intervals crossed by the skip connection, one should also consider the model's recurrent skip coefficient, which can serve as a guide for introducing more powerful skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we first introduced a general formulation of RNN architectures, which provides a solid framework for the architectural complexity analysis. We then proposed three architectural complexity measures: recurrent depth, feedforward depth, and recurrent skip coefficients capturing both short term and long term properties of RNNs. We also found empirical evidences that increasing recurrent depth and feedforward depth might yield performance improvements, increasing feedforward depth might not help on long term dependency tasks, while increasing the recurrent skip coefficient can largely improve performance on long term dependency tasks. These measures and results can provide guidance for the design of new recurrent architectures for particular learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>To show theorem 3.2, we first consider the most general case in which d r is defined (Theorem A.1). Then we discuss the mild assumptions under which we can reduce to the original limit (Proposition A.1.1). Additionally, we introduce some notations that will be used throughout the proof.</p><p>If v = (t, p) ∈ G un is a node in the unfolded graph, it has a corresponding node in the folded graph, which is denoted byv = (t, p). Theorem A.1. Given an RNN cyclic graph and its unfolded representation (G c , G un ), we denote C(G c ) the set of directed cycles in G c . For ϑ ∈ C(G c ), denote l(ϑ) the length of ϑ and σ s (ϑ) the sum of σ along ϑ. Write d i = lim sup k→∞ Di(n) n . <ref type="bibr" target="#b9">10</ref> we have :</p><p>• The quantity d i is periodic, in the sense that d i+m = d i , ∀i ∈ N.</p><formula xml:id="formula_10">• Let d r = max i d i , then d r = max ϑ∈C(Gc) l(ϑ) σ s (ϑ)<label>(3)</label></formula><p>Proof. The first statement is easy to prove. Because of the periodicity of the graph, any path from time step i to i + n corresponds to an isomorphic path from time step i + m to i + m + n. Passing to limit, and we can deduce the first statement.</p><p>Now we prove the second statement. Write ϑ 0 = argmax ϑ l(ϑ) σs(ϑ) . First we prove that d ≥ l(ϑ0) σs(ϑ0) . Let c 1 = (t 1 , p 1 ) ∈ G un be a node such that if we denote c 1 = (t 1 , p 1 ) the image of c 1 on the cyclic graph, we have c 1 ∈ ϑ 0 . Consider the subsequence S 0 =</p><formula xml:id="formula_11">D t 1 (kσs(ϑ0)) kσs(ϑ0) ∞ k=1 of D t 1 (n) n ∞ n=1</formula><p>.</p><p>From the definition of D and the fact that ϑ 0 is a directed circle, we have D t1 (kσ s (ϑ 0 )) ≥ kl(ϑ 0 ), by considering the path on G un corresponding to following ϑ 0 k -times. So we have</p><formula xml:id="formula_12">d r ≥ lim sup k→+∞ D i (n) n ≥ lim sup k→+∞ D t1 (kσ s (ϑ 0 )) kσ s (ϑ 0 ) ≥ kl(ϑ 0 ) kσ s (ϑ 0 ) = l(ϑ 0 ) σ s (ϑ 0 )</formula><p>Next we prove d r ≤ l(ϑ0) σs(ϑ0) . It suffices to prove that, for any ≥ 0, there exists N &gt; 0, such that for any path γ : {(t 0 , p 0 ), (t 1 , p 1 ), · · · , (t nγ , p nγ )} with t nγ − t 1 &gt; N , we have nγ tn γ −t1 ≤ l(ϑ0) σs(ϑ0) + . We denoteγ as the image of γ on the cyclic graph.γ is a walk with repeated nodes and edges. Also, we assume there are in total Γ nodes in cyclic graph G c .</p><p>We first decomposeγ into a path and a set of directed cycles. More precisely, there is a path γ 0 and a sequence of directed cycles C = C 1 (γ), C 2 (γ), · · · , C w (γ) on G c such that:</p><p>• The starting and end nodes of γ 0 is the same as γ. (If γ starts and ends at the same node, take γ 0 as empty.)</p><p>• The catenation of the sequences of directed edges E(γ 0 ), E(C 1 (γ)), E(C 2 (γ)), · · · , E(C w (γ)) is a permutation of the sequence of edges of E(γ).</p><p>The existence of such a decomposition can be proved iteratively by removing directed cycles from γ. Namely, if γ is not a paths, there must be some directed cycles C on γ. Removing C from γ, we can get a new walk γ . Inductively apply this removal, we will finally get a (possibly empty) path and a sequence of directed cycles. For a directed path or loop γ, we write D(γ) the distance between the ending node and starting node when travel through γ once. We have</p><formula xml:id="formula_13">D(γ 0 ) := t nγ − t 0 + |γ0| i=1 σ(e i )</formula><p>where e i , i ∈ {1, 2, · · · , |γ 0 |} is all the edges of γ 0 .t denotes the module of t: t ≡t(mod m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>So we have:</head><formula xml:id="formula_14">|D(γ 0 )| ≤ m + Γ · max e∈Gc σ(e) = M</formula><p>For convenience, we denote l 0 , l 1 , · · · , l w to be the length of path γ 0 and directed cycles C 1 (γ), C 2 (γ), · · · , C w (γ). Obviously we have:</p><formula xml:id="formula_15">n γ = w i=0 l i</formula><p>And also, we have</p><formula xml:id="formula_16">t nγ − t 1 = w i=1 σ s (C i ) + D(γ 0 ) So we have: n γ t nγ − t 1 = l 0 t nγ − t 1 + w i=1 l i t nγ − t 1 ≤ Γ N + w i=1 l i t nγ − t 1</formula><p>In which we have for all i ∈ {1, 2, · · · , w} :</p><formula xml:id="formula_17">l i t nγ − t 1 = l i σ s (C i ) · σ s (C i ) t nγ − t 1 ≤ l(ϑ 0 ) σ s (ϑ 0 ) σ s (C i ) t nγ − t 1 So we have: w i=1 l i t nγ − t 1 ≤ l(ϑ 0 ) σ s (ϑ 0 ) 1 − D(γ 0 ) t nγ − t 1 ≤ l(ϑ 0 ) σ s (ϑ 0 ) + M N</formula><p>in which M and Γ are constants depending only on the RNN G c .</p><p>Finally we have:</p><formula xml:id="formula_18">n γ t nγ − t 1 ≤ l(ϑ 0 ) σ s (ϑ 0 ) + M + Γ N</formula><p>take N &gt; M +Γ , we can prove the fact that d r ≤ l(ϑ0) σs(ϑ0) .</p><p>Proposition A.1.1. Given an RNN and its two graph representations G un and G c , if ∃ϑ ∈ C(G c ) such that (1) ϑ achieves the maximum in Eq. <ref type="formula" target="#formula_10">(3)</ref> and <ref type="formula" target="#formula_9">(2)</ref> the corresponding path of ϑ in G un visits nodes at every time step, then we have</p><formula xml:id="formula_19">d r = max i∈Z lim sup n→+∞ D i (n) n = lim n→+∞ D i (n) n</formula><p>Proof. We only need to prove, in such a graph, for all i ∈ Z we have</p><formula xml:id="formula_20">lim inf n→+∞ D i (n) n ≥ max i∈Z lim sup n→+∞ D i (n) n = d r</formula><p>Because it is obvious that</p><formula xml:id="formula_21">liminf n→+∞ D i (n) n ≤ d r</formula><p>Namely, it suffice to prove, for all i ∈ Z, for all &gt; 0, there is an N &gt; 0, such that when n &gt; N , we have Di(n) n ≥ d r − . On the other hand, for k ∈ N, if we assume (k+1)σ s (ϑ)+i &gt; n ≥ i+k·σ s (ϑ), then according to condition (2) we have</p><formula xml:id="formula_22">D i (n) n ≥ k · l(ϑ) (k + 1)σ s (ϑ) = l(ϑ) σ s (ϑ) − l(ϑ) σ s (ϑ) 1 k + 1</formula><p>We can see that if we set k &gt; σs(ϑ) l(ϑ) , the inequality we wanted to prove.</p><p>We now prove Proposition 3.3.1 and Theorem 3.4 as follows.</p><p>Lastly, we show Theorem 3.6. Theorem A.3. Given an RNN cyclic graph and its unfolded representation (G c , G un ), we denote C(G c ) the set of directed cycles in G c . For ϑ ∈ C(G c ), denote l(ϑ) the length of ϑ and σ s (ϑ) the sum of σ along ϑ. Write s i = lim inf k→∞ di(n) n . We have : • The quantity s i is periodic, in the sense that s i+m = s i , ∀i ∈ N.</p><p>• Let s = min i s i , then</p><formula xml:id="formula_23">d r = min ϑ∈C(Gc) l(ϑ) σ s (ϑ) .<label>(4)</label></formula><p>Proof. The proof is essentially the same as the proof of the first theorem. So we omit it here.</p><p>Proposition A.3.1. Given an RNN and its two graph representations G un and G c , if ∃ϑ ∈ C(G c ) such that (1) ϑ achieves the minimum in Eq. <ref type="formula" target="#formula_23">(4)</ref> and <ref type="formula" target="#formula_9">(2)</ref> the corresponding path of ϑ in G un visits nodes at every time step, then we have</p><formula xml:id="formula_24">s = min i∈Z lim inf n→+∞ d i (n) n = lim n→+∞ d i (n) n .</formula><p>Proof. The proof is essentially the same as the proof of the Proposition A.1.1. So we omit it here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 RNNs with tanh</head><p>In this section we explain the functional dependency among nodes in RNNs with tanh in detail.</p><p>The transition function for each node is the tanh function. The output of a node v is a vector h v . To compute the output for a node, we simply take all incoming nodes as input, and sum over their affine transformations and then apply the tanh function (we omit the bias term for simplicity).</p><formula xml:id="formula_25">h v = tanh   u∈In(v) W(u)h u   ,</formula><p>where W(·) represents a real matrix.</p><p>u v p q As a more concrete example, consider the "bottom-up" architecture in <ref type="figure" target="#fig_4">Figure 3</ref>, with which we did the experiment described in Section 4.2. To compute the output of node v,</p><formula xml:id="formula_26">h v = tanh(W(u)h u + W(p)h p + W(q)h q ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 LSTMs</head><p>In this section we explain the Multidimensional LSTM (introduced by [1]) which we use for experiments with LSTMs.</p><p>The output of a node v of the LSTM is a 2-tuple (c v ,h v ), consisting of a cell memory state c v and a hidden state h v . The transition function F is applied to each node indistinguishably. We describe the computation of F below in a sequential manner (we omit the bias term for simplicity).</p><formula xml:id="formula_27">z = g   u∈In(v) W z (u)h u   block input i = σ   u∈In(v) W i (u)h u   input gate o = σ   u∈In(v) W o (u)h u   output gate {f u } =    σ   u ∈In(v) W fu (u )h u   |u ∈ In(v)    A set of forget gates c v = i z + u∈In(v) f u c u cell state h v = o c v hidden state</formula><p>Note that the Multidimensional LSTM includes the usual definition of LSTM as a special case, where the extra forget gates are 0 (i.e., bias term set to -∞) and extra weight matrices are 0. We again consider the architecture bu in <ref type="figure" target="#fig_4">Fig. 3</ref>. We first compute the block input, the input gate and the output gate by summing over all affine transformed outputs of u, p, q, and then apply the activation function. For example, to compute the input gate, we have</p><formula xml:id="formula_28">i = σ (W i (u)h u + W i (p)h p + W i (q)h q ) .</formula><p>Next, we compute one forget gate for each pair of (v, u), (v, p), (v, q). The way of computing a forget gate is the same as computing the other gates. For example, the forget gate in charge of the connection of u → v is computed as,</p><formula xml:id="formula_29">f u = σ (W fu (u)h u + W fu (p)h u + W fu (q)h u ) .</formula><p>Then, the cell state is simply the sum of all element-wise products of the input gate with the block output and forget gates with the incoming nodes' cell memory states,</p><formula xml:id="formula_30">c v = i z + f u c u + f p c p + f q c q .</formula><p>Lastly, the hidden state is computed as usual,</p><formula xml:id="formula_31">h v = o c v .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Recurrent Depth is Non-trivial</head><p>The validation curves of the 4 different connecting architectures sh, st, bu and td on text8 dataset for both tanhRNN-small and LSTM-small are shown below:   <ref type="figure">Figure 5</ref> shows all the validation curves for the 9 architectures on text8 dataset, with their d r = 1, 2, 3 and d f = 2, 3, 4 respectively. We initialize hidden-to-hidden matrices from uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Full Comparisons on Depths</head><p>Also, to see if increasing feedforward depth/ recurrent depth helps for long term dependency problems, we evaluate these 9 architectures on sequential MNIST task, with roughly the same number of parameters( 8K, where the first architecture with d r = 1 and d f = 2 has hidden size of 90.). Hidden-to-hidden matrices are initialized from uniform distribution. <ref type="figure">Figure 6</ref> clearly show that, as the feedforward depth increases, the model performance stays roughly the same. In addition, note that increasing recurrent depth might even result in performance decrease. This is possibly because that larger recurrent depth amplifies the gradient vanishing/exploding problems, which is detrimental on long term dependency tasks.</p><p>1 ×10 5 more easily in the beginning, but in the long run, (3) seems to be more superior, because of its more prominent skipping effect over time.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) An example of an RNN's Gc and Gun.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 2 ,</head><label>2</label><figDesc>d f = 7 2 and s = 2. (b) 5 more examples. (1) and (2) have dr = 2, 3 2 , (3) has d f = 5, (4) and (5) has s = 2, 3 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Left: (a) The architectures for sh, st, bu and td, with their (dr, d f ) equal to (1, 2), (1, 3), (1, 3) and (2, 3), respectively. The longest path in td are colored in red. (b) The 9 architectures denoted by their (d f , dr) with dr = 1, 2, 3 and d f = 2, 3, 4. In both (a) and (b), we only plot hidden states at two adjacent time steps and the connections between them (the period number is 1). Right: (a) Various architectures that we consider in Section 4.4. From top to bottom are baseline s = 1, and s = 2, s = 3. (b) Proposed architectures that we consider in Section 4.5 where we take k = 3 as an example. The shortest paths in (a) and (b) that correspond to the recurrent skip coefficients are colored in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, bottom-left panel, shows that our simple architecture improves upon the uRNN by 2.6% on pMNIST, stanh s = 1 s = 5 s = 9 s = 13 s = 21 MNIST 34.9 46.9 74.9 85.4 87.8 s = 1 s = 3 s = 5 s = 7 s = 9 pMNIST 49.8 79.1 84.3 88.9 88.0 LSTM s = 1 s = 3 s = 5 s = 7 s = 9 MNIST 56.2 87.2 86.4 86.4 84.8 s = 1 s = 3 s = 4 s = 5 s = 6 pMNIST 28.5 25.0 60.8 62.2 65.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>"Bottom-up" architecture (bu).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Validation curves for sh, st, bu and td on test8 dataset. Left: results for tanhRNN-small. Right: results for LSTM-small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Test curves on MNIST/pMNIST, with tanh and LST M . The numbers in the legend denote the recurrent skip coefficient s of each architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Test curves on MNIST/pMNIST for architecture (1), (2), (3) and (4), with tanh. The recurrent skip coefficient s of each architecture is shown in the legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Left</figDesc><table /><note>: Test BPCs of sh, st, bu, td for tanh RNNs and LSTMs. Right: Test BPCs of tanh RNNs with recurrent depth dr = 1, 2, 3 and feedforward depth d f = 2, 3, 4 respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 ,</head><label>1</label><figDesc>left panel, shows that the td architecture outperforms all the other architectures for all the different models. Specifically, td in tanh RNN achieves a test BPC of 1.49 on PennTreebank, which is comparable to the BPC of 1.48 reported in [27] using stabilization techniques. Similar improvements are shown for LSTMs, where td architecture in LSTM-large achieves BPC of 1.49 on text8, outperforming the BPC of 1.54 reported in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 ,</head><label>1</label><figDesc>right panel, displays results on the text8 dataset. We observed that when fixing feedforward depth d f = 2, 3 (or fixing recurrent depth d r = 1, 2), increasing recurrent depth d r from 1 to 2 (or increasing feedforward depth d f from 2 to 3) does improve the model performance. The best test BPC is achieved by the architecture with d f = 3, d r = 2. This suggests that reasonably increasing d r and d f can aid in better capturing the over-time nonlinearity of the input sequence. However, for too large d r (or d f ) like d r = 3 or d f = 4, increasing d f (or d r ) only hurts models performance. This can potentially be attributed to the optimization issues when modelling large input-to-output dependencies (see Appendix B.4 for more details). With sequential MNIST dataset, we next examined the effects of d f and d r when modelling long term dependencies (more in Appendix B.4). In particular, we observed that increasing d f does not bring any improvement to the model performance, and increasing d</figDesc><table /><note>r might even be detrimental for training. Indeed, it appears that d f only captures the local nonlinearity and has less effect on the long term prediction. This result seems to contradict previous claims [17] that stacked RNNs (d f &gt; 1, d r = 1) could capture information in different time scales and would thus be more capable of dealing with learning long-term dependencies. On the other hand, a large d r indicates multiple transformations per time step, resulting in greater gradient vanishing/exploding issues</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results for MNIST/pMNIST. Top-left: Test accuracies with different s for tanh RNN. Top-right:Test accuracies with different s for LSTM. Bottom-left: Compared to previous results.</figDesc><table><row><cell>Bottom-right: Test</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A directed multigraph is a directed graph that allows multiple directed edges connecting two nodes.<ref type="bibr" target="#b2">3</ref> A directed cycle is a closed walk with no repetitions of edges.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Without loss of generality, we assume the unidirectional RNN approaches positive infinity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">See a full treatment of the limit in general cases in Theorem A.1 and Proposition A.1.1 in Appendix.<ref type="bibr" target="#b5">6</ref> Conventionally, an architecture with depth 1 is a three-layer architecture containing one hidden layer. But in our definition, since it goes through two transformations, we count the depth as 2 instead of 1. This should be particularly noted with the concept of feedforward depth, which can be thought as the conventional depth plus 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">See Proposition A.3.1 in Appendix.<ref type="bibr" target="#b7">8</ref> One would find this definition very similar to the definition of the recurrent depth. Therefore, we refer readers to examples inFigure 1for some illustrations.<ref type="bibr" target="#b8">9</ref> http://mattmahoney.net/dc/textdata.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Di(n) is not defined when there does not exist a path from time i to time i + n. We simply omit undefined cases when we consider the limsup. In a more rigorous sense, it is the limsup of a subsequence of {Di(n)} ∞ n=1 , where Di(n) is defined.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">It will be more clear if one checks the length of the shortest path from an node at time t to to a node at time t + k in both architectures.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors acknowledge the following agencies for funding and support: NSERC, Canada Research Chairs, CIFAR, Calcul Quebec, Compute Canada, Samsung, ONR Grant N000141310721, ONR Grant N000141512791 and IARPA Raytheon BBN Contract No. D11PC20071. The authors thank the developers of Theano <ref type="bibr" target="#b27">[28]</ref> and Keras <ref type="bibr" target="#b28">[29]</ref>, and also thank Nicolas Ballas, Tim Cooijmans, Ryan Lowe, Mohammad Pezeshki, Roger Grosse and Alex Schwing for their insightful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proposition A.1.2. Given an RNN with recurrent depth d r , we denote</p><p>The supremum d f exists and we have the following least upper bound:</p><p>Proof. We first prove that d f &lt; +∞. Write d f (i) = sup n∈Z D * i (n) − n · d r . It is easy to verify d f (·) is m−periodic, so it suffices to prove for each i ∈ N, d f (i) &lt; +∞. Hence it suffices to prove lim sup n→∞ (D * i (n) − n · d r ) &lt; +∞.</p><p>From the definition, we have</p><p>From the proof of Theorem A.1, there exists two constants M and Γ depending only on the RNN G c , such that</p><p>Theorem A.2. Given an RNN and its two graph representations G un and G c , we denote ξ(G c ) the set of directed path that starts at an input node and ends at an output node in G c . For γ ∈ ξ(G c ), denote l(γ) the length and σ s (γ) the sum of σ along γ. Then we have:</p><p>Proof. Let γ : {(t 0 , 0), (t 1 , p 1 ), · · · , (t nγ , p)} be a path in G un from an input node (t 0 , 0) to an output node (t nγ , p), where t 0 = i and t nγ = i + n. We denoteγ as the image of γ on the cyclic graph. From the proof of Theorem A.1, for eachγ in G c , we can decompose it into a path γ 0 and a sequence of directed cycles C = C 1 (γ), C 2 (γ), · · · , C w (γ) on G c satisfying those properties listed in Theorem A.1. We denote l 0 , l 1 , · · · , l w to be the length of path γ 0 and directed cycles C 1 (γ), C 2 (γ), · · · , C w (γ). We know l k σs(C k ) ≤ d r for all k = 1, 2, . . . , w by definition. Thus,</p><p>for all time step i and all integer n. The above inequality suggests that in order to take the supremum over all paths in G un , it suffices to take the maximum over a directed path in G c . On the other hand, the equality can be achieved simply by choosing the corresponding path of γ 0 in G un . The desired conclusion then follows immediately.</p><p>number of iterations </p><p>number of iterations </p><p>number of iterations    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Recurrent Skip Coefficients</head><p>The test curves for all the experiments are shown in <ref type="figure">Figure 7</ref>. In <ref type="figure">Figure 7</ref>, we observed that obtaining good performance on MNIST requires larger s than for pMNIST. We hypothesize that this is because, for the sequential MNIST dataset, each training example contains many consecutive zero-valued subsequences, each of length 10 to 20. Thus within those subsequences, the input-output gradient flow could tend to vanish. However, when the recurrent skip coefficient is large enough to cover those zero-valued subsequences, the model starts to perform better. With pMNIST, even though the random permuted order seems harder to learn, the permutation on the other hand blends zeros and ones to form more uniform sequences, and this may explain why training is easier, less hampered by by the long sequences of zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Recurrent Skip Coefficients vs. Skip Connections</head><p>Test curves for all the experiments are shown in <ref type="figure">Figure 8</ref>. Observe that in most cases, the test accuracy of (3) is worse than (2) in the beginning while beating (2) in the middle of the training. This is possibly because in the first several time steps, it is easier for (2) to pass information to the output thanks to the skip connections, while only after multiples of k time steps, (3) starts to show its advantage with recurrent skip connections <ref type="bibr" target="#b10">11</ref> . The shorter paths in (2) make its gradient flow</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc Vv</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler. Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning recurrent neural networks with hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen. Diploma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Lstm: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04069</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning complex, extended sequences using the principle of history compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="242" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><forename type="middle">El</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="493" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning made easier by linear transformations in perceptrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="924" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training and analysing deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6026</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies is not as difficult with NARX recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1329" to="1338" />
			<date type="published" when="1996-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal-kernel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="243" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A clockwork rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1863" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Son</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<ptr target="http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf" />
		<imprint>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06464</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08400</idno>
		<title level="m">Regularizing rnns by stabilizing activations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The Theano Development</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoly</forename><surname>Belikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<title level="m">A python framework for fast computation of mathematical expressions. arXiv preprint</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<ptr target="https://github.com/fchollet/keras" />
	</analytic>
	<monogr>
		<title level="j">François Chollet. Keras. GitHub</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-dimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Artificial Neural Networks, ICANN&apos;07</title>
		<meeting>the 17th International Conference on Artificial Neural Networks, ICANN&apos;07<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
