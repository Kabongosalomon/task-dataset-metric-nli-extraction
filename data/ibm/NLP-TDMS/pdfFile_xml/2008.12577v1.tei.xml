<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
							<email>rudolph@tnt.uni-hannover.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Leibniz University Hanover</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
							<email>wandt@tnt.uni-hannover.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Leibniz University Hanover</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
							<email>rosenhahn@tnt.uni-hannover.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Leibniz University Hanover</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The detection of manufacturing errors is crucial in fabrication processes to ensure product quality and safety standards. Since many defects occur very rarely and their characteristics are mostly unknown a priori, their detection is still an open research question. To this end, we propose Dif-ferNet: It leverages the descriptiveness of features extracted by convolutional neural networks to estimate their density using normalizing flows. Normalizing flows are well-suited to deal with low dimensional data distributions. However, they struggle with the high dimensionality of images. Therefore, we employ a multi-scale feature extractor which enables the normalizing flow to assign meaningful likelihoods to the images. Based on these likelihoods we develop a scoring function that indicates defects. Moreover, propagating the score back to the image enables pixel-wise localization. To achieve a high robustness and performance we exploit multiple transformations in training and evaluation. In contrast to most other methods, ours does not require a large number of training samples and performs well with as low as 16 images. We demonstrate the superior performance over existing approaches on the challenging and newly proposed MVTec AD <ref type="bibr" target="#b3">[4]</ref> and Magnetic Tile Defects <ref type="bibr" target="#b13">[14]</ref> datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In industrial manufacturing processes the quality of the products is constantly monitored and improved. Hence, small defects during fabrication need to be detected reliably. However, manufacturers do not know in advance which types of defects will occur and most of them appear so infrequently that no defective examples are available. Even if some defect types are known, new types can still occur any time due to unforeseeable events during manufacturing. Consequently, reliable defect detection cannot be done with supervised machine learning approaches. We propose a solution for semi-supervised defect detection where only pos-  <ref type="figure">Figure 1</ref>. DifferNet assigns likelihoods to inputs which makes it usable to detect defects. In contrast to the top left image without any defect, the top middle image is assigned to a high anomaly score due to the defect (see the enlarged patch on the right side). Additionally, DifferNet identifies the defective region by backpropagating the likelihood loss up to the input which gives a gradient map (top right image). This allows for a detailed analysis of defect position and shape. itive examples and no defective examples are present during training. This is also known as anomaly detection.</p><p>In general, anomaly detection describes the problem of determining whether a data sample differs from a set of given normal data. There are various approaches for general anomaly detection on images, summarized in Section 2. Defect detection is a specific sub-problem where visually similar normal samples and only slightly different anomalous samples are present. While traditional anomaly detection methods are well-suited to data with high intra-class variance, they are not able to capture subtle differences. We tackle this problem by employing an accurate density estimator on image features extracted by a convolutional neural network. The feature distribution of normal samples is captured by utilizing the latent space of a normalizing flow <ref type="bibr" target="#b21">[22]</ref>. Unlike other generative models such as variational autoencoders <ref type="bibr" target="#b16">[17]</ref> or GANs <ref type="bibr" target="#b11">[12]</ref>, there exists a bijective mapping between feature space and latent space in which each vector is assigned to a likelihood. This enables DifferNet to calculate a likelihood for each image. From this likelihood we derive a scoring function to decide if an image contains an anomaly. <ref type="figure">Figure 1</ref> visualizes the core idea behind our method. The most common samples are assigned to a high likelihood whereas uncommon images are assigned to a lower likelihood. Since defects are not present during training, they are mapped to a low likelihood. We further improve the descriptiveness of the feature extractor by using multi-scale inputs. To derive a meaningful scoring function, we include likelihoods of several transformations of the image. Thereby DifferNet gains flexibility and robustness to detect various types of defects. We show that our method considers even small changes in data while other approaches struggle to detect them. Moreover, due to the efficiency of the feature extractor and the proposed image transformations, our approach even outperforms the state of the art when trained with a low number of training samples. Besides defect detection, DifferNet's architecture allows for localization of defects by having expressive gradients of input pixels regarding the scoring functions. A high magnitude of the gradient in an image region signals an area with anomalous features which helps to identify the defect.</p><p>Our work comprises the following contributions:</p><p>• Detection of anomalies via the usage of likelihoods provided by a normalizing flow on multi-scale image features with multi-transform evaluation.</p><p>• Anomaly localization without training labels, the necessity of any pixel-wise optimization and sub-image detection.</p><p>• Applicability on small training sets. Even with a low number of training examples our approach achieves competitive results.</p><p>• State-of-the-art detection performance on MVTec AD and Magnetic Tile Defects.</p><p>• Code is available on GitHub 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Anomaly Detection</head><p>Existing methods for anomaly detection can be roughly divided into approaches based on generative models and pretrained networks. The most relevant methods to our work are briefly presented in the following subsections. Note that we focus on works that deal with image anomaly 1 https://github.com/marco-rudolph/differnet detection rather than anomaly localization to keep the focus on our main problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Detection with Pretrained Networks</head><p>There are several works which use the feature space of a pretrained network to detect anomalies. In most cases simple traditional machine learning approaches are used to obtain an anomaly score. Andrews et al. <ref type="bibr" target="#b1">[2]</ref> apply a One-Class-SVM on VGG <ref type="bibr" target="#b27">[28]</ref> features of the training images. Nazare et al. <ref type="bibr" target="#b20">[21]</ref> evaluated different normalization techniques to have a 1-Nearest-Neighbor-classifier on a PCAreduced representation of the feature space. Localization of the anomalies is achieved by evaluating the method on many overlapping patches. However, this is very costly. Sabokrou et al. <ref type="bibr" target="#b24">[25]</ref> models the anomaly-free feature distribution as an unimodal Gaussian distribution. Therefore, they cannot capture multimodal distributions as opposed to our method.</p><p>These techniques only work for particular classes in defect detection. In contrast to our proposed DifferNet, existing approaches do not appear to be robust and powerful enough for defect detection. None of the above techniques take advantage of the flexibility of another neural network on top of the pretrained model. Another benefit of our method compared to these approaches is being able to compute gradients w.r.t. the inputs which can be utilized to compute anomaly maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Generative Models</head><p>Generative models, such as autoencoders <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref> and GANs <ref type="bibr" target="#b11">[12]</ref>, are able to generate samples from the manifold of the training data. Anomaly detection approaches using these models are based on the idea that the anomalies cannot be generated since they do not exist in the training set.</p><p>Autoencoder-based approaches try to detect anomalies by comparing the output of an autoencoder to its input. Thus, a high reconstruction error should indicate an anomalous region. Bergmann et al. <ref type="bibr" target="#b4">[5]</ref> proposes SSIM-loss to make the reconstruction error dependent on visual similarity. In many cases autoencoder-based methods fail because they generalize too strongly, i.e. anomalies can be reconstructed as good as normal samples. Gong et al. <ref type="bibr" target="#b10">[11]</ref> tackle the generalization problem by employing memory modules which can be seen as a discretized latent space. Zhai et al. <ref type="bibr" target="#b29">[30]</ref> connect regularized autoencoders with energy-based models to model the data distribution and classify samples with high energy as an anomaly.</p><p>GAN-based approaches assume that only positive samples can be generated. Schlegl et al. <ref type="bibr" target="#b25">[26]</ref> propose a twostage training method: The GAN is learned first and an encoder is optimized as an inverse generator. Using the generator as decoder enables the calculation of a reconstruc- tion loss alongside the difference in discriminator features of original and reconstructed image to obtain an anomaly score. Akcay et al. <ref type="bibr" target="#b0">[1]</ref> make use of adversarial training by letting an autoencoder directly act as generating part of the GAN. This enforces the property of the decoder to only generate normal-like samples which can be measured by the difference between the embedding of the original and the reconstructed data.</p><p>We argue that generative models are appropriate for a wide range of defect detection scenarios since they strongly depend on the anomaly type. For example, the size and structure of the defective area heavily influence the anomaly score. If the region of interest shows high frequency structures, they cannot be represented accurately. Often, other instance-specific structures influence the reconstruction error more than the anomaly. In contrast, we show that Dif-ferNet handles significantly more and various defect types. Additionally, our method does not rely on a large number of training samples compared to generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Normalizing Flows</head><p>Normalizing Flows (NF) <ref type="bibr" target="#b21">[22]</ref> are neural networks that are able to learn transformations between data distributions and well-defined densities. Their special property is that their mapping is bijective and they can be evaluated in both directions. First, they assign likelihoods to a given sample. Second, data is generated by sampling from the modeled distribution. The bijectivity is ensured by stacking layers of affine transforms which are fixed or autoregressive. A common class of such autoregressive flows is MADE (Germain et al. <ref type="bibr" target="#b8">[9]</ref>) which makes use of the Bayesian chain rule to decompose the density. These models can learn distributions of large datasets containing of mostly small images. In contrast, we capture the distribution of a comparably small number of images at a high resolution. Autoregressive flows compute likelihoods fast, but are slow at sampling. Inverse autoregressive flows, proposed by Kingma et al. <ref type="bibr" target="#b15">[16]</ref>, show the exact opposite behavior. Real-NVP <ref type="bibr" target="#b7">[8]</ref> can be seen as a special inverse autoregressive flow which is simplified such that both forward and backward pass can be processed quickly. Similar to Ardizzone et al. proposed for Invertible Neural Networks <ref type="bibr" target="#b2">[3]</ref>, we integrate a parametrized clamping mechanism to the affine transformation of the Real-NVPlayers to obtain a more stable training process. Details to the so-called coupling layers and the clamping mechanism of Real-NVP are explained in Section 3.1.</p><p>The property of normalizing flows as an adequate estimator of probability densities to detect anomalies has not raised much attention yet, although some works present promising results using Real-NVP and MADE <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b6">7]</ref>. However, none of the works deal with visual data. <ref type="figure" target="#fig_1">Figure 2</ref> shows an overview of our pipeline. Our method is based on density estimation of image features y ∈ Y from the anomaly-free training images x ∈ X. Let f ex : X − → Y be the mapping of a pretrained feature extractor which is not further optimized. The estimation of p Y (y), provided by f ex (x), is achieved by mapping from Y to a latent space Z -with a well-defined distribution p Z (z) -by applying a normalizing flow f NF : Y − → Z. Likelihoods for image samples are directly calculated from p Z (z). Features of anomalous samples should be out of distribution and hence have lower likelihoods than normal images. Likelihoods of multiple transforms on the image are maximized in training and used in inference for a robust prediction of the anomaly score. To capture structures at different scales and thus having a more descriptive representation in y, we further define f ex as the concatenation of features at 3 scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Normalizing Flow</head><p>The normalizing flow acts as a bijector between feature space Y and latent space Z by using affine transformations. Likelihoods are computed from Z according to the modelled distribution. We model z ∼ N (0, I) in our latent space which gives us a well-defined density p Z (z) for z. Following from bijectivity, for every feature input there is a unique z of the same dimension and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Architecture</head><p>We use an architecture of coupling layers as proposed in Real-NVP <ref type="bibr" target="#b7">[8]</ref>. The detailed structure of one block is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. The design of f NF is a chain of such blocks,   To apply the scale and shift operations, the input y in is split into y in,1 and y in,2 that manipulate each other by regressing multiplicative and additive components in subnetworks s and t; these manipulations are applied to their respective counterpart successively. The scale and shift operations are described by y out,2 = y in,2 e s1(yin,1) + t 1 (y in,1 )</p><formula xml:id="formula_0">y out,1 = y in,1 e s2(yout,2) + t 2 (y out,2 ),<label>(1)</label></formula><p>with as the element-wise product. Using an exponential function before scaling preserves the affinity property by ensuring non-zero coefficients. The internal functions s and t can be any differentiable function, which in our case is implemented as a fully-connected network that regresses both components by splitting the output (see <ref type="figure" target="#fig_3">Figure 3</ref>). Similar to Ardizzone et al. <ref type="bibr" target="#b2">[3]</ref>, we apply soft-clamping to the values of s to preserve model stability which is crucial in our case for better convergence. This is achieved by using the activation</p><formula xml:id="formula_1">σ α (h) = 2α π arctan h α<label>(2)</label></formula><p>as the last layer of s. This prevents large scaling components by restricting s(y) to the interval (−α, α). Each block first performs a predefined random permutation on the features to allow each dimension to affect all other dimensions at some point. The output of one coupling block is given by the concatenation of y out,1 and y out,2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Training</head><p>The goal during training is to find parameters for f NF that maximize likelihoods for extracted features y ∈ Y which are quantifiable in the latent space Z. With the mapping z = f NF (y) and according to the change-of-variables formula Eq. 3, we describe this problem as maximizing</p><formula xml:id="formula_2">p Y (y) = p Z (z) det ∂z ∂y .<label>(3)</label></formula><p>This is equivalent to maximizing the log-likelihood, which is more convenient here, since the terms simplify when inserting the density function of a standard normal distribution as p Z (z). We use the negative log-likelihood loss L(y) to obtain a minimization problem:</p><formula xml:id="formula_3">log p Y (y) = log p Z (z) + log det ∂z ∂y L(y) = z 2 2 2 − log det ∂z ∂y .<label>(4)</label></formula><p>Intuitively, we want f NF to map all y as close as possible to z = 0 while penalizing trivial solutions with scaling coefficients close to zero 2 . The latter in ensured by the negative log determinant of the Jacobian ∂z ∂y in L(y). In our case the log determinant of the Jacobian is the sum of scaling coefficients before exponentiation.</p><p>During Training, L(y) is optimized for features y of different transformations of an input image for a fixed epoch length. Section 4.2 describes the training in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Scoring Function</head><p>We use the calculated likelihoods as a criterion to classify a sample as anomalous or normal. To get a robust anomaly score τ (x), we average the negative loglikelihoods using multiple transformations T i (x) ∈ T of an image x:</p><formula xml:id="formula_4">τ (x) = E Ti∈T [− log p Z (f NF (f ex (T i (x))))].<label>(5)</label></formula><p>As T we choose rotations and manipulations of brightness and contrast. An image is classified as anomalous if the anomaly score τ (x) is above the threshold value θ. Thus, the decision can be expressed as</p><formula xml:id="formula_5">A(x) = 1 for τ (x) ≥ θ 0 for τ (x) &lt; θ ,<label>(6)</label></formula><p>where A(x) = 1 indicates an anomaly. In Section 4 θ is varied to calculate the Receiver Operating Characteristic (ROC). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Localization</head><p>In contrast to several other approaches, ours is not optimized for localizing the defects on the image. Nevertheless, our method localizes areas where anomalous features occur. Our method allows for propagating the negative loglikelihood L back to the input image x. The gradient ∇x c of each input channel x c is a value indicating how much the pixels influence the error which relates to an anomaly. For better visibility we blur these gradients with a Gaussian kernel G and sum the absolute values over the channels C according to</p><formula xml:id="formula_6">g x = c∈C |G * ∇x c |,<label>(7)</label></formula><p>with * as 2D convolution and | · | as the element-wise absolute value, which results in the gradient map g x . Averaging the maps of multiple rotations of one single image -after rotating back the obtained maps -gives a robust localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>In this paper, we evaluate our approach on real-world defect detection problems. We use the challenging and newly proposed datasets MVTec AD <ref type="bibr" target="#b3">[4]</ref> and Magnetic Tile Defects (MTD) <ref type="bibr" target="#b13">[14]</ref>. The difficulty in these datasets lies in the similarity of anomalies and normal examples.</p><p>To the best of our knowledge, MVTec AD is the only publicly available multi-object and multi-defect anomaly dataset. It contains 5354 high-resolution color images of 10 object and 5 texture categories. The number of training samples per category ranges from 60 to 320, which is challenging for the estimation of the distribution of normal samples. Several defect types per category, such as little cracks, deformations, discolorizations and scratches are occurring in the test set. Some of which are shown in <ref type="figure" target="#fig_8">Figure 8</ref>. In total, the dataset includes 70 defect types. The anomalies differ in their size, shape and structure and thus cover several scenarios in industrial defect detection.</p><p>Magnetic Tile Defects <ref type="bibr" target="#b13">[14]</ref> comprises grayscale images of magnetic tiles under different illuminations with and without defects. Such tiles should provide a constant magnetic potential in engines. We split the 952 defect-free images randomly into a train and a test set, where the test set contains 20% of the data. All 392 defect images are used for testing. These show frayed or uneven areas, cracks, breaks and blowholes as anomalies, as shown in <ref type="figure" target="#fig_4">Figure 4</ref>. A lot of defect-free images contain variations that are similar to anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For all experiments, we use the convolutional part of AlexNet <ref type="bibr" target="#b17">[18]</ref> as the feature extractor and apply global average pooling on each feature map at each scale. We tested more complex topologies, for instance ResNet <ref type="bibr" target="#b12">[13]</ref> and VGG <ref type="bibr" target="#b27">[28]</ref>, but did not observe better performance. We prefer the smaller AlexNet since it is sufficient for our purpose. The feature extractor is pretrained on ImageNET and remains fixed during training. We use features at 3 scales with input image sizes of 448 × 448, 224 × 224 and 112 × 112 pixels -resulting in 3 · 256 = 768 features. The normalizing flow consists of 8 coupling blocks with fully connected networks as internal functions s and t. These include 3 hidden dense layers with a size of 2048 neurons and ReLU activations. We set the mentioned clamping parameter α = 3. For training, we use the Adam Optimizer <ref type="bibr" target="#b14">[15]</ref> with the authorsuggested βparameters and a learning rate of 2 · 10 −4 . As transformations T , random rotations, which are uniformly distributed in the interval [0, 2π], are applied. In addition, we manipulated the contrast and brightness of the magnetic tiles with an uniformly distributed random factor in the interval [0.85, 1.15]. In each case of training and inference the same transformations are applied. We train our models for 192 epochs with a batch size of 96.  non-defects defects <ref type="figure">Figure 6</ref>. Normalized histogram of DifferNet's anomaly scores for the test images of MTD. As can be seen, the score is a reliable indicator for defects except for a narrow range of some borderline cases. Note that the rightmost bar summarizes all scores above 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Detection</head><p>For reporting the performance of our method regarding the detection of anomalies, we follow <ref type="bibr" target="#b0">[1]</ref> and compute the Area Under Receiver Operator Characteristics (AUROC) depending on the scoring function which we obtained as described in Section 3. It measures the area under the true positive rate as a function of the false positive rate. The AUROC metric is not sensitive to any threshold or the percentage of anomalies in the test set. Besides other anomaly detection methods, we compare our method to the baselines one-class SVM (OCSVM) <ref type="bibr" target="#b1">[2]</ref> and the distance to the nearest neighbor (1-NN) after PCA reduction to 64 dimensions and z-score normalization <ref type="bibr" target="#b20">[21]</ref>. Note that both methods OCSVM and 1-NN are adapted to our setting: We used every technique of our pipeline (see <ref type="figure" target="#fig_1">Figure 2</ref>) but replaced the normalizing flow with them. We evaluate several transformations and used the mean respective score. Apart from these approaches and some state-of-the-art models, we compare our method with GeoTrans <ref type="bibr" target="#b9">[10]</ref> which cannot be assigned to generative and pretrained methods described in Section 2. GeoTrans computes an anomaly score based on the classification of conducted transformations. <ref type="table">Table 1</ref> shows the results for MVTec AD. Compared to other approaches, our method outperforms existing methods in almost every category, up to a large margin of 15%. In all of the 15 categories our method achieves an AUROC of at minimum 84%, which shows that our approach is not limited to a specific set of defects or features. The fact that 1-NN outperforms other competitors except us, demonstrates that our feature extraction and evaluation is well-suited for the problem.</p><p>We can observe similar characteristics on MTD, seen in <ref type="table" target="#tab_2">Table 2</ref>. The ROC-Curve in <ref type="figure" target="#fig_5">Figure 5</ref> shows that our method provides a much higher true positive rate for any false positive rate. DifferNet achieves a recall of about 50% without any false positive among 191 defect-free test images. The histogram of anomaly scores is visualized in <ref type="figure">Figure 6</ref>. There is a large subset of defective samples whose scores differ significantly from all scores of non-defective samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method AUROC [%]</head><p>GeoTrans <ref type="bibr" target="#b9">[10]</ref> 75.5 GANomaly <ref type="bibr" target="#b0">[1]</ref> 76.6 DSEBM <ref type="bibr" target="#b29">[30]</ref> 57.2 ADGAN <ref type="bibr" target="#b5">[6]</ref> 46.4 OCSVM <ref type="bibr" target="#b1">[2]</ref> 58.7 1-NN <ref type="bibr" target="#b20">[21]</ref> 80.0 DifferNet (ours) 97.7  The assignment of extremely high scores without any false positive is a characteristic of our method and can be similarly observed for other evaluated product categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Localization</head><p>Results of the localization procedure described in Section 3.3 are shown in <ref type="figure" target="#fig_8">Figure 8</ref>. The localizations are accurate for many types, sizes and shapes of anomalies; despite the average pooling of feature maps before being processed by the normalizing flow. Our architecture produces meaningful gradients which can be explained by the models architecture: First, AlexNet is relatively shallow such that noisy or vanishing gradients are prevented. Second, the bijectivity of the normalizing flow causes a direct relation between all image features y and all values of z with nonzero gradients. The gradients tend to appear speckled for larger anomalous regions. We conject the reason is that pixels, leading to features influencing the anomaly score, are usually not located evenly distributed in the corresponding region. However, our method enables the human to perceive the defective region and interpret which areas influenced the networks decision to what extent.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>To quantify the effects of individual strategies used in our work, we performed an ablation study by comparing the performance on MVTec AD <ref type="bibr" target="#b3">[4]</ref> when modifying the strategies. In addition, the model's behavior for different characteristics of the training set is analyzed. Preprocessing Pipeline and Evaluation. <ref type="table" target="#tab_4">Table 3</ref> compares the detection performance on MVTec AD for different configurations regarding multi-scaling, the usage of transformations in training and the number of used transformations T i for evaluation. Having one test transformation means that only the original image was used for evaluation. Note that we outperform existing methods even without the proposed transformations and multi-scale strategy. Since relevant features could appear at any scale, it is beneficial to include features at multiple scales which is shown by an AU-ROC improvement of 4.7%. Having transformed samples in training is crucial as it enables multi-transform evaluation and helps for generalization and data augmentation. The similar performances of configuration C and D reflect that applying transformations in training is only useful if they are performed in inference as well. The more of these transformations are then used, the more meaningful the score is, as the rising performance of configurations D to G shows. Number of Training Samples. We investigate the effect of the training set size on the detection performance as shown on <ref type="figure" target="#fig_7">Figure 7</ref> and on the right of <ref type="table">Table 1</ref>. The reported results are the average over three runs with different random subsets per training set size. It can be seen that our model and training procedure allows for a stable training even on small training sets. This can be explained by the usage of multiple transformations and the averaging of feature maps. Our model profits from this strategy which is a mixture of augmentation and compression. DifferNet requires only 16 training samples to outperform existing approaches that use the whole training set with 369 samples per class on average. For some classes, the subsets cannot represent the feature variation of normal samples. Multimodality. The feature distributions of the evaluated categories are unimodal. We also investigated the performance on multimodal distributions. Therefore, we also ob- served the detection performance when using all 15 categories of MVTec as training data. To capture this more complex distribution, we used 12 coupling blocks. The result is a mean AUROC of 90.2% which shows that our method is able to handle multimodal distributions well. The regressing sub-blocks inside the NF appear to capture the modes and switch between them depending on their input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented DifferNet to detect defects in images by utilizing a normalizing-flow-based density estimation of image features at multiple scales. Likelihoods of several transformations of a single image are used to compute a ro-bust anomaly score. Therefore, there is no need for a large amount of training samples. The design and scoring function is chosen such that image gradients can be exploited to localize defects. As shown, the method also scales to multimodal distributions which resembles real-world settings. In the future we plan to refine the concept in order to find anomalies in video data comparable to <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our pipeline: Multiple scales of a transformed input image are fed into a feature extractor. The distribution of its concatenated outputs is captured by transforming it via a normalizing flow (NF) into a normal distribution by maximum likelihood training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of one block inside the normalizing flow: After a fixed random permutation, the input is split into two parts that regress scale and shift parameters to transform their respective counterpart. Symbols and ⊕ denote element-wise multiplication and addition, respectively. Numerical operations are symbolized by grey blocks. White blocks contain variable names. consisting of permutations followed by scale and shift operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Samples of defect-free and defective images from Magnetic Tile Defects<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>ROC-Curve for different methods for detecting defects in MTD. DifferNet is significantly more accurate in detecting the defects compared to other approaches. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Detection performance of DifferNet, measured by AU-ROC, depending on the training set size of MTD and of some categories of MVTec AD. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Localization of anomalous regions of different categories in MVTec AD. The upper rows shows the original anomaly images, the mid rows the localizations provided by DifferNet and the lower rows the superimposition of both. They were generated by backpropagating the negative log-likelihood loss to the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Area under ROC in % for detecting anomalies on MTD</figDesc><table><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AUROC [%]</cell><cell>70 80</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50 60</cell><cell>1 16</cell><cell>64</cell><cell>128 # Training Samples</cell><cell>256 Transistor Wood Zipper Carpet Magnets</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Average detection performance for all categories of MVTec AD when modifying our proposed training and evaluation strategy. The columns show parameter configurations named from A to F. Parameters that differ from our proposed configuration are underlined.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The exponentiation inhibits the coefficients from being zero.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy within the Cluster of Excellence PhoenixD (EXC 2122).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Amir Atapour Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breckon</surname></persName>
		</author>
		<idno>abs/1805.06725</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Transfer representation-learning for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerone</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Griffin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynton</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Lüth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>Köthe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02392</idno>
		<title level="m">Guided image generation with conditional invertible neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02011</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adgan: A scalable gan-based architecture for image anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (IT-NEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="987" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Madson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">César</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ticiana Lc Da</forename><surname>Mattos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antônio F De Macedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Wellington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05958</idno>
		<title level="m">Anomaly detection in trajectory data with normalizing flows</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Made: Masked autoencoder for distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="881" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9758" to="9769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Surface defect saliency of magnetic tile. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalization and network design strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionism in Perspective</title>
		<editor>R. Pfeifer, Z. Schreter, F. Fogelman, and L. Steels</editor>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gaussian process for activity modeling and anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Photogrammetry and Remote Sensing ISA workshop</title>
		<meeting><address><addrLine>La Grande Motte, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Are pretrained cnns good feature extractors for anomaly detection in surveillance videos?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Nazare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moacir</forename><surname>Ponti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08495</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structuring autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Normalizing flows for deep anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Ryzhikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Borisyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ustyuzhanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Derkach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09323</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep-anomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zahra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">f-anogan: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Normalizing flows for novelty detection in industrial time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Simic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06904</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video event recognition and anomaly detection by combining gaussian process and hierarchical dirichlet process models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Photogrammetric Engineering &amp; Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep structured energy based models for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
