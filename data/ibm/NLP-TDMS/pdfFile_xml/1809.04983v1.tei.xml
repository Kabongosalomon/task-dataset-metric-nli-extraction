<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Part-based Graph Convolutional Network for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpit</forename><surname>Thakkar</surname></persName>
							<email>kalpit.thakkar@research.iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Information Technology (CVIT)</orgName>
								<orgName type="department" key="dep2">Kohli Center for Intelligent Systems (KCIS)</orgName>
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Information Technology (CVIT)</orgName>
								<orgName type="department" key="dep2">Kohli Center for Intelligent Systems (KCIS)</orgName>
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Part-based Graph Convolutional Network for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>KALPIT THAKKAR, P J NARAYANAN: PART-BASED GCN FOR ACTION RECOGNITION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human actions comprise of joint motion of articulated body parts or "gestures". Human skeleton is intuitively represented as a sparse graph with joints as nodes and natural connections between them as edges. Graph convolutional networks have been used to recognize actions from skeletal videos. We introduce a part-based graph convolutional network (PB-GCN) for this task, inspired by Deformable Part-based Models (DPMs). We divide the skeleton graph into four subgraphs with joints shared across them and learn a recognition model using a part-based graph convolutional network. We show that such a model improves performance of recognition, compared to a model using entire skeleton graph. Instead of using 3D joint coordinates as node features, we show that using relative coordinates and temporal displacements boosts performance. Our model achieves state-of-the-art performance on two challenging benchmark datasets NTURGB+D and HDM05, for skeletal action recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recognizing human actions in videos is necessary for understanding them. Video modalities such as RGB, depth and skeleton provide different types of information for understanding human actions. The S-video (or Skeletal modality) provides 3D joint locations, which is a relatively high level information compared to RGB or depth. With the release of several multi-modal datasets <ref type="bibr">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref>, action recognition from S-video has gained significant traction recently <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Graph convolutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref> have been used to learn high level features from arbitrary graph structure. State-of-the-art action recognition from S-videos <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref> use graph convolutions, wherein the whole skeleton is treated as a single graph. It is, however, natural to think of human skeleton as a combination of multiple body parts. A body-part based representation can learn the importance of each part and their relations across space and time. We present a model using part-based graph convolutional network for recognizing actions from S-videos, using a novel part-based graph convolution scheme. The model attains better performance for recognition than a model entire skeleton as a single graph. Current models for skeletal action recognition <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref> use 3D coordinates as features at each vertex. Geometric features such as relative joint coordinates and motion features such as temporal displacements can be more informative for action recognition. Optical flow helps in action recognition from RGB videos <ref type="bibr" target="#b29">[30]</ref> and Manhattan line map helps in generating 3D layout from single image c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.  <ref type="bibr" target="#b36">[37]</ref>. Geometric feature <ref type="bibr" target="#b35">[36]</ref> and kinematic features <ref type="bibr" target="#b33">[34]</ref> have been used for skeletal action recognition before. Inspired by these observations, we use a geometric feature that encodes relative joint coordinates and motion feature that encodes temporal displacements at each vertex in our part-based graph convolution model to significant impact. The major contributions of this paper are: (i) Formulation of a general part-based graph convolutional network (PB-GCN) which can be learned for any graph with well-known properties and its application to recognize actions from S-videos, (ii) Use of geometric and motion features in place of 3D joint locations at each vertex to boost recognition performance, and (iii) Exceeding the state-of-the-art on challenging benchmark datasets NTURGB+D and HDM05. The overview of our representation and signals is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Non graph-based methods</head><p>Skeletal action recognition has been approached using techniques such as handcrafted feature encodings, complex LSTM networks, image encodings with pretrained CNNs and noneuclidean methods based on manifolds. Non-deep learning methods worked well initially and proved usefulness of several extracted information from S-videos such as joint angles <ref type="bibr" target="#b21">[22]</ref>, distances <ref type="bibr" target="#b31">[32]</ref> and kinematic features <ref type="bibr" target="#b33">[34]</ref>. These methods learn from hand designed On the other hand, LSTM-based methods were used because S-videos can be thought of as time sequences of features. Spatio-temporal LSTMs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, attention-based LSTM <ref type="bibr" target="#b25">[26]</ref> and simple LSTM networks with part-based skeleton representation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> have been used. These methods either use complex LSTM models which have to be trained very carefully or use part-based representation with a simple LSTM model. We propose a part-based graph convolutional network that has good learning capacity and uses a part-based representation, inheriting the good qualities of both types of aforementioned approaches. Image encodings of skeletons were proposed to facilitate usage of Imagenet pretrained CNNs to extract spatiotemporal features. Ke et al. <ref type="bibr" target="#b11">[12]</ref> generate images using relative coordinates while Du et al. <ref type="bibr" target="#b5">[6]</ref> and Li et al. <ref type="bibr" target="#b14">[15]</ref> proposed a body part-based image encoding. Due to inherent differences in information in such image encodings and RGB images, it is almost impossible to interpret the learned filters. In contrast, our method is intuitive as it uses a graph-based representation for human skeleton.</p><p>Manifold learning techniques have been used for skeletal action recognition, where actions are represented as curves on Lie groups <ref type="bibr" target="#b27">[28]</ref> and Riemannian manifold <ref type="bibr" target="#b4">[5]</ref>. Deep learning on these manifolds is difficult <ref type="bibr" target="#b10">[11]</ref> while deep learning on graphs (also a manifold) has developed recently <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>. Our method uses a human skeleton graph and learns a model using part-based graph convolutional network, exploiting the benefits of deep learning on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph-based methods</head><p>Representing S-videos as skeleton graph sequences for recognizing actions had not been explored until recently. Li and Leung <ref type="bibr" target="#b16">[17]</ref> construct graphs using a statistical variance measure dependent on joint distances and match them for recognition. Recently, Yan et al. <ref type="bibr" target="#b32">[33]</ref> and Li et al. <ref type="bibr" target="#b15">[16]</ref> proposed a spatio-temporal graph convolutional network for action recognition from S-videos. Both the methods construct graphs where the human skeleton is treated as a single graph. Our formulation explores a partitioned skeleton graph with a part-based graph convolutional network and we show that it improves recognition performance. Also, we use relative coordinates and temporal displacements as features at each vertex instead of 3D joint</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>A graph is defined as G = (V, E) where V is the set of vertices and E ⊆ (V × V) is the set of edges. A is the graph adjacency matrix having</p><formula xml:id="formula_0">A(i, j) = w, w ∈ R \ {0} if (v i , v j ) ∈ E and A(i, j) = 0 otherwise. N k : v → V defines the set of vertices V in k-neighborhood of v which</formula><p>includes neighbors having shortest path length atmost k from vertex v. A labeling function L : V → {0, 1, . . . , L − 1} assigns a label to each vertex in a vertex set V, where L is the number of unique labels. The adjacency matrix is normalized using a degree matrix as:</p><formula xml:id="formula_1">D(i, i) = ∑ j A(i, j); A norm = D −1/2 AD −1/2<label>(1)</label></formula><p>Graph convolutions can be formulated using spectral graph theory <ref type="bibr" target="#b3">[4]</ref> or spatial convolution <ref type="bibr" target="#b20">[21]</ref> on graphs. We focus on spatial convolutions in this paper as they resemble convolutions on regular grid graphs like RGB images <ref type="bibr" target="#b20">[21]</ref>. A graph CNN can then be formed by stacking multiple graph convolution units. Graph convolution (shown in <ref type="figure" target="#fig_6">Figure 2</ref>) can be defined as <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_2">Y(v i ) = ∑ v j ∈N k (v i ) W(L(v j ))X(v j )<label>(2)</label></formula><p>where, v i is the root vertex at which the convolution is centered (like center pixel in an image convolution), W(·) is a filter weight vector of size of L indexed by the label assigned to neighbor v j in the k-neighborhood N k (v i ), X(v j ) is the input feature at v j and Y(v i ) is the convolved output feature at root vertex v i . Equation 2 can be written in terms of adjacency matrix as:</p><formula xml:id="formula_3">Y(v i ) = ∑ j A norm (i, j) W(L(v j )) X(v j )<label>(3)</label></formula><p>A norm (i, j) basically defines the neighbors at distance 1 and hence, Equation 2 captures a more general form of convolution by using k-order neighborhood N k (v i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Part-based Graph</head><p>Graphs representing real world manifolds can often be thought of as being made up of several parts. For instance, a graph representing a complex molecule consists of several simple structures, such as structure of a protein biomolecule, which can be divided into several polypeptide chains that make up the complex. Similarly, human body can be visualized as connected rigid parts, much like a deformable part-based model <ref type="bibr" target="#b7">[8]</ref>. The graph of the skeleton of human body can be divided into parts, where each subgraph represents a part of the human body.</p><p>In general, a part-based graph can be constructed as a combination of subgraphs where each subgraph has certain properties that define it. Let us consider that a graph G has been divided into n partitions. Formally:</p><formula xml:id="formula_4">G = p∈{1,...,n} P p | P p = (V p , E p )<label>(4)</label></formula><p>P p is the partition (or subgraph) p of the graph G. We consider scenarios in which the partitions can share vertices or have edges connecting them. We proceed to explain how the part-based graph convolution is defined for the part-based graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Part-based Graph Convolutions</head><p>In essence, graph convolutions over parts are aimed at capturing high-level properties of parts and learn the relations between them. In a Deformable Part-based Model, different parts are identified and relations between them are learned through the deformation of the connections between them. Similarly, graph convolutions over a part identifies the properties of that subgraph and an aggregation across subgraphs learns the relations between them. For a part-based graph, convolutions for each part are performed separately and the results are combined using an aggregation function F agg . Using F agg over edges across partitions:</p><formula xml:id="formula_5">Y p (v i ) = ∑ v j ∈N kp (v i ) W p (L p (v j ))X p (v j ), p ∈ {1, . . . , n} (5) Y(v i ) = F agg (Y p1 (v i ), Y p2 (v j )) | (v i , v j ) ∈ E (p1,p2) , (p1, p2) ∈ {1, . . . , n} × {1, . . . , n}<label>(6)</label></formula><p>Using F agg for common vertices across partitions:</p><formula xml:id="formula_6">Y(v i ) = F agg (Y p1 (v i ), Y p2 (v i )) | (p1, p2) ∈ {1, . . . , n} × {1, . . . , n}<label>(7)</label></formula><p>The convolution parameters W p can be shared across parts or kept separate, while the neighbors of v i only in that part (N kp (v i )) are considered. In order to combine the information across parts, the function F agg combines information at shared vertices (equation 7) or shares information through edges crossing parts (equation 6, E (p1,p2) contains all edges connecting parts p1 and p2), according to the partition configuration. A sophisticated F agg can be employed to make the model powerful. Using graph convolutions, part-based graph models can learn rich representations and we demonstrate the strength of this model through application to action recognition from S-videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Spatio-temporal Part-based Graph Convolutions</head><p>The S-videos are represented as spatio-temporal graphs. In order to include the temporal dimension, corresponding joints in each part are connected temporally. <ref type="figure" target="#fig_2">Figure 3</ref>(b) shows the spatio-temporal graph for torso over five frames. Adapting select-assemble-normalize (PATCHY-SAN) proposed by Niepert et al. <ref type="bibr" target="#b20">[21]</ref> we present an overview of convolution formulation for our spatio-temporal graph by extending ideas from section 3.2. For in-depth understanding, we refer the reader to <ref type="bibr" target="#b20">[21]</ref>. We perform a spatial convolution on each partition following equation 5, combine the convolved partitions using F agg and perform temporal convolution on the graph obtained by aggregating the partitions. In effect, we spatially convolve each partition independently for each frame, aggregate them at each frame and perform temporal convolution on the temporal dimension of the aggregated graph. For a possible partitioning of human skeleton, this phenomenon is shown in <ref type="figure" target="#fig_2">Figure 3</ref>(c) for spatial convolution for a vertex common to torso and head, 3(d) for spatial convolutions in different frames, 3(e) for applying F agg on head + torso and 3(f) for convolution on temporal dimension of the combined graph. We first define the spatial and temporal neighborhood of a vertex in spatio-temporal graph and assign labels to the vertices in the neighborhoods, which is required to perform convolutions. For each vertex, we use 1-neighborhood (k = 1) for spatial dimension (N 1 ) as the skeleton graph is not very large and a τ-neighborhood (k = τ) for the temporal dimension (N τ ). <ref type="figure" target="#fig_2">Figure 3</ref>(a) (dashed polygons) shows the spatial &amp; temporal neighborhood for a root vertex. The different neighborhood sets for our model are defined as (d(v i , v j ) = length of shortest path between v i and v j ):</p><formula xml:id="formula_7">N 1p (v i ) = {v j | d(v i , v j ) ≤ 1, v i , v j ∈ V p } (8) N τ (v it a ) = {v it b | d(v it a , v it b ) ≤ τ 2 } (9)</formula><p>where, t a &amp; t b represent two time instants and p ∈ {1, . . . , n} is the partition index. The set of vertices V p differs for each part, with some vertices shared between parts <ref type="figure" target="#fig_0">(Figure 1(c)</ref>). As temporal convolution is performed on the aggregated spatio-temporal graph, N τ is not part-specific. <ref type="figure" target="#fig_2">Figure 3</ref>(a) shows the spatial and temporal neighborhoods for a root vertex in torso. For ordering vertices in the receptive fields (or neighborhoods), we use a single label spatially (L S : V → {0}) to weigh vertices in N 1p of each vertex equally and τ labels temporally (L T : V → {0, . . . , τ − 1}) to weigh vertices across frames in N τ differently. The labeling functions are defined as:</p><formula xml:id="formula_8">L S (v jt ) = {0 | v jt ∈ N 1p (v it )} (10) L T (v it b ) = {((t b − t a ) + τ 2 ) | v it b ∈ N τ (v it a )}<label>(11)</label></formula><p>Using the labeled spatial and temporal receptive fields, we define the spatial and temporal convolutions as (adapted from <ref type="bibr" target="#b13">[14]</ref>):</p><formula xml:id="formula_9">Y p (v it ) = ∑ v jt ∈N 1p (v it ) A p (i, j) Z p (v jt ) | p ∈ {1, . . . , n}<label>(12)</label></formula><formula xml:id="formula_10">Z p (v jt ) = W p (L S (v jt )) X p (v jt )<label>(13)</label></formula><formula xml:id="formula_11">Y S (v it ) = F agg ({Y 1 (v it ), . . . , Y n (v it )})<label>(14)</label></formula><formula xml:id="formula_12">Y T (v it a ) = ∑ v jt b ∈N τ (v ita ) W T (L T (v it b )) Y S (v it b )<label>(15)</label></formula><p>where, A p is a normalized adjacency matrix as explained in section 3 for part p. L S for each part is same but N 1p is part-specific. W p ∈ R C ×C×1×1 is a part-specific channel transform kernel (pointwise operation) and W T ∈ R C ×C ×τ×1 is the temporal convolution kernel. Z p is the output from applying W p on input features X p at each vertex. Y S is the output obtained after aggregating all partition graphs at one frame and Y T is the output after applying temporal convolution on Y S output of τ frames. We use a weighted sum fusion as our F agg :</p><formula xml:id="formula_13">F agg ({Y 1 , . . . , Y n }) = ∑ i W agg (i) Y i<label>(16)</label></formula><p>Human skeleton can be divided into two major components: (1) Axial skeleton and (2) Appendicular skeleton. The body parts included in these two components are shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. Human skeleton can be divided into parts based on these components. Different division schemes are shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>, 1(c) and 1(d) and we use these schemes for experiments to test our PB-GCN.</p><p>For the final representation, we divide the human skeleton into four parts: head, hands, torso and legs, which corresponds to a division scheme where each of the axial and appendicular skeleton are divided into upper and lower components, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(c). We consider left and right parts of hands and legs together in order to be agnostic to laterality <ref type="bibr" target="#b30">[31]</ref> (handedness / footedness) of the human when performing an action. To show how being agnostic to laterality is helpful, we divide the upper and lower components of appendicular skeleton into left and right (shown in <ref type="figure" target="#fig_0">Figure 1(d)</ref>), resulting in six parts and show results on it. To cover all natural connections between joints in skeleton graph, we include an overlap of atleast one joint between two adjacent parts. For example, in <ref type="figure" target="#fig_0">Figure 1(c)</ref>, shoulder joints are common between the head and hands. For the lower appendicular skeleton (viz. legs), we also include the joint at the base of spine to get a good overlap with lower axial skeleton.</p><p>Architecture and Implementation We represent each subgraph by its adjacency matrix, normalized by corresponding degree matrix D. Our model takes as input a tensor having features for each vertex in the spatio-temporal graph of S-video and outputs a vector of class scores for the video. The architecture of the graph convolutional network is similar to Yan et al. <ref type="bibr" target="#b32">[33]</ref> and consists of 9 spatio-temporal graph convolution units (each unit with the four W p kernels, one W T kernel and a residual) with an initial spatio-temporal head unit, based on a Resnet-like model <ref type="bibr" target="#b8">[9]</ref>. First three layers have 64 output channels, next three have 128 and last three have 256. We also use a learnable edge weight mask for learning edge weights in each subgraph <ref type="bibr" target="#b32">[33]</ref>. We use the Pytorch framework <ref type="bibr" target="#b22">[23]</ref> for our implementation. The code and models are made publicly available: https://github.com/dracarys983/pb-gcn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Geometric &amp; Kinematic Signals</head><p>Yan et al. <ref type="bibr" target="#b32">[33]</ref> use the 3D coordinates of each joint directly as the signal at each graph node. Relative coordinates <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref> and temporal displacements <ref type="bibr" target="#b33">[34]</ref> of joints have been used earlier for action recognition. Derived information like optical flow and Manhattan line map has been found useful on RGB images also <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37]</ref>. Even a CNN framework can be more effective and efficient if relevant derived information is supplied as input to the network.</p><p>We use a signal at each node that combines temporal displacements across time and relative coordinates, with respect to shoulders and hips <ref type="bibr" target="#b11">[12]</ref>. This representation provides translation invariance to the representation <ref type="bibr" target="#b28">[29]</ref> and improves skeletal action recognition performance significantly. <ref type="figure" target="#fig_0">Figure 1(a)</ref>   skeleton video frame. We show the effect of relative joint coordinates (geometric signal) and temporal displacements (kinematic signal) individually and the performance improvement obtained by using a combination of these signals for a baseline one-part model as well as our four part-based model in the <ref type="table">Table 1</ref>(b). The improvement in performance obtained using the geometric and kinematic signals is noteworthy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup and Results</head><p>We use SGD as the optimizer and run the training for 80 epochs (NTURGB+D) / 120 epochs (HDM05). We set the initial learning rate to 0.1 and all the experiments are run on a cluster with 4 Nvidia GTX 1080Ti GPUs. The batch size is set to 64. Learning rate decay schedule (set to decay by 0.1 at epochs 20, 50 and 70 for NTURGB+D, and at epoch 80 for HDM05) is finalized using a validation set. No augmentation is performed for any of the experiments, consistent with graph-based method <ref type="bibr" target="#b32">[33]</ref>. We perform ablation studies on the large-scale NTURGB+D dataset (shown in <ref type="table">Table 1</ref>) and then compare with state-of-the-art on both HDM05 and NTURGB+D using the best configuration of our model (shown in <ref type="table" target="#tab_3">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>NTURGB+D <ref type="bibr" target="#b23">[24]</ref> This is currently the largest RGBD dataset for action recognition to the best of our knowledge. It has 56,880 video sequences shot with three Microsoft Kinect v2 cameras from different viewing angles. There are 60 classes among the action sequences and 3D coordinates of 25 joints are provided for each human skeleton tracked. There is a large variation in viewpoint, intra-class subjects and sequence lengths, which makes this dataset challenging. We remove 302 of the captured samples having missing or incomplete skeleton data. The protocol mentioned in Shahroudy et al. <ref type="bibr" target="#b23">[24]</ref> is followed for comparisons with previous methods.</p><p>HDM05 <ref type="bibr" target="#b19">[20]</ref> This dataset was captured by using an optical marker-based Vicon system. It contains 2337 action sequences ranging across 130 motion classes performed by five actors. This dataset currently has the largest number of motion classes. The actors are named "bd", "bk", "dg", "mm" and "tr", and 31 joints are annotated for each skeleton. This dataset is challenging due to intra-class variations induced by multiple realizations of same action and large number of motion classes. We follow the protocol given in <ref type="bibr" target="#b9">[10]</ref> which is used by recent deep learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Discussion</head><p>PART-BASED GRAPH MODEL: Our motivation to use a part-based graph model is derived primarily from the fact that human actions are made up of "gestures" which represent motion of a body part. The seminal success of DPMs <ref type="bibr" target="#b7">[8]</ref> in detecting humans in images reinforces the motivation further. We discuss the effect of proposed spatio-temporal part-based graph model below.</p><p>(a) How many parts to have? We start with a coarse-grained scheme where entire skeleton is a single part and progress towards finer representations. The different partitions are, two parts: dividing skeleton into axial and appendicular skeleton, four parts: as explained in section 4 and six parts: Assigning left and right in hands and legs. The feature at each vertex in the input is 3D coordinate of the corresponding joint. From <ref type="table">Table 1</ref>(a), we can see that using two parts improves over one and four improves over two. This shows that partitioning the skeleton graph into subgraphs with useful properties helps. However, dividing upper and lower skeletons into left and right in four part scheme does not improve performance, as per our intuition about laterality mentioned in section 4. This experiment suggests that part-based model improves performance over single part and being agnostic to laterality is helpful. Our final model uses the four part division of the human skeleton.</p><p>(b) Comparison to graph-based models From Table 2(a) and Table 1(b), it can be seen that our part-based model performs better than graph based model of Yan et al. <ref type="bibr" target="#b32">[33]</ref> even when using J loc as the feature at each vertex. The graph construction in <ref type="bibr" target="#b32">[33]</ref> uses a spatial partitioning scheme for their final model which divides the skeleton graph egde set into several partitions, while the vertex set has no partitions and contains all the joints. The difference in our model is that we divide the entire skeleton into smaller parts similar to human body parts and hence we use different edge set and vertex set for each part. Compared to graph based model of Li et al. <ref type="bibr" target="#b15">[16]</ref>, our model performs significantly better on NTURGB+D as well as HDM05. However, it is possible that this is because the number of layers in the network in <ref type="bibr" target="#b15">[16]</ref> is much smaller (2 vs 9) compared to our model. Our model outperforms both the previous graph based models proposed for skeleton action recognition on the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GEOMETRIC + KINEMATIC SIGNALS:</head><p>Providing an explicit cue to a convolutional network, such as optical flow when performing action recognition from RGB videos <ref type="bibr" target="#b24">[25]</ref>, which is significant for the task at hand helps learn a richer representation by focusing on the cue. This motivates the use of geometric and kinematic features for skeletal action recognition. For the final configuration of our model, we concatenate the geometric and kinematic signals.</p><p>(a) Kinematic: temporal displacements Temporal displacements provide information about the amount of motion happening between two frames. This information is synonymous to 3D scene flow of a very sparse set of points. We hypothesize that these displacements provide explicit motion information (like optical flow) which makes the model consider displacements as strong features and learn from them. Improvement in performance using this signal can be seen from Table 1(b), for both four-part as well as one-part model across both splits of NTURGB+D.</p><p>(b) Geometric: relative coordinates These provide translation invariant features as explained in <ref type="bibr" target="#b28">[29]</ref> and they have been used effectively to encode skeletons by Ke et al. <ref type="bibr" target="#b11">[12]</ref> into images. Also, Zhang et al. <ref type="bibr" target="#b35">[36]</ref> used relative coordinates as a geometric feature which performs much better than 3D joint locations using a simple stacked LSTM network. We  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Accuracy</head><p>SPDNet <ref type="bibr" target="#b9">[10]</ref> 61.45 ± 1.12 Lie Group <ref type="bibr" target="#b27">[28]</ref> 70.26 ± 2.89 LieNet <ref type="bibr" target="#b10">[11]</ref> 75.78 ± 2.26 P-LSTM <ref type="bibr" target="#b23">[24]</ref> 73.  of results correspond to non-graph based methods and the middle corresponds to GCN based methods. PB-GCN is our part-based graph convolutional network. Evaluation protocols used: CS (Cross Subject) and CV (Cross View) for NTURGB+D <ref type="bibr" target="#b23">[24]</ref>; 10-fold cross sample validation for HDM05 <ref type="bibr" target="#b9">[10]</ref>.</p><p>can see improvements in performance provided by relative coordinates in <ref type="table">Table 1</ref>(b) for both global (one part) and four part-based models, which are the worst and best performing models according to <ref type="table">Table 1</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison to state of the art</head><p>NTURGB+D: For this dataset, we outperform all previous state-of-the-art methods by a large margin. Even without using the signals introduced in section 5, we outperform the previous methods which can be seen in <ref type="table">Table 1</ref>(b) (J loc results). We outperform the previous state-of-the-art graph based method of Yan et al. <ref type="bibr" target="#b32">[33]</ref> (STGCN) which is also the state-ofthe-art for skeleton based action recognition to the best of our knowledge, by a margin of 6% and~5% for the two protocols.</p><p>HDM05: This is a~20x smaller dataset compared to NTURGB+D but contains more than twice the number of classes in NTURGB+D. The length of sequences in this dataset is longer and some of the action classes have only one sequence <ref type="bibr">[2]</ref>. Using the protocol of <ref type="bibr" target="#b9">[10]</ref> is therefore very challenging, on which we obtain state-of-the-art results using our model. We outperform the previous state-of-the-art Deep STGC <ref type="bibr" target="#b15">[16]</ref>, which is a network based on spectral graph convolutions for skeleton action recognition by~3% at the mean accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we define a partition of skeleton graph on which spatio-temporal convolutions are formalized through a part-based GCN for the task of action recognition. Such a partbased GCN learns the relations between parts and understands the importance of each part in human actions more effectively than a model that considers entire body as a single graph. We also demonstrate the benefit of giving explicit cues to the convolutional model which are significant from the point of view of the task at hand, such as relative coordinates and temporal displacements for skeletal action recognition. As a result, our model achieves state-of-the-art performance on two challenging action recognition datasets. As a future work, we would like to explore the use of part-based graph model for tasks other than action recognition, such as object detection, measuring image similarity, etc. In this document, we present findings from further quantitative analysis on the action recognition results. Specifically, we compute the confusion matrices of the performance of different models and explain the useful model properties based on our observations. We find that graph-based models can understand actions which involve more motion better than those where skeleton motion is very less and contains object interactions. We also show the importance of using geometric and kinematic features instead of 3D joint locations by performing an experiment on graph-based model of Yan et al. <ref type="bibr">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part-based Graph Convolutional Network for Action Recognition: Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Quantitative Analysis</head><p>We compute the confusion matrices for performance of our part-based graph model, graph model using only one part and Yan's graph model <ref type="bibr">[2]</ref>. We did not include Li's graph model <ref type="bibr">[1]</ref> as no code has been provided by the authors to reproduce the results. The performance for cross subject (CS) evaluation protocol is considered as it is more challenging than the cross view (CV) evaluation protocol. The confusion matrices for different models are shown in <ref type="figure" target="#fig_0">Figure 1</ref> (model-1), 2 (model-2) and 3 (model-3). The recognition accuracy for each of these models for cross subject (CS) evaluations is 85.6, 87.5 and 81.5 respectively. The model corresponding to <ref type="figure" target="#fig_0">Figure 1</ref> is a one-part graph model which does not divide the skeleton graph into parts and it takes a combination of relative joint coordinates D R and temporal displacements D T as input. The model corresponding to 2 is our four-part graph model with D R and D T as input. Finally, <ref type="figure" target="#fig_2">Figure 3</ref> corresponds to graph-based model introduced in Yan et al. <ref type="bibr">[2]</ref> for skeleton action recognition. We proceed to identifying the action classes for which the recognition performance is bad, explain what the reasons are for such performance, propose a possible solution and then compare performance across different classes for models with respect to model-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Commonly confused classes</head><p>The confusion matrices have boxes marked around certain values. These boxes represent the confused classes which are consistent across all models. For example, one of the boxes is around action classes 11 &amp; 12, which correspond to "reading" and "writing" actions. These actions are mostly confused amongst each other and also with actions such as "playing with the phone / tablet" or "typing on a keyboard" (actions 29 &amp; 30 present in the other marked box) which is clear from the confusion matrices. In all these actions, there is almost no skeleton motion and the differences are manifested in the form of interaction with different   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy CS CV Yan <ref type="bibr">[2]</ref> (model-2) 81.5 88.3 Model-2 + D R ||D T 86.3 92.1 <ref type="table">Table 1</ref>: Results on NTURGB+D for model-2 <ref type="bibr">[2]</ref>, with and without the combined signal D R ||D T (relative coordinates and temporal displacements).</p><p>objects. Due to these properties, models using skeleton information for recognizing actions give lower performance for these action classes as they do not have access to object information. A possible approach to overcome this limitation on recognition potential is to use RGB information along with skeleton information in order to get information about objects as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Model-1 vs Model-2</head><p>Model-2 improves over Model-1 by using a part-based graph representation instead of considering the entire graph as one part. Model-2 achieves better recognition performance by improving over action classes such as "brushing teeth" (class 3), "cheer up" (class 22), "make a phone call/answer phone" (class 28), etc. These actions have a strong correlation with movement of both hands and legs. Due to this correlation, our part-based graph model is able to achieve better performance as it learns from these parts specifically and uses an intuitive way to divide the human body into parts. Being agnostic to parts in human skeleton helps in learning a global representation but learning importance of parts using such a model is difficult, compared to a part-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Model-3 vs Model-2</head><p>Spatio-temporal model of Yan et al. <ref type="bibr">[2]</ref> confuses the action of "clapping" as well along with the actions mentioned in section 1.1. The model proposed by Yan <ref type="bibr">[2]</ref> partitions the edge set and uses the same vertex set for each partition of edge set. We believe that their model learns the importance of different edges in the skeleton graph and does not learn the importance of parts like our part-based graph model. In order to understand the influence of geometric and kinematic signals as input to a graph-based model, we use the signals on top of model-3 and we find that we get a boost in recognition performance for model-3. The recognition accuracy on NTURGB+D is shown in <ref type="table">Table 1</ref>. This experiment shows that the signals help in improving recognition performance for different graph-models for skeleton action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Conclusion</head><p>Using a part-based model works better than using a model that does not partition the skeleton graph. However, using only skeletal data for action recognition is not enough as different actions might have similar dynamics of parts in the skeleton but different object interactions. In such cases, RGB information can be used to disambiguate interactions with objects. Providing the network with a cue that is known apriori to work well for the task at hand, viz. relative coordinates and temporal displacements for skeletal action recognition, can improve recognition performance by a large amount as we show in our experiment on previous stateof-the-art model for NTURGB+D <ref type="bibr">[2]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Geometric and Kinematic features, (b) Appendicular and axial body parts: two parts, (c) Dividing the appendicular and axial skeletons into upper and lower parts: four parts, (d) Dividing appendicular upper and lower skeletons into left and right: six parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Figure 2 :</head><label>12</label><figDesc>(a) Graph feature (b) Convolution for receptive field of (c) Final convolution equation and adjacency matrices a chosen root vertex f Equation-based formulation and illustration of a graph convolution features using shallow models which do not model spatio-temporal properties of actions very well and constrain learning capacity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Spatio-temporal neighborhood for root node (in green) and depiction of convolutions in space and time dimensions. Effect of application of F agg is shown, where the common vertices are in darker shade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison for different number of parts in the skeleton graph and signals at vertices using our PB-GCN, on NTURGB+D [24] (CS: Cross Subject, CV: Cross View). The symbols for signals, J loc : Absolute 3D joint locations, D R : Relative coordinates, D T : Temporal displacements and D R ||D T : Concatenation of D R and D T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 :</head><label>1</label><figDesc>Confusion matrix for model with one part and combined geometric + kinematic features as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Confusion matrix for model with four parts and combined geometric + kinematic features as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Confusion matrix for Yan's graph-based model[2]  having 3D joint locations as input signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>illustrates the computation of the two signals for a single</figDesc><table><row><cell cols="2">(a) Performance with number of parts</cell><cell>(b) Performance with various signals for best &amp; worst number of parts</cell></row><row><cell>#Parts One Two Four Six</cell><cell>Accuracy CS CV 79.4 87.9 80.2 88.4 82.8 90.3 81.4 89.1</cell><cell>Signals J loc D R D T D R ||D T 85.6 91.8 87.5 93.2 Accuracy #Parts=1 #Parts=4 CS CV CS CV 79.4 87.9 82.8 90.3 83.6 87.7 84.6 88.4 84.3 91.6 85.4 92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with previous methods on two benchmark datasets. The top group</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">KALPIT THAKKAR, P J NARAYANAN: PART-BASED GCN FOR ACTION RECOGNITION coordinates (seeFigure 1(a)) which improves action recognition performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">KALPIT THAKKAR, P J NARAYANAN: PART-BASED GCN FOR ACTION RECOGNITION</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1809.04983v1 [cs.CV] 13 Sep 2018 2 KALPIT THAKKAR, P J NARAYANAN: PART-BASED GCN FOR ACTION RECOGNITION</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">KALPIT THAKKAR, P J NARAYANAN: PART-BASED GCN FOR ACTION RECOGNITION</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">KALPIT THAKKAR, P J NARAYANAN: PART-BASED GCN FOR ACTION RECOGNITION</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying and visualizing motion capture sequences using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Applications (VISAPP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="122" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pku-mmd: A large scale benchmark for continuous multi-modal human action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Chunhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Yueyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yanghao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Sijie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Jiaying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mohamed Daoudi, and Alberto Del Bimbo. 3-d human action recognition by shape analysis of motion trajectories on riemannian manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Devanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Pala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1340" to="1352" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A riemannian network for spd matrix learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc J Van</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning on lie groups for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengde</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6099" to="6108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ferdous Sohel, and Farid Boussaid. A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senjian</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4570" to="4579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d skeleton based action recognition by video-domain translation-scale invariant mapping and multi-scale dilated cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolution for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph-based approach for 3d human skeletal action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="195" to="202" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Documentation mocap database hdm05</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meinard</forename><surname>Mãijller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tido</forename><surname>Rãűder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Clausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjãűrn</forename><surname>Krãijger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence of the most informative joints (smij): A new representation for human skeletal action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rizwan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregorij</forename><surname>Kurillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruzena</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="38" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">Z</forename><surname>Andrew Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-vipdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Moving poselets: A discriminative and interpretable skeletal motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="61" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feastnet: Feature-steered graph convolutions for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018-IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Definition of laterality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Laterality" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Chih</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On geometric features for skeletonbased action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08999</idno>
		<title level="m">Layoutnet: Reconstructing the 3d room layout from a single rgb image</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolution for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
