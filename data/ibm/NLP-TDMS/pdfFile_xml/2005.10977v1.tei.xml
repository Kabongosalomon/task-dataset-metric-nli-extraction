<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Qiao</surname></persName>
							<email>qiaozhi@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
							<email>zhouyu@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
							<email>yangdongbao@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucan</forename><surname>Zhou</surname></persName>
							<email>zhouyucan@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
							<email>wangweiping@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene text recognition is a hot research topic in computer vision. Recently, many recognition methods based on the encoder-decoder framework have been proposed, and they can handle scene texts of perspective distortion and curve shape. Nevertheless, they still face lots of challenges like image blur, uneven illumination, and incomplete characters. We argue that most encoder-decoder methods are based on local visual features without explicit global semantic information. In this work, we propose a semantics enhanced encoder-decoder framework to robustly recognize low-quality scene texts. The semantic information is used both in the encoder module for supervision and in the decoder module for initializing. In particular, the state-of-theart ASTER method is integrated into the proposed framework as an exemplar. Extensive experiments demonstrate that the proposed framework is more robust for low-quality text images, and achieves state-of-the-art results on several benchmark datasets. The source code will be available. † * The corresponding author † https://github.com/Pay20Y/SEED hird theatro hour theatre hard house reval royal you body promos promod Existing Encoder-Decoder Framework Low Quality Images Ours</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene text detection and recognition have attracted great attention in recent years owing to its various applications such as autonomous driving, road sign recognition, helping visual impaired and so on. Inspired by object detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">58]</ref>, scene text detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b5">6]</ref> achieved convincing performance. Despite the maturity of conventional text recognition in documents, scene text recognition is still a challenging task.</p><p>With the development of deep learning, recent works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b52">53]</ref> on scene text recognition have shown promising results. However, existing methods are still facing various problems when dealing with image blur, background interference, occlusion <ref type="bibr">Figure 1</ref>. The comparison of our SEED with the existing encoderdecoder framework such as <ref type="bibr" target="#b44">[45]</ref>. The first column shows the examples of some challenging scene text including image blur, occlusion, and background interference. The second column is the results of the existing encoder-decoder framework and the third column gives the predictions of our approach. It shows that our proposed method is more robust to the low-quality images. and incomplete characters as shown in <ref type="figure">Fig. 1</ref>.</p><p>Recently, inspired by neural machine translation of the natural language processing field, the encoder-decoder framework with attention mechanism has been widely used in scene text recognition. For regular text recognition <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref>, the encoder is based on CNN with RNN and another RNN with attention mechanism is used as the decoder to predict character at each time step. For irregular text recognition, the rectification based methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b52">53]</ref>, the multi-direction encoding method <ref type="bibr" target="#b7">[8]</ref> and the 2D-attention based methods <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b22">23]</ref> are proposed. Rectification based methods first rectify the irregular images, then the following pipeline is as those of regular recognition. The multi-direction encoding method uses CNN with two LSTMs to encode four different directions. The 2D-attention based methods use 2D-attention mechanism to deal with irregular text which handles feature map from two dimensions directly.</p><p>The existing methods define the text recognition task as a sequence character classification task locally, but ignore the global information of the whole word. As a result, they may struggle to handle low-quality images such as image blur, occlusion and incomplete characters. However, people can deal with these low-quality cases well by considering the global information of the text.</p><p>To address this problem, we propose the Semantics Enhanced Encoder-Decoder framework (SEED), in which an additional semantic information is predicted acting as the global information. The semantic information is then used to initialize the decoder as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref> (c). The semantic information has two main advantages, 1) it can be supervised by a word embedding in natural language processing field, 2) it can reduce the gap between the encoder focusing on the visual feature and the decoder focusing on the language information, since the text recognition can be regarded as a cross-modality task. Specifically, we get the word embedding from a pre-trained language model and compute a loss between the semantic information and the word embedding during training. By this way, the semantic information contains richer semantics, then the predicted semantic information is used to guide the decoding process. As a result, the decoding process can be limited in a semantic space, and the performance of recognition will be better. Some examples are shown in <ref type="figure">Fig. 1</ref>. As an example, in the fourth sub-image of <ref type="figure">Fig. 1</ref>, the last two characters "se" are recognized as "R" because of the occlusion, but it can be corrected in our framework with the global semantic information. In other words, the semantic information works as an "intuition", which is like a glimpse before people read a word carefully.</p><p>Predicting semantic information from images directly has already been studied before. <ref type="bibr" target="#b11">[12]</ref> predicts semantic concepts directly from a word image with a CNN and a weighted ranking loss. <ref type="bibr" target="#b50">[51]</ref> tries to embed image features into a word embedding space for text spotting. <ref type="bibr" target="#b20">[21]</ref> proposes to learn embedding of the word images and the text labels in an endto-end way. These works validate that semantic information is helpful to the text related tasks.</p><p>The main contributions are as follows: 1. We propose SEED for scene text recognition, which predicts additional global semantic information to guide the decoding process, and the predicted semantic information is supervised by the word embedding from a pre-trained language model.</p><p>2. We integrate the state-of-the-art ASTER method <ref type="bibr" target="#b44">[45]</ref> to our framework as an exemplar.</p><p>3. Extensive experiments on several public scene text benchmarks demonstrate the proposed framework can obtain state-of-the-art performance, especially on the low-quality datasets ICDAR2015 and SVT-Perspective, and it is particularly more robust for incomplete characters.</p><p>The rest of this paper is organized as follows: Sec. 2 reviews the related works, Sec. 3 describes the proposed framework and the exemplar, Sec. 4 conducts profuse experiments and Sec. 5 concludes the work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Scene Text Recognition</head><p>Existing scene text recognition methods can be divided into two categories, namely traditional methods and deep learning based methods.</p><p>Traditional methods usually adopt a bottom-up approach which detects and classifies characters first and then groups them to a word or text line with heuristic rules, language models or lexicons. They design various hand-craft features then use these features to train a classifier such as SVM. For example, <ref type="bibr" target="#b33">[34]</ref> uses a set of computationally expensive features like aspect ratio, hole area ratio, etc. <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b48">49]</ref> use sliding windows with HOG descriptors, and <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b2">3]</ref> use Hough voting with random forest classifier. Most traditional methods suffer from designing various hand-crafted features, and these features are limited for high-level representation.</p><p>With the development of deep learning, most methods use CNN to perform a top-down approach which recognizes word or text line directly. <ref type="bibr" target="#b15">[16]</ref> treats a word as a class, then converts the recognition problem into the image classification problem. Recently, most works treat the recognition problem as the sequence prediction problem. Existing methods can be almost divided into two techniques namely Connectionist Temporal Classication (CTC) and attention mechanism. For CTC-based decoding, <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref> propose to use CNN and RNN to encode the sequence features and use CTC for character alignment. For attention-based decoding, <ref type="bibr" target="#b21">[22]</ref> proposes recursive CNN to capture longer contextual dependencies and uses an attention-based decoder for sequence generation. <ref type="bibr" target="#b6">[7]</ref> introduces the problem of attention drift, and proposes focusing attention for better performance.</p><p>However, these works all assume that the text is horizontal, and can not handle the text of irregular shapes such as perspective distortion and curvature. To solve the problem of irregular text recognition, <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> propose to rectify the text first based on Spatial Transformer Network <ref type="bibr" target="#b16">[17]</ref> and then treat it as horizontal text. Furthermore, <ref type="bibr" target="#b56">[57]</ref> gets better performance with iterative rectication and <ref type="bibr" target="#b52">[53]</ref> rectifies with some geometric constraints. <ref type="bibr" target="#b31">[32]</ref> rectifies text by predicting pixels offset. Instead of rectifying the whole text, <ref type="bibr" target="#b27">[28]</ref> takes an approach of detecting and rectifying individual characters. In spite of rectification, <ref type="bibr" target="#b7">[8]</ref> encodes the images in four directions and proposes a filter gate to fuse the features. <ref type="bibr" target="#b53">[54]</ref> introduces an auxiliary dense character detection task and an alignment loss into the 2D attention based network. <ref type="bibr" target="#b22">[23]</ref> proposes a tailored 2D attention based framework for irregular text recognition. Without encoder-decoder framework, <ref type="bibr" target="#b24">[25]</ref> converts irregular text recognition into character segmentation with fully convolutional network <ref type="bibr" target="#b30">[31]</ref>. <ref type="bibr" target="#b51">[52]</ref> proposes a new loss function for more effective decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantics in Scene Text</head><p>Many works try to bring semantics into the tasks of text recognition or text retrieval. <ref type="bibr" target="#b11">[12]</ref> predicts semantic concepts directly from a word image with CNN. <ref type="bibr" target="#b35">[36]</ref> proposes to generate contextualized lexicons for scene images with only visual information, and word-spotting task benefits a lot from the lexicons. <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b20">21]</ref> learn to map the word images to a word embedding space and apply it into word spotting system. <ref type="bibr" target="#b17">[18]</ref> tries to detect and recognize text in online images with the help of context information such as tags, comments, and titles. <ref type="bibr" target="#b41">[42]</ref> introduces to use the language model and the semantic correlation between scene and text to re-rank the recognition results. <ref type="bibr" target="#b36">[37]</ref> proposes to boost the performance of text spotting with the object information. <ref type="bibr" target="#b10">[11]</ref> uses the text embedded in advertisement images to enhance the image classication. <ref type="bibr" target="#b58">[59]</ref> proposes to use a pre-trained language model to correct the inaccurate recognition results with the text context in the image.</p><p>As discussed before, state-of-the-art recognition methods do not utilize the semantics of the text well. The related semantics works do not integrate the semantics into the recognition pipeline explicitly and effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section we describe the proposed method in detail. The general framework is shown in <ref type="figure" target="#fig_0">Fig. 2</ref> (c), which consists of 4 major components: 1) The encoder including CNN backbone and RNN for extracting visual features; 2) The semantic module for predicting semantic information from the visual features; 3) The pre-trained language model for supervising the semantic information predicted by semantic module; 4) The decoder including RNN with attention mechanism for generating the recognition results. First we review the encoder-decoder framework in Sec. 3.1, and introduce the pre-trained language model detailedly in Sec. 3.2. In Sec. 3.3, we describe our proposed method. Specifically, we present the general framework in Sec. 3.3.1. After that, we show the details of the proposed method which integrate state-of-the-art method ASTER <ref type="bibr" target="#b44">[45]</ref> into proposed framework in Sec. 3.3.2. Finally, the loss function and the training strategies are presented in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoder-Decoder Framework</head><p>The Encoder-decoder framework is widely used in neural machine translation, speech recognition, text recognition and so on. <ref type="bibr" target="#b46">[47]</ref> first introduces the structure of the framework and applies it into neural machine translation. For simplicity, we call this framework plain encoder-decoder framework. As visualized in <ref type="figure" target="#fig_0">Fig. 2 (a)</ref>, the encoder extracts rich features and generates a context vector C which contains global information of the inputs, then the decoder converts the context vector to target outputs. Source inputs and target outputs are different due to different tasks, as for text recognition, the inputs are images and target outputs are the texts in the images. The specific composition of encoder and decoder is not fixed, CNN and LSTM are all common choices. Despite great effectiveness, the plain encoder-decoder framework has an obvious drawback, where the context information has limited ability to represent the whole inputs. Inspired by human visual attention, researchers introduce the attention mechanism into the encoder-decoder framework, which is defined as the attention-based encoder-decoder framework. As shown in <ref type="figure" target="#fig_0">Fig. 2 (b)</ref>, attention mechanism attempts to build shortcuts between the context and the whole inputs. The decoder can select the appropriate context at each decoding step which is capable of resolving long-range dependency problems, and the alignment between encoder and decoder is trained in a weakly supervised way.</p><p>For scene text recognition, the decoder only depends on the limited local visual features for decoding in both the plain encoder-decoder framework and the attention-based encoder-decoder framework, so it is difficult to deal with some low-quality images without global information. In our proposed framework, the encoder learns explicit global semantic information and uses it as guidance for the decoder. We use FastText <ref type="bibr" target="#b3">[4]</ref> to generate word embedding as the supervision of the semantic information in that it can solve the problem of "out of vocabulary".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">FastText Model</head><p>We choose FastText as our pre-trained language model, which is based on skip-gram. Let T = {w i−l , . . . , w i+l } be a sentence in a text corpus. l indicates the length of the sentence and is a hyper-parameter. In skip-gram, a word w i is represented by a single embedding vector v i and then input to a simple feed-forward neural network, which aims to predict the context represented as</p><formula xml:id="formula_0">C i = {w i−l , . . . , w i−1 , w i+1 , . . . , w i+l }.</formula><p>With training the feedforward network, the embedding vector is simultaneously optimized, and the final embedding vector of a word is close to the words with similar semantics.</p><p>FastText additionally embeds subwords and uses them to generate final embedding of the word w i . Given the hyperparamters l min and l max denoting a minimum and a maximum length of the subwords. For example, let l min = 2, l max = 4 and the word be "where", the set of subwords is {wh, he, er, re, whe, her, ere, wher, here}. The word representation is obtained by the combination of the embedding vectors of all subwords and the word itself. Accordingly, FastText model can handle the problem of "out of vocabulary". There are some novel words or incomplete words in the benchmark datasets such as ICDAR2015 and SVT-Perspective, so FastText is suitable for our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SEED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">General Framework</head><p>Many scene text recognition methods are based on the encoder-decoder framework with attention. The decoder focuses on specific regions of visual features and outputs corresponding characters step by step. The framework works well in most scenarios except in low-quality images. In some low-quality images, texts may be blurred or occluded. To address these problems, utilizing global semantic information is an alternative. The proposed framework is shown in <ref type="figure" target="#fig_0">Fig. 2 (c)</ref>. Different from the attention-based encoderdecoder framework, the proposed semantic module predicts extra semantic information. Further, we use the word embedding from a pre-trained language model as the supervision to improve the performance. After that, the semantic information is fed into the decoder along with the visual features. The semantic module predicts semantic information from the outputs of the encoder which is fed into decoder as the guidance.</p><p>In this way, our method is robust to low-quality images and can correct recognition mistakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Architecture of Semantics Enhanced ASTER</head><p>We use ASTER <ref type="bibr" target="#b44">[45]</ref> as an exemplar for our proposed framework, and we call the proposed method Semantics Enhanced ASTER (SE-ASTER). The SE-ASTER is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. There are four modules: the rectification module is to rectify the irregular text images, the encoder is to extract rich visual features, the semantic module is to predict semantic information from the visual features, and the decoder transcribes the final recognition results. First, the image is input to the rectification module to predict control points with a shallow CNN, then Thin-plate Splines <ref type="bibr" target="#b4">[5]</ref> is applied to the image. In this way, the distorted text image will be rectified. This module is the same as <ref type="bibr" target="#b44">[45]</ref>, so we don't describe it in detail. Thereafter, the rectified image will be input to the encoder, and rich visual features can be generated. Specifically, the encoder consists of a 45-layer ResNet based CNN same as <ref type="bibr" target="#b44">[45]</ref> and a 2-layer Bidirectional LSTM <ref type="bibr" target="#b12">[13]</ref> (BiLSTM) network with 256 hidden units. The output of the encoder is a feature sequence h = (h 1 , . . . , h L ) with the shape of L × C, where L is the width of the last feature map in CNN, and C is the depth.</p><p>The feature sequence h has two functions, one is to predict the semantic information by the semantic module and the other is as the input of the decoder. For predicting semantic information, we first flatten the feature sequence into a one-dimensional feature vector I with dimension of K, where K = L × C. The semantic information S is predicted with two linear functions as following:</p><formula xml:id="formula_1">S = W 2 σ(W 1 I + b 1 ) + b 2 .<label>(1)</label></formula><p>where W 1 , W 2 , b 1 , b 2 are trainable weights in the linear function, σ is a ReLU activation function. We also evaluate predicting the semantic information with the final hidden state h L of BiLSTM in the encoder, and it gets worse performance. It may originate from that predicting semantic information needs larger feature contexts and it is more proper to use the BiLSTM outputs. The semantic information will be supervised by the word embedding provided by the pre-trained FastText model. The loss function used here will be introduced in Sec. 3.4. The decoder adopts the Bahdanau-Attention mechanism <ref type="bibr" target="#b0">[1]</ref> which consists of a single layer attentional GRU <ref type="bibr" target="#b8">[9]</ref> with 512 hidden units and 512 attention units. Different from <ref type="bibr" target="#b44">[45]</ref> we use a single direction decoder here. In particular, the semantic information S is used to initialize the states of GRU after a linear function for transforming the dimension. Instead of using zero-state initializing, the decoding process will be guided with global semantics, so the decoder uses not only local visual information but also global semantic information to generate more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function and Training Strategy</head><p>We add supervision at both the semantic module and the decoder module. SE-ASTER is trained end-to-end. The loss function is as follows:</p><formula xml:id="formula_2">L = L rec + λL sem .<label>(2)</label></formula><p>where L rec is the standard cross-entropy loss of the predicted probabilities with respect to the ground-truth, and L sem is the cosine embedding loss of the predicted semantic information with respect to the word embedding of the transcription label from the pre-trained FastText model. λ is hyper-parameters to balance the loss, and we set it to 1 here. Note that we just use a simple cosine based loss function here instead of contrastive loss for faster training speed.</p><formula xml:id="formula_3">L sem = 1 − cos(S, em) .<label>(3)</label></formula><p>where S is the predicted semantic information and em is the word embedding from pre-trained FastText model. There are two training strategies. The first is initializing the state of the decoder with the word embedding from the pre-trained FastText model rather than the predicted semantic information. Another is to use the predicted semantic information directly. We evaluate these two strategies, and their performances are similar. We use the second training strategy which trains the model in a pure end-to-end way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments to verify the effectiveness of our proposed method. First, we introduce the datasets used for training and evaluation, and the implementation details are described. Next, we perform ablation studies to analyze the performance of the different strategies. Finally, our method is compared with state-of-theart methods on several benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>IIIT5K-Words (IIIT5K) <ref type="bibr" target="#b32">[33]</ref> contains 5000 images, most of which are regular samples. There are 3000 images for testing. Each sample in test set is associated with a 50-word lexicon and a 1k-word lexicon.</p><p>Street View Text (SVT) <ref type="bibr" target="#b48">[49]</ref> consists of 647 cropped word images from 249 street view images. Most of word images are horizontal, but some of them are severely corrupted by noise, blur, and low resolution. A 50-word lexicon is provided for each image.</p><p>SVT-Perspective (SVTP) <ref type="bibr" target="#b38">[39]</ref> contains 645 word images for evaluation. most images suffer in heavy perspective distortions which are difficult for recognition. Each image is associated with a 50-word lexicon.</p><p>ICDAR2013 (IC13) <ref type="bibr" target="#b19">[20]</ref> consists of 1015 images for testing, most of which are regular text images. Some of them are under uneven illumination.</p><p>ICDAR2015 (IC15) <ref type="bibr" target="#b18">[19]</ref> was collected without careful capture. Most of images are with various distortions and blurry which are challenging for most existing methods.</p><p>CUTE80 (CUTE) <ref type="bibr" target="#b40">[41]</ref> consists of 288 word images only for evaluation. Most of them are curved but with high resolution, no lexicon is provided.</p><p>Synth90K <ref type="bibr" target="#b15">[16]</ref> consists of 9 million synthetic images generated from a lexicon of 90K words. It has been widely used in text recognition task. We use it as one of our training datasets. It contains words from the testing set of the IC13 and SVT.</p><p>SynthText <ref type="bibr" target="#b13">[14]</ref> is another synthetic dataset for text detection task. We crop the words with ground-truth word bounding boxes and use for training our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The proposed SE-ASTER is implemented in Py-Torch <ref type="bibr" target="#b34">[35]</ref>. The pre-trained FastText model is the officially available model 1 trained on Common Crawl 2 and Wikipedia 3 . In total 97 symbols are recognized, including digits, uppercase and lower-case letters, 32 punctuation marks, end-ofsequence symbol, padding symbol, and unknown symbol. The size of input images are resized to 64 × 256 without keeping ratio, and we adopt the ADADELTA <ref type="bibr" target="#b55">[56]</ref> to minimize the objective function. Without any pre-training and data augmentation, our model is trained on SynthText and Synth90K for 6 epochs with the batch size of 512, the learning rate is set to 1.0 and is decayed to 0.1 and 0.01 at the 4th epoch and the 5th epoch. The model is trained on one NVIDIA M40 graphics card.</p><p>For evaluation, we resize the input images to the same size as for training. We use beam search for GRU decoding, which keeps the k candidates with the highest accumulative scores, where k is set to 5 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>There are two steps about the semantic module, one is the word embedding supervision and the other is initializing decoder with the predicted semantic information. We evaluate these two steps separately by using the Synth90K and SynthText as training data consistently. The results are shown in Tab. 2. The model supervised with word embedding only does not improve the performance compared with the baselines. Using predicted holistic features from the encoder to initialize decoder improves the performance by almost 0.2% in ICDAR13, but gets worse performance on SVTP and IC15. It shows that learning global information in an implicit weakly supervised way still struggles with low-quality images. A combination of these two steps gets the best performance. The improvements of 1.9%, 2.3% and 1.6% are obtained on IC13, SVTP and IC15 respectively. Compared with ASTER without word embedding supervision, it improves the accuracy by 1.7% on IC13, 3.3% on SVTP and 3.9% on IC15, which verifies that the supervision with word embedding is quite important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance with Inaccurate Bounding Boxes</head><p>Scene text recognition in real applications is always combined with the detection branch to achieve an end-to-end pipeline. However, the detection branch may not output ideal bounding boxes. If text recognition is robust to inaccurate detection results, the overall end-to-end performance can be more satisfactory. Limited by the receptive field of CNNs, the most frequent inaccurate detection is incomplete characters. We conduct experiments to show our method is robust with this situation. Here we also use SE-ASTER as an exemplar. Note that the SE-ASTER is only trained on Synth90K and SynthText without any data augmentation such as random cropping. We first generate two shrink datasets IC13-sr and IC15-sr based on IC13 and IC15 respectively as follows.</p><p>We randomly remove the original word images up to 15% in the left, right, top and bottom directions simultaneously. All of the cropped images still have an intersection over union with the original ones larger or equal than (1−0.15×2) 2 = 0.49. According to the evaluation protocol of detection, these cropped images are all positive localizations because the IoU is above the standard threshold of 0.  word embedding, the model still struggles with the shrink images. Using the holistic information from encoder as the guidance of the decoder gets better results with 16.5% and 13.0% decline. SE-ASTER gets the best results, which shows that our model is more robust with incomplete characters. Some visualizing examples are illustrated in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Generalization of Proposed Framework</head><p>To verify the generalization of SEED, we integrate another state-of-the-art recognition method SAR <ref type="bibr" target="#b22">[23]</ref>. SAR is a 2D-attention based recognition method without rectification on input images, and it already adopts an LSTM to generate a holistic feature. However as we mentioned before, the holistic feature may be not effective in a weakly supervised training strategy, so we make some modifications and call our new model Semantics Enhanced SAR (SE-SAR).</p><p>In SE-SAR, we replace the max-pooling along the vertical axis with a shallow CNN. The output of the shallow CNN is a feature map with the height of 1, then the feature map is fed into a 2-layer LSTM to extract context information. Two linear functions are applied to the output of LSTM to predict the semantic information. Except for the 2Dattention decoder in SAR, we apply another decoder to the output of the LSTM and supervise with the transcription labels. In this way, the output of LSTM contains richer information and helps predict semantic information. Finally, the semantic information is used to initialize the LSTM of the decoder. The model is trained on Synth90K and SynthText for 2 epochs with the batch size of 128.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results and Visualization</head><p>We visualize low-quality images including blur or occlusion. Some examples are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. As can be seen, our proposed methods SE-ASTER and SE-SAR are robust with low-quality images. We explain that semantic information will provide an effective global feature to decoder, which is robust to the interference in the images.</p><p>We also perform experiments on IIIT5K to visualize the validity of the predicted semantic information. As illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>, we compute the cosine similarity between the predicted semantic information and the word embedding of each word from lexicons <ref type="bibr">(</ref>  <ref type="table">Table 5</ref>. Lexicon-free performance on public benchmarks. Bold represents the best performance. Underline represents the second best result. * indicates using both word-level and character-level annotations to train model. <ref type="figure" target="#fig_3">Fig. 5 (a)</ref>, the predicted semantic information is very related to the words which have similar semantics. For example, "home", "house", and "lodge' all have the meaning of residence. "Tom", "Paul" and "Charles" are all common names. The second row illustrates the robustness of the predicted semantic information. For example, "house" and "horse" have a similar spelling and are of the edit distance of 1, but their semantics are quite different as shown in <ref type="figure" target="#fig_3">Fig. 5 (b)</ref>.</p><p>With the help of global semantic information, the model can distinguish them easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Comparison with State-of-the-art</head><p>We also compare our methods with previous state-of-theart methods on several benchmarks. The results are shown in Tab. 5. Compared with other methods, we achieve 2 best results and 3 second best results out of 6 in the lexicon-free scenario with only word-level annotations.</p><p>Our proposed method works effectively on some lowquality datasets such as IC15 and SVTP compared with other methods. Especially, SE-ASTER improve 3.9% on IC15 (from 76.1% to 80.0%) and 2.9% on SVTP (from 78.5% to 81.4%) compared with ASTER <ref type="bibr" target="#b44">[45]</ref>. It also outperforms state-of-the-art method ScRN <ref type="bibr" target="#b52">[53]</ref> 0.6% on SVTP and 1.3% on IC15, although our method is based on a weaker backbone and without character-level annotations.</p><p>SE-ASTER also gets superior or comparable results on several high-quality datasets. Compared with ASTER <ref type="bibr" target="#b44">[45]</ref> we get 0.4% and 4.1% improvements on IIIT5K and CUTE respectively. On SVT and IC13, our method gets accuracies of 89.6% and 92.8%, which are slightly worse than ESIR <ref type="bibr" target="#b56">[57]</ref> and <ref type="bibr" target="#b1">[2]</ref> by 0.6% and 1.6%. Note that our framework is very flexible and can be integrated with most existing methods, and we believe that if we replace a stronger baseline model better results can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Works</head><p>In this work, we propose the semantics enhanced encoderdecoder framework for scene text recognition. Our framework predicts an additional global semantic information supervised by the word embedding from a pre-trained language model. Using the predicted semantic information as the decoder initialization, the recognition accuracy can be improved especially for low-quality images. By integrating the state-of-the-art method ASTER into our framework, we can achieve superior results on several standard benchmark datasets. In the future, we will extend our framework to an end-to-end text spotting system. In this way, more semantic information can be utilized.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of three kinds of framework. "C" represents context information. The plain encoder-decoder framework gets incorrect results due to limited context representation. The attention-based encoder-decoder framework works better but still can not handle incomplete characters without global information. Our proposed encoder-decoder framework predicts the correct result with the help of global semantic information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Details of our SE-ASTER. It consists of four main modules, rectification module, encoder, semantic module, and decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Examples of low-quality images and recognition results in four methods. Red characters are the wrong results, and green ones are the correct.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of cosine similarity of the predicted semantic information from the image w.r.t the word embedding of the words from lexicons. Larger value means more similar semantics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Visualization of the recognition results on the two shrink datasets. Red: wrong results; Green: correct results. Performance comparison between different strategies. WES represents word embedding supervision. INIT represents initializing the state of the GRU in the decoder. ASTER-r represents the model re-trained by ourselves.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IC13-sr</cell><cell></cell><cell></cell><cell cols="2">IC15-sr</cell><cell>lote, note, lots, notes, notes</cell></row><row><cell></cell><cell>Images</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>xerr, verry, merri, merry, merry</cell></row><row><cell cols="2">Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ASTER</cell><cell>ier</cell><cell>for</cell><cell></cell><cell>irst</cell><cell>cooker</cell><cell>poon</cell><cell>echars</cell><cell>mest</cell><cell>xerr</cell></row><row><cell></cell><cell>ASTER+WES</cell><cell>herb</cell><cell cols="2">room</cell><cell>its</cell><cell>ooker</cell><cell>spoon</cell><cell>rechery</cell><cell>inest</cell><cell>verry</cell></row><row><cell></cell><cell>ASTER+INIT</cell><cell>here</cell><cell cols="2">look</cell><cell>first</cell><cell>looker</cell><cell>poon</cell><cell>keekers</cell><cell>inest</cell><cell>merri</cell></row><row><cell></cell><cell>SE-ASTER</cell><cell>here</cell><cell cols="2">room</cell><cell>first</cell><cell>cookery</cell><cell>spoon</cell><cell>kechers</cell><cell>finest</cell><cell>merry</cell></row><row><cell>Methods</cell><cell cols="4">WES INIT IC13 SVTP IC15</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASTER [45]</cell><cell></cell><cell>91.8</cell><cell>78.5</cell><cell>76.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASTER-r ASTER ASTER ASTER</cell><cell cols="4">90.9 90.8 ! 91.1 78.1 76.1 79.1 78.4 79.2 77.0 ! ! 92.8 81.4 80.0 !</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 3. Results on the shrink datasets, GAP indicates the decline between two datasets.The quantitative results are illustrated in Tab. 3. The performances of the ASTER baseline drop 19.5% and 12.8% on the IC13-sr dataset and the IC15-sr dataset respectively, which reveal that the ASTER baseline suffers a lot from the incomplete characters. However, with the supervision of</figDesc><table><row><cell>Input Images</cell><cell>SAR SE-SAR</cell><cell>ASTER SE-ASTER</cell><cell></cell><cell></cell></row><row><cell></cell><cell>baf</cell><cell>batf</cell><cell></cell><cell></cell></row><row><cell></cell><cell>bar</cell><cell>bar</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ale</cell><cell>ale</cell><cell></cell><cell></cell></row><row><cell></cell><cell>sale</cell><cell>sale</cell><cell></cell><cell></cell></row><row><cell></cell><cell>orchano</cell><cell>orghand</cell><cell></cell><cell></cell></row><row><cell></cell><cell>orchard</cell><cell>orchard</cell><cell></cell><cell></cell></row><row><cell></cell><cell>nex</cell><cell>me</cell><cell></cell><cell></cell></row><row><cell></cell><cell>mex</cell><cell>mex</cell><cell></cell><cell></cell></row><row><cell></cell><cell>martis</cell><cell>martia</cell><cell></cell><cell></cell></row><row><cell></cell><cell>martin</cell><cell>martin</cell><cell></cell><cell></cell></row><row><cell></cell><cell>xccessorize</cell><cell>recessorize</cell><cell></cell><cell></cell></row><row><cell></cell><cell>accessorize</cell><cell>accessorize</cell><cell></cell><cell></cell></row><row><cell></cell><cell>hilfiger</cell><cell>hilfgger</cell><cell></cell><cell></cell></row><row><cell></cell><cell>hilfger</cell><cell>hilfger</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.</cell></row><row><cell></cell><cell></cell><cell cols="3">Some examples are shown in Tab. 1.</cell></row><row><cell></cell><cell></cell><cell>Methods</cell><cell>IC13 IC13-sr</cell><cell>GAP</cell><cell>IC15 IC15-sr</cell><cell>GAP</cell></row><row><cell></cell><cell></cell><cell>ASTER</cell><cell>90.9 71.4</cell><cell>-19.5</cell><cell>78.4 65.6</cell><cell>-12.8</cell></row><row><cell></cell><cell></cell><cell>ASTER+WES</cell><cell>90.8 71.9</cell><cell>-18.9</cell><cell>77.0 62.8</cell><cell>-14.2</cell></row><row><cell></cell><cell></cell><cell>ASTER+INIT</cell><cell>91.1 74.6</cell><cell>-16.5</cell><cell>76.1 63.1</cell><cell>-13.0</cell></row><row><cell></cell><cell></cell><cell>SE-ASTER</cell><cell>92.8 77.4</cell><cell>-15.4</cell><cell>80.0 70.0</cell><cell>-10.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Recognition performance on SAR and SE-SAR.We conduct some experiments on IC13, IC15, SVT, and SVTP to show the effectiveness of the SE-SAR. The results are demonstrated in Tab. 4. Compared with the baseline, our SE-SAR improves 4.2%, 1.3% and 2.3% on IC15, SVT, and SVTP respectively. SE-SAR is only comparable with SAR in that low-quality images are scarce in IC13.</figDesc><table><row><cell>Methods</cell><cell cols="2">IC13 IC15 SVT SVTP</cell></row><row><cell cols="2">SAR [23] 91.0 69.2 84.5</cell><cell>76.4</cell></row><row><cell>SE-SAR</cell><cell>90.9 73.4 85.8</cell><cell>78.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>50 words for each image). In</figDesc><table><row><cell>Methods</cell><cell cols="6">IIIT5K SVT IC13 IC15 SVTP CUTE</cell></row><row><cell>Shi et al. [43]</cell><cell>81.2</cell><cell>82.7</cell><cell>89.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Shi et al. [44]</cell><cell>81.9</cell><cell>81.9</cell><cell>88.6</cell><cell>-</cell><cell>71.8</cell><cell>59.2</cell></row><row><cell>Lee et al. [22]</cell><cell>78.4</cell><cell>80.7</cell><cell>90.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Yang et al. [54]*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.8</cell><cell>69.3</cell></row><row><cell>Cheng et al. [7]*</cell><cell>87.4</cell><cell>85.9</cell><cell>93.3</cell><cell>70.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Cheng et al. [8]</cell><cell>87.0</cell><cell>82.8</cell><cell>-</cell><cell>68.2</cell><cell>73.0</cell><cell>76.8</cell></row><row><cell>Liu et al. [28]*</cell><cell>92.0</cell><cell>85.5</cell><cell>91.1</cell><cell>74.2</cell><cell>78.9</cell><cell>-</cell></row><row><cell>Bai et al. [2]*</cell><cell>88.3</cell><cell>87.5</cell><cell>94.4</cell><cell>73.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Liu et al. [30]*</cell><cell>87.0</cell><cell>-</cell><cell>92.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Liu et al. [29]</cell><cell>89.4</cell><cell>87.1</cell><cell>94.0</cell><cell>-</cell><cell>73.9</cell><cell>62.5</cell></row><row><cell>Liao et al. [25]*</cell><cell>91.9</cell><cell>86.4</cell><cell>91.5</cell><cell>-</cell><cell>-</cell><cell>79.9</cell></row><row><cell>Zhan et al. [57]</cell><cell>93.3</cell><cell>90.2</cell><cell>91.3</cell><cell>76.9</cell><cell>79.6</cell><cell>83.3</cell></row><row><cell>Xie et al. [52]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.9</cell><cell>70.1</cell><cell>82.6</cell></row><row><cell>Li et al. [23]</cell><cell>91.5</cell><cell>84.5</cell><cell>91.0</cell><cell>69.2</cell><cell>76.4</cell><cell>83.3</cell></row><row><cell>Luo et al. [32]</cell><cell>91.2</cell><cell>88.3</cell><cell>92.4</cell><cell>74.7</cell><cell>76.1</cell><cell>77.4</cell></row><row><cell>Yang et al. [53]*</cell><cell>94.4</cell><cell>88.9</cell><cell>93.9</cell><cell>78.7</cell><cell>80.8</cell><cell>87.5</cell></row><row><cell>ASTER [45]</cell><cell>93.4</cell><cell>89.5</cell><cell>91.8</cell><cell>76.1</cell><cell>78.5</cell><cell>79.5</cell></row><row><cell>ASTER baseline reproduced</cell><cell>93.5</cell><cell>87.2</cell><cell>90.9</cell><cell>78.4</cell><cell>79.1</cell><cell>82.3</cell></row><row><cell>SE-ASTER (Ours)</cell><cell>93.8</cell><cell>89.6</cell><cell>92.8</cell><cell>80.0</cell><cell>81.4</cell><cell>83.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://fasttext.cc/docs/en/crawl-vectors.html 2 https://commoncrawl.org/ 3 https://www.wikipedia.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is supported by the National Key R&amp;D Program of China (2017YFB1002400) and the Strategic Priority Research Program of Chinese Academy of Sciences (XDC02000000). In addition, we sincerely thank Mingkun Yang for his help.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Edit probability for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1508" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale mid-level representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2789" to="2802" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Principal warps: Thin-plate splines and the decomposition of deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="567" to="585" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Constrained relation network for character detection in scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PRICAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="137" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5076" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AON: Towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangliu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual attention models for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Suman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Valveny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="943" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Don&apos;t only feel read: Using scene text to understand advertisements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Suman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valveny</surname></persName>
		</author>
		<idno>abs/1806.08279</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Naila Murray, and Florent Perronin. LEWIS: Latent embeddings for word images and their semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Almazán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1242" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reading scene text in deep convolutional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3501" to="3508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurlPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detection and recognition of text embedded in online images via neural context models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulmoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk</forename><forename type="middle">I</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4103" to="4110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">ICDAR 2015 competition on robust reading. In ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez I Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Sergi Robles Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mas</surname></persName>
		</author>
		<title level="m">ICDAR 2013 robust reading competition. In ICDAR</title>
		<meeting><address><addrLine>David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word spotting and recognition using deep embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8610" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TextBoxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scene text recognition from two-dimensional perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengming</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8714" to="8721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Char-Net: A character-aware neural network for distorted scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7154" to="7162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Synthetically supervised feature learning for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Wassell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="435" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SqueezedText: A real-time scene text recognition by binary convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengbo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><forename type="middle">Ling</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7194" to="7201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">MORAN: A multi-object rectified attention network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PR</publisher>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurlPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Marçal Rusinol, and Dimosthenis Karatzas. Dynamic lexicon generation for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="395" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Using object information for spotting text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitala</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams Wai Kin</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="540" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Curved text detection in natural scene images with semi-and weakly-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xugong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="559" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahnakote</forename><surname>Trung Quy Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangxuan</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurlPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhar</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahankote</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Chee Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ESA</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual re-ranking with natural language understanding for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ASTER: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Accurate recognition of words in scenes without character segmentation using recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PR</publisher>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="397" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurlPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic and verbatim word spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Brun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICFHR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Aggregation cross-entropy for sequence recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zecheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoxiong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6538" to="6547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Symmetryconstrained rectification network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushuo</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaigui</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9147" to="9156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning to read irregular text with attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dafang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3280" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4042" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">CoRR preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">ESIR: End-to-end scene text recognition via iterative image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2059" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">FreeAnchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurlPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Deep neural network for semantic-based text recognition in images. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Betke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">EAST: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
