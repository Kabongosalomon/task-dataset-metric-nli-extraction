<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Sheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric I-Chao</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature warping is a core technique in optical flow estimation; however, the ambiguity caused by occluded areas during warping is a major problem that remains unsolved. In this paper, we propose an asymmetric occlusionaware feature matching module, which can learn a rough occlusion mask that filters useless (occluded) areas immediately after feature warping without any explicit supervision. The proposed module can be easily integrated into end-to-end network architectures and enjoys performance gains while introducing negligible computational cost. The learned occlusion mask can be further fed into a subsequent network cascade with dual feature pyramids with which we achieve state-of-the-art performance. At the time of submission, our method, called MaskFlownet, surpasses all published optical flow methods on the MPI Sintel, KITTI 2012 and 2015 benchmarks. Code is available at https: //github.com/microsoft/MaskFlownet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Optical flow estimation is a core problem in computer vision and a fundamental building block in many realworld applications <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>. Recent development towards fast, accurate optical flow estimation has witnessed great progress of learning-based methods using a principled network design -feature pyramid, warping, and cost volume -proposed by PWC-Net <ref type="bibr" target="#b31">[32]</ref> and LiteFlowNet <ref type="bibr" target="#b10">[11]</ref>, and used in many follow-up works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref>. Feature warping effectively resolves the long-range matching problem between the extracted feature maps for the subsequent cost volume computation. However, we observe that a major problem of the warping operation is that it introduces unreliable information in the presence of occlusions. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the warped image as well as the warped feature map can be even "doubled" at the occluded areas (also called the ghosting effect). It remains unclear that <ref type="bibr">Figure 2</ref>. The overall architecture of MaskFlownet. MaskFlownet consists of two stages -the first end-to-end network named MaskFlownet-S (left), and the second cascaded network (right) that aims to perform refinements using dual pyramids. Dashed lines across pyramids represent shared weights. MaskFlownet generally utilizes the proposed AsymOFMM whenever possible. The learnable occlusion mask is coarse-to-fine predicted and fed into the new occlusion-aware feature pyramid. See §4 for the network details.</p><p>whether the source image would be mismatched to such areas yet raises a natural question: are they really distinguishable without being supervised of occlusions?</p><p>We answer this question positively by showing that the network can indeed learn to mask such areas without any explicit supervision. Rather than enforcing the network to distinguish useful parts from those confusing information, we propose to apply a multiplicative learnable occlusion mask immediately on the warped features (see <ref type="figure" target="#fig_0">Fig. 1</ref>). We can see that there is a clear distinction between the black and white areas in the learned occlusion mask, indicating that there exists solid gradient propagation. The masked image (features) has much cleaner semantics, which could potentially facilitate the subsequent cost volume processing.</p><p>The masking process interprets how those areas can be distinguished in a clear way. While previous works commonly believe that the feature symmetricity is crucial for the cost volume processing, we in contrast demonstrate that the network further benefits from a simple asymmetric design despite the explicit masking. The combined asymmetric occlusion-aware feature matching module (AsymOFMM) can be easily integrated into end-to-end network architectures and achieves significant performance gains.</p><p>We demonstrate how the proposed AsymOFMM would contribute to the overall performance using a two-stage architecture named MaskFlownet (see <ref type="figure">Fig. 2</ref>). MaskFlownet is trained on standard optical flow datasets (not using the occlusion ground truth), and predicts the optical flow together with a rough occlusion mask in a single forward pass. At the time of submission, MaskFlownet surpasses all published optical flow methods on the MPI Sintel (on both clean and final pass), KITTI 2012 and 2015 benchmarks while using only two-frame inputs with no additional assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Optical Flow Estimation. Conventional approaches formulate optical flow estimation as an energy minimization problem based on brightness constancy and spatial smoothness since <ref type="bibr" target="#b9">[10]</ref> with many follow-up improvements <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref>. Estimating optical flow in a coarse-to-fine manner achieves better performance since it better solves large displacements <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>. Later works propose to use CNN extractors for feature matching <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39]</ref>. However, their high accuracy is at the cost of huge computation, rendering those kinds of methods impractical in real-time settings.</p><p>An important breakthrough of deep learning techniques in optical flow estimation is made by FlowNet <ref type="bibr" target="#b7">[8]</ref>, which proposes to train end-to-end CNNs on a synthetic dataset and first achieves a promising performance. Although they only investigate two types of simple CNNs (FlowNetS and FlowNetC), the correlation layer in FlowNetC turns out to be a key component in the modern architectures. Flownet2 <ref type="bibr" target="#b13">[14]</ref> explores a better training schedule and makes significant improvements by stacking multiple CNNs which are stage-wise trained after fixing the previous ones.</p><p>SpyNet <ref type="bibr" target="#b26">[27]</ref> explores a light-weight network architecture using feature pyramid and warping, but the learned features are not correlated so it can only achieve a comparable performance to FlowNetS. PWC-Net <ref type="bibr" target="#b31">[32]</ref> and Lite-FlowNet <ref type="bibr" target="#b10">[11]</ref> present a compact design using feature pyramid, warping, and cost volume, and achieve remarkable performance over most conventional methods while preserving high efficiency, and they further make some slight improvements in the later versions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>. VCN <ref type="bibr" target="#b39">[40]</ref> recently exploits the high-dimensional invariance during cost volume processing and achieves state-of-the-art performance.</p><p>Note that this paper focuses on the feature matching process prior to the correlation layer, which is independent of the improvement made by VCN. To our knowledge, none of those works realizes that an asymmetric design of the feature matching process can achieve better performance.</p><p>Occlusions and Optical Flow. Occlusions and optical flow are closely related. Optical flow methods such as FlowNet and FlowNet2 can be easily extended to joint optical flow and occlusion estimation with slight modifications as proposed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>. IRR-PWC <ref type="bibr" target="#b12">[13]</ref> presents an iterative residual refinement approach with joint occlusion estimation using bilateral refinement. However, all those methods can only explicitly learn from the ground-truth occlusions, which require additional efforts on the training labels that limit their applicability <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Unsupervised or self-supervised learning of optical flow is another promising direction. Handling occlusions is clearly a vital aspect in such setting, since the brightness error does not make sense at occluded pixels. Some initial works show the feasibility of the unsupervised learning guided by the photometric loss <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>. Later works realize that the occluded pixels should be excluded from the loss computation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref>. Occlusions also facilitate multiframe estimation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>. However, the only occlusion estimation approach used by those kinds of methods is the forward-backward consistency <ref type="bibr" target="#b32">[33]</ref> that requires bidirectional flow estimation, which limits its flexibility and could lead to noisy predictions. We would like to remark that our promising approach can jointly estimate occlusions without any explicit supervision in a single forward pass, which we expect can be helpful to future unsupervised or self-supervised learning methods.</p><p>Occlusion-Aware Techniques in Other Applications. Occlusions commonly exist in object detection and might affect the performance of standard approaches in some scenarios, e.g., crowded pedestrian detection <ref type="bibr" target="#b40">[41]</ref>. Recent works propose to explicitly learn a spatial attention mask that highlights the foreground area for occluded pedestrian detection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42]</ref>, which requires additional supervising information. Occlusions in face recognition is also a major problem which can be addressed by the guided mask learning <ref type="bibr" target="#b29">[30]</ref>. Our work is also related to the attention mechanism in computer vison <ref type="bibr" target="#b33">[34]</ref>, which addresses a different problem of capturing pixel-wise long-range dependencies. None of those works realizes a global attention mask can be learned to filter occluded areas with no explicit supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Occlusion-Aware Feature Matching</head><p>Given an image pair I 1 (the source image) and I 2 (the target image), the task is to estimate the flow displace- ment φ, representing the correspondence between I 1 (x) and I 2 (x + φ(x)). Image warping is the process of constructing</p><formula xml:id="formula_0">(φ • I 2 )(x) I 2 (x + φ(x))<label>(1)</label></formula><p>using the estimated displacement φ, and ideally we have</p><formula xml:id="formula_1">I 1 (x) ≈ (φ • I 2 )(x) at all non-occluded pixels x.</formula><p>This operation is differentiable w.r.t. both inputs because nonintegral points can be dealt with bilinear interpolation. Feature warping is similarly defined by replacing I 2 in Eq. (1) with the extracted feature map. Feature warping followed by a correlation layer is the common practice to compute the cost volume for non-local feature matching in recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. The feature extractor of an image is a pyramid of convolutional layers, which are proposed to be symmetric for I 1 and I 2 in the sense that they share the same convolution kernels. The cost volume at pyramid level l can be formulated as</p><formula xml:id="formula_2">c(F l (I 1 ), φ • F l (I 2 )),<label>(2)</label></formula><p>where F l denotes the shared feature extractor for level l, and φ denotes the flow displacement predicted by the previous level. c represents the correlation layer that computes the element-wise dot product between the two feature maps within a maximum displacement. <ref type="figure" target="#fig_6">Fig. 4</ref>(a) illustrates this process, which we call a feature matching module (FMM). We observe that a major consequence caused by the warping operation is the ambiguity in the presence of occlusions. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates a simplified example, where the foreground object has a large movement while the background stays still. During warping, a copy of the foreground object is revealed in the occluded background area. Such areas in the warped image (features) are useless and cause confusion to the subsequent flow inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corr. Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost Volume Feature Matching Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_3">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_4">F l (I1) F l (I2) φ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_5">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_6">F l (I1) F l (I2)</formula><p>(a) Feature matching module (FMM) as used in PWC-Net <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_7">F l (I1) F l (I2) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_8">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407 Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_9">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corr. Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost Volume Occlusion-Aware</head><formula xml:id="formula_10">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_11">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren  Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_12">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_13">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_14">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_15">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_16">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_17">F l (I1) F l (I2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deform.</head><p>Conv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corr. Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost Volume Asymmetric Occlusion-Aware Feature Matching Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren  Occlusion-Aware Feature Matching Module (OFMM).</p><p>The occlusion-aware feature matching module incorporates a learnable occlusion mask that filters useless information immediately after feature warping (see <ref type="figure" target="#fig_6">Fig. 4(b)</ref>). The warped feature tensor of shape (B, C, H, W ) is elementwise multiplied by the (soft) learnable occlusion mask θ of shape (B, 1, H, W ) with broadcasting and then added with an additional feature tensor µ of shape (B, C, H, W ). The resulting cost volume at level l is formulated as</p><formula xml:id="formula_18">c(F l (I 1 ), (φ • F l (I 2 )) ⊗ θ ⊕ µ).<label>(3)</label></formula><p>θ is assumed to be within the range of [0, 1]. µ acts as a trade-off term that facilitates the learning of occlusions, as it provides extra information at the masked areas. OFMM learns to mask the occluded areas simply because it realizes they are useless comparing to the trade-off term, even if there is no explicit supervision to the occlusions at all. Although the OFMM itself might not contribute significantly to the performance, it can learn a rough occlusion mask at negligible cost, which can be further fed into the new occlusion-aware feature pyramid (see §4).</p><p>Asymmetric Occlusion-Aware Feature Matching Module (AsymOFMM). We suggest that an asymmetric design of the feature extraction layers consistently gains the performance. Intuitively, the warping operation induces ambiguity to the occluded areas and breaks the symmetricity of the feature matching process, so an asymmetric design might be helpful to conquer this divergence.</p><p>Based on the OFMM, we introduce an extra convolutional layer prior to the warping operation, which is asymmetrically drawn on the feature extraction process of only I 2 . In practice, we replace the extra convolutional layer and the warping operation by a deformable convolution. In the general setting of deformable convolutional networks <ref type="bibr" target="#b6">[7]</ref>, different locations in each of the convolution kernels are associated with different offsets, but here we introduce a specialized setting where each convolution kernel is warped in parallel according to the corresponding flow displacement at center. The deformable convolution slightly differs from the initial design since it reverts the order of convolution and (bilinear) interpolation, which is proved to be better in the experiments. As illustrated in <ref type="figure" target="#fig_6">Fig. 4(c)</ref>, the resulting cost volume can be formulated as</p><formula xml:id="formula_19">c(F l (I 1 ), D l (F l (I 2 ), φ) ⊗ θ ⊕ µ),<label>(4)</label></formula><p>where D l (·, φ) denotes the deformable convolution layer at level l using the displacement φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MaskFlownet</head><p>The overall architecture of the proposed MaskFlownet is illustrated in <ref type="figure">Fig. 2</ref>. MaskFlownet consists of two cascaded subnetworks. The first stage, named MaskFlownet-S, generally inherits the network architecture from PWC-Net <ref type="bibr" target="#b31">[32]</ref>, but replaces the feature matching modules (FMMs) by the proposed AsymOFMMs.</p><p>MaskFlownet-S first generates a 6-level shared feature pyramid as PWC-Net, and then makes predictions from level 6 to 2 in a coarse-to-fine manner. The final predictions at level 2 are 4-time upsampled to level 0. <ref type="figure">Fig. 5</ref> illustrates the network details at each level l (modifications needed at level 6 and level 2). The previous level is responsible for providing φ l+1 , θ l+1 , and µ l+1 , which are then upsampled and fed into the AsymOFMM. φ l+1 , θ l+1 are upsampled using bilinear interpolation; µ l+1 is upsampled</p><formula xml:id="formula_20">AsymOFMM Convolutional Layers θ l+1 μ l+1 φ l+1 φ l+1 θ l+1 μ l+1 conv</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren Due on Nov, XX 2019</p><p>Yue Dong YaoClass 70 2017011407</p><formula xml:id="formula_21">F l (I1) F l (I2) φ l+1 θ l+1 µ l+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulas:</head><p>Instructed by Zhuren</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due on Nov, XX 2019</head><p>Yue Dong YaoClass 70 2017011407 <ref type="figure">Figure 5</ref>. Network connections at each level. This figure adopts to the first stage (MaskFlownet-S). The learnable occlusion mask is generated through the sigmoid activation from the previous level, and then upsampled and fed into the AsymOFMM. using a deconvolutional layer (of 16 channels), followed by a convolutional layer to match the channels of the pyramid feature extractor. All convolutional layers have a kernel size of 3; all deconvolutional layers have a kernel size of 4. The feature matching module at level 6 is simply a correlation layer since there is no initial flow for warping. The maximum displacement of the correlation layers is kept to be 4. The resulting cost volume is concatenated with F l (I 1 ), the upsampled displacement, and the upsampled features, and then passed through 5 densely connected convolutional layers as PWC-Net. The final layer predicts the flow displacement φ l with residual link from the previous flow estimation, the occlusion mask θ l after the sigmoid activation, and the features µ l passing to the next level. Level 2 only predicts the flow displacement, ended with the context network (as PWC-Net) that produces the final flow prediction.</p><formula xml:id="formula_22">F l (I1) F l (I2) φ l+1 θ l+1 µ l+1</formula><p>Occlusion-Aware Feature Pyramid. The learned occlusion mask, concatenated with the warped image, is fed into the occlusion-aware feature pyramid for the subsequent flow refinement. The occlusion mask is subtracted by 0.5 before concatenation; a zero mask is concatenated with I 1 for symmetricity. The occlusion-aware pyramid extracts features for the concatenated images (both with 4 channels) with shared convolutional layers as usual. We suggest that the occlusion mask facilitates the feature representation of the warped image, given the vast existence of occluded areas during warping.</p><p>Cascaded Flow Inference with Dual Pyramids. We propose to cascade the network by utilizing dual feature pyramids. The occlusion-aware feature pyramid provides abundant information about the warped image from the previous flow estimation for refinement, but it cannot feedback to the new coarse-to-fine flow predictions. Hence, we suggest that the network can still gain complementary information from the original feature pyramid.</p><p>The network architecture of this stage is similar to the former stage except some modifications and the incorporation of the new occlusion-aware feature pyramid (see <ref type="figure">Fig. 2</ref>). The original feature pyramid is directly placed into this stage using the same parameters. The maximum displacement of all correlation layers in this stage is set to 2 since we expect it to perform mainly refinements. Correlation layers are used as the feature matching modules for the occlusion-aware feature pyramid since there is no need for feature warping. At each level, the resulting cost volumes from dual feature pyramids are concatenated together with the other terms including an extra flow predicted from the previous stage at the current level. As suggested in FlowNet2 <ref type="bibr" target="#b13">[14]</ref>, we fix all the parameters in the first stage (MaskFlownet-S) when training the whole MaskFlownet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation</head><p>We implement a Python-trainable code framework using MXNet <ref type="bibr" target="#b5">[6]</ref>. Mostly, we follow the training configurations as suggested in PWC-Net+ <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> and FlowNet2 <ref type="bibr" target="#b13">[14]</ref>. More details can be found in the supplementary material.</p><p>Training Schedule. MaskFlownet-S is first trained on FlyingChairs <ref type="bibr" target="#b7">[8]</ref> and then tuned on FlyingThings3D <ref type="bibr" target="#b18">[19]</ref> following the same schedule as PWC-Net <ref type="bibr" target="#b31">[32]</ref>. When finetuning on Sintel, we use the same batch configuration (2 from Sintel, 1 from KITTI 2015, and 1 from HD1K) and a longer training schedule (1000k iterations) referring to the cyclic learning rate proposed by PWC-Net+ <ref type="bibr" target="#b30">[31]</ref>. When training the whole MaskFlownet, we fix all the parameters in MaskFlownet-S as suggested in FlowNet2 <ref type="bibr" target="#b13">[14]</ref> and follow again the same schedule except that it is shorter on FlyingChairs (800k iterations). For submission to KITTI, we fine-tune our model on the combination of KITTI 2012 and 2015 datasets based on the tuned checkpoint on Sintel, while the input images are resized to 1280 × 576 (before augmentation and cropping) since the decreased aspect ratio better balances the vertical and horizontal displacement.</p><p>Data Augmentation. We implement geometric and chromatic augmentations referring to the implementation of FlowNet <ref type="bibr" target="#b7">[8]</ref> and IRR-PWC <ref type="bibr" target="#b12">[13]</ref>. We suppress the degree of augmentations when fine-tuning on KITTI as suggested. For sparse ground-truth flow in KITTI, the augmented flow is weighted averaged based on the interpolated valid mask.</p><p>Training Loss. We follow the multi-scale end-point error (EPE) loss when training on FlyingChairs and FlyingTh-ings3D, and its robust version on Sintel and KITTI, using the same parameters as suggested in PWC-Net+ <ref type="bibr" target="#b30">[31]</ref>. Weight decay is disabled since we find it of little help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Time</head><p>Sintel  <ref type="bibr" target="#b31">[32]</ref>; our time is measured on an NVIDIA TITAN Xp GPU, which is comparable to the NVIDIA TITAN X used by <ref type="bibr" target="#b31">[32]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Overlay Ground Truth PWC-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Main Results</head><p>MaskFlownet outperforms all published optical flow methods on the MPI Sintel <ref type="bibr" target="#b4">[5]</ref>, KITTI 2012 <ref type="bibr" target="#b8">[9]</ref> and KITTI 2015 <ref type="bibr" target="#b22">[23]</ref> benchmarks as presented in <ref type="table">Table 1</ref>, while the end-to-end MaskFlownet-S achieves a satisfactory result as well. Values listed in the training sets only consider the models that have never seen them and hence comparable; note that training on the synthetic datasets is of limited generalizability. <ref type="figure" target="#fig_7">Fig. 6</ref> visualizes the predicted flows as a comparison. All samples are chosen from the Sintel training set (final pass). We can observe that MaskFlownet in general better separates moving objects from the background, and cascading significantly weakens the checkerboard effect while preserving clear object boundaries. <ref type="figure">Fig. 7</ref> qualitatively demonstrates that the learnable occlusion mask matches the inverse ground-truth occlusion map fairly well, even if it is learned without any explicit supervision.  <ref type="figure">Figure 9</ref>. The trade-off term facilitates the learning of occlusions. Without the trade-off term, the learnable occlusion mask fails to achieve a clear estimation; if there is an additive shortcut that skips over warping, only motion boundaries are learned. With the trade-off term, large occlusions are successfully learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warped Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverse Ground-Truth Occlusion Map Learnable Occlusion Mask</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>Feature Matching Module.    OFMM achieves relative gains compared with the original FMM, the proposed AsymOFMM significantly outperforms the symmetric variants.</p><p>Asymmetricity. We demonstrate the effectiveness of the asymmetricity by comparing AsymOFMM (i.e., "OFMM + deform-conv", MaskFlownet-S) with the raw OFMM, "OFMM + sym-conv" (adding an extra convolutional layer at each feature pyramid level), and "OFMM + asym-conv" (adding an asymmetric convolutional layer prior to the warping operation at each level). As shown in <ref type="table" target="#tab_3">Table 3</ref>, increasing the depth of convolutional layers has a limited impact in the symmetric setting, while a simple asymmetric design achieves consistently better performance. It also indicates that our deformable convolution can be a better choice over the "asym-conv" version. Although it is commonly believed that matched patterns should be embedded into similar feature vectors, we suggest that the network can</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Occ. Mask</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features w/ Mask</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warped Image</head><p>Features w/o Mask <ref type="figure" target="#fig_0">Figure 10</ref>. Occlusion mask facilitates feature extraction in the occlusion-aware feature pyramid. Comparing the learned features with and without (i.e., replaced by constant) mask, we can see that the occlusion mask draws a significant effect on smoothing the feature map at the occluded areas.</p><p>really benefit from learning very different feature representations as visualized in <ref type="figure">Fig. 8</ref>.</p><p>Learnable Occlusion Mask. <ref type="table" target="#tab_4">Table 4</ref> presents the results if the mask or the trade-off term is disabled. Interestingly, only the two factors combined lead to performance gains. A possible explanation is that the occlusion mask helps if and only if it is learned properly, where the trade-off term plays a vital role (see <ref type="figure">Fig. 9</ref>).</p><p>Network Cascading. <ref type="table" target="#tab_5">Table 5</ref> indicates that MaskFlownet consistently benefits from dual feature pyramids over a single new pyramid, while the concatenated occlusion mask gains the performance on the Sintel final pass. We hypothesize that the occlusion-aware feature pyramid mainly contributes to the harder final pass since the occluded areas can be more easily mismatched, but it might be overfitted on the easier clean pass. We demonstrate how the learned occlusion mask could affect the extracted feature map in <ref type="figure" target="#fig_0">Fig. 10</ref>. The occluded areas are smoothened during feature extraction and hence become more distinguishable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose the AsymOFMM, which incorporates a learnable occlusion mask that filters occluded areas immediately after feature warping without any explicit supervision. AsymOFMM can be easily integrated into an endto-end network while introducing negligible computational cost. We further propose a two-stage network -Mask-Flownet -which exploits dual pyramids and achieves superior performance on all modern optical flow benchmarks. Our approach opens a promisingly new perspective on dealing with occlusions for both supervised and unsupervised optical flow estimation, and we also expect it as an initiative and effective component in many other applications.  Data Augmentation. We implement geometric and chromatic augmentations referring to the implementation of FlowNet <ref type="bibr" target="#b7">[8]</ref> and IRR-PWC <ref type="bibr" target="#b12">[13]</ref>. Details about the sampling ranges for each training stage are provided in <ref type="table">Table 6</ref> (for geometric augmentations) and <ref type="table">Table 7</ref> (for chromatic augmentations). We use the same augmentations on Fly-ingThings3D as FlyingChairs. We finally apply a random crop (within valid areas) using a size of 448 × 320 on Fly-ingChairs, 768×384 on FlyingThings3D, 768×320 on Sintel, and 896 × 320 on KITTI. To avoid out-of-bound areas after cropping, we compute the minimum degree of zoom that forces the existence of a valid crop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. More Visualizations</head><p>More visualizations of the learnable occlusion mask and the flow predictions are presented in <ref type="figure" target="#fig_0">Fig. 12</ref> and <ref type="figure" target="#fig_0">Fig. 13</ref>. Note that the learned occlusion masks are relatively vague at the image boundary, since the network cannot learn to mask out-of-bound features that are already zeros. We expect that the estimation results can be further improved if out-of-bound areas are manually regarded as occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Screenshots on Benchmarks</head><p>At the time of submission, MaskFlownet ranks first on the MPI Sintel benchmark on both clean pass (see <ref type="figure" target="#fig_0">Fig. 14)</ref> and final pass (see <ref type="figure" target="#fig_0">Fig. 15</ref>). Note that the top entry (Scope-Flow) at the time of screenshot (Nov. 23th, 2019) on the final pass is a new anonymous submission, with a relatively poor performance on the clean pass. Remarkably, Mask-Flownet outperforms the previous top entry on the clean pass (MR-Flow <ref type="bibr" target="#b37">[38]</ref>) that uses the rigidity assumption while being very slow, as well as the previous top entry on the final pass (SelFlow <ref type="bibr" target="#b17">[18]</ref>) that uses multi-frame inputs.</p><p>On the KITTI 2012 and 2015 benchmarks, MaskFlownet surpasses all optical flow methods (excluding the anonymous entries) at the time of submission (see <ref type="figure" target="#fig_0">Fig. 16</ref> and <ref type="figure" target="#fig_0">Fig. 17</ref>). Note that the top 3 entries on the KITTI 2015 benchmark are scene flow methods that use stereo images and thus not comparable.  Clean Clean <ref type="figure" target="#fig_0">Figure 14</ref>. Screenshot on the MPI Sintel clean pass (printed as PDF). Final Final <ref type="figure" target="#fig_0">Figure 15</ref>. Screenshot on MPI Sintel final pass (printed as PDF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warped Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverse Ground-Truth Occlusion Map Learnable Occlusion Mask</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Motivation of the learnable occlusion mask. (a) Image warping induces ambiguity in the occluded areas (see the doubled hands). (b) The same problem exists in the feature warping process. Such areas can be masked without any explicit supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>A simplified case of occlusions. The top image is warped to the bottom image according to the illustrated flow. The foreground object (shaded area) generates a large displacement (tracked by the red lines) while the background stays still (tracked by the blue lines). However, a copy of the foreground object still stays at the occluded area after warping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Due on Nov, XX 2019 Dong YaoClass 70 2017011407 Formulas: Instructed by Zhuren Due on Nov, XX 2019 Dong YaoClass 70 2017011407 (b) Occlusion-aware feature matching module (OFMM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Instructed by ZhurenDue on Nov, XX 2019 Dong YaoClass 70 2017011407 (c) Asymmetric occlusion-aware feature matching module (AsymOFMM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Feature matching modules. The figures illustrate the proposed AsymOFMM and OFMM in comparison with the FMM. FMM warps the target feature maps with the flow displacement φ. OFMM introduces the multiplicative learnable occlusion mask θ followed by the additive trade-off term µ. AsymOFMM further replaces the warping operation by a deformable convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparison among PWC-Net [32], MaskFlownet-S, and MaskFlownet. Highlights for each row: (a) weakening the checkerboard effect; (b) separating background from two fighting figures; (c) preserving the weakly connected head of the moving figure; (d) maintaining a fluent flow for the background at left-bottom; (e) preserving boundary details of the flying creature and buildings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Learnable occlusion mask. MaskFlownet can jointly learn a rough occlusion mask without any explicit supervision. Asymmetricity in the learned feature maps. The source features and the target features are level-2 feature maps prior to the correlation layer in the AsymOFMM. This figure presents the input image overlay, image 1 features (source features), the warped image 2 features after masking (masked features) and after trade-off (target features). Comparing the source features with the target features, we can see that the AsymOFMM enables the network to learn very different feature representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>A shorter schedule for the second stage on FlyingChairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Learning rate schedules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>More visualizations of the learnable occlusion mask. All samples are chosen from the Sintel training set (final pass). The learnable occlusion masks are expected to (roughly) match the inverse ground-truth occlusion maps, even if they are learned without any explicit supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 Table 2 .</head><label>22</label><figDesc>Feature matching module.</figDesc><table><row><cell>presents the results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Asymmetricity and deformable convolution.</figDesc><table><row><cell></cell><cell cols="3">Trained on Chairs</cell></row><row><cell>Module (AsymOFMM)</cell><cell>Chairs</cell><cell cols="2">Sintel (train)</cell></row><row><cell></cell><cell>test</cell><cell>clean</cell><cell>final</cell></row><row><cell>w/o mask w/o trade-off</cell><cell>1.58</cell><cell>3.08</cell><cell>4.29</cell></row><row><cell>w/ mask w/o trade-off</cell><cell>1.60</cell><cell>3.06</cell><cell>4.32</cell></row><row><cell>w/o mask w/ trade-off</cell><cell>1.58</cell><cell>2.97</cell><cell>4.30</cell></row><row><cell>(w/ mask w/ trade-off)</cell><cell>1.56</cell><cell>2.88</cell><cell>4.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Learnable occlusion mask and the trade-off term.</figDesc><table><row><cell></cell><cell cols="2">Tuned on Sintel</cell></row><row><cell>Network</cell><cell cols="2">Sintel (val)</cell></row><row><cell></cell><cell>clean</cell><cell>final</cell></row><row><cell>MaskFlownet-S</cell><cell>2.70</cell><cell>4.07</cell></row><row><cell>+ single pyramid w/o mask</cell><cell>2.53</cell><cell>3.90</cell></row><row><cell>+ single pyramid w/ mask</cell><cell>2.55</cell><cell>3.88</cell></row><row><cell>+ dual pyramids w/o mask</cell><cell>2.52</cell><cell>3.85</cell></row><row><cell>+ dual pyramids w/ mask</cell><cell>2.52</cell><cell>3.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Network cascading with dual pyramids.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Geometric augmentations. Chromatic augmentations.</figDesc><table><row><cell>Geometric Aug.</cell><cell>Chairs</cell><cell>Sintel</cell><cell>KITTI</cell></row><row><cell>Horizontal Flip</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>Squeeze</cell><cell>0.9</cell><cell>0.9</cell><cell>0.95</cell></row><row><cell>Translation</cell><cell>0.1</cell><cell>0.1</cell><cell>0.05</cell></row><row><cell>Rel. Translation</cell><cell>0.025</cell><cell>0.025</cell><cell>0.0125</cell></row><row><cell>Rotation</cell><cell cols="2">17°17°5°R</cell><cell></cell></row><row><cell>el. Rotation</cell><cell cols="2">4.25°4.25°1.25°Z</cell><cell></cell></row><row><cell>oom</cell><cell cols="3">[0.9, 2.0] [0.9, 1.5] [0.95, 1.25]</cell></row><row><cell>Rel. Zoom</cell><cell>0.96</cell><cell>0.96</cell><cell>0.98</cell></row><row><cell>Chromatic Aug.</cell><cell>Chairs</cell><cell>Sintel</cell><cell>KITTI</cell></row><row><cell>Contrast</cell><cell cols="3">[−0.4, 0.8] [−0.4, 0.8] [−0.2, 0.4]</cell></row><row><cell>Brightness</cell><cell>0.1</cell><cell>0.1</cell><cell>0.05</cell></row><row><cell>Channel</cell><cell>[0.8, 1.4]</cell><cell>[0.8, 1.4]</cell><cell>[0.9, 1.2]</cell></row><row><cell>Saturation</cell><cell>0.5</cell><cell>0.5</cell><cell>0.25</cell></row><row><cell>Hue</cell><cell>0.5</cell><cell>0.5</cell><cell>0.1</cell></row><row><cell>Noise</cell><cell>0.04</cell><cell>0</cell><cell>0.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Figure 13. More visualizations for qualitative comparison among PWC-Net<ref type="bibr" target="#b31">[32]</ref>, MaskFlownet-S, and MaskFlownet. All samples are chosen from the Sintel training set (final pass). We replicate PWC-Net using the PyTorch reimplementation<ref type="bibr" target="#b24">[25]</ref> that provides a pretrained model of the "PWC-Net ROB" version<ref type="bibr" target="#b30">[31]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EPE=2.33</cell><cell>EPE=1.21</cell><cell>EPE=1.13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">About About</cell><cell cols="2">Downloads Downloads</cell><cell>Results Results</cell><cell>FAQ FAQ</cell><cell>Contact Contact</cell><cell>Signup Signup</cell><cell>Login Login</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EPE=29.88</cell><cell>EPE=19.18</cell><cell>EPE=18.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EPE=14.16</cell><cell>EPE=11.97</cell><cell>EPE=11.87</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EPE=1.15</cell><cell>EPE=1.03</cell><cell>EPE=0.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EPE=0.84</cell><cell>EPE=0.78</cell><cell>EPE=0.68</cell></row><row><cell cols="2">Final Final</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EPE=25.24</cell><cell>EPE=17.64</cell><cell>EPE=16.48</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">[1]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results Results</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EPE=1.24</cell><cell>EPE=1.26</cell><cell>EPE=1.11</cell></row><row><cell cols="6">MaskFlownet</cell><cell cols="3">[2]</cell><cell>2.521</cell><cell cols="2">0.989</cell><cell>15.032</cell><cell>2.742</cell><cell>0.908</cell><cell>0.291</cell><cell>0.361</cell><cell>1.285</cell><cell>16.261</cell><cell>Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results Results</cell></row><row><cell cols="3">MR-Flow</cell><cell></cell><cell>[3]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.527</cell><cell cols="2">0.954</cell><cell>15.365</cell><cell>2.866</cell><cell>0.710</cell><cell>0.420</cell><cell>0.446</cell><cell>1.715</cell><cell>14.826</cell><cell>Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EPE=0.28</cell><cell>EPE=0.26</cell><cell>EPE=0.21 Results Results</cell></row><row><cell cols="7">ProFlow_ROB</cell><cell cols="2">[4]</cell><cell>2.709</cell><cell cols="2">1.013</cell><cell>16.549</cell><cell>2.843</cell><cell>0.723</cell><cell>0.518</cell><cell>0.485</cell><cell>1.586</cell><cell>16.470</cell><cell>Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results Results</cell></row><row><cell cols="8">MaskFlownet-S</cell><cell>[5]</cell><cell>2.771</cell><cell cols="2">1.077</cell><cell>16.608</cell><cell>2.901</cell><cell>0.996</cell><cell>0.342</cell><cell>0.419</cell><cell>1.404</cell><cell>17.777</cell><cell>Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EPE=0.13</cell><cell>EPE=0.10</cell><cell>EPE=0.06 Results Results</cell></row><row><cell>VCN</cell><cell>[6]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.808</cell><cell cols="2">1.108</cell><cell>16.682</cell><cell>3.267</cell><cell>0.867</cell><cell>0.418</cell><cell>0.646</cell><cell>1.669</cell><cell>16.302</cell><cell>Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results Results</cell></row><row><cell cols="3">ProFlow</cell><cell cols="2">[7]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.818</cell><cell cols="2">1.027</cell><cell>EPE=0.14 17.428</cell><cell>2.892</cell><cell>0.751</cell><cell>EPE=0.12 0.496 0.469</cell><cell>1.626</cell><cell>17.369</cell><cell>Results Results EPE=0.06 Visualize Visualize</cell></row><row><cell cols="2">SfM-PM</cell><cell cols="3">[8]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.910</cell><cell cols="2">1.016</cell><cell>18.357</cell><cell>2.797</cell><cell>0.756</cell><cell>0.479</cell><cell>0.559</cell><cell>1.732</cell><cell>17.431</cell><cell>Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results Results</cell></row><row><cell cols="6">FlowFields++</cell><cell cols="3">[9]</cell><cell>2.943</cell><cell cols="2">0.850</cell><cell>EPE=9.41 20.027</cell><cell>2.550</cell><cell>0.603</cell><cell>EPE=7.62 0.403 0.560</cell><cell>1.859</cell><cell>17.401</cell><cell>EPE=6.75 Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results Results</cell></row><row><cell cols="6">LiteFlowNet3</cell><cell cols="3">[10]</cell><cell>2.994</cell><cell cols="2">1.148</cell><cell>18.077</cell><cell>3.000</cell><cell>0.985</cell><cell>0.498</cell><cell>0.559</cell><cell>1.670</cell><cell>18.302</cell><cell>Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results Results</cell></row><row><cell cols="5">FlowFields+</cell><cell cols="3">[11]</cell><cell></cell><cell>3.102</cell><cell cols="2">0.820</cell><cell>EPE=11.64 21.718</cell><cell>2.340</cell><cell>0.616</cell><cell>EPE=6.44 0.373 0.593</cell><cell>1.865</cell><cell>18.549</cell><cell>EPE=5.39 Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results Results</cell></row><row><cell cols="4">DIP-Flow</cell><cell cols="2">[12]</cell><cell></cell><cell></cell><cell></cell><cell>3.103</cell><cell cols="2">0.881</cell><cell>21.227</cell><cell>2.574</cell><cell>0.681</cell><cell>0.419</cell><cell>0.548</cell><cell>1.801</cell><cell>18.979</cell><cell>Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results Results</cell></row><row><cell>Image Overlay PST</cell><cell>[13]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Ground Truth 3.110 0.942</cell><cell>20.809</cell><cell>PWC-Net 2.759</cell><cell>0.664</cell><cell>0.378</cell><cell>MaskFlownet-S 0.635 2.069</cell><cell>17.919</cell><cell>MaskFlownet Visualize Visualize</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Results Results</cell></row><row><cell></cell><cell cols="2">[14]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">ambush 2, ambush 6, bamboo 2, cave 4, market 6, temple 2.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. More Implementation Details</head><p>Training Schedule. When fine-tuning on Sintel, we use a longer schedule (see <ref type="figure">Fig. 11</ref>(a)) referring to the cyclic learning rate proposed by PWC-Net+ <ref type="bibr" target="#b30">[31]</ref>. When training the second stage, we follow again the same schedule as the first stage for all datasets except that it is shorter on FlyingChairs (see <ref type="figure">Fig. 11(b)</ref>). For submission to the test set, we train on the whole training set and reduce randomness by averaging 3 independent runs due to the huge variance. <ref type="bibr">sintel</ref> background interpolation as explained in the corresponding header file in the development kit. For each method we show:</p><p>Out-Noc: Percentage of erroneous pixels in non-occluded areas Out-All: Percentage of erroneous pixels in total Avg-Noc: Average disparity / end-point error in non-occluded areas Avg-All: Average disparity / end-point error in total Density: Percentage of pixels for which ground truth has been provided by the method Note: On 04.11.2013 we have improved the ground truth disparity maps and flow fields leading to slightly improvements for all methods. Please download the stereo/flow dataset with the improved ground truth for training again, if you have downloaded the dataset prior to 04.11.2013. Please consider reporting these new number for all future submissions. Links to last leaderboards before the updates: stereo and flow! Important Policy Update: As more and more non-published work and re-implementations of existing work is submitted to KITTI, we have established a new policy: from now on, only submissions with significant novelty that are leading to a peer-reviewed paper in a conference or journal are allowed. Minor modifications of existing algorithms or student research projects are not allowed. Such work must be evaluated on a split of the training set. To ensure that our policy is adopted, new users must detail their status, describe their work and specify the targeted venue during registration. Furthermore, we will regularly delete all entries that are 6 months old but are still anonymous or do not have a paper associated with them. For conferences, 6 month is enough to determine if a paper has been accepted and to add the bibliography information. For longer review cycles, you need to resubmit your results.  <ref type="figure">Figure 16</ref>. Screenshot on the KITTI 2012 benchmark (printed as PDF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional information used by the methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2019/11/23</head><p>The KITTI Vision Benchmark Suite www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow 2/11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Download development kit (3 MB)</head><p>Our evaluation table ranks all methods according to the number of erroneous pixels. All methods providing less than 100 % density have been interpolated using simple background interpolation as explained in the corresponding header file in the development kit. Legend: . The leaderboards for the KITTI 2015 stereo benchmarks did not change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Important Policy Update:</head><p>As more and more non-published work and re-implementations of existing work is submitted to KITTI, we have established a new policy: from now on, only submissions with significant novelty that are leading to a peer-reviewed paper in a conference or journal are allowed. Minor modifications of existing algorithms or student research projects are not allowed. Such work must be evaluated on a split of the training set. To ensure that our policy is adopted, new users must detail their status, describe their work and specify the targeted venue during registration. Furthermore, we will regularly delete all entries that are 6 months old but are still anonymous or do not have a paper associated with them. For conferences, 6 month is enough to determine if a paper has been accepted and to add the bibliography information. For longer review cycles, you need to resubmit your results.  <ref type="figure">Figure 17</ref>. Screenshot on the KITTI 2015 benchmark (printed as PDF). MaskFlownet-S ranks 14th.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional information used by the methods</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cnnbased patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3250" to="3259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">196</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="500" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8981" to="8989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07414</idno>
		<title level="m">A lightweight optical flow cnn-revisiting data fidelity and regularization</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iterative residual refinement for joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="614" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Yu Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Selflow: Self-supervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dense estimation and object-based segmentation of the optical flow with robust techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Mémin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="703" to="719" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Continual occlusion and optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Neoral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janšochman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A reimplementation of PWC-Net using PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<ptr target="https://github.com/sniklaus/pytorch-pwc" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mask-guided attention network for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4967" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Occlusion robust face recognition based on mask learning with pairwise differential siamese network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxue</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="773" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Models matter, so does training: An empirical study of cnns for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05571</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dense point trajectories by gpu-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4884" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structure-and motion-adaptive regularization for high accuracy optic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1663" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optical flow in mostly rigid scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1289" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Occlusion-aware r-cnn: detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6995" to="7003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
