<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inferring and Executing Programs for Visual Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Inferring and Executing Programs for Visual Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many applications, computer-vision systems need to answer sophisticated queries by reasoning about the visual world ( <ref type="figure">Figure 1</ref>). To deal with novel object interactions or object-attribute combinations, visual reasoning needs to be compositional: without ever having seen a "person touching a bike", the model should be able to understand the phrase by putting together its understanding of "person", "bike" and "touching". Such compositional reasoning is a hallmark of human intelligence, and allows people to solve a plethora of problems using a limited set of basic skills <ref type="bibr" target="#b28">[28]</ref>.</p><p>In contrast, modern approaches to visual recognition learn a mapping directly from inputs to outputs; they do not explicitly formulate and execute compositional plans. Direct input-output mapping works well for classifying images <ref type="bibr" target="#b26">[26]</ref> and detecting objects <ref type="bibr" target="#b9">[10]</ref> for a small, fixed set of categories. However, it fails to outperform strong baselines on tasks that require the model to understand an exponentially large space of objects, attributes, actions, and interactions, such as visual question answering (VQA) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b51">51]</ref>. Instead, models that learn direct input-output mappings tend How many chairs are at the table? Is there a pedestrian in my lane?</p><p>Is the person with the blue hat touching the bike in the back?</p><p>Is there a matte cube that has the same size as the red metal object? <ref type="figure">Figure 1</ref>. Compositional reasoning is a critical component needed for understanding the complex visual scenes encountered in applications such as robotic navigation, autonomous driving, and surveillance. Current models fail to do such reasoning <ref type="bibr" target="#b19">[19]</ref>.</p><p>to learn dataset biases but not reasoning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>.</p><p>In this paper, we argue that to successfully perform complex reasoning tasks, it might be necessary to explicitly incorporate compositional reasoning in the model structure. Specifically, we investigate a new model for visual question answering that consists of two parts: a program generator and an execution engine. The program generator reads the question and produces a plan or program for answering the question by composing functions from a function dictionary. The execution engine implements each function using a small neural module, and executes the resulting module network on the image to produce an answer. Both the program generator and the modules in the execution engine are neural networks with generic architectures; they can be trained separately when ground-truth programs are available, or jointly in an end-to-end fashion.</p><p>Our model builds on prior work on neural module networks that incorporate compositional reasoning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Prior module networks do not generalize well to new problems, because they rely on a hand-tuned program generator based on syntactic parsing, and on hand-engineered modules. By contrast, our model does not rely on such heuristics: we only define the function vocabulary and the "universal" module architecture by hand, learning everything else. We evaluate our model on the recently released CLEVR dataset <ref type="bibr" target="#b19">[19]</ref>, which has proven to be challenging for stateof-the-art VQA models. The CLEVR dataset contains ground-truth programs that describe the compositional reasoning required to answer the given questions. We find that with only a small amount of reasoning supervision (9000 ground truth programs which is 2% of those available), our model outperforms state-of-the-art non-compositional VQA models by ∼20 percentage points on CLEVR. We also show that our model's compositional nature allows it to generalize to novel questions by composing modules in ways that are not seen during training.</p><p>Though our model works well on the algorithmically generated questions in CLEVR, the true test is whether it can answer questions asked by humans in the wild. We collect a new dataset of human-posed free-form natural language questions about CLEVR images. Many of these questions have out-of-vocabulary words and require reasoning skills that are absent from our model's repertoire. Nevertheless, when finetuned on this dataset without additional program supervision, our model learns to compose its modules in novel but intuitive ways to best answer new types of questions. The result is an interpretable mapping of freeform natural language to programs, and a ∼9 point improvement in accuracy over the best competing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is related to to prior research on visual question answering, reasoning-augmented models, semantic parsers, and (neural) program-induction methods.</p><p>Visual question answering (VQA) is a popular proxy task for gauging the quality of visual reasoning systems <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b44">44]</ref>. Like the CLEVR dataset, benchmark datasets for VQA typically comprise a set of questions on images with associated answers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b51">51]</ref>; both questions and answers are generally posed in natural language. Many systems for VQA employ a very similar architecture <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b45">45]</ref>: they combine an RNN-based embedding of the question with a convolutional network-based embedding of an image in a classification model over possible answers. Recent work has questioned whether such systems are capable of developing visual reasoning capabilities: (1) very simple baseline models were found to perform competitively on VQA benchmarks by exploiting biases in the data <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b10">11]</ref> and (2) experiments on CLEVR, which was designed to control such biases, revealed that current systems do not learn to reason about spatial relationships or to learn disentangled representations <ref type="bibr" target="#b19">[19]</ref>.</p><p>Our model aims to address these problems by explicitly constructing an intermediate program that defines the reasoning process required to answer the question. We show that our model succeeds on several kinds of reasoning where other VQA models fail.</p><p>Reasoning-augmented models add components to neural network models to facilitate the development of reasoning processes in such models. For example, models such as neural Turing machines <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, memory networks <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b38">38]</ref>, and stack-augmented recurrent networks <ref type="bibr" target="#b20">[20]</ref> add explicit memory components to neural networks to facilitate learning of reasoning processes that involve long-term memory. While long-term memory is likely to be a crucial component of intelligence, it is not a prerequisite for reasoning, especially the kind of reasoning that is required for answering questions about images. <ref type="bibr" target="#b0">1</ref> Therefore, we do not consider memory-augmented models in this study.</p><p>Module networks are an example of reasoningaugmented models that use a syntactic parse of a question to determine the architecture of the network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">16]</ref>. The final network is composed of trained neural modules that execute the "program" produced by the parser. The main difference between our models and existing module networks is that we replace hand-designed off-the-shelf syntactic parsers <ref type="bibr" target="#b24">[24]</ref>, which perform very poorly on complex questions such as those in CLEVR <ref type="bibr" target="#b19">[19]</ref>, by a learnt program generator that can adapt to the task at hand.</p><p>Semantic parsers attempt to map natural language sentences to logical forms. Often, the goal is to answer natural language questions using a knowledge base <ref type="bibr" target="#b30">[30]</ref>. Recent approaches to semantic parsing involve a learnt programmer <ref type="bibr" target="#b29">[29]</ref>. However, the semantics of the program and the execution engine are fixed and known a priori, while we learn both the program generator and the execution engine.</p><p>Program-induction methods learn programs from input-output pairs by fitting the parameters of a neural network to predict the output that corresponds to a particular input value. Such models can take the form of a feedforward scoring function over operators in a domain-specific language that can be used to guide program search <ref type="bibr" target="#b3">[4]</ref>, or of a recurrent network that decodes a vectorial program representation into the actual program <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49]</ref>. The recurrent networks may incorporate compositional structure that allows them to learn new programs by combining previously learned sub-programs <ref type="bibr" target="#b36">[36]</ref>.</p><p>Our approach differs from prior work on program induction in (1) the type of input-output pairs that are used and (2) the way the domain-specific language is implemented. Prior work on neural program interpreters considers simple algorithms such as sorting of a list of integers; by contrast, we consider inputs that comprise an image and an associ-ated question (in natural language). Program induction approaches also assume knowledge of the low-level operators such as arithmetic operations. In contrast, we use a learnt execution engine and assume minimal prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We develop a learnable compositional model for visual question answering. Our model takes as input an image x and a visual question q about the image. The model selects an answer a ∈ A to the question from a fixed set A of possible answers. Internally, the model predicts a program z representing the reasoning steps required to answer the question. The model then executes the predicted program on the image, producing a distribution over answers.</p><p>To this end, we organize our system into two components: a program generator, z = π(q), which predicts programs from questions, and an execution engine, a = φ(x, z), which executes a program z on an image x to predict an answer a. Both the program generator and the execution engine are neural networks that are learned from data. In contrast to prior work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, we do not manually design heuristics for generating or executing the programs.</p><p>We present learning procedures both for settings where (some) ground-truth programs are available during training, and for settings without ground-truth programs. In practice, our models need some program supervision during training, but we find that the program generator requires very few of such programs in order to learn to generalize (see <ref type="figure" target="#fig_2">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Programs</head><p>Like all programming languages, our programs are defined by syntax giving rules for building valid programs, and semantics defining the behavior of valid programs. We focus on learning semantics for a fixed syntax. Concretely, we fix the syntax by pre-specifying a set F of functions f , each of which has a fixed arity n f ∈ {1, 2}. Because we are interested in visual question answering, we include in the vocabulary a special constant Scene, which represents the visual features of the image. We represent valid programs z as syntax trees in which each node contains a function f ∈ F, and in which each node has as many children as the arity of the function f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Program generator</head><p>The program generator z = π(q) predicts programs z from natural-language questions q that are represented as a sequence of words. We use a prefix traversal to serialize the syntax tree, which is a non-sequential discrete structure, into a sequence of functions. This allows us to implement the program generator using a standard LSTM sequence-tosequence model; see <ref type="bibr" target="#b39">[39]</ref> for details.</p><p>When decoding at test time, we simply take the argmax function at each time step. The resulting sequence of func- Question: Are there more cubes than yellow things? <ref type="figure">Figure 2</ref>.</p><p>System overview. The program generator is a sequence-to-sequence model which inputs the question as a sequence of words and outputs a program as a sequence of functions, where the sequence is interpreted as a prefix traversal of the program's abstract syntax tree. The execution engine executes the program on the image by assembling a neural module network <ref type="bibr" target="#b1">[2]</ref> mirroring the structure of the predicted program.</p><p>tions is converted to a syntax tree; this is straightforward since the arity of each function is known. Some generated sequences do not correspond to prefix traversals of a tree. If the sequence is too short (some functions do not have enough children) then we pad the sequence with Scene constants. If the sequence is too long (some functions have no parents) then unused functions are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Execution engine</head><p>Given a predicted program z and and an input image x, the execution engine executes the program on the image, a = φ(x, z), to predict an answer a. The execution engine is implemented using a neural module network <ref type="bibr" target="#b1">[2]</ref>: the program z is used to assemble a question-specific neural network that is composed from a set of modules. For each function f ∈ F, the execution engine maintains a neural network module m f . Given a program z, the execution engine creates a neural network m(z) by mapping each function f to its corresponding module m f in the order defined by the program: the outputs of the "child modules" are used as input into their corresponding "parent module".</p><p>Our modules use a generic architecture, in contrast to <ref type="bibr" target="#b1">[2]</ref>. A module of arity n receives n features maps of shape C×H×W and produces a feature map of shape C×H×W . Each unary module is a standard residual block <ref type="bibr" target="#b14">[14]</ref> with two 3×3 convolutional layers. Binary modules concatenate their inputs along the channel dimension, project from 2C to C channels using a 1 × 1 convolution, and feed the result to a residual block. The Scene module takes visual features as input (conv 4 features from ResNet-101 <ref type="bibr" target="#b14">[14]</ref> pretrained on ImageNet <ref type="bibr" target="#b37">[37]</ref>) and passes these features through four convolutional layers to output a C×H×W feature map.</p><p>Using the same architecture for all modules ensures that every valid program z corresponds to a valid neural network which inputs the visual features of the image and outputs a feature map of shape C×H×W . This final feature map is flattened and passed into a multilayer perceptron classifier that outputs a distribution over possible answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>Given a VQA dataset containing (x, q, z, a) tuples with ground truth programs z, we can train both the program generator and execution engine in a supervised manner. Specifically, we can (1) use pairs (q, z) of questions and corresponding programs to train the program generator, which amounts to training a standard sequence-to-sequence model; and (2) use triplets (x, z, a) of the image, program, and answer to train the execution engine, using backpropagation to compute the required gradients (as in <ref type="bibr" target="#b1">[2]</ref>).</p><p>Annotating ground-truth programs for free-form natural language questions is expensive, so in practice we may have few or no ground-truth programs. To address this problem, we opt to train the program generator and execution engine jointly on (x, q, a) triples without ground-truth programs. However, we cannot backpropagate through the argmax operations in the program generator. Instead we replace the argmaxes with sampling and use REINFORCE <ref type="bibr" target="#b42">[42]</ref> to estimate gradients on the outputs of the program generator; the reward for each of its outputs is the negative zero-one loss of the execution engine, with a moving-average baseline.</p><p>In practice, joint training using REINFORCE is difficult: the program generator needs to produce the right program without understanding what the functions mean, and the execution engine has to produce the right answer from programs that may not accurately implement the question asked. We propose a more practical semi-supervised learning approach. We first use a small set of ground-truth programs to train the program generator, then fix the program generator and train the execution engine using predicted programs on a large dataset of (x, q, a) triples. Finally, we use REINFORCE to jointly finetune the program generator and execution engine. Crucially, ground-truth programs are only used to train the initial program generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our model on the recent CLEVR dataset <ref type="bibr" target="#b19">[19]</ref>. Standard VQA methods perform poorly on this dataset, showing that it is a challenging benchmark. All questions are equipped with ground-truth programs, allowing for experiments with varying amounts of supervision.</p><p>We first perform experiments using strong supervision in the form of ground-truth programs. We show that in this strongly supervised setting, the combination of program generator and execution engine works much better on CLEVR than alternative methods. Next, we show that this strong performance is maintained when a small number of ground-truth programs, which capture only a fraction of question diversity, is used for training. Finally, we evaluate the ability of our models to perform compositional generalization, as well as generalization to free-form questions posed by humans. Code reproducing the results of our experiments is available from https://github.com/ facebookresearch/clevr-iep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines</head><p>Johnson et al. <ref type="bibr" target="#b19">[19]</ref> tested several VQA models on CLEVR. We reproduce these models as baselines here.</p><p>Q-type mode: This baseline predicts the most frequent answer for each of the question types in CLEVR.</p><p>LSTM: Similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">33]</ref>, questions are processed with learned word embeddings followed by a word-level LSTM <ref type="bibr" target="#b15">[15]</ref>. The final LSTM hidden state is passed to a multi-layer perceptron (MLP) that predicts a distribution over answers. This method uses no image information, so it   CNN+LSTM: Images and questions are encoded using convolutional network (CNN) features and final LSTM hidden states, respectively. These features are concatenated and passed to a MLP that predicts an answer distribution.</p><p>CNN+LSTM+SA <ref type="bibr" target="#b46">[46]</ref>: Questions and images are encoded using a CNN and LSTM as above, then combined using two rounds of soft spatial attention; a linear transform of the attention output predicts the answer.</p><p>CNN+LSTM+SA+MLP: Replaces the linear transform with an MLP for better comparison with the other methods.</p><p>The models that are most similar to ours are neural module networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Unfortunately, neural module networks use a hand-engineered, off-the-shelf parser to produce programs, and this parser fails 2 on the complex questions in CLEVR <ref type="bibr" target="#b19">[19]</ref>. Therefore, we were unable to include module networks in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Strongly and semi-supervised learning</head><p>We first experiment with a model trained using full supervision: we use the ground-truth programs for all ques-2 See supplemental material for example parses of CLEVR questions. tions in CLEVR to train both the program generator and the execution engine separately. The question answering accuracy of the resulting model on CLEVR is shown in Table 1 (Ours-strong). The results show that using strong supervision, our model can achieve near-perfect accuracy on CLEVR (even outperforming Mechanical Turk workers).</p><p>In practical scenarios, ground-truth programs are not available for all questions. We use the semi-supervised training process described in Section 3.4 to determine how many ground-truth programs are needed to match fully su-pervised models. First, the program generator is trained in a supervised manner using a small number of questions and ground-truth programs; next, the execution engine is trained on all CLEVR questions, using predicted rather than ground-truth programs. Finally, both components are jointly finetuned without ground-truth programs. <ref type="table" target="#tab_0">Table 1</ref> shows the accuracy of semi-supervised models trained with 9K and 18K ground-truth programs (Ours-semi).</p><p>The results show that 18K ground-truth programs are sufficient to train a model that performs almost on par with a fully supervised model (that used all 700K programs for training). This strong performance is not due to the program generator simply remembering all programs: the total number of unique programs in CLEVR is approximately 450K. This implies that after observing only a small fraction (≤ 4%) of all possible programs, the model is able to understand the underlying structure of CLEVR questions and use that understanding to generalize to new questions. <ref type="figure" target="#fig_2">Figure 4</ref> analyzes how the accuracy of the predicted programs and the final answer vary with the number of groundtruth programs used. We measure the accuracy of the program generator by deserializing the function sequence produced by the program generator, and marking it as correct if it matches the ground-truth program exactly. <ref type="bibr" target="#b2">3</ref> Our results show that with about 20K ground-truth programs, the program generator achieves near perfect accuracy, and the final answer accuracy is almost as good as strongly-supervised training. Training the execution engine using the predicted programs from the program generator instead of groundtruth programs leads to a loss of about 3 points in accuracy, but some of that loss is mitigated after joint finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">What do the modules learn?</head><p>To obtain additional insight into what the modules in the execution engine have learned, we visualized the parts of the image that are being used to answer different questions; see <ref type="figure" target="#fig_1">Figure 3</ref>. Specifically, the figure displays the norm of the gradient of the sum of the predicted answer scores (softmax inputs) with respect to the final feature map. This visualization reveals several important aspects of our model.</p><p>First, it clearly attends to the correct objects even for complicated referring expressions involving spatial relationships, intersection and union of constraints, etc.</p><p>Second, the examples show that changing a single module (swapping purple/blue, left/right, and/or) results in drastic changes in both the predicted answer and model attention, demonstrating that the individual modules do in fact perform their intended functions. Modules learn specialized functions such as localization and set operations without explicit supervision of their outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth question:</head><p>Is the number of matte blocks in front of the small yellow cylinder greater than the number of red rubber spheres to the left of the large red shiny cylinder? Program length: 20 A: yes Ground-truth question: How many objects are big rubber objects that are in front of the big gray thing or large rubber things that are in front of the large rubber sphere? Program length: 16 A: 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted program (translated):</head><p>Is the number of matte blocks in front of the small yellow cylinder greater than the number of large red shiny cylinders? Program length: 15 A: no</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted program (translated):</head><p>How many objects are big rubber objects in front of the big gray thing or large rubber spheres? Program length: 12 A: 2 <ref type="figure">Figure 6</ref>. Examples of long questions where the program and answer were predicted incorrectly when the model was trained on short questions, but both program and answer were correctly predicted after the model was finetuned on long questions. Above each image we show the ground-truth question and its program length; below, we show a manual English translation of the predicted program and answer before finetuning on long questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generalizing to new attribute combinations</head><p>Johnson et al. <ref type="bibr" target="#b19">[19]</ref> proposed the CLEVR-CoGenT dataset for investigating the ability of VQA models to perform compositional generalization. The dataset contains data in two different conditions: in Condition A, all cubes are gray, blue, brown, or yellow and all cylinders are red, green, purple, or cyan; in Condition B, cubes and cylinders swap color palettes. Johnson et al. <ref type="bibr" target="#b19">[19]</ref> found that VQA models trained on data from Condition A performed poorly on data from Condition B, suggesting the models are not well capable of generalizing to new conditions.</p><p>We performed experiments with our model on CLEVR-CoGenT: in <ref type="figure">Figure 5</ref>, we report accuracy of the semisupervised variant of our model trained on data from Condition A and evaluated on data from Condition B. Although the resulting model performs better than all baseline meth-ods in Condition B, it still appears to suffer from the problems identified by <ref type="bibr" target="#b19">[19]</ref>. A more detailed analysis of the results revealed that our model does not outperform the CNN+LSTM+SA baseline for questions about an object's shape or color. This is not surprising: if the model never sees red cubes, it has no incentive to learn that the attribute "red" refers to the color and not to the shape.</p><p>We also performed experiments in which we used a small amount of training data without ground-truth programs from condition B for finetuning. We varied the amount of data from condition B that is available for finetuning. As shown in <ref type="figure">Figure 5</ref>, our model learns the new attribute combinations from only ∼10K questions (∼1K images), and outperforms similarly trained baselines across the board. <ref type="bibr" target="#b3">4</ref> We believe that this is because the model's compositional nature allows it to quickly learn new semantics of attributes such as "red" from little training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Generalizing to new question types</head><p>Our experiments in Section 4.2 showed that relatively few ground-truth programs are required to train our model effectively. Due to the large number of unique programs in CLEVR, it is impossible to capture all possible programs with a small set of ground-truth programs; however, due to the synthetic nature of CLEVR questions, it is possible that a small number of programs could cover all possible program structures. In real-world scenarios, models should be able to generalize to questions with novel program structures without observing associated ground-truth programs.</p><p>To test this, we divide CLEVR questions into two categories based on their ground-truth programs: short and long. CLEVR questions are divided into question families, where all questions in the same family share the same program structure. A question is short if its question family has a mean program length less than 16; otherwise it is long. <ref type="bibr" target="#b4">5</ref> We train the program generator and execution engine on short questions in a semi-supervised manner using 18K ground-truth short programs, and test the resulting model on both short and long questions. This experiment tests the ability of our model to generalize from short to long chains of reasoning. Results are shown in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>The results show that when evaluated on long questions, our model trained on short questions underperforms the CNN+LSTM+SA model trained on the same set. Presumably, this result is due to the program generator learning a bias towards short programs. Indeed, <ref type="figure">Figure 6</ref> shows that the program generator produces programs that refer to the right objects but that are too short.</p><p>We can undo this short-program bias through joint fine- <ref type="bibr" target="#b3">4</ref> Note that this finetuning hurts performance on condition A. Joint finetuning on both conditions will likely alleviate this issue. <ref type="bibr" target="#b4">5</ref> Partitioning at the family level rather than the question level allows for better separation of program structure between short and long questions. tuning of the program generator and execution engine on the combined set of short and long questions, without groundtruth programs. To pinpoint the problem of short-program bias in the program generator, we leave the execution engine fixed during finetuning; it is only used to compute REIN-FORCE rewards for the program generator. After finetuning, our model substantially outperforms baseline models that were trained on the entire dataset; see <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Generalizing to human-posed questions</head><p>The fact that questions in the CLEVR benchmark were generated algorithmically may favor some approaches over others. In particular, natural language tends to be more ambiguous than algorithmically generated questions. We performed an experiment to assess the extent to which models trained on CLEVR can be finetuned to answer human questions. To this end, we collected a new dataset of naturallanguage questions and answers for CLEVR images.</p><p>The CLEVR-Humans Dataset. Inspired by VQA <ref type="bibr" target="#b2">[3]</ref>, workers on Amazon Mechanical Turk were asked to write questions about CLEVR images that would be hard for a smart robot to answer; workers were primed with questions from CLEVR and restricted to answers in CLEVR. We filtered questions by asking three workers to answer each question, and removed questions that a majority of workers could not correctly answer. We collected one question per image; after filtering, we obtained 17,817 training, 7,202 validation, and 7,145 test questions on CLEVR images. The data is available from the first author's website.</p><p>The human questions are more challenging than synthetic CLEVR questions because they exhibit more linguistic variety. Unlike existing VQA datasets, however, the CLEVR-Humans questions do not require common-sense knowledge: they focus entirely on visual reasoning abilities, which makes them a good testbed for evaluating reasoning. <ref type="figure" target="#fig_3">Figure 7</ref> shows some example human questions. Some questions are rewordings of synthetic CLEVR questions; others are answerable using the same basic functions as CLEVR but potentially with altered semantics for those skills. For example, people use spatial relationships "left", "right", etc. differently than their meanings in CLEVR questions. Finally, some questions require skills not needed for answering synthetic questions.  Results. We train our model on CLEVR, and then finetune only the program generator on the CLEVR-Humans training set to adapt it to the additional linguistic variety; we do not adapt the execution engine due to the limited quantity of data. No ground-truth programs are available during finetuning. The embeddings in the sequence-to-sequence model of question words that do not appear in CLEVR synthetic questions are initialized randomly before finetuning.</p><p>During finetuning, our model learns to reuse the reasoning skills it has already mastered in order to answer the linguistically more diverse natural-language questions. As shown in <ref type="figure" target="#fig_3">Figure 7</ref>, it learns to map novel words ("box") to known modules. When human questions are not expressible using CLEVR functions, our model still learns to produce reasonable programs closely approximating the question's intent. Our model often fails on questions that cannot be reasonably approximated using our model's module inventory, such as the rightmost example in <ref type="figure" target="#fig_3">Figure 7</ref>. Quantitatively, the results in <ref type="table" target="#tab_4">Table 3</ref> show that our model outperforms all baselines on the CLEVR-Humans test set both with and without finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Future Work</head><p>Our results show that our model is able to generalize to novel scenes and questions and can even infer programs for free-form human questions using its learned modules. Whilst these results are encouraging, there still are many questions that cannot be reasonably approximated using our fixed set of modules. For example, the question "What color is the object with a unique shape?" requires a model to identify unique shapes, for which no module is currently available. Adding new modules to our model is straightforward due to our generic module design, but automatically identifying and learning new modules without program supervision is still an open problem. One path forward is to design a Turing-complete set of modules; this would allow for all programs to be expressed without learning new modules. For example, by adding ternary operators (if/then/else) and loops (for/do), the question "What color is the object with a unique shape?" can be answered by looping over all shapes, counting the objects with that shape, and returning it if the count is one. These control-flow operators could be incorporated into our framework: for example, a loop could apply the same module to an input set and aggregate the results. We emphasize that learning such programs with limited supervision is an open research challenge, which we leave to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper fits into a long line of work on incorporating symbolic representations into (neural) machine learning models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b36">36]</ref>. We have shown that explicit program representations can make it easier to compose programs to answer novel questions about images. Our generic program representation, learnable program generator and universal design for modules makes our model much more flexible than neural module networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and thus more easily extensible to new problems and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Program Generator</head><p>In all experiments our program generator is an LSTM sequence-to-sequence model <ref type="bibr" target="#b39">[39]</ref>. It comprises two learned recurrent neural networks: the encoder receives the naturallanguage question as a sequence of words, and summarizes the question as a fixed-length vector; the decoder receives this fixed-length vector as input and produces the predicted program as a sequence of functions. The encoder and decoder do not share weights.</p><p>The encoder converts the discrete words of the input question to vectors of dimension 300 using a learned word embedding layer; the resulting sequence of vectors is then processed with a two-layer LSTM using 256 hidden units per layer. The hidden state of the second LSTM layer at the final timestep is used as the input to the decoder network.</p><p>At each timestep the decoder network receives both the function from the previous timestep (or a special &lt;START&gt; token at the first timestep) and the output from the encoder network. The function is converted to a 300-dimensional vector with a learned embedding layer and concatenated with the decoder output; the resulting sequence of vectors is processed by a two-layer LSTM with 256 hidden units per layer. At each timestep the hidden state of the second LSTM layer is used to compute a distribution over all possible functions using a linear projection.</p><p>During supervised training of the program generator, we use Adam <ref type="bibr" target="#b23">[23]</ref> with a learning rate of 5 × 10 −4 and a batch size of 64; we train for a maximum of 32,000 iterations, employing early stopping based on validation set accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Execution Engine</head><p>The execution engine uses a Neural Module Network <ref type="bibr" target="#b1">[2]</ref> to compile a custom neural network architecture based on the predicted program from the program generator. The input image is first resized to 224 × 224 pixels, then passed through a convolutional network to extract image features; the architecture of this network is shown in <ref type="table">Table 4</ref>.</p><p>The predicted program takes the form of a syntax tree; the leaves of the tree are Scene functions which receive visual input from the convolutional network. For ground-truth programs, the root of the tree is a function corresponding to one of the question types from the CLEVR dataset <ref type="bibr" target="#b19">[19]</ref>, such as count or query shape. For predicted programs the root of the program tree could in principle be any function, but in practice we find that trained models tend only to  <ref type="table">Table 4</ref>. Network architecture for the convolutional network used in our execution engine. The ResNet-101 model is pretrained on ImageNet <ref type="bibr" target="#b37">[37]</ref> and remains fixed while the execution engine is trained. The output from this network is passed to modules representing Scene nodes in the program.</p><p>predict as roots those function types that appear as roots of ground-truth programs. Each function in the predicted program is associated with a module which receives either one or two inputs; this association gives rise to a custom neural network architecture corresponding to each program. Previous implementations of Neural Module networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> used different architectures for each module type, customizing the module architecture to the function the module was to perform. In contrast we use a generic design for our modules: each module is a small residual block <ref type="bibr" target="#b14">[14]</ref>; the exact architectures used for our unary and binary modules are shown in <ref type="table" target="#tab_8">Tables 5 and 6</ref> respectively.</p><p>In initial experiments we used Batch Normalization <ref type="bibr" target="#b17">[17]</ref> after each convolution in the modules, but we found that this prevented the model from converging. Since each image in a minibatch may have a different program, our implementation of the execution engine iterates over each program in the minibatch one by one; as a result each module is only run with a batch size of one during training, leading to poor convergence when modules contain Batch Normalization.</p><p>The output from the final module is passed to a classifier which predicts a distribution over answers; the exact architecture of the classifier is shown in <ref type="table" target="#tab_10">Table 7</ref>.</p><p>When training the execution engine alone (using either ground-truth programs or predicted programs from a fixed program generator), we train using Adam <ref type="bibr" target="#b23">[23]</ref> with a learning rate of 1 × 10 −4 and a batch size of 64; we train for a maximum of 200,000 iterations and employ early stopping based on validation set accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Joint Training</head><p>When jointly training the program generator and execution engine, we train using Adam with a learning rate of 5 × 10 −5 and a batch size of 64; we train for a maximum of 100,000 iterations, again employing early stopping based on validation set accuracy.</p><p>We use a moving average baseline to reduce the variance of gradients estimated using REINFORCE; in particular our baseline is an exponentially decaying moving average of past rewards, with a decay factor of 0.99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>Output   <ref type="table">Table 6</ref>. Architecture for binary modules in the execution engine. These modules receive the output from two other modules. The binary modules in our system are intersect, union, equal size, equal color, equal material, equal shape, equal integer, less than, and greater than.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>Output  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Baselines</head><p>We reimplement the baselines used in <ref type="bibr" target="#b19">[19]</ref>: LSTM. Our LSTM baseline receives the input question as a sequence of words, converts the words to 300dimensional vectors using a learned word embedding layer, and processes the resulting sequence with a two-layer LSTM with 512 hidden units per layer. The LSTM hidden state from the second layer at the final timestep is passed to an MLP with two hidden layers of 1024 units each, with ReLU nonlinearities after each layer.</p><p>CNN+LSTM. Like the LSTM baseline, the CNN+LSTM model encodes the question using learned 300-dimensional word embeddings followed by a twolayer LSTM with 512 hidden units per layer. The image is encoded using the same CNN architecture as the execution engine, shown in <ref type="table">Table 4</ref>. The encoded question and (flattened) image features are concatenated and passed to a two-layer MLP with two hidden layers of 1024 units each, with ReLU nonlinearities after each layer.</p><p>CNN+LSTM+SA. The question and image are encoded in exactly the same manner as the CNN+LSTM baseline. However rather than concatenating these representations, they are fed to two consecutive Stacked Attention layers <ref type="bibr" target="#b46">[46]</ref> with a hidden dimension of 512 units; this results in a 512-dimensional vector which is fed to a linear layer to predict answer scores.</p><p>This matches the CNN+LSTM+SA model as originally described by Yang et al. <ref type="bibr" target="#b46">[46]</ref>; this also matches the CNN+LSTM+SA model used in <ref type="bibr" target="#b19">[19]</ref>.</p><p>CNN+LSTM+SA+MLP. Identical to CNN+LSTM+ SA; however the output of the final stacked attention module is fed to a two-layer MLP with two hidden layers of 1024 units each, with ReLU nonlinearities after each layer.</p><p>Since all other other models (LSTM, CNN+LSTM, and ours) terminate in an MLP to predict the final answer distribution, the CNN+LSTM+SA+MLP gives a more fair comparison with the other methods.</p><p>Surprisingly, the minor architectural change of replacing the linear transform with an MLP significantly improves performance on the CLEVR dataset: CNN+LSTM+SA achieves an overall accuracy of 69.8, while CNN+LSTM+SA+MLP achieves 73.2. Much of this gain comes from improved performance on comparison questions; for example on shape comparison questions CNN+LSTM+SA achieves an accuracy of 50.9 and CNN+LSTM+SA+MLP achieves 69.7.</p><p>Training. All baselines are trained using Adam with a learning rate of 5 × 10 −4 with a batch size of 64 for a maximum of 360,000 iterations, employing early stopping based on validation set accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Module Network parses</head><p>The closest method to our own is that of Andreas et al. <ref type="bibr" target="#b0">[1]</ref>. Their dynamic neural module networks first perform a dependency parse of the sentence; heuristics are then used to generate a set of layout fragments from the dependency parse. These fragments are heuristically combined, giving a set of candidate layouts; the final network layout is selected from these candidates through a learned reranking step.</p><p>Unfortunately we found that the parser used in <ref type="bibr" target="#b0">[1]</ref> for VQA questions did not perform well on the longer questions in CLEVR. In <ref type="table">Table 8</ref> we show random questions from the CLEVR training set together with the layout frag-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>The brown object that is the same shape as the green shiny thing is what size? Fragments:</p><p>( what thing) Question:</p><p>What material is the big purple cylinder? Fragments: (material purple);(material big);(material (and purple big))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>How big is the cylinder that is in front of the green metal object left of the tiny shiny thing that is in front of the big red metal ball? Fragments:</p><p>( what thing)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>Are there any metallic cubes that are on the right side of the brown shiny thing that is behind the small metallic sphere to the right of the big cyan matte thing? Fragments:</p><p>(is brown);(is cubes);(is (and brown cubes))</p><p>Question:</p><p>Is the number of cyan things in front of the purple matte cube greater than the number of metal cylinders left of the small metal sphere? Fragments:</p><p>(is cylinder);(is cube);(is (and cylinder cube)) Question:</p><p>Are there more small blue spheres than tiny green things? Fragments:</p><p>(is blue);(is sphere);(is (and blue sphere)) Question:</p><p>Are there more big green things than large purple shiny cubes? Fragments:</p><p>(is cube);(is purple);(is (and cube purple))</p><p>Question:</p><p>What number of things are large yellow metallic balls or metallic things that are in front of the gray metallic sphere? Fragments:</p><p>(number gray);(number ball);(number (and gray ball)) Question:</p><p>The tiny cube has what color? Fragments:</p><p>( what thing)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>There is a small matte cylinder; is it the same color as the tiny shiny cube that is behind the large red metallic ball?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fragments:</head><p>( what thing) <ref type="table">Table 8</ref>. Examples of random questions from the CLEVR training set, parsed using the code by Andreas et al. <ref type="bibr" target="#b0">[1]</ref> for parsing questions from the VQA dataset <ref type="bibr" target="#b2">[3]</ref>. Each parse gives a set of layout fragments separated by semicolons; in <ref type="bibr" target="#b0">[1]</ref> these fragments are combined to produce candidate layouts for the module network. When the parser fails, it produces the default fallback fragment ( what thing).</p><p>ments computed using the parser from <ref type="bibr" target="#b0">[1]</ref>. For many questions the parser fails, falling back to the fragment ( what thing); when this happens then the resulting module network will not respect the structure of the question at all. For questions where the parser does not fall back to the default layout, the resulting layout fragments often fail to capture key elements from the question; for example, after parsing the question What material is the big purple cylinder?, none of the resulting fragments mention the cylinder.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Visualizations of the norm of the gradient of the sum of the predicted answer scores with respect to the final feature map. From left to right, each question adds a module to the program; the new module is underlined in the question. The visualizations illustrate which objects the model attends to when performing the reasoning steps for question answering. Images are from the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Accuracy of predicted programs (left) and answers (right) as we vary the number of ground-truth programs. Blue and green give accuracy before and after joint finetuning; the dashed line shows accuracy of our strongly-supervised model. can only model question-conditional biases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Examples of questions from the CLEVR-Humans dataset, along with predicted programs and answers from our model. Question words that do not appear in CLEVR questions are underlined. Some predicted programs exactly match the semantics of the question (green); some programs closely match the question semantics (yellow), and some programs appear unrelated to the question (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Compare Integer Query Compare Method Exist Count Equal Less More Size Color Mat. Shape Size Color Mat. Shape Overall Q-type mode 50.2 34.6 51.4 51.6 50.5 50.1 13.4 50.8 33.5 50.3 52.5 50.2 51.8 42.1 LSTM 61.8 42.5 63.0 73.2 71.7 49.9 12.2 50.8 33.2 50.5 52.5 49.7 51.8 47.0 CNN+LSTM 68.2 47.8 60.8 74.3 72.5 62.5 22.4 59.9 50.9 56.5 53.0 53.8 55.5 54.3 CNN+LSTM+SA [46] 68.4 57.5 56.8 74.9 68.2 90.1 83.3 89.8 87.6 52.1 55.5 49.7 50.9 69.8 CNN+LSTM+SA+MLP 77.9 59.7 60.3 83.7 76.7 85.4 73.1 84.5 80.7 72.3 71.2 70.1 69.7 73.2 Human † [19] 96.6 86.7 79.0 87.0 91.0 97.0 95.0 94.0 94.0 94.0 98.0 96.0 96.0 92.6 Ours-strong (700K prog.) 97.1 92.7 98.0 99.0 98.9 98.8 98.4 98.1 97.3 99.8 98.5 98.9 98.4 96.9 Ours-semi (18K prog.) 95.3 90.1 93.9 97.1 97.6 98.1 97.1 97.7 96.6 99.0 97.6 98.0 97.3 95.4 Ours-semi (9K prog.) 89.7 79.7 85.2 76.1 77.9 94.8 93.3 93.1 89.2 97.8 94.5 96.6 95.1 88.6 Question answering accuracy (higher is better) on the CLEVR dataset for baseline models, humans, and three variants of our model. The strongly supervised variant of our model uses all 700K ground-truth programs for training, whereas the semi-supervised variants use 9K and 18K ground-truth programs, respectively.</figDesc><table /><note>† Human performance is measured on a 5.5K subset of CLEVR questions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>What shape is the. . . . . . purple thing? A: cube . . . blue thing? A: sphere . . . red thing right of the blue thing? A: sphere . . . red thing left of the blue thing? A: cube Q: How many cyan things are. . . . . . right of the gray cube?</figDesc><table><row><cell></cell><cell>. . . left of the small cube?</cell><cell>. . . right of the gray cube</cell><cell>. . . right of the gray cube</cell></row><row><cell></cell><cell></cell><cell>and left of the small cube?</cell><cell>or left of the small cube?</cell></row><row><cell>A: 3</cell><cell>A: 2</cell><cell>A: 1</cell><cell>A: 4</cell></row></table><note>Q:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Train Short</cell><cell cols="2">Finetune Both</cell></row><row><cell cols="5">Method Short Long Short Long</cell></row><row><cell>LSTM</cell><cell>46.4</cell><cell>48.6</cell><cell>46.5</cell><cell>49.9</cell></row><row><cell>CNN+LSTM</cell><cell>54.0</cell><cell>52.8</cell><cell>54.3</cell><cell>54.2</cell></row><row><cell>CNN+LSTM+SA+MLP</cell><cell>74.2</cell><cell>64.3</cell><cell>74.2</cell><cell>67.8</cell></row><row><cell>Ours (25K prog.)</cell><cell>95.9</cell><cell>55.3</cell><cell>95.6</cell><cell>77.8</cell></row></table><note>. Question answering accuracy on short and long CLEVR questions. Left columns: Models trained only on short questions; our model uses 25K ground-truth short programs. Right columns: Models trained on both short and long questions. Our model is trained on short questions then finetuned on the entire dataset; no ground-truth programs are used during finetuning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Question answering accuracy on the CLEVR-Humans test set of four models after training on just the CLEVR dataset (left) and after finetuning on the CLEVR-Humans dataset (right).</figDesc><table><row><cell></cell><cell>Train</cell><cell>Train CLEVR,</cell></row><row><cell cols="3">Method CLEVR finetune human</cell></row><row><cell>LSTM</cell><cell>27.5</cell><cell>36.5</cell></row><row><cell>CNN+LSTM</cell><cell>37.7</cell><cell>43.2</cell></row><row><cell>CNN+LSTM+SA+MLP</cell><cell>50.4</cell><cell>57.6</cell></row><row><cell>Ours (18K prog.)</cell><cell>54.0</cell><cell>66.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Is there a blue box in the items? A: yes</figDesc><table><row><cell></cell><cell>Q: What shape object</cell><cell></cell><cell>Q: Are all the balls small?</cell><cell>Q: Is the green block to the</cell><cell>Q: Two items share a color, a</cell></row><row><cell></cell><cell>is farthest right?</cell><cell></cell><cell>A: no</cell><cell>right of the yellow sphere?</cell><cell>material, and a shape; what</cell></row><row><cell></cell><cell>A: cylinder</cell><cell></cell><cell></cell><cell>A: yes</cell><cell>is the size of the rightmost</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>of those items? A: large</cell></row><row><cell>Predicted Program:</cell><cell>Predicted Program:</cell><cell></cell><cell>Predicted Program:</cell><cell>Predicted Program:</cell><cell>Predicted Program:</cell></row><row><cell>exist</cell><cell>query shape</cell><cell></cell><cell>equal size</cell><cell>exist</cell><cell>count</cell></row><row><cell>filter shape[cube]</cell><cell>unique</cell><cell></cell><cell>query size</cell><cell>filter shape[cube]</cell><cell>filter shape[cube]</cell></row><row><cell>filter color[blue]</cell><cell>relate[right]</cell><cell></cell><cell>unique</cell><cell>filter color[green]</cell><cell>same material</cell></row><row><cell>scene</cell><cell>unique</cell><cell cols="2">filter shape[sphere]</cell><cell>relate[right]</cell><cell>unique</cell></row><row><cell></cell><cell cols="2">filter shape[cylinder]</cell><cell>scene</cell><cell>unique</cell><cell>filter shape[cylinder]</cell></row><row><cell></cell><cell>filter color[blue]</cell><cell></cell><cell>query size</cell><cell>filter shape[sphere]</cell><cell>scene</cell></row><row><cell></cell><cell>scene</cell><cell></cell><cell>unique</cell><cell>filter color[yellow]</cell></row><row><cell></cell><cell></cell><cell cols="2">filter shape[sphere]</cell><cell>scene</cell></row><row><cell></cell><cell></cell><cell></cell><cell>filter size[small]</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>scene</cell><cell></cell></row><row><cell>Predicted Answer:</cell><cell>Predicted Answer:</cell><cell></cell><cell>Predicted Answer:</cell><cell>Predicted Answer:</cell><cell>Predicted Answer:</cell></row><row><cell>yes</cell><cell>cylinder</cell><cell></cell><cell>no</cell><cell>yes</cell><cell>0</cell></row></table><note>Q:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>ResNet-101 [14] conv4 6 1024 × 14 × 14 Conv(3 × 3, 1024 → 128) 128 × 14 × 14</figDesc><table><row><cell>Layer</cell><cell>Output size</cell></row><row><cell>Input image</cell><cell>3 × 224 × 224</cell></row><row><cell>ReLU</cell><cell>128 × 14 × 14</cell></row><row><cell>Conv(3 × 3, 128 → 128)</cell><cell>128 × 14 × 14</cell></row><row><cell>ReLU</cell><cell>128 × 14 × 14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Architecture for unary modules used in the execution engine. These modules receive the output from one other module, except for the special Scene module which instead receives input from the convolutional network(Table 4).</figDesc><table><row><cell>Index</cell><cell>Layer</cell><cell>Output size</cell></row><row><cell>(1)</cell><cell>Previous module output</cell><cell>128 × 14 × 14</cell></row><row><cell>(2)</cell><cell>Previous module output</cell><cell>128 × 14 × 14</cell></row><row><cell>(3)</cell><cell>Concatenate (1) and (2)</cell><cell>256 × 14 × 14</cell></row><row><cell>(4)</cell><cell cols="2">Conv(1 × 1, 256 → 128) 128 × 14 × 14</cell></row><row><cell>(5)</cell><cell>ReLU</cell><cell>128 × 14 × 14</cell></row><row><cell>(6)</cell><cell cols="2">Conv(3 × 3, 128 → 128) 128 × 14 × 14</cell></row><row><cell>(7)</cell><cell>ReLU</cell><cell>128 × 14 × 14</cell></row><row><cell>(8)</cell><cell cols="2">Conv(3 × 3, 128 → 128) 128 × 14 × 14</cell></row><row><cell>(9)</cell><cell cols="2">Residual: Add (5) and (8) 128 × 14 × 14</cell></row><row><cell>(10)</cell><cell>ReLU</cell><cell>128 × 14 × 14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Network architecture for the classifier used in our execution engine. The classifier receives the output from the final module and predicts a distribution over answers A.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Memory is likely indispensable in more complex settings such as visual dialogues or SHRDLU<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b43">43]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that this may underestimate the true accuracy, since two different programs can be functionally equivalent.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Ranjay Krishna, Yuke Zhu, Kevin Chen, and Dhruv Batra for helpful comments and discussion. J. Johnson is partially supported by an ONR MURI grant.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We will release code to reproduce our experiments. We also detail some key implementation details here.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepcoder: Learning to write programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Making neural programming architectures generalize via recursion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual dialog. In CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04467</idno>
		<title level="m">Zitnick. Exploring nearest neighbor approaches for image captioning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Hybrid computing using a neural network with dynamic external memory. Nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hassabis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inferring algorithmic patterns with stack-augmented recurrent nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visual question answering: Datasets, algorithms, and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1610.01465</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural GPUs learn algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>NIPS. 2012. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural random-access machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00020</idno>
		<title level="m">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning dependencybased compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning models for actions and person-object interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural programmer-interpreters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-toend memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Movieqa: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno>1992. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Understanding Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Visual question answering: A survey of methods and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1607.05910</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning simple algorithms from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00521</idno>
		<title level="m">Reinforcement learning neural turing machines</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Yin and yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visual7W: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
