<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-parametric Uni-modality Constraints for Deep Ordinal Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-15">June 15, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soufiane</forename><surname>Belharbi</surname></persName>
							<email>soufiane.belharbi.1@ens.etsmtl.ca</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIVIA</orgName>
								<orgName type="institution">Université du Québec</orgName>
								<address>
									<addrLine>École de technologie supérieure</addrLine>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><forename type="middle">Ben</forename><surname>Ayed</surname></persName>
							<email>ismail.benayed@etsmtl.ca</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIVIA</orgName>
								<orgName type="institution">Université du Québec</orgName>
								<address>
									<addrLine>École de technologie supérieure</addrLine>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Mccaffrey</surname></persName>
							<email>luke.mccaffrey@mcgill.caeric.granger@etsmtl.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Oncology</orgName>
								<orgName type="institution" key="instit1">Rosalind and Morris Goodman Cancer Research Centre</orgName>
								<orgName type="institution" key="instit2">McGill University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Granger</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIVIA</orgName>
								<orgName type="institution">Université du Québec</orgName>
								<address>
									<addrLine>École de technologie supérieure</addrLine>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-parametric Uni-modality Constraints for Deep Ordinal Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-15">June 15, 2020</date>
						</imprint>
					</monogr>
					<note>Belharbi et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new constrained-optimization formulation for deep ordinal classification, in which uni-modality of the label distribution is enforced implicitly via a set of inequality constraints over all the pairs of adjacent labels. Based on (c − 1) constraints for c labels, our model is non-parametric and, therefore, more flexible than the existing deep ordinal classification techniques. Unlike these, it does not restrict the learned representation to a single and specific parametric model (or penalty) imposed over all the labels. Therefore, it enables the training to explore larger solution spaces, while removing the need for ad hoc choices, and scaling up to large numbers of labels. Our formulation can be employed in conjunction with any standard classification loss and deep architecture. To address this challenging optimization problem, we solve a sequence of unconstrained losses based on a powerful extension of the log-barrier method. This effectively handles competing constraints and accommodates standard SGD for deep networks, while avoiding computationally expensive Lagrangian dual steps and substantially outperforming penalty methods. Furthermore, we propose a new Sides Order Index (SOI) performance metric for ordinal classification, as a proxy to measure distribution uni-modality. We report comprehensive set of evaluations and comparisons with state-of-the-art methods on benchmark public datasets for several ordinal classification tasks, showing the merits of our approach in terms of label consistency, classification accuracy and scalability. Importantly, enforcing label consistency with our model does not incur higher classification errors, unlike many existing ordinal classification methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Different research has suggested using a set of surrogate losses for OC training <ref type="bibr" target="#b37">(Pedregosa et al., 2017;</ref><ref type="bibr" target="#b39">Rennie and Srebro, 2005)</ref>. However, cross-entropy and/or mean squared error remain a primary choice for deep models due to their differentiability, simplicity, and capability to deal with many classes. Such standard classification losses, e.g., cross-entropy, do not impose any prior on the structure of the labels. Designed to penalize the error between predicted and true labels for each data sample, they do not account for the semantic relationships that might exist between the labels. However, in a wide range of classification tasks, the set of labels exhibits a natural structure, for instance, in the form of a specific order. A typical example is classifying biopsy samples, with the labels encoding cancer-aggressiveness levels (or grades), which are ordered. Ordinal classification (OC) attempts to leverage such natural order of the labels, and is useful in a breadth of applications, such as movies rating <ref type="bibr" target="#b10">(Crammer and Singer, 2002;</ref><ref type="bibr" target="#b27">Koren and Sill, 2011)</ref>, market bonds rating <ref type="bibr" target="#b31">(Moody and Utans, 1994)</ref>, age estimation <ref type="bibr" target="#b34">Pan et al., 2018;</ref><ref type="bibr" target="#b46">Zhu et al., 2019)</ref>, emotion estimation <ref type="bibr" target="#b23">(Jia et al., 2019;</ref><ref type="bibr" target="#b44">Xiong et al., 2019;</ref><ref type="bibr" target="#b45">Zhou et al., 2015)</ref>, cancer grading <ref type="bibr" target="#b17">(Gentry et al., 2015)</ref>, Code: https://github.com/sbelharbi/Deep-Ordinal-Classification-with-Inequality-Constraints diabetic retinopathy grading <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref>, photographs dating <ref type="bibr" target="#b33">(Palermo et al., 2012)</ref>, among many others. <ref type="figure">Figure 1</ref>: An example of two different posterior distributions of the same input sample with the same probability of the predicted label g 5 and the same cross-entropy loss: d 1 corresponds to paradoxical predictions, ranking grade g 1 right after predicted grade g 5 , despite their significant semantic difference, whereas d 1 corresponds to consistent (ordered) predictions.</p><p>Fig.1 depicts a typical OC task, which consists of grading cancer (8 classes, for illustration). It shows two different posterior distributions (PDs) of class predictions for the same input. While the two PDs have exactly the same cross-entropy loss, d 2 corresponds to a uni-modal distribution of the posteriors as it concentrates its probability mass around predicted label g 5 and, as we move away from the latter, it decreases monotonically. In contrast, d 1 distributes its mass over labels that are far away from each other, yielding inconsistent class predictions. Therefore, d 1 corresponds to predictions that are semantically paradoxical. The model gave the second highest posterior probability to grade g 1 , ranking it right after predicted grade g 5 , despite the significant semantic difference between the two grades. In practice, such inconsistent distributions may raise serious interpretability issues, and impede the deployment of the models, more so when important actions are associated with the predictions. As an example, consider d 1 with the following ordered actions for the top-3 labels {mild-chemotherapy, do-nothing, immediate-surgery-with-intensive-chemotherapy}. This order of top-3 actions is confusing, and may not be considered due to its discordance.</p><p>Several recent deep learning works addressed ordinal classification by imposing a uni-modality prior on the predicted posterior distributions following some parametric model. This is often done by enforcing uni-modality either on the label distributions <ref type="bibr" target="#b1">(Beckham and Pal, 2016;</ref><ref type="bibr" target="#b9">Cheng et al., 2008;</ref><ref type="bibr" target="#b14">Gao et al., 2017;</ref><ref type="bibr" target="#b15">Geng, 2016;</ref><ref type="bibr" target="#b16">Geng et al., 2013;</ref><ref type="bibr" target="#b22">Huo et al., 2016;</ref><ref type="bibr" target="#b34">Pan et al., 2018)</ref> or on the network outputs <ref type="bibr" target="#b2">(Beckham and Pal, 2017;</ref><ref type="bibr" target="#b11">Da Costa and Cardoso, 2005)</ref>. While both type of approaches yield consistent predictions, they constrain the output distribution to have a specific form, following the choice of a parametric uni-modal prior, e.g., Poisson <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref> or Gaussian <ref type="bibr" target="#b14">(Gao et al., 2017;</ref><ref type="bibr" target="#b15">Geng, 2016;</ref><ref type="bibr" target="#b16">Geng et al., 2013;</ref><ref type="bibr" target="#b22">Huo et al., 2016)</ref>, imposed as a single penalty on all the labels . Therefore, in general, they require several task-dependent choices, including tuning carefully the hyper-parameters that control the form (or shape) of the parametric model and complex network-architecture design. They might also lead to models that do not scale well for large numbers of labels, as is the case of (Beckham and Pal, 2017) 1 , which limits their applicability. We argue that restricting the posterior distribution to a single parametric model of a specific form is not necessary for ensuring uni-modality and order consistency, and propose a non-parametric, constrained-optimization solution for deep ordinal classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>We propose a novel constrained-optimization formulation for deep ordinal classification. Our model enforces uni-modality and label-order consistency via a set of inequality constraints over all pairs of adjacent labels, which can be imposed on any standard classification loss and integrated with any deep architecture. Based on (c − 1) constraints for c labels, our model is non-parametric and, therefore, more flexible than the existing deep ordinal classification techniques. Unlike these, it does not restrict the learned representation to a single and specific parametric model (or penalty) imposed on all the labels. Therefore, it enables the training to explore larger spaces of solutions, while removing the need for ad hoc choices and scaling up to large numbers of labels.</p><p>To tackle the ensuing challenging optimization problem, we solve a sequence of unconstrained losses based on a powerful extension of the log-barrier method. This handles effectively competing constraints and accommodates standard SGD for deep networks, while avoiding computationally expensive Lagrangian dual steps and outperforming substantially penalty methods. Furthermore, we introduce a new performance metric for OC, as a proxy to measure distribution uni-modality, referred to as the Sides Order Index (SOI). We report comprehensive evaluations and comparisons to state-of-the-art methods on benchmark public datasets for several OC tasks (breast cancer grading, age estimation, and historical color image dating). The results indicate that our approach outperforms substantially several state-of-the-art ordinal classification methods in terms of label consistency, while scaling up to large numbers of labels. Importantly, enforcing label consistency with our model does not incur higher classification errors, unlike many existing ordinal classification methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Several recent work converted the hard target label into a prior distribution <ref type="bibr" target="#b14">(Gao et al., 2017;</ref><ref type="bibr" target="#b15">Geng, 2016;</ref><ref type="bibr" target="#b16">Geng et al., 2013;</ref><ref type="bibr" target="#b22">Huo et al., 2016)</ref>. One way to impose such a prior distribution over the labels is, for instance, to optimize a divergence loss, such as Kullback-Leibler (KL), for training the network <ref type="bibr" target="#b15">(Geng, 2016)</ref>. A typical choice in the literature is to use a parametric uni-modal Gaussian to model label distribution, with the mean of the Gaussian encoding the true label, while the variance is set through validation or through prior knowledge. The main motivation behind this class of labeldistribution methods is to deal with the ambiguity and uncertainty of discrete labels, in tasks such as age estimation, head pose estimation, and semantic segmentation. Using prior distributions over the labels is also related to the well-known label-smoothing regularization for improving the training of deep networks <ref type="bibr" target="#b41">(Szegedy et al., 2016)</ref>. Such a regularization perturbs the hard label with a uniform distribution, embedding uncertainty in the ground truth. In <ref type="bibr" target="#b9">(Cheng et al., 2008)</ref>, the prior order of the labels is encoded using a step function. Instead of a standard one-hot encoding, a binary-vector encoding the labels is used as a target for network training. Another class of methods in literature penalize directly the deviations of the softmax predictions of the network from a uni-modal Gaussian, which is constrained to have the same mean as the true label and minimal variance <ref type="bibr" target="#b1">(Beckham and Pal, 2016;</ref><ref type="bibr" target="#b34">Pan et al., 2018)</ref>. These methods do not impose specific prior knowledge on the variance, but attempt to push it towards zero. In <ref type="bibr" target="#b2">(Beckham and Pal, 2017;</ref><ref type="bibr" target="#b11">Da Costa and Cardoso, 2005)</ref> specific parametric distributions are directly encoded within the network output, e.g., binomial or Poisson distributions. The network output is a single scalar used to model the output distribution and to infer the probability of each label. Other methods <ref type="bibr" target="#b29">(Liu et al., 2018;</ref><ref type="bibr" target="#b43">Xia et al., 2007)</ref> seek to reinforce the order between samples of adjacent labels within the feature space, not in the output space. However, such methods require considerable changes to the network architecture <ref type="bibr" target="#b29">(Liu et al., 2018)</ref>.</p><p>While the above mentioned techniques enforce the prior order and uni-modality of the output distributions, they have several shortcomings. The techniques in <ref type="bibr" target="#b14">(Gao et al., 2017;</ref><ref type="bibr" target="#b15">Geng, 2016;</ref><ref type="bibr" target="#b16">Geng et al., 2013;</ref><ref type="bibr" target="#b22">Huo et al., 2016)</ref> constrain the output distribution to have a specific form (or shape), following the choice of a parametric uni-modal model, e.g., Gaussian. This requires ad hoc (manual) setting of model parameters, e.g., the variance of a Gaussian, which might have a direct but unclear impact on the results. The models in <ref type="bibr" target="#b1">(Beckham and Pal, 2016;</ref><ref type="bibr" target="#b34">Pan et al., 2018)</ref> learn such variance parameter by pushing it towards zero, yielding a sharp Gaussian that approaches a Dirac function. Choosing a sharp or flat Gaussian has a direct impact on the labels and their order, but it is not clear how to make such a choice. In general, the choices that one has to make as to the form of the parametric model are task-dependent. Directly encoding a specific parametric distribution within the network output, as in <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref>, also requires several choices, including complex network architecture design and setting ad hoc parameters. In particular, the performance of the method in <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref> seems to depend strongly on a hyper-parameter that controls the variance of the distribution. Such a hyper-parameter should be set empirically with some care since its value changes the distribution shape from uniform to Gaussian. The method in <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref> was evaluated on only two ordinal datasets that have very few classes (5 and 8). The authors of <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref> stated that the method does not scale well to large numbers of labels, in particular, with the Poisson distribution, due to the nature of the latter. With this distribution choice, the label probabilities have a stair-like shape with a constant (deterministic) variation in each step.</p><p>As detailed in the next section, our approach circumvents the need to pre-define a parametric unimodal model for network outputs, and to set its parameters. We enforce uni-modality and a consistent order between the labels, but without constraining the learned representation to any specific parametric model, allowing the training to explore a larger space of solutions. To this end, we describe the uni-modality property through ordering adjacent labels, thereby ensuring decreasing monotonicity of the probabilities on both sides of the target label. Such an order is represented using a set of inequality constraints on all pairs of adjacent labels. The competing constraints are optimized with a powerful extension of the log-barrier method <ref type="bibr" target="#b6">(Boyd and Vandenberghe, 2004;</ref><ref type="bibr" target="#b26">Kervadec et al., 2019b)</ref>, which is well-known in the context of interior-point methods in convex optimization. Unlike <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref>, our method scales up to a very large number of labels. Furthermore, we provide a new metric, as a proxy, to asses the uni-modality of a distribution by measuring the order between adjacent labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Uni-modality via pairwise inequality constraints</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let us consider a set of training samples</head><formula xml:id="formula_0">D = {(X (i) , y (i) )} n i=1 where X (i)</formula><p>is an input sample, a realization of the discrete random variable X with support set X ; y (i) is the sample label, a realization of the discrete random variable y with support set Y = {1, · · · , c} that exhibits an overall order between the labels, y 1 ≺ y 2 ≺ . . . ≺ y c−1 ≺ y c , where a ≺ b means that the event described by the label a is ordered before the event described by the label b. In this work, we propose to use pairwise inequality constraints to enforce implicitly the uni-modality of the posterior probability, with the latter decreasing monotonically as we move further away from the target label. For notation simplicity and clarity in this section, we omit sample index (·) (i) and the expected value of losses over all the samples. We definep(y|X) as the posterior probability estimated by a neural-based model (function) M(X; θ). s ∈ R c denotes the logit scores 2 obtained by the model M, where the posterior probability is computed using standard softmax function,p(k|X) = exp(s k )/ c j=1 exp(s j ). Let dom f and ran f denote the domain and range of function f , respectively.</p><p>We describe the uni-modality of a function with respect to a target point (or label) as a decreasing monotonicity of the function above and below the point, according to some pre-defined order. To ensure such decreasing monotonicity in a non-parametric way, we embed hard pairwise constraints on the order between every two adjacent points, within each of two sets of points, one including those below the target and the other including those above. Instead of ordering probabilities, we consider ordering scores. For a sample (X, y), and its score vector s, we formulate adjacent ordering as a constrained-optimization problem using the following set of hard pairwise inequality constraints:</p><formula xml:id="formula_1">minimize θ C(M(X ; θ), y) subject to s k &lt; s k+1 , for k &lt; y , and s k+1 &lt; s k , for y ≤ k &lt; c ,<label>(1)</label></formula><p>where C(·, ·) is a standard classification loss such as the cross-entropy (CE): C(M(X; θ), y) = − logp(y|X). The cross-entropy will be used in our experiments, but our constrained optimization can be integrated with any other classification loss in a straightforward manner.</p><p>Our constrained optimization problem in Eq.(1) is very challenging for modern deep networks involving large numbers of trainable parameters <ref type="bibr" target="#b25">(Kervadec et al., 2019a;</ref><ref type="bibr" target="#b30">Márquez-Neila et al., 2017;</ref><ref type="bibr" target="#b36">Pathak et al., 2015;</ref><ref type="bibr" target="#b38">Ravi et al., 2019)</ref>. In the context of deep networks, hard constraints are typically addressed with basic penalty methods <ref type="bibr" target="#b20">(He et al., 2017;</ref><ref type="bibr" target="#b24">Jia et al., 2017;</ref><ref type="bibr" target="#b25">Kervadec et al., 2019a)</ref> as they accommodate SGD optimization, avoiding explicit primal-dual steps and projections. However, for a large set of constraints, penalty methods might have difficulty guaranteeing constraint satisfaction as they require careful and manual tuning of the weight of each constraint. In principle, standard Lagrangian-dual optimization seeks automatically the optimal weight of the constraints, and have well-established advantages over penalty methods, in the general context of convex optimization <ref type="bibr" target="#b6">(Boyd and Vandenberghe, 2004)</ref>. However, as shown and discussed in several recent deep learning works <ref type="bibr" target="#b30">(Márquez-Neila et al., 2017;</ref><ref type="bibr" target="#b36">Pathak et al., 2015;</ref><ref type="bibr" target="#b38">Ravi et al., 2019;</ref><ref type="bibr" target="#b25">Kervadec et al., 2019a;</ref>, in problems other than ordinal classification, those advantages do not materialize in practice for deep networks due mainly to the interplay between the dual steps and SGD optimization, causing instability, and to the incurred computational complexity. To solve our problem in Eq.</p><p>(1), we consider two alternatives to explicit Lagrangian-dual optimization: a penalty-based method and a powerful extension of the log-barrier method, which is well-known in the context of interior-point methods <ref type="bibr" target="#b6">(Boyd and Vandenberghe, 2004;</ref><ref type="bibr" target="#b26">Kervadec et al., 2019b)</ref>. In particular, the log-barrier method is well-suited to our problem. It approximates Lagrangian optimization via implicit dual variables, handling effectively large numbers of constraints, while accommodating standard SGD and avoiding explicit Lagrangian-dual steps and projections.</p><p>Penalty-based optimization: C in Eq.1 is augmented by converting each inequality constraint into a penalty term H that increases when the corresponding constraint is violated <ref type="bibr" target="#b5">(Bertsekas, 1995)</ref>. To impose the inequality constraint a &lt; b, a quadratic penalty is used,</p><formula xml:id="formula_2">H(δ b a ) = (δ b a + ) 2 if δ b a ≥ 0. 0 otherwise , where δ b a = a − b, a, b</formula><p>∈ R, and ∈ R * + is a slack constant to avoid the equality case. In this case, our problem in Eq.(1) becomes,</p><formula xml:id="formula_3">minimize θ C(M(X; θ), y) + λ y−1 j=1 H(∆ k+1 k (s)) + c−1 j=y H(∆ k k+1 (s)) ,<label>(2)</label></formula><p>where ∆ l m (s) = s m − s l , and λ ∈ R + is a model hyper-parameter, which balances the contribution of all the penalties encouraging constraint satisfaction in Eq. (2); and determined using a validation set. This method is referred to as PN (Eq.2).</p><p>Log-barrier optimization: While PN-based methods are simple and straightforward, they do not guarantee constraints satisfaction and require an empirical tuning of the importance coefficient(s) <ref type="bibr" target="#b13">(Fletcher, 1987;</ref><ref type="bibr" target="#b18">Gill et al., 1981)</ref>. Moreover, once a constraint is satisfied, the penalty is zero. Consequently, constraints that are satisfied in one iteration may not be satisfied in the next one since the penalty does not play a role of a barrier at the feasible set of solutions. This can be problematic when dealing with a large number of constraints at once. To avoid such well known issues with PN methods, we consider log-barrier methods (LB) as an alternative. LB-methods belong to interior-point methods (IP) <ref type="bibr" target="#b6">(Boyd and Vandenberghe, 2004)</ref>, which aim to approximate Lagrangian optimization with a sequence of unconstrained problems and implicit dual variable, avoiding dual steps and projections <ref type="bibr" target="#b6">(Boyd and Vandenberghe, 2004)</ref>. Eq.(1) can be re-written in a standard form of a LB method: minimize </p><p>LB methods are widely used for inequality constrained problems <ref type="bibr" target="#b6">(Boyd and Vandenberghe, 2004)</ref>. The main aim of LB methods is to convert a constrained problem of the form in <ref type="formula" target="#formula_4">(3)</ref>) into an unconstrained one via an indicator function I(·) that has zero penalty when the constraint is satisfied, and a penalty of +∞ otherwise. Instead of using I, LB methods employ an approximate,Î, using the logarithmic function, where the penalty decreases the further we get away from violating the inequality, forming a barrier between feasible and infeasible solutions. To solve Eq.(3), a strictly feasible set of parameters θ are required as a starting point. Such a set is found through Lagrangian minimization of inequality constraints (phase I <ref type="figure" target="#fig_1">(Boyd and Vandenberghe, 2004)</ref>), which turns out to be a problem of similar difficulty as the constrained optimization in Eq.</p><p>(3) <ref type="bibr" target="#b6">(Boyd and Vandenberghe, 2004)</ref>. Such strategy makes standard LB methods impractical for constraining deep models. We use an extension of the standard log-barrier based on a different approximation of the indicator function <ref type="bibr" target="#b26">(Kervadec et al., 2019b)</ref>. The main advantage is that this algorithm does not require starting from a feasible solution -i.e., domÎ is no longer restricted to feasible points of θ. A direct consequence is that stochastic optimization techniques such as SGD can be directly applied without the need for a feasible starting point. We replace problem Eq.(3) by the following sequence of unconstrained problems, parameterized by a temperature t:</p><formula xml:id="formula_5">minimize θ C(M(X; θ), y) + y−1 j=1Î (∆ k+1 k (s)) + c−1 j=yÎ (∆ k k+1 (s)) ,<label>(4)</label></formula><p>whereÎ is a log-barrier extension, which is convex, continuous, and twice differentiable <ref type="bibr" target="#b26">(Kervadec et al., 2019b)</ref>:Î</p><formula xml:id="formula_6">(r) = − 1 t log(−r) if r ≤ − 1 t 2 , tr − 1 t log( 1 t 2 ) + 1 t otherwise .<label>(5)</label></formula><p>One can show that optimizing log-barrier extensions approximate Lagrangian optimization of the original constrained problem with implicit dual variables, with a duality gap upper bounded by a factor of 1/t; see Proposition 2 in <ref type="bibr" target="#b26">(Kervadec et al., 2019b)</ref>. Therefore, in practice, we use a varying parameter t and optimize a sequence of losses of the form (4), increasing gradually the value of t by a constant factor. The network parameters evaluated at the current t and epoch are used as a starting point for the next t and epoch. In our experiments, we refer to this LB method as the extended log-barrier (ELB) (Eq.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Performance metrics: We denoteŷ as the label predicted by the model, E as the expectation value, and T as an evaluation dataset. For our evaluation, we report performances using two metrics: 1) The mean absolute error, which is often used in OC setups: MAE = E (X,y)∈T |ŷ − y| ; and 2)</p><p>We propose a new metric that measures how well labels are ordered, above and below the true or predicted label. Following the non-monotonic index defined in <ref type="bibr" target="#b4">(Ben-David, 1995;</ref><ref type="bibr" target="#b19">Gutiérrez and García, 2016)</ref>, we propose the Side-Order Index (SOI) that counts the number of satisfactions (non-violations) of the order constraints over adjacent labels above and below a reference label ν,</p><formula xml:id="formula_7">SOI ν = E(X,y)∈T 1 c−1 ( ν−1 j=1 1 ∆ j+1 j (p)&lt;0 + c−1 j=ν 1 ∆ j j+1 (p)&lt;0</formula><p>) . This metric can be seen as a proxy to describe how well a distribution is uni-modal with respect to a reference label ν. Furthermore, it is appropriate for evaluating the performance of a constrained-optimization method for our problem in (1) as it evaluates constraint satisfaction. We compute the expected value over a normalized measure so that the metric is independent of the total number of labels, hence, independent of the number of pairs of c − 1 adjacent labels. Consequently, the range of the measure is in [0, 1], where 0 indicates that all the adjacent labels are un-ordered, and 1 indicates a perfect order. In our experiments, we consider the case where the reference label ν is the predicted label. It is noted SOIŷ. In this case, we measure the consistency of the model's predictions with respect to the predicted label and independently from the true label. This measure is the most important as it assesses the model's consistency when it is evaluated in a real scenario, where the true label is unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and training protocol:</head><p>We consider datasets that naturally exhibit order between the labels. We target datasets that have large number of labels. Applying OC over datasets with very small number of labels is unlikely to show the power and limitation of different methods. We consider 3 different applications: breast cancer grading, photographs dating, and age estimation using public benchmarks. (1) Breast cancer grading: This task consists in classifying histology images of breast biopsy into different grades that are ordered with respect to the cancer aggressivity. We use the dataset BACH (Part A) Breast cancer <ref type="bibr" target="#b0">(Aresta et al., 2019)</ref> referred to here as ICIAR. The dataset contains a total of 400 images, and 4 classes: normal, benign, in Situ, and invasive (in this order). Following the protocol described in <ref type="bibr" target="#b40">Rony et al., 2019)</ref>, we perform two splits of the dataset where in each split we take 50% of samples per class for test, and perform 5-fold cross-validation to build the train/validation sets. (2) Photographs dating: This task consists of predicting in which decade a color image was taken. We consider the dataset Historical Color Image dataset (for classification by decade) 3 <ref type="bibr" target="#b33">(Palermo et al., 2012)</ref>, referred to here as HCI, along with the experimental protocol in <ref type="bibr" target="#b33">(Palermo et al., 2012)</ref>. The dataset contains 5 decades from 1930s to 1970s, each containing 265 images, for a total of 1325 images. We took 50 random images per decade to form a test set, with the rest of the data used for training and validation sets using 10-fold cross-validation. This process is repeated 10 times. (3) Age estimation: This task consists of estimating someone's age based on their picture. We consider three datasets: (q) FG-NET dataset 4 <ref type="bibr" target="#b35">(Panis et al., 2016)</ref>, referred to here as FGNET: It is a very early database used for age estimation, which contains 1,002 face images from 82 individuals, with ages ranging from 0 to 69 (70 classes); (b) Asian Face Age Dataset (AFAD) 5 <ref type="bibr" target="#b32">(Niu et al., 2016)</ref> light (AFAD-Lite), with a total of 59,344 samples, and age ranges from 15 to 39, and a total number of classes of 22; and (c) AFAD-Full <ref type="bibr" target="#b32">(Niu et al., 2016)</ref>, with a total of 165,501 samples, an age ranges from 15 to 72, and a total number of classes of 58. The same protocol is conducted over the three datasets. Following the experimental setup in <ref type="bibr" target="#b7">(Chang et al., 2011;</ref><ref type="bibr" target="#b8">Chen et al., 2013;</ref><ref type="bibr" target="#b32">Niu et al., 2016;</ref><ref type="bibr" target="#b42">Wang et al., 2015)</ref>, we randomly select 20% of the entire dataset for testing, and perform 5-fold cross-validation to build train and validation sets. This process is repeated 10 times. All the splits are done randomly using a deterministic code that we provide publicly along with the splits. We report the mean and standard deviation of each metric.</p><p>For a fair comparison, all the methods use exactly the same training setup including the shuffling, the order of processing samples, and the experimental environment (device and code). We use a pre-trained Resnet18 <ref type="bibr" target="#b21">(He et al., 2016)</ref> as a model M, with WILDCAT <ref type="bibr" target="#b12">(Durand et al., 2017)</ref> pooling layer and hyper-parameters kmax = 0.1, α = 0. Randomly cropped patches of size 256 × 256 are used for training. We train for 1000 epochs using SGD with a learning rate of 0.001, 6 which is decayed every 100 epochs by 0.1, with an allowed minimum value of 1e − 7, a batch size of 8, a momentum of 0.9 and weight decay of 1e − 5. For PN (Eq.2), λ is selected using validation from the set {1e − 1, 1e − 2, 1e − 3, 1e − 4, 1e − 5}, and set to 1e − 2; = 1e − 1. For LB methods, t is initialized to 1, and iteratively increased after each epoch by a factor of 1.001, with an allowed maximum value of 5. Due to the large size of AFAD-Lite-Full datasets, training is done using a batch size of 64 for 100 epochs. 7 For AFAD-Full, t is initialized to 4.5 and increased by a factor of 1.01. For the case of age estimation task, we do not use any face detector, face cropping, nor face alignment. We feed the network the raw image. For efficient computation and scalability, we implement the difference ∆ l m (s) (Eq.2, 3) using 1D convolution with fixed weights [+1, −1] as a differentiator from left to right. The difference in the other direction is the same as the one from left to right but with sign (−).  <ref type="bibr" target="#b9">(Cheng et al., 2008)</ref> 0.23 ± 0.028 54.94 ± 1.38 0.63 ± 0.03 57.08 ± 1.46 LD <ref type="bibr" target="#b15">(Geng, 2016)</ref> 0.49 ± 0.03 91.02 ± 1.33 1.00 ± 0.04 84.42 ± 1.33 MV <ref type="bibr" target="#b34">(Pan et al., 2018)</ref> 0.61 ± 0.04 62.99 ± 2.09 0.69 ± 0.03 71.82 ± 1.93 PO <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref>  The quantitative results obtained with the different methods over the different test sets are presented in Tabs.1-Tab.2. We recall the notation of the different methods -CE: cross-entropy method; REN <ref type="bibr" target="#b9">(Cheng et al., 2008)</ref>: re-encode the hard target into a vector of binary values and use the mean squared error as a loss. The threshold is set to 0.5; LD <ref type="bibr" target="#b14">(Gao et al., 2017;</ref><ref type="bibr" target="#b15">Geng, 2016;</ref><ref type="bibr" target="#b16">Geng et al., 2013;</ref><ref type="bibr" target="#b22">Huo et al., 2016)</ref>: label distribution learning with Bayes rule prediction. The variance is set to 1; MV <ref type="bibr" target="#b34">(Pan et al., 2018)</ref>: mean-variance loss combined with softmax where the predicted label is the round function of the expected value of the labels. Following <ref type="bibr" target="#b34">(Pan et al., 2018)</ref>, we set λ 1 = 0.2, λ 2 = 0.05; PO <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref>: Hard-wire Poisson distribution at the network output, and use cross-entropy for learning. τ is fixed and set to 1 as in the paper <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref>, and the prediction is based on the expected value of the labels; PN (Eq.2): penalty-based 5 http://afad-dataset.github.io/ 6 Except for HCI dataset, where we use a learning rate of 0.0001. 7 For AFAD-Full, PN and ELB are trained only for 80 epochs due to time constraint.  <ref type="bibr" target="#b9">(Cheng et al., 2008)</ref> 3.00 ± 0.01 58.91 ± 1.16 3.19 ± 0.02 55.97 ± 0.30 4.53 ± 0.44 53.32 ± 0.34 LD <ref type="bibr" target="#b15">(Geng, 2016)</ref> 5.03 ± 0.02 66.52 ± 0.19 5.15 ± 0.028 61.82 ± 0.12 18.25 ± 0.86 51.60 ± 0.20 MV <ref type="bibr" target="#b34">(Pan et al., 2018)</ref> 2.96 ± 0.02 83.32 ± 0.95 3.20 ± 0.02 70.56 ± 0.58 5.21 ± 0.65 59.72 ± 1.95 PO <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref>   <ref type="bibr" target="#b21">(He et al., 2016)</ref>), except PO <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref> where we add a dense layer that maps from c (number of classes) into one (positive score). Based on the obtained results, we note the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and discussion</head><p>(1) In term of SOIŷ consistency, we observe that ELB method yields the best results, achieving almost perfect ordering SOIŷ &gt; 97% over all datasets. Then, comes second the PN method, but with a large gap of 60% &lt; SOIŷ &lt; 84% in comparison to ELB, and a slightly better performance than CE (54% &lt; SOIŷ &lt; 80%). We note that the gap between PN and ELB increases with the number of classes: ∼ 15%, ∼ 16% on ICIAR and HCI with 4 and 5 classes, respectively; ∼ 19%, ∼ 19% on AFAD-Lite with 22 classes, and AFAD-Full with 58 classes, respectively; and ∼ 38% on FGNET with 70 classes. This is expected, since the PN method does not cope well with the interplay between different constraints, more so when the number of constraints is large unlike LB methods which approximate Lagrangian optimization. The different methods REN, LD, MV, and PO yield a SOIŷ not far from the CE method. LD achieves good results over ICIAR and HCI with SOIŷ of ∼ 91% and ∼ 84% ranking in the second place after ELB. However, its performance drops to ∼ 66%, ∼ 61% over AFAD-Lite -Full, and to ∼ 51% over FGNET suggesting that it does not handle well large number of classes. Compared to LD, MV performs better on large number of classes achieving ∼ 83%, ∼ 70%, ∼ 59% over AFAD-Lite -Full, and FGNET, respectively. REN performance is usually worse than CE. The case of PO is particular. It is expected to obtain SOIŷ = 100% but since the predicted label is the expected value of the label (and not the argmax of the scores), it achieves a low level of performance. When PO uses argmax, it does not represent a fair comparison to other methods since the order in PO is hardwired, while the order is learned with all other methods. PO <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref> does not scale up to large numbers of classes, and, in <ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref>, it was evaluated only on two datasets, with 5 and 8 classes. These results show the amount of disorder with a CE prediction in an OC setup, which confirms its inadequacy to such context. Furthermore, we observe the power of LB methods compared to PN method, with the former achieving much better constraint satisfaction than the latter. This shows the effectiveness of LB methods and, in particular, its extension <ref type="bibr" target="#b26">(Kervadec et al., 2019b)</ref>, for optimizing with inequality constraints on a model output <ref type="figure" target="#fig_1">(Fig.2).</ref> (2) In term of MAE, in all the datasets, we observe that combining CE with inequality constraints always helps improving the performance. Over ICIAR and HCI datasets, ELB obtained the best performance with MAE = 0.17, 0.62, respectively. Over AFAD-Lite, the MV method yields MAE = 2.96, while REN obtains a state-of-the-art error over AFAD-Full with MAE = 3.19, compared to MAE = 3.34 reported in <ref type="bibr" target="#b32">(Niu et al., 2016)</ref>. In both datasets, ELB ranks third with MAE = 3.15, 3.40, respectively. All our experiments are repeated 10 times, while <ref type="bibr" target="#b32">(Niu et al., 2016)</ref> repeats experiments for 100 times. On the FGNET dataset, which has 70 labels, ELB obtained the best performance of MAE = 4.27, followed by REN, and MV with MAE = 4.53, 5.21, respectively. Combining the inequality constraints with the CE to promote a consistent output prediction helps always to improve the MAE performance.</p><p>(3) Training time: PN and LB methods do not add a significant computation overhead compared to CE.</p><p>(4) Which method to choose in practice?: this is an important question and the answer depends mostly on the specific application. Based on the above empirical evidence we suggest that: (a) for critical applications where an agent is used who expects an explanation for the model's decision (e.g., in the medical domain, where model interpretability/consistency is a priority), our method is a better choice; (b) For applications where the MAE performance is a priority (e.g., control and automatic applications) without agent, other methods can be a good choice. However, our method can be considered as well since it yields competitive MAE. (c) for applications where both metrics are crucial, our method is a reasonable choice. Independently from the chosen method, the proposed SOI metric provides a helpful tool for the agent to quickly and reliably assess the prediction's consistency along with the PD visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a new constrained-optimization formulation for ordinal classification, with uni-modality of the label distribution imposed implicitly via a set of inequality constraints over pairs of adjacent labels. To tackle the ensuing challenging optimization problem, we solve a sequence of unconstrained losses based on a powerful extension of the log-barrier method, which is well-known in the context of interior-point methods. This accommodates standard SGD, and avoids computationally expensive Lagrangian dual steps and projections, while outperforming substantially standard penalty methods.</p><p>Our non-parametric model is more flexible than the existing ordinal classification techniques: it does not restrict the learned representation to a specific parametric model, allowing the training to explore larger spaces of solutions and removing the need for ad hoc choices, while scaling up to large numbers of labels. It can be used in conjunction with any standard classification loss and any deep architecture. We also propose a new performance metric for ordinal classification, as a proxy to measure a distribution uni-modality, referred to as the Sides Order Index (SOI). We report comprehensive evaluations and comparisons to state-of-the-art methods on benchmark public datasets for several ordinal classification tasks, showing the merits of our approach in terms of label consistency and scalability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>&lt; 0, for k &lt; y, and ∆ k k+1 (s) &lt; 0, for y ≤ k &lt; c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Moving average of MAE and SOIŷ metrics over the validation set (one run over fold-0, split-0) of AFAD-Full (top-left), FGNET (top-right), AFAD-Lite (bottom-left), and ICIAR (bottomright) for CE, PN and ELB. (Best visualized in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of different methods over the test sets of ICIAR and HCI datasets (classification datasets).</figDesc><table><row><cell>Method</cell><cell></cell><cell>ICIAR</cell><cell></cell><cell>HCI</cell></row><row><cell></cell><cell>MAE</cell><cell>SOIŷ (%)</cell><cell>MAE</cell><cell>SOIŷ (%)</cell></row><row><cell>CE</cell><cell>0.19 ± 0.02</cell><cell>80.05 ± 1.68</cell><cell>0.68 ± 0.05</cell><cell>78.05 ± 1.67</cell></row><row><cell>REN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of different methods over the test sets of AFAD-Lite, AFAD-Full, and FGNET datasets. −− indicates that the method does not scale to a large number of classes.</figDesc><table><row><cell>Method</cell><cell cols="2">AFAD-Lite</cell><cell cols="2">AFAD-Full</cell><cell></cell><cell>FGNET</cell></row><row><cell></cell><cell>MAE</cell><cell>SOIŷ (%)</cell><cell>MAE</cell><cell>SOIŷ (%)</cell><cell>MAE</cell><cell>SOIŷ (%)</cell></row><row><cell>CE</cell><cell>3.69 ± 0.06</cell><cell>68.67 ± 1.00</cell><cell>3.73 ± 0.06</cell><cell>63.69 ± 0.47</cell><cell>7.79 ± 0.79</cell><cell>54.11 ± 1.18</cell></row><row><cell>REN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">June 15, 2020Belharbi et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The experiments in<ref type="bibr" target="#b2">(Beckham and Pal, 2017)</ref> were limited to 8 labels</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For generality, we assume that such scores are unbounded.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://graphics.cs.cmu.edu/projects/historicalColor/ 4 https://yanweifu.github.io/FG_NET_data/index.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bach: Grand challenge on breast cancer histology images. medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aresta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="122" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A simple squared-error reformulation for ordinal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno>abs/1612.00775</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unimodal probability distributions for deep ordinal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="411" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Min-max entropy for weakly supervised pointwise localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mccaffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<idno>abs/1907.12934</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monotonicity maintenance in information-theoretic machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Athena scientific. Nonlinear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ordinal hyperplanes ranker with cost sensitivities for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A neural network approach to ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pollastri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Joint Con. on Neural Networks</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1279" to="1284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pranking with ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classification of ordinal data using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Da Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="690" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5957" to="5966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Practical methods of optimization john wiley &amp; sons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page">80</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep label distribution learning with label ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2825" to="2838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1734" to="1748" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Facial age estimation by learning from label distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2401" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Penalized ordinal regression methods for predicting stage of cancer in high-dimensional covariate spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Gentry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jackson-Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Archer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17277</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Practical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<publisher>Academic Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Current prospects on ordinal and monotonic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="171" to="179" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to play in a day: Faster deep reinforcement learning by optimality tightening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep age distribution learning for apparent age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facial emotion distribution learning by exploiting low-rank label correlations locally</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9841" to="9850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Constrained deep weak supervision for histopathology image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE tran. on medical imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2376" to="2388" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Constrained-cnn losses for weakly supervised segmentation. medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="88" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Constrained deep networks: Lagrangian optimization via log-barrier extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<idno>abs/1904.04205</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ordrec: An ordinal model for predicting personalized item rating distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conf. on Recommender Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ordinal deep feature learning for facial age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Con. on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="157" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A constrained deep neural network for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wai Kin Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keong</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="831" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imposing hard constraints on deep networks: Promises and limitations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Márquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Negative Results in Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Architecture selection strategies for neural networks: Application to corporate bond rating prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Utans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks in the capital markets</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="277" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ordinal regression with multiple output cnn for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4920" to="4928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dating historical color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Palermo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="499" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mean-variance loss for deep age estimation from a face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5285" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Overview of research on facial ageing using the fg-net ageing database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Panis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsapatsoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iet Biometrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1796" to="1804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the consistency of ordinal regression methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1769" to="1803" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Explicitly imposing constraints in deep networks via conditional gradients gives improved generalization and faster convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lokhande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Loss functions for preference levels: Regression with discrete ordered labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI multidisciplinary workshop on advances in preference handling</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="180" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep weakly-supervised learning methods for classification and localization in histology images: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mccaffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<idno>abs/1909.03354</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deeply-learned feature for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="534" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recursive feature extraction for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Con. on Neural Networks</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Structured and sparse annotations for image emotion distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Emotion distribution recognition from facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">annual con. on multimedia</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Ordinal distribution regression for gait-based age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<idno>abs/1905.11005</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
