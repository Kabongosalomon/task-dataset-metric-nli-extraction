<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Learning Deep Representation for Face Alignment with Auxiliary Attributes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
						</author>
						<title level="a" type="main">TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Learning Deep Representation for Face Alignment with Auxiliary Attributes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Face Alignment</term>
					<term>Face Landmark Detection</term>
					<term>Deep Learning</term>
					<term>Convolutional Network !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this study, we show that landmark detection or face alignment task is not a single and independent problem. Instead, its robustness can be greatly improved with auxiliary information. Specifically, we jointly optimize landmark detection together with the recognition of heterogeneous but subtly correlated facial attributes, such as gender, expression, and appearance attributes. This is non-trivial since different attribute inference tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, which not only learns the inter-task correlation but also employs dynamic task coefficients to facilitate the optimization convergence when learning multiple complex tasks. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing face alignment methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art methods based on cascaded deep model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Face alignment, or detecting semantic facial landmarks (e.g., eyes, nose, mouth corners) is a fundamental component in many face analysis tasks, such as facial attribute inference <ref type="bibr" target="#b0">[1]</ref>, face verification <ref type="bibr" target="#b1">[2]</ref>, and face recognition <ref type="bibr" target="#b2">[3]</ref>. Though great strides have been made in this field (see Sec. 2), robust facial landmark detection remains a formidable challenge in the presence of partial occlusion and large head pose variations ( <ref type="figure" target="#fig_0">Fig. 1)</ref>.</p><p>Landmark detection is traditionally approached as a single and independent problem. Popular approaches include template fitting approaches <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and regression-based methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. More recently, deep models have been applied too. For example, Sun et al. <ref type="bibr" target="#b12">[13]</ref> propose to detect facial landmarks by coarse-to-fine regression using a cascade of deep convolutional neural networks (CNN). This method shows superior accuracy compared to previous methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref> and existing commercial systems. Nevertheless, the method requires a complex and unwieldy cascade architecture of deep model.</p><p>We believe that facial landmark detection is not a standalone problem, but its estimation can be influenced by a number of heterogeneous and subtly correlated factors. Changes on a face are often governed by the same rules determined by the intrinsic facial structure. For instance, when a kid is smiling, his mouth is widely opened (the second image in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). Effectively discovering and exploiting such an intrinsically correlated facial attribute would help in detecting the mouth corners more accurately. Also, the inter-ocular distance is smaller in faces with large yaw rotation (the first image in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). Such pose information can be leveraged as an additional source of information to constrain the solution space of landmark estimation. Indeed, the input and solution spaces of face alignment can be effectively divided given auxiliary face attributes. In a small experiment, we average a set of face images according to different attributes, as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>), where the frontal and smiling faces show the mouth corners, while there are no specific details for the image averaged over the whole dataset. Given the rich auxiliary information, treating facial landmark detection in isolation is counterproductive.</p><p>This study aims to investigate the possibility of optimizing facial landmark detection (the main task) by leveraging auxiliary information from attribute inference tasks. Potential auxiliary tasks include head pose estimation, gender classification, age estimation <ref type="bibr" target="#b14">[15]</ref>, facial expression recognition, or facial attribute inference <ref type="bibr" target="#b15">[16]</ref>. Given the multiple tasks, deep convolutional network appears to be a viable model choice since it allows for joint features learning and multi-objective inference. Typically, one can formulate a cost function that encompasses all the tasks and use the cost function in the network back-propagation learning. We show that this conventional multi-task learning scheme is challenging in our problem. There are several reasons. First, the different tasks of face alignment and attribute inference are inherently different in learning difficulties. For instance, learning to identify "wearing glasses" attribute is easier than determining if one is smiling. Second, we rarely have auxiliary tasks with similar number of positive/negative cases. For instance, male/female classification enjoys more balanced samples than facial expression recognition. As a result, different tasks have different convergence   <ref type="bibr" target="#b12">[13]</ref>, and the proposed Tasks-Constrained Deep Convolutional Network (TCDCN). More accurate detection can be achieved by optimizing the detection task jointly with related/auxiliary tasks. (b) Average face images with different attributes. The image in blue rectangle is averaged among the whole training faces, while the one in red is from the smiling faces with frontal pose. It indicates that the input and solution space can be effectively divided into subsets, which are in different distributions. This lowers the learning difficulty.</p><p>rates. In many cases we observe that the joint learning with a specific auxiliary task improves the convergence of landmark detection at the beginning of the training procedure, but become ineffective when the auxiliary task training encounters local minima or over-fitting.</p><p>Continuing the training with all tasks jeopardizes the network convergence, leading to poor landmark detection performance.</p><p>Our study is the first attempt to demonstrate that face alignment can be jointly optimized with the inference of heterogeneous but subtly correlated auxiliary attributes. We show that the supervisory signal of auxiliary tasks can be back-propagated jointly with that of face alignment to learn the underlying regularities of face representation. Nonetheless, the learning is non-trivial due to the different natures and convergence rates of different tasks. Our key contribution is a newly proposed Tasks-Constrained Deep Convolutional Network (TCDCN), with new objective function to address the aforementioned challenges. In particular, our model considers the following aspects to make the learning effective:</p><p>• Dynamic task coefficient -Unlike existing multi-task deep models <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> that treat all tasks as equally important, we assign and weight each auxiliary task with a coefficient, which is adaptively and dynamically adjusted based on training and validation errors achieved so far in the learning process. Thus a task that is deemed not beneficial to the main task is prevented from contributing to the network learning. This approach can be seen as a principled way of achieving "early stopping" on specific task. In the experiments, we show that the dynamic task coefficient is essential to reach the peak performance for face alignment. • Inter-task correlation modeling -We additionally model the relatedness of heterogeneous tasks in a covariance matrix in the objective function. Different from the dynamic task coefficient that concerns on the learning convergence, inter-task correlation modeling helps better exploiting the relation between tasks to achieve better feature learning.</p><p>All the network parameters, including the filters, dynamic task coefficients, and inter-task correlation are learned automatically using a newly proposed alternating optimization approach. Thanks to the effective shared representation learned from multiple auxiliary attributes, the proposed approach outperforms other deep learning based approaches for face alignment, including the cascaded CNN model <ref type="bibr" target="#b12">[13]</ref> on five facial point detection. We demonstrate that shared representation learned by a TCDCN for sparse landmarks can be readily transferred to handle an entirely different configuration with more landmarks, e.g. 68 points in the 300-W dataset <ref type="bibr" target="#b19">[20]</ref>. With the transferred configuration, our method further outperforms other existing methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> on the challenging 300-W dataset, as well as the Helen <ref type="bibr" target="#b22">[23]</ref> and COFW <ref type="bibr" target="#b7">[8]</ref> dataset.</p><p>In comparison to our earlier version of this work <ref type="bibr" target="#b23">[24]</ref>, we introduce the new dynamic task coefficient to generalize the original idea of task-wise early stopping <ref type="bibr" target="#b23">[24]</ref> (discussed in Sec. 3.1). Specifically, we show that the dynamic task coefficient is a relatively more effective mechanism to facilitate the convergence of a heterogeneous task network. In addition, we formulate a new objective function that learns different tasks and their correlation jointly, which further improves the performance and allows us to analyze the usefulness of auxiliary tasks more comprehensively. Apart from the methodology, the paper was also substantially improved by providing more technical details and more extensive experimental evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Facial landmark detection: Conventional facial landmark detection methods can be divided into two categories, namely regression-based method and template fitting method. A regression-based method estimates landmark locations explicitly by regression using image features. For example, Valstar et al. <ref type="bibr" target="#b24">[25]</ref> predict landmark location from local image patch with support vector regression. Cao et al. <ref type="bibr" target="#b8">[9]</ref> and Burgos-Artizzu et al. <ref type="bibr" target="#b7">[8]</ref> employ cascaded fern regression with pixel-difference features. A number of studies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> use random regression forest to cast votes for landmark location based on local image patch with Haar-like features. Most of these methods refine an initial guess of the landmark location iteratively, the first guess/initialization is thus critical. By contrast, our deep model takes raw pixels as input without the need of any facial landmark initialization. Importantly, our method differs in that we exploit auxiliary tasks to facilitate landmark detection learning.</p><p>A template fitting method builds face templates to fit input images <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Part-based model has recently been used for face fitting <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Zhu and Ramanan <ref type="bibr" target="#b4">[5]</ref> show that face detection, facial landmark detection, and pose estimation can be jointly addressed. Our method differs in that we do not limit the learning of specific tasks, i.e. the TCDCN is readily expandable to be trained with additional auxiliary tasks. Specifically, apart from pose, we show that other facial attributes such as gender and expression, can be useful for learning a robust landmark detector. Another difference to <ref type="bibr" target="#b4">[5]</ref> is that we learn feature representation from raw pixels rather than pre-defined HOG as face descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Landmark detection by deep learning:</head><p>The methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b31">[32]</ref> that use deep learning for face alignment are close to our approach. The methods usually formulate the face alignment as a regression problem and use multiple deep models to locate the landmarks in a coarse-to-fine manner, such as the cascaded CNN by Sun et al. <ref type="bibr" target="#b12">[13]</ref>. The cascaded CNN requires a pre-partition of faces into different parts, each of which are processed by separate deep CNNs. The resulting outputs are subsequently averaged and channeled to separate cascaded layers to process each facial landmark individually. Similarly, Zhang et al. <ref type="bibr" target="#b11">[12]</ref> uses successive auto-encoder networks to perform coarse-to-fine alignment. Instead, our model requires neither pre-partition of faces nor cascaded networks, leading to drastic reduction in model complexity, whilst still achieving comparable or even better accuracy. This opens up possibility of application in computational constrained scenario, such as the embedded systems. In addition, the use of auxiliary task can reduce the overfitting problem of deep model because the local minimum for different tasks might be in different places. Another important difference is that our method performs feature extraction in the whole face image automatically, instead of handcraft local regions. Learning multiple tasks in neural network: Multitask learning (MTL) is the process of learning several tasks simultaneously with the aim of mutual benefit. This is an old idea in machine learning. Caruana <ref type="bibr" target="#b32">[33]</ref> provides a good overview focusing on neural network. Deep model is well suited for learning multiple tasks since it allows for joint features learning and multi-objective inference. Joint learning of multiple tasks has also proven effective in many computer vision problems <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, existing deep models <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b33">[34]</ref> are not suitable to solve our problem because they assume similar learning difficulties and convergence rates across all tasks. For example, in the work of <ref type="bibr" target="#b18">[19]</ref>, the algorithm simultaneously learns a human pose regressor and multiple body-part detectors. This algorithm optimizes multiple tasks directly without learning the task correlation. In addition, it uses predefined task coefficients in the iterative learning process. Applying this method on our problem leads to difficulty in learning convergence, as shown in Sec. 4. We mitigate this shortcoming by introducing dynamic task coefficients in the deep model. This new formulation generalizes the idea of early stopping. Early stopping of neural network can date back to the work of Caruana <ref type="bibr" target="#b32">[33]</ref>, but it is heuristic and limited to shallow multilayer perceptrons. The scheme is also not scalable for a large quantity of tasks. Different from the work of <ref type="bibr" target="#b34">[35]</ref>, which learns the task priority to handle outlier tasks, the dynamic task coefficient in our approach is based on the training and validation error, and aims to coordinate tasks of different convergence rates. We show that dynamic task coefficient is important for joint learning multiple objectives in deep convolutional network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LANDMARK DETECTION WITH AUXILIARY ATTRIBUTES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We cast facial landmark detection as a nonlinear transformation problem, which transforms the raw pixels of a face image to the positions of dense landmarks. The proposed framework is illustrated in <ref type="figure">Fig.  2</ref>, showing that the highly nonlinear function is modeled A 60 × 60 image is taken as input. In the first layer, we convolve it with 20 different 5 × 5 filters, using a stride of 1. The obtained feature map is 56 × 56 × 20, which is subsampled to 28 × 28 × 20 with a 2 × 2 max-pooling operation. Similar operations are repeated in layer 2, 3, 4, as the parameters shown in the figure. The last layer is fully-connected. Then the output is obtained by regression.</p><p>as a DCN, which is pre-trained by five landmarks and then fine-tuned to predict the dense landmarks. Since dense landmarks are expensive to label, the pre-training step is essential because it prevents DCN from overfitting to small dataset. In general, the pre-training and fine-tuning procedures are similar, except that the former step initializes filters by a standard normal distribution, while the latter step initializes filters using the pretrained network.</p><p>As shown in <ref type="figure">Fig. 2</ref>, DCN extracts a high-level representation x ∈ R D×1 on a face image I using a set of filters</p><formula xml:id="formula_0">K = {k s } S s=1 , x = φ(I|K), where φ(·)</formula><p>is the nonlinear transformation learned by DCN. With the extracted feature x, we jointly estimate landmarks and attributes, where landmark detection is the main task and attribute prediction is the auxiliary task. Let {y m } M m=1 denote a set of real values, representing the x-,y-coordinate of the landmarks, and let {l t } T t=1 denote a set of binary labels of the face attributes, ∀l t ∈ {0, 1}. Specifically, M equals 5 × 2 = 10 in the pre-training step, implying that the landmarks include two centers of the eyes, nose, and two corners of the mouth. M represents the number of dense landmarks in the fine-tuning step, such as M = 194 × 2 = 388 in Helen dataset <ref type="bibr" target="#b22">[23]</ref> and M = 68 × 2 = 136 in 300-W dataset <ref type="bibr" target="#b19">[20]</ref>. This work investigates the effectiveness of 22 attributes in landmark detection, i.e. T = 22.</p><p>Both landmark detection and attribute prediction can be learned by the generalized linear models <ref type="bibr" target="#b35">[36]</ref>.</p><formula xml:id="formula_1">Suppose W = [w y 1 , w y 2 , ..., w y M , w l 1 , w l 2 , ..., w l T ] be a weight matrix, W ∈ R D×(M +T )</formula><p>, where each column vector corresponds to the parameters of a single task. For example, w y 2 ∈ R D×1 indicates the parameter vector for the y-coordinate of the first landmark. With these parameters, we have</p><formula xml:id="formula_2">y m = w y m T x + y m ,<label>(1)</label></formula><p>where y m represents an additive random error variable that is distributed according to a normal distribution with mean zero and variance σ 2 m , i.e. y m ∼ N (0, σ 2 m ). Similarly, each w l t represents the parameter vector of the t-th attribute, which is model as</p><formula xml:id="formula_3">l t = w l t T x + l t ,<label>(2)</label></formula><p>where l t is distributed following a standard logistic distribution, i.e. l t ∼ Logistic(0, 1). If all the tasks are independent, W can be simply modeled as a product of the multivariate normal distribution, i.e. ∀w y m , w l t ∼ N (0, ε 2 I), where 0 is a D × 1 zero vector, I denote a D × D identity matrix, and ε 2 is a D × 1 vector representing the diagonal elements of the covariance matrix. However, this work needs to explore which auxiliary attribute is crucial to landmark detection, implying that we need to model the correlations between tasks. Therefore, we assume W is distributed according to a matrix normal distribution <ref type="bibr" target="#b36">[37]</ref></p><formula xml:id="formula_4">, i.e. W ∼ MN D×(M +T ) (0, Υ, ε 2 I), where 0 is a D × (M + T ) zero matrix and Υ is a (M + T ) × (M + T )</formula><p>task covariance matrix. The matrix Υ is learned in the training process and can naturally capture the correlation between the weight of different tasks.</p><p>As landmark detection and attribute prediction are heterogenous tasks, different auxiliary attribute behaves differently in the training procedure. They may improve the convergence of landmark detection at the beginning of the training procedure, but may become ineffective as training proceeds when local minima or over-fitting is presented. Thus, each auxiliary attribute is assigned with a dynamic task coefficient λ t , t = 1...T , which is adjusted adaptively during training. λ t is distributed according to a normal distribution with mean µ t and variance σ 2</p><formula xml:id="formula_5">t , i.e. λ t ∼ N (µ t , σ 2 t ),</formula><p>where we assume σ 2 t = 1 and µ t is determined based on the training and validation errors (detailed in Sec. 3.3).</p><p>It is worth pointing out that in in the early version of this work <ref type="bibr" target="#b23">[24]</ref>, we introduce a task-wise early stopping scheme to halt a task after it is no longer beneficial to the main task. This method is heuristic and the criterion to determine when to stop learning a task is empirical. In addition, once a task is halted, it will never resume during the training process. In contrast to this earlier proposal, the dynamic task coefficient is dynamically updated. Thus a halted task may be resumed automatically if it is found useful again during the learning process. In particular, the dynamic task coefficient has no single optimal solution across the whole learning process. Instead, its value is updated to fit the current training status.</p><p>In summary, given a set of face images and their labels, we jointly estimate the filters K, the weight matrix W, the task covariance matrix Υ, and the dynamic coefficients Λ = {λ t } T t=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Formulation</head><p>The above problem can be formulated as a probabilistic framework. Given a data set with N training samples,</p><formula xml:id="formula_6">denoted as {I, Y, L}, where I = {I i } N i=1 , Y = {{y m i } M m=1 } N i=1 , and L = {{l t i } T t=1 } N i=1</formula><p>, and a set of parameters {K, W, Υ, Λ}, we optimize the parameters by maximizing a posteriori probability (MAP)</p><formula xml:id="formula_7">K * , W * , Υ * , Λ * = argmax K,W,Υ,Λ p(K, W, Υ, Λ|I, Y, L). (3) Eqn.(3) is proportional to p(K, W, Υ, Λ|I, Y, L) ∝ p(Y|I, K, W M )p(L|I, K, W T , Λ)· p(W|Υ)p(Λ)p(K),<label>(4)</label></formula><p>where the first two terms are the likelihood probabilities and the last three terms are the prior probabilities. Moreover, W M and W T represent the first M columns and the last T columns of W, respectively. In the following, we will introduce each term of Eqn.(4) in detail.</p><p>• The likelihood probability p(Y|I, K, W M ) measures the accuracy of landmark detection. As discussed in Eqn. <ref type="bibr" target="#b0">(1)</ref>, each variable of landmark position can be modeled as a linear regression plus a Gaussian noise. The likelihood can be factorized as</p><formula xml:id="formula_8">p(Y|I, K, W M ) = N i=1 M m=1 N (w y m T x i , σ 2 m ).<label>(5)</label></formula><p>• The likelihood probability p(L|I, K, W T , Λ) measures the accuracy of attribute prediction. As introduced in Eqn. <ref type="bibr" target="#b1">(2)</ref>, each binary attribute is predicted by a linear function plus a logistic distributed random noise, implying that the probability of l t i is a sigmoid function, which is p</p><formula xml:id="formula_9">(l t i = 1|x i ) = f (w l t T x i ), where f (x) = 1/(1 + exp{−x})</formula><p>. Thus, the likelihood can be defined as product of Bernoulli distributions</p><formula xml:id="formula_10">p(L|I, K, W T , Λ) ∝ N i=1 T t=1 {p(l t i = 1|x i ) l t i 1 − p(l t i = 1|x i ) 1−l t i } λt .<label>(6)</label></formula><p>• The prior probability of the weight matrix, p(W|Υ), is modeled by a matrix normal distribution with mean zero <ref type="bibr" target="#b36">[37]</ref>, which is able to capture the correlations between landmark detection and auxiliary attributes. It is written as</p><formula xml:id="formula_11">p(W|Υ) = exp − 1 2 tr[(ε 2 I) −1 WΥ −1 W T ] (2π) D(M +T ) 2 |ε 2 I| M +T 2 |Υ| D 2 ,<label>(7)</label></formula><p>where tr(·) calculates the trace of a matrix and Υ is a positive semi-definite matrix modeling the task covariance, denoted as Υ 0, Υ ∈ R (M +T )×(M +T ) . Referring to Eqn. <ref type="bibr" target="#b6">(7)</ref>, the variance between the mth landmark and the t-th attribute is obtained by</p><formula xml:id="formula_12">D d=1 W (d,m) Υ −1 (m,m+t) W (d,m+t)</formula><p>, where W (d,m) denotes the element in the d-th row and m-th column, showing that the relation of a pair of tasks is measured by their corresponding weights with respect to each feature dimension d. For instance, if two different tasks select or reject the same set of features, they are highly correlated. More clearly, Eqn. <ref type="formula" target="#formula_11">(7)</ref> is a matrix form of the multivariate normal distribution. They are equivalent if W is reshaped as a long vector.</p><p>• The prior probability of the tasks' dynamic coefficients is defined as a product of the normal distributions,</p><formula xml:id="formula_13">p(Λ) = T t=1 N (µ t , σ 2 t ),</formula><p>where the mean is adjustable based on the training and validation errors. It has significant difference with the task covariance matrix. For example, the auxiliary attribute 'wearing glasses' is probably related to the landmark positions of eyes. Their relation can be measured by Υ. However, if 'wearing glasses' converges more quickly than the other tasks, it becomes ineffective because of local minima or over-fitting. Therefore, its dynamic coefficient could be decreased to avoid these side-effects.</p><p>• The DCN filters can be initialized as a standard multivariate normal distribution as previous methods <ref type="bibr" target="#b37">[38]</ref> did. In particular, we define</p><formula xml:id="formula_14">p(K) = S s=1 p(k s ) = S s=1 N (0, I).</formula><p>By taking the negative logarithm of Eqn.(4) and combining Eqn. <ref type="bibr" target="#b4">(5)</ref>, <ref type="bibr" target="#b5">(6)</ref>, and <ref type="formula" target="#formula_11">(7)</ref>, we obtain the MAP objective function</p><formula xml:id="formula_15">argmin K,W,Λ,Υ 0 N i=1 M m=1 (y m i − w y m T x i ) 2 − N i=1 T t=1 λ t l t i ln f (w l t T x i ) + (1 − l t i ) ln 1 − f (w l t T x i ) + tr(WΥ −1 W T ) + D ln |Υ| + S s=1 k T s k s + T t=1 (λ t − µ t ) 2 .<label>(8)</label></formula><p>Eqn.(8) contains six terms. For simplicity of discussion, we remove the terms that are constant. We also assume the variance parameters such as σ m , ∀m = 1...M , σ t , ∀t = 1...T , and ε equal one. Thus, the regularization parameters of the above terms are comparable and can be simply ignored.</p><p>Eqn. <ref type="formula" target="#formula_15">(8)</ref> can be minimized by updating one parameter with the remaining parameters fixed. First, although the first three terms are likely to be jointly convex with respect to W, x i in the first two terms is a highly nonlinear transformation with respect to K, i.e. x i = Φ(I i |K). In this case, no global optima are guaranteed. Therefore, following the optimization strategies of CNN <ref type="bibr" target="#b38">[39]</ref>, we apply stochastic gradient descent (SGD) <ref type="bibr" target="#b37">[38]</ref> with weight decay <ref type="bibr" target="#b39">[40]</ref> to search the suitable local optima for both W and K. This method has been demonstrated working reasonably well in practice <ref type="bibr" target="#b37">[38]</ref>. Here, the fifth term can be considered as the weight decay of the filters. Second, the third term in Eqn.(8) is a convex function regarding Υ, but the fourth term is concave since negative logarithm is a convex function. In other words, learning Υ directly is a convex-concave problem <ref type="bibr" target="#b40">[41]</ref>. However, with a well-known lemma <ref type="bibr" target="#b41">[42]</ref>, ln |Υ| has a convex upper bound, ln |Υ| ≤ tr(Υ) − M − T . Thus, the fourth term can be replaced by Dtr(Υ). Both the third and the fourth terms are now convex regarding Υ. Finally, since the dynamic coefficients in Eqn.(8) are linear and independent, finding each λ t has a closed form solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Algorithm</head><p>We solve the MAP problem in an iterative manner. First, we jointly update the DCN filters K and the weight matrix W with the tasks' dynamic coefficients Λ and covariance matrix Υ fixed. Second, we update the covariance matrix Υ by fixing all the other parameters with their current values. Third, we update Λ in a similar way to the second step.</p><p>In the first step, we optimize W and K in the DCN and fix Υ and Λ with their current values. In this case, the fourth and the last terms in Eqn.(8) are constant and thus can be removed. We write the loss function in a matrix form as follows</p><formula xml:id="formula_16">E(I) = N i=1 ||y i − W T M x i || 2 − T i diag(Λ) ln f (W T T x i ) − (1 − i ) T diag(Λ) ln 1 − f (W T T x i ) + tr(WΥ −1 W T ) + tr(KK T ),<label>(9)</label></formula><p>where y is a M ×1 vector, and , 1 are both T ×1 vectors. diag(Λ) represents a diagonal matrix with λ 1 , ..., λ T being the values in the diagonal. The fourth term in Eqn. <ref type="bibr" target="#b8">(9)</ref> can be considered as the parameterized weight decay of W, while the last term is the weight decay of the filters K, i.e. tr(KK T ) = S s=1 k T s k s . Eqn.(9) combines the least square loss and the cross-entropy loss to learn the DCN, which can be optimized by SGD <ref type="bibr" target="#b37">[38]</ref>, since they are defined over individual sample. <ref type="figure">Fig. 2</ref> illustrates the architecture of DCN, containing four convolutional layers and one fully-connected layer. This architecture is a tradeoff between accuracy of landmark detection and computational cost, and it works well in practice. Note that the learning method introduced in this work is naturally compatible with any deep network structure, but exploring them is out of the scope of this paper. Now we introduce the learning procedure. At the very beginning, each column of W and each filter of K are initialized according to a multivariate standard normal distribution. To learn the weight matrix W, we calculate its derivative, ∆W = −η ∂E ∂W = −η ∂E ∂o ∂o ∂f ∂f ∂W , where o and η denote the network outputs (predictions) and the step size of the gradient descent, respectively. By simple derivation, we have</p><formula xml:id="formula_17">∂E ∂W M = x i (y i − o i ) T ,<label>(10)</label></formula><formula xml:id="formula_18">∂E ∂W T = x i ( i − o i ) T diag(Λ),<label>(11)</label></formula><p>where o i is the corresponding tasks' predictions. For example, o i = W T M x i in Eqn. <ref type="bibr" target="#b9">(10)</ref> indicates the predictions of the landmark positions, while o i = f (W T T x i ) in Eqn. <ref type="bibr" target="#b10">(11)</ref> indicates the predictions of auxiliary attributes. In summary, the entire weight matrix in the (j + 1)-th iteration is updated by</p><formula xml:id="formula_19">W j+1 = W j − η 1 ∂E ∂Wj − η 2 (2W j Υ −1 )</formula><p>, where η 1 , η 2 are the regularization parameters of the gradient and the weight decay.</p><p>To update filters K, we propagate the errors of DCN from top to bottom, following the well-known backpropagation (BP) strategy <ref type="bibr" target="#b42">[43]</ref>, where the gradient of each filter is computed by the cross-correlation between the corresponding input channel and the error map <ref type="bibr" target="#b38">[39]</ref>. In particular, at the fully-connected layer as shown in <ref type="figure">Fig. 2</ref>, the errors are obtained by first summing over the losses of both landmark detection and attribute predictions, and then the sum is multiplied by the transpose of the weight matrix. For each convolutional layer, the errors are achieved by the de-convolution <ref type="bibr" target="#b38">[39]</ref> between its filters and the back-propagated errors. Several pairs of face images and their features obtained by filters K are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, which shows that the learned features are robust to large poses and expressions. For example, the features of smiling faces or faces have similar poses exhibit similar patterns.</p><p>In the second step, we optimize the covariance matrix Υ with W, K, and Λ fixed. As discussed in Eqn. <ref type="bibr" target="#b7">(8)</ref>, the logarithm of Υ can be relaxed by its upper bound. The optimization problem for finding Υ then becomes</p><formula xml:id="formula_20">min Υ tr(WΥ −1 W T ) s.t. Υ 0, tr(Υ) ≤ η.<label>(12)</label></formula><p>For simplicity, we assume η = 1. Problem (12) with respect to Υ is a naive semi-definite programming problem and has a simple closed form solution, which is Υ = (W T W) <ref type="bibr">1 2</ref> tr (W T W) <ref type="bibr">1 2</ref> .</p><p>In the third step, we update the dynamic coefficients Λ with W, K, and Υ fixed. By ignoring the constant terms in Eqn. <ref type="bibr" target="#b7">(8)</ref>, the optimization problem becomes</p><formula xml:id="formula_21">min Λ 1 N N i=1 T t=1 −λ t {l t i ln f (w l t T x i ) + (1 − l t i ) ln 1 − f (w l t T x i ) } + 1 2 T t=1 (λ t − µ t ) 2 , s.t. 1 ≥ λ t ≥ , t = 1, 2, ..., T<label>(13)</label></formula><p>where is a small constant close to zero. Each λ t has a analytical solution, which is λ t = min 1, max ,</p><formula xml:id="formula_22">µ t + 1 N N i=1 l t i ln f (w l t T x i ) + (1 − l t i ) ln 1 − f (w l t T x i )</formula><p>, implying that each dynamic coefficient is determined by its expected value and the loss value averaged over N training samples. Here, we can define µ t similar to the task-wise early stopping <ref type="bibr" target="#b23">[24]</ref>. Suppose the current iteration is j, let E t val (j), and E t tr (j) be the values of the loss function of task t on the validation set and training set, respectively. We can have</p><formula xml:id="formula_23">µ t = ρ × E t val (j − τ ) − E t val (j) E t val (j − τ ) × E t tr (j − τ ) − E t tr (j) E t tr (j − τ ) ,<label>(14)</label></formula><p>where ρ is a constant scale factor, and τ controls a training strip of length τ . The second term in Eqn. <ref type="bibr" target="#b13">(14)</ref> represents the tendency of the validation error. If the validation error drops rapidly within a period of length τ , the value of the first term is large, indicating that training should be emphasized as the task is valuable. Similarly, the third term measures the tendency of the training error. We can see that the task-wise early stopping strategy proposed in <ref type="bibr" target="#b23">[24]</ref> can be treated as a special case of the dynamic coefficient λ t . In addition, we does not need a tuned threshold to decide whether to stop a task as <ref type="bibr" target="#b23">[24]</ref>, and we can provide better performance (see Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Transferring TCDCN for Dense Landmarks</head><p>After training the TCDCN model on sparse landmarks and auxiliary attributes, it can be readily transferred from sparse landmark detection to handle more landmark points, e.g. 68 points as in 300-W dataset <ref type="bibr" target="#b19">[20]</ref>.</p><p>In particular, we initialize the network (i.e. , the lower part of <ref type="figure">Fig. 2</ref>) with the learned shared representation and fine-tune using a separate training set only labeled with dense landmark points. Since the shared representation of the pre-trained TCDCN model already captures the information from attributes, the auxiliary tasks learning is not necessary in the fine-tuning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION AND EXPERIMENTS</head><p>Network Structure. <ref type="figure">Fig. 2</ref> shows the network structure of TCDCN. The input of the network is 60×60 gray-scale face image (normalized to zero-mean and unit-variance). The feature extraction stage contains four convolutional layers, three pooling layers, and one fully connected layer. The kernels in each convolutional layer produce multiple feature maps. The commonly used rectified linear unit is selected as the activation function. For the pooling layers, we conduct max-pooling on nonoverlap regions of the feature map. The fully connected layer following the fourth convolutional layer produces a feature vector that is shared by the multiple tasks in the estimation stage. Evaluation metrics: In all cases, we report our results on two popular metrics <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b26">[27]</ref>, i.e. mean error and failure rate. The mean error is measured by the distances between estimated landmarks and the ground truths, and normalized with respect to the inter-ocular distance. Mean error larger than 10% is reported as a failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Multi-Attribute Facial Landmark (MAFL) <ref type="bibr" target="#b0">1</ref> : To facilitate the training of TCDCN, we construct a new dataset by annotating 22 facial attributes on 20,000 faces randomly chosen from the Celebrity face dataset <ref type="bibr" target="#b43">[44]</ref>. The attributes are listed in <ref type="table" target="#tab_2">Table 1</ref> and all the attributes are binary, indicating the attribute is presented or not. We divide the attributes into four groups to facilitate the following analyses. The grouping criterion is based on the main face region influenced by the associated attributes. In addition, we divide the face into one of five categories according to the degree of yaw rotation. This results in the fifth group named as "head pose". All the faces in the dataset are accompanied with five facial landmarks locations (eyes, nose, and mouth corners), which are used as the target of the face alignment task. We randomly select 1,000 faces for testing and the rest for training. Example images are provided in <ref type="figure" target="#fig_0">Fig. 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotated Facial Landmarks in the Wild (AFLW) [45]:</head><p>AFLW contains 24,386 face images gathered from Flickr. This dataset is selected because it is more challenging than other conventional datasets, such as BioID <ref type="bibr" target="#b45">[46]</ref> and LFPW <ref type="bibr" target="#b13">[14]</ref>. Specifically, AFLW has larger pose variations (39% of faces are non-frontal in our testing images) and severe partial occlusions. Each face is annotated with 21 landmarks at most. Some landmarks are not annotated due to out-of-plane rotation or occlusion. We randomly select 3,000 faces for testing. <ref type="figure" target="#fig_0">Fig. 12</ref> depicts some examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caltech Occluded Faces in the Wild (COFW) [8]:</head><p>This dataset is collected from the web. It is designed to present faces in occlusions due to pose, the use of accessories (e.g., sunglasses), and interaction with objects (e.g., food, hands). This dataset includes 1,007 faces, annotated with 29 landmarks, as shown in <ref type="figure" target="#fig_0">Fig. 15</ref>. Helen [23]: Helen contains 2,330 faces from the web, annotated densely with 194 landmarks <ref type="figure" target="#fig_0">(Fig. 13)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>300-W [20]:</head><p>This dataset is well-known as a standard benchmark for face alignment. It is a collection of 3,837 faces from existing datasets: LFPW <ref type="bibr" target="#b13">[14]</ref>, AFW <ref type="bibr" target="#b4">[5]</ref>, Helen <ref type="bibr" target="#b22">[23]</ref> and XM2VTS <ref type="bibr" target="#b46">[47]</ref>. It also contains faces from an additional subset called IBUG, consisting images with difficult poses and expressions for face alignment, as shown in <ref type="figure" target="#fig_0">Fig. 16</ref>. Each face is densely annotated with 68 landmarks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training with Dynamic Task Coefficient</head><p>Dynamic task coefficient is essential in TCDCN to coordinate the learning of different tasks with different convergence rates. To verify its effectiveness, we train the proposed TCDCN with and without this technique. <ref type="figure">Fig. 4 (a)</ref> plots the main task's error of the training and validation sets up to 200,000 iterations. Without dynamic task coefficient, the training error converges slowly and exhibits substantial oscillations. In contrast, convergence rates of both the training and validation sets are fast and stable when using the proposed dynamic task coefficient.</p><p>In addition, we illustrate the dynamic task coefficients of two attributes in <ref type="figure">Fig. 4 (b)</ref>. We observe that the values of their coefficients drop after a few thousand iterations, preventing these auxiliary tasks from overfitting. The coefficients may increase when these tasks become effective in the learning process, as shown by the sawtooth-like pattern of the coefficient curves. These two behaviours work together, facilitating the smooth convergence of the main task, as shown in <ref type="figure">Fig. 4 (a)</ref>. In addition, we compare the dynamic tasks coefficient with the task-wise early stopping proposed in the earlier version of this work <ref type="bibr" target="#b23">[24]</ref>. As shown in <ref type="table" target="#tab_3">Table 2</ref>, dynamic task coefficient achieves better performance than the task-wise early stopping scheme. This is because the new method is more dynamic in coordinating the different auxiliary tasks across the whole training process (see Sec. 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inter-task Correlation Learning</head><p>To investigate how the auxiliary tasks help facial landmark detection, we study the learned correlation between these tasks and the facial landmarks. In particular, as we have learned the task covariance matrix Υ, given the relation between correlation matrix and covariance matrix, we can compute the correlation between any two tasks, by normalizing their covariance with the square root of the product of their variances. iteration (×1000) training error with dynamic task coefficient validation error with dynamic task coefficient training error without dynamic task coefficient validation error without dynamic task coefficient <ref type="figure">Fig. 4. (a)</ref> Facial landmark localization error curve with and without dynamic task coefficient. The error is measured in L2-norm with respect to the ground truth of the 10 coordinates values (normalized to [0,1]) for the 5 landmarks. (b) Task coefficients for the "Big Nose" and "Arched Eyebrows" attributes over the training process.</p><p>In <ref type="figure">Fig. 5</ref>, we present the learned correlation between the attribute groups and facial landmarks. In particular, for each attribute group, we compute the average absolute value of the correlation with the five facial landmarks, respectively. It is shown that for the group of "mouth", the correlations with the according landmarks (i.e. , mouth corners) are higher than the others. Similar trends can be observed in the group of "nose" and "eyes". For the group of "global", the correlations are roughly even for different landmarks because the attributes are determined by the global face structure. The correlation of the "pose" group is much higher than that of the others. This is because the head rotation directly affects the landmark distribution. Moreover, in <ref type="figure">Fig. 6</ref>, we randomly choose one attribute from each attribute group and visualize its correlation to other landmarks. For clarification, for each attribute, we normalize the correlation among the landmarks (i.e. , the sum of the correlation on the five landmarks equals one). We can also observe that the attributes are more likely to be correlated to its according landmarks.</p><p>In addition, we visualize the learned correlation between the auxiliary tasks in <ref type="figure" target="#fig_7">Fig. 7</ref>. Because the attributes of "Left Profile", "Left", "Frontal", "Right", "Right Profile" are mutually exclusive (i.e., only one attribute can be positive for a face) and describe the yaw rotation, we aggregate these five attributes as one attribute (i.e. "pose"), by computing the average absolute correlation with other attributes. One can observe some intuitive results in this <ref type="figure">figure.</ref> For examples, the head pose is unrelated to other attributes; "Heavy Makeup" has high positive correlation with "Attractive", and high negative correlation with "Male". In <ref type="table" target="#tab_3">Table 2</ref>, we show the mean errors of facial landmark localization on MAFL  dataset with and without inter-task correlation learning (without correlation learning means that we simply apply multiple tasks as targets and do not use the term of Υ in Eq. <ref type="formula" target="#formula_15">(8)</ref>). It demonstrates the effectiveness of task correlation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluating the Effectiveness of Auxiliary Task</head><p>To further examine the influence of auxiliary tasks more comprehensively, we evaluate different variants of the proposed model. In particular, the first variant is trained only on facial landmark detection. We train another five model variants on facial landmark detection along with the auxiliary tasks in the groups of "eyes", "nose", "mouth", "global", "head pose", respectively. In addition, we synthesize a task with random objective and train it along with the facial landmark detection task, which results in the sixth model variant. The full model is trained using all the attributes. For simplicity, we name each variant by facial landmark detection (FLD) and the auxiliary tasks, such as "FLD only", "FLD+eyes", "FLD+pose", "FLD+all". It is evident from <ref type="figure">Fig. 8</ref> that optimizing landmark detection with auxiliary tasks is beneficial. In particular, "FLD+all" outperforms "FLD" by a large margin, with a reduction of over 7% in failure rate. When single auxiliary task group is present, "FLD+pose" and "FLD+global" perform better than the others. This is not surprising since the pose variation affects locations of all landmarks directly and the "global' attribute group influences the whole face region. The other auxiliary tasks such as "eyes" and "mouth" are   observed to have comparatively smaller influence to the final performance, since they mainly capture local information of the face. As for "FLD+random" the performance is hardly improved. This result shows that the main task and auxiliary task need to be related for any performance gain in the main task. In addition, we show the relative improvement caused by different groups of attributes for each landmark in <ref type="figure">Fig. 9</ref>. In particular, we define relative improvement = reduced error original error , where original error is produced by the model of "FLD only". We can observe a trend that each group facilitates the landmarks in the according face region. For example, for the group of "mouth", the benefits are mainly observed at the corners of mouth. This observation is intuitive since attributes like smiling drives the lower part of the faces, involving Zygomaticus and levator labii superioris muscles, more than the upper facial region. The learning of these attributes develops a shared representation that describes lower facial region, which in turn facilitates the localization of corners of mouth. Similarly, the improvement of eye location is much more significant than mouth and nose for the attribute group of "eye". However, we observe the group of "nose" improves the eye and mouth localization remarkably. This is mainly because the nose is in the central of the face, there exists constrain between the nose location and other landmarks. The horizontal coordinate of the nose is likely to be the mean of the eyes in frontal face. As for the group of "pose" and "global", the improvement is significant in all landmarks. <ref type="figure" target="#fig_0">Fig. 10</ref> depicts improvements led by adding "eye" and "mouth" attributes. <ref type="figure" target="#fig_0">Fig. 12</ref> shows more example results, demonstrating the effectiveness on various face appearances of TCDCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with Deep Learning based Methods</head><p>Although the TCDCN, cascaded CNN <ref type="bibr" target="#b12">[13]</ref> and CFAN <ref type="bibr" target="#b11">[12]</ref> are built upon deep model, we show that the proposed model can achieve better detection accuracy with lower computational cost and model complexity. We use the full model "FLD+all", and the publicly available binary code of cascaded CNN <ref type="bibr" target="#b12">[13]</ref> and CFAN <ref type="bibr" target="#b11">[12]</ref> in this experiment. Landmark localization accuracy: In this experiment, we employ the testing images of MAFL and AFLW <ref type="bibr" target="#b44">[45]</ref> for evaluation. It is observed from <ref type="figure" target="#fig_0">Fig. 11</ref> that the overall accuracy of the proposed method is superior to that of cascaded CNN and CFAN.  <ref type="figure" target="#fig_0">Fig. 11</ref>. Cumulative error curves of the proposed method, cascaded CNN <ref type="bibr" target="#b12">[13]</ref> and CFAN <ref type="bibr" target="#b11">[12]</ref>, on the MAFL and AFLW <ref type="bibr" target="#b44">[45]</ref> dataset <ref type="bibr">(5 landmarks)</ref>. The number in the legend indicates the according mean error (×10 −2 ). Model complexity: The proposed method only has one CNN, whereas the cascaded CNN <ref type="bibr" target="#b12">[13]</ref> deploys multiple CNNs in different cascaded layers (23 CNNs in its implementation). Also, for each CNN, both our method and cascaded CNN <ref type="bibr" target="#b12">[13]</ref> have four convolutional layers and two fully connected layers, with comparable input image size. However, the convolutional layer in <ref type="bibr" target="#b12">[13]</ref> uses locally unshared kernels. Hence, TCDCN has much lower computational cost and model complexity. The cascaded CNN requires 0.12s to process an image on an Intel Core i5 CPU, whilst TCDCN only takes 18ms, which is 7 times faster. Also, the TCDCN costs 1.5ms on a NVIDIA GTX760 GPU. Similarly, the complexity is larger in CFAN <ref type="bibr" target="#b11">[12]</ref> due to the use of multiple auto-encoders, each of which contains fully connected structures in all layers. <ref type="table" target="#tab_5">Table 3</ref> shows the details of the running time and network complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Face Representation Transfer and Comparison with State-of-the-art Methods</head><p>As we discussed in Section 3.4, we can transfer the trained TCDCN to handle more landmarks beyond the five major facial points. The main idea is to pretrain a TCDCN on sparse landmark annotations and multiple auxiliary tasks, followed by fine-tuning with dense landmark points. We compare against various state-of-the-arts. The first class of methods use regression methods that directly predict the facial landmarks: (1) Robust Cascaded Pose Regression (RCPR) <ref type="bibr" target="#b7">[8]</ref>; (2) Explicit Shape Regression (ESR) <ref type="bibr" target="#b8">[9]</ref>; (3) Supervised Descent Method (SDM) <ref type="bibr" target="#b20">[21]</ref>; (4) Regression based on Local Binary Features (LBF) <ref type="bibr" target="#b21">[22]</ref>; (5) Regression based on Ensembles of Regression Trees <ref type="bibr" target="#b47">[48]</ref> (ERT); (6) Coarse-to-Fine Auto-Encoder Networks (CFAN) <ref type="bibr" target="#b11">[12]</ref> (as this method can predict dense  <ref type="figure" target="#fig_0">Fig. 13</ref>. Example alignment results on Helen <ref type="bibr" target="#b22">[23]</ref>.</p><p>landmarks, we compare with it again); (7) Coarse-to-Fine Shape Searching (CFSS) <ref type="bibr" target="#b48">[49]</ref>. The second class of methods employ a face template: (8) Tree Structured Part Model (TSPM) <ref type="bibr" target="#b4">[5]</ref>, which jointly estimates the head pose and facial landmarks; (9) A Cascaded Deformable Model (CDM) <ref type="bibr" target="#b5">[6]</ref>; (10) STASM <ref type="bibr" target="#b49">[50]</ref>, which is based on Active Shape Model <ref type="bibr" target="#b50">[51]</ref>; <ref type="bibr" target="#b10">(11)</ref> Componentbased ASM <ref type="bibr" target="#b22">[23]</ref>; <ref type="bibr" target="#b11">(12)</ref> Robust discriminative response map fitting (DRMF) method <ref type="bibr" target="#b30">[31]</ref>; (13) Gauss-newton deformable part models (GN-DPM) <ref type="bibr" target="#b6">[7]</ref>; In addition, we compare with the commercial face analysis software: <ref type="bibr" target="#b13">(14)</ref> Face++ API <ref type="bibr" target="#b51">[52]</ref>. For the methods of RCPR <ref type="bibr" target="#b7">[8]</ref>, SDM <ref type="bibr" target="#b20">[21]</ref>, CFAN <ref type="bibr" target="#b11">[12]</ref>, TSPM <ref type="bibr" target="#b4">[5]</ref>, CDM <ref type="bibr" target="#b5">[6]</ref>, STASM <ref type="bibr" target="#b49">[50]</ref>, DRMF <ref type="bibr" target="#b30">[31]</ref>, and Face++ <ref type="bibr" target="#b51">[52]</ref>, we use their publicly available implementation. For the methods which include their own face detector (like TSPM <ref type="bibr" target="#b4">[5]</ref> and CDM <ref type="bibr" target="#b5">[6]</ref>), we avoid detection errors by cropping the image around the face. For methods that do not release the code, we report their results on the related literatures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on Helen [23]:</head><p>Helen consists of 2,000 training and 330 testing images as specified in <ref type="bibr" target="#b22">[23]</ref>.</p><p>In particular, the 194-landmark annotation is from the original dataset and the 68-landmark annotation is from <ref type="bibr" target="#b19">[20]</ref>. <ref type="table" target="#tab_6">Table 4</ref> reports the performance of the competitors and the proposed method. Most of the images are in high resolution and the faces are nearfrontal. Although our method just uses the input of 60 × 60 grey image, it still achieves better result. <ref type="figure" target="#fig_0">Fig. 13</ref> visualizes some of our results. It can be observed that driven by rich facial attributes, our model can capture various facial expression accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on 300-W [20]:</head><p>We follow the same protocol in <ref type="bibr" target="#b21">[22]</ref>: the training set contains 3,148 faces, including  the AFW, the training sets of LFPW, and the training sets of Helen. The test set contains 689 faces, including IBUG, the testing sets of LFPW, and the testing sets of Helen. <ref type="table" target="#tab_7">Table 5</ref> demonstrates the superior of the proposed method. In particular, for the challenging subset (IBUG faces) TCDCN produces a significant error reduction of over 10% in comparison to the state-of-the-art <ref type="bibr" target="#b48">[49]</ref>. As can be seen from <ref type="figure" target="#fig_0">Fig. 16</ref>, the proposed method exhibits superior capability of handling difficult cases with large head rotation and exaggerated expressions, thanks to the shared representation learning with auxiliary tasks. <ref type="figure" target="#fig_0">Fig. 17</ref> shows more results of the proposed method on Helen <ref type="bibr" target="#b22">[23]</ref>, IBUG <ref type="bibr" target="#b19">[20]</ref>, and LPFW <ref type="bibr" target="#b13">[14]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on COFW [8]:</head><p>The testing protocol is the same as <ref type="bibr" target="#b7">[8]</ref>, where the training set includes LFPW <ref type="bibr" target="#b13">[14]</ref> and 500 COFW faces, and the testing set includes the remaining 507 COFW faces. This dataset is more challenging as it is collected to emphasize the occlusion cases. The quantitative evaluation is reported in <ref type="figure" target="#fig_0">Fig. 14.</ref> Example results of our algorithm are depicted in <ref type="figure" target="#fig_0">Fig. 15</ref>.</p><p>It is worth pointing out that the proposed method achieves better performance than RCPR <ref type="bibr" target="#b7">[8]</ref> even that we do not explicitly learn and detect occlusions as <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Instead of learning facial landmark detection in isolation, we have shown that more robust landmark detection can be achieved through joint learning with heterogeneous but subtly correlated auxiliary tasks, such as appearance attribute, expression, demographic, and head pose. The proposed Tasks-Constrained DCN allows errors of auxiliary tasks to be back-propagated in deep hidden layers for constructing a shared representation to be for the method of TSPM <ref type="bibr" target="#b4">[5]</ref>, CDM <ref type="bibr" target="#b5">[6]</ref>, ESR <ref type="bibr" target="#b8">[9]</ref>, SDM <ref type="bibr" target="#b20">[21]</ref>, Face++ <ref type="bibr" target="#b51">[52]</ref>, RCPR <ref type="bibr" target="#b7">[8]</ref> and the proposed method. <ref type="figure" target="#fig_0">Fig. 15</ref>. Example alignment results on COFW <ref type="bibr" target="#b7">[8]</ref> dataset. relevant to the main task. We also show that by learning dynamic task coefficient, we can utilize the auxiliary tasks in a more efficient way. Thanks to learning with the auxiliary attributes, the proposed model is more robust to faces with severe occlusions and large pose variations compared to existing methods. We have observed that a deep model needs not be cascaded <ref type="bibr" target="#b12">[13]</ref> to achieve the better performance. The lighter-weight CNN allows real-time performance without the usage of GPU or parallel computing techniques. Future work will explore deep learning with auxiliary information for other vision problems. TCDCN <ref type="figure" target="#fig_0">Fig. 16</ref>. Results of ESR <ref type="bibr" target="#b8">[9]</ref>, SDM <ref type="bibr" target="#b20">[21]</ref>, LBF <ref type="bibr" target="#b21">[22]</ref> and our method on the IBUG faces <ref type="bibr" target="#b19">[20]</ref>. <ref type="figure" target="#fig_0">Fig. 17</ref>. Example alignment results on Helen <ref type="bibr" target="#b22">[23]</ref>, IBUG <ref type="bibr" target="#b19">[20]</ref>, and LPFW <ref type="bibr" target="#b13">[14]</ref> datasets (68 landmarks). Red rectangles indicate wrong cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Examples of facial landmark detection by a single conventional CNN, the cascaded CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Fig. 2 .</head><label>22</label><figDesc>Structure specification for TCDCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The TCDCN learns shared features for facial landmark detection and auxiliary tasks. The first row shows the face images and the second row shows the corresponding features in the shared feature space, where the face images with similar poses and attributes are close with each other. This reveals that the learned feature space is robust to pose, expression, and occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Correlation of each attribute group with different landmarks. Normalized correlation of the attributes with different landmarks. The attributes are randomly selected from each attribute group. The correlation is normalized among the five landmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Pairwise correlation of the auxiliary tasks learned by TCDCN (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Improvement over different landmarks by different attribute groups. Examples of improvement by attribute group of "eye" and "mouth".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Example alignment results on MAFL dataset (the first row), and AFLW<ref type="bibr" target="#b44">[45]</ref> datasets (the second row). Red rectangles indicate wrong cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Mean errors on COFW<ref type="bibr" target="#b7">[8]</ref> dataset(29 landmarks)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1408.3967v4 [cs.CV] 11 Aug 2015</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pose</cell><cell>smiling</cell><cell>not smiling</cell><cell>wearing glasses</cell><cell>not wearing glasses</cell></row><row><cell></cell><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Cascaded CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>all</cell></row><row><cell></cell><cell>TCDCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Auxiliary Attributes</cell><cell>Wearing glasses smiling gender pose</cell><cell>× × female right profile</cell><cell>× √ male frontal</cell><cell>√ × female frontal</cell><cell>× × female left</cell><cell>√ × male frontal</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 Annotated</head><label>1</label><figDesc>Face Attributes in MAFL Dataset</figDesc><table><row><cell>Group</cell><cell>Attributes</cell></row><row><cell>eyes</cell><cell>bushy eyebrows, arched eyebrows, narrow eyes, bags under eyes, eyeglasses</cell></row><row><cell>nose</cell><cell>big nose, pointy nose</cell></row><row><cell>mouth</cell><cell>mouth slightly open, no beard, smiling, big lips, mustache</cell></row><row><cell>global</cell><cell>gender, oval face, attractive, heavy makeup, chubby</cell></row><row><cell>head pose</cell><cell>frontal, left, left profile, right, right profile</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Comparison of mean error (×10 −2 ) on MAFL dataset</cell></row><row><cell cols="3">under different network configurations</cell></row><row><cell></cell><cell>without inter-task</cell><cell>with inter-task</cell></row><row><cell></cell><cell>correlation learning</cell><cell>correlation learning</cell></row><row><cell>task-wise early stopping [24]</cell><cell>8.35</cell><cell>8.21</cell></row><row><cell>dynamic task coefficient</cell><cell>8.07</cell><cell>7.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">Comparison of Different Deep Modes for Facial</cell></row><row><cell cols="2">Landmark Detection</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">#Parameter Time (per face)</cell></row><row><cell>Cascaded CNN [13]</cell><cell>∼990K</cell><cell>120ms</cell></row><row><cell>CFAN [12]</cell><cell>∼18,500K</cell><cell>30ms</cell></row><row><cell>TCDCN</cell><cell>∼100K</cell><cell>18ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Mean errors (%) on Helen [23] dataset</figDesc><table><row><cell>Method</cell><cell cols="2">194 landmarks 68 landmarks</cell></row><row><cell>STASM [50]</cell><cell>11.1</cell><cell>-</cell></row><row><cell>CompASM [23]</cell><cell>9.10</cell><cell>-</cell></row><row><cell>DRMF [31]</cell><cell>-</cell><cell>6.70</cell></row><row><cell>ESR [9]</cell><cell>5.70</cell><cell>-</cell></row><row><cell>RCPR [8]</cell><cell>6.50</cell><cell>5.93</cell></row><row><cell>SDM [21]</cell><cell>5.85</cell><cell>5.50</cell></row><row><cell>LBF [22]</cell><cell>5.41</cell><cell>-</cell></row><row><cell>CFAN [12]</cell><cell>-</cell><cell>5.53</cell></row><row><cell>CDM [6]</cell><cell>-</cell><cell>9.90</cell></row><row><cell>GN-DPM [7]</cell><cell>-</cell><cell>5.69</cell></row><row><cell>ERT [48]</cell><cell>4.90</cell><cell>-</cell></row><row><cell>CFSS [49]</cell><cell>4.74</cell><cell>4.63</cell></row><row><cell>TCDCN</cell><cell>4.63</cell><cell>4.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Mean errors (%) on 300-W [20] dataset (68 landmarks)</figDesc><table><row><cell>Method</cell><cell cols="3">Common Subset Challenging Subset Fullset</cell></row><row><cell>CDM [6]</cell><cell>10.10</cell><cell>19.54</cell><cell>11.94</cell></row><row><cell>DRMF [31]</cell><cell>6.65</cell><cell>19.79</cell><cell>9.22</cell></row><row><cell>RCPR [8]</cell><cell>6.18</cell><cell>17.26</cell><cell>8.35</cell></row><row><cell>GN-DPM [7]</cell><cell>5.78</cell><cell>-</cell><cell>-</cell></row><row><cell>CFAN [12]</cell><cell>5.50</cell><cell>16.78</cell><cell>7.69</cell></row><row><cell>ESR [9]</cell><cell>5.28</cell><cell>17.00</cell><cell>7.58</cell></row><row><cell>SDM [21]</cell><cell>5.57</cell><cell>15.40</cell><cell>7.50</cell></row><row><cell>ERT [48]</cell><cell>-</cell><cell>-</cell><cell>6.40</cell></row><row><cell>LBF [22]</cell><cell>4.95</cell><cell>11.98</cell><cell>6.32</cell></row><row><cell>CFSS [49]</cell><cell>4.73</cell><cell>9.98</cell><cell>5.76</cell></row><row><cell>TCDCN</cell><cell>4.80</cell><cell>8.60</cell><cell>5.54</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Facetracer: A search engine for large collections of images with faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surpassing human-level face verification performance on LFW with GaussianFace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coupling alignments with recognition for still-to-video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3296" to="3303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pose-free facial landmark fitting via optimized part mixtures and cascaded deformable shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1944" to="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gauss-newton deformable part models for face alignment in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2887" to="2894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust and accurate shape model fitting using random forest regression voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Ionita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="278" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sieving regression forest votes for facial feature detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1936" to="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coarse-to-fine autoencoder networks (CFAN) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A deep sum-product architecture for robust facial attributes analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2864" to="2871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training hierarchical feed-forward visual recognition models using transfer learning from pseudo-tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="69" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Heterogeneous multi-task learning for human pose estimation with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: the first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facial point detection using boosted regression and graph models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Binefa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2729" to="2736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1078" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time facial feature detection using conditional regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2578" to="2585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="109" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generic face alignment using boosted appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using a deformation field model for localizing faces and facial points under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3694" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3444" to="3451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep regression for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Baoguang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5230</idno>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weighted task regularization for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raghavendra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Generalized linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Chapman and Hall London</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Nagar</surname></persName>
		</author>
		<title level="m">Matrix variate distributions</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="950" to="957" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The concave-convex procedure (cccp)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1033" to="1040" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust face detection using the hausdorff distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jesorsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Kirchberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frischholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Audio-and Video-Based Biometric Person Authentication</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Xm2vtsdb: the extended m2vts database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lttin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Audio and Video-based Biometric Person Authentication</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Josephine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face alignment by coarseto-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Locating facial features with an extended active shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milborrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nicolls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megvii</forename><surname>Inc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Face++</surname></persName>
		</author>
		<ptr target="http://www.faceplusplus.com" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
