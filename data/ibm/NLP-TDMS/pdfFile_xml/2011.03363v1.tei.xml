<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adaptive Person Re-Identification via Coupling Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Adaptive Person Re-Identification via Coupling Optimization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
						<meeting>the 28th ACM International Conference on Multimedia (MM &apos;20) <address><addrLine>Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413904</idno>
					<note>ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Information systems → Information retrieval KEYWORDS Domain Adaptive Person Re-Identification</term>
					<term>Domain-Invariant Map- ping</term>
					<term>Global-Local Distance Optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptive person Re-Identification (ReID) is challenging owing to the domain gap and shortage of annotations on target scenarios. To handle those two challenges, this paper proposes a coupling optimization method including the Domain-Invariant Mapping (DIM) method and the Global-Local distance Optimization (GLO), respectively. Different from previous methods that transfer knowledge in two stages, the DIM achieves a more efficient one-stage knowledge transfer by mapping images in labeled and unlabeled datasets to a shared feature space. GLO is designed to train the ReID model with unsupervised setting on the target domain. Instead of relying on existing optimization strategies designed for supervised training, GLO involves more images in distance optimization, and achieves better robustness to noisy label prediction. GLO also integrates distance optimizations in both the global dataset and local training batch, thus exhibits better training efficiency. Extensive experiments on three large-scale datasets, i.e., Market-1501, DukeMTMC-reID, and MSMT17, show that our coupling optimization outperforms state-of-the-art methods by a large margin. Our method also works well in unsupervised training, and even outperforms several recent domain adaptive methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Person Re-Identification (ReID) <ref type="bibr">[2, 13, 15-17, 28, 35, 37, 45, 54]</ref> targets to match a query person image against a gallery set. Recent works have achieved promising performance in supervised scenario <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>. However, supervised ReID models suffer Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM <ref type="bibr">'20, October 12-16, 2020</ref>  Global-Local distance Optimization (GLO). In DIM, green and red dots denote labeled and unlabeled samples, respectively. In GLO, red, green, and black arrows denote directions of push, pull and update for anchor, respectively. In the shared feature space, dots in different colors denote samples of different identities.</p><p>from the expensive data annotation and substantial performance drop when applied on different target domains. To address those issues, recent works focus on domain adaptive person ReID by transferring knowledge learned from the labeled domain to the target domain <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b57">58]</ref>. Transferred models exhibit better generalization ability and can be applied on target domains without labeled data. More details of related works are summarized in Sec. 2.</p><p>Despite the significant success, domain adaptive person ReID is still a challenging task and there remain several open issues unexplored. Firstly, previous works commonly conduct the knowledge transfer in two stages, i.e., first transfer labeled images to the target domain with Generative Adversarial Networks (GANs), then train the ReID model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref> using transferred images. However, GANs could be hard to tune.The image generation is also challenging and sensitive to various factors like backgrounds, lighting, etc.. Secondly, person ReID models for the unlabeled target domain can be optimized by predicted labels. This procedure requires an optimization strategy robust to noisy labels. Existing unsupervised person ReID works commonly adopt the optimization strategies in supervised ReID training, e.g., triplet loss <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b56">57]</ref>, which are not specifically optimized for unsupervised training.</p><p>This paper is motivated to study an efficient optimization for domain adaptive person ReID. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, we first propose an efficient one-stage knowledge transfer model. It trains the target ReID model using labeled source dataset and unlabeled target dataset, without requiring image transfer and GANs training. An efficient unsupervised optimization is further proposed to conduct the optimization on the target domain. Those two optimizations jointly boost the performance of domain adaptive person ReID.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the DIM method is proposed to effectively transfer discriminative cues from the labeled dataset to the target dataset. DIM bridges the domain gap between labeled and unlabeled datasets in feature space in an adversarial learning manner. Specifically, a domain discriminator is trained based on the learned person ReID feature to discriminate where samples are from. The feature extractor is optimized to confuse the domain discriminator. Those two models are interactively optimized, leading to a discriminative feature space shared by both the labeled and unlabeled datasets. The DIM directly bridges the domain gap in feature space, hence is more efficient than the two-stage transfer models.</p><p>Individual image triplets or pairs contain a small number of images, making triplet loss sensitive to errors in label prediction. As illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>, to enhance the robustness to noisy labels, one possible way is to involve more images during distance optimization. This intuition leads to the GLO strategy. GLO consists of Global Optimization (GO) and Local Optimization (LO), both compute the distance among a larger number of samples. GO optimizes the distance of each anchor image and its positive and negative images in the training set. GO is achieved with the memory bank <ref type="bibr" target="#b40">[41]</ref>, which caches the features of the entire training set. LO is introduced for distance optimization in each training batch. Note that, each training batch samples a small portion of training set. This sampling strategy guarantees images inside each training batch likely to be from different persons. The LO is hence computed by pushing all images away from each other. Compared with GO, the LO is more efficient to compute and accelerates the training convergence.</p><p>Our person ReID model is jointly optimized with the above two training algorithms. We hence call our method as coupling optimization. We test our model on three large-scale datasets, i.e., Market-1501 <ref type="bibr" target="#b51">[52]</ref>, DukeMTMC-reID <ref type="bibr" target="#b52">[53]</ref>, and MSMT17 <ref type="bibr" target="#b37">[38]</ref>. Comparison with recent works shows that our methods outperform state-of-theart works by a large margin. For instance, using DukeMTMC-reID as the source domain, our method achieves Rank-1 accuracy of 88.3% on Market-1501, outperforming recent PAST <ref type="bibr" target="#b46">[47]</ref> and SSG [6] by 9.9% and 8.3%, respectively. We also test our method in unsupervised scenario, i.e., training the ReID model only with GLO. Our method achieves Rank-1 accuracy of 77.4% on Market-1501, significantly outperforming recent BUC <ref type="bibr" target="#b22">[23]</ref> and DBC [5] by 11.2% and 8.2%, respectively. It is worth noting that, our unsupervised training also outperforms several domain adaptive methods that use extra source domain for training, such as PAUL <ref type="bibr" target="#b42">[43]</ref> and DA_2S <ref type="bibr" target="#b11">[12]</ref>.</p><p>In summary, this work iteratively runs DIM and GLO to optimize the domain adaptive person ReID model. DIM transfers discriminative cues from the source domain to the target domain. GLO achieves unsupervised optimization based on predicted labels on the target domain. Compared with existing works, DIM and GLO exhibit advantages in efficiency and robustness to label noises, respectively. Those properties guarantee the superior performance of our method in comparisons with state-of-the-art works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This work is closely related with unsupervised domain adaptation, supervised and domain adaptive person ReID. This section briefly reviews those three categories of works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unsupervised Domain Adaptation</head><p>Unsupervised domain adaptation (UDA) is similar to transfer learning for person ReID. However, UDA holds a strong assumption that the unlabeled domain has the same categories with the labeled domain <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42]</ref>. While in domain adaptive person ReID, labeled and unlabeled domains usually have totally different identities. The same issue that both tasks suffer from is the domain gap. Several works on UDA have been proposed to tackle this issue <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>. <ref type="bibr">Ganin et al. [7]</ref> propose a gradient reversal layer to learn domaininvariant features, which suffers gradient vanish issue in training. Kumagai et al. <ref type="bibr" target="#b13">[14]</ref> and Long et al. <ref type="bibr" target="#b28">[29]</ref> propose to transform the feature distribution of labeled domain to approach the feature distribution of unlabeled domain. However, those methods may suffer from the noisy from mean feature computation. Moreover, encouraging the distributions of labeled and unlabeled domains to be identical is not reasonable for person ReID task because different domains have different identities. For instance, colors of clothe in unlabeled domain could be different from labeled domain. Tzeng et al. <ref type="bibr" target="#b33">[34]</ref> present an adversarial loss to encourage the feature domain of unlabeled dataset to approach the one of labeled dataset. However, this restriction is possible to disturb the distance optimization on unlabeled dataset. Different from <ref type="bibr" target="#b33">[34]</ref>, proposed DIM restricts features from both labeled and unlabeled datasets to approach to each other, which alleviates the disturbance on distance optimization and is also more effective at narrowing domain gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Supervised Person Re-Identification</head><p>Supervised person ReID task has been widely studied and many methods have been proposed from many respects, such as parts feature extraction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>, distance metric learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b60">61]</ref>, attention learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>, etc. However, manual person ID annotation is expensive and rarely available in real-world application. Although these supervised methods have achieved superior performance on existing labeled datasets, person ReID in unsupervised scenario still remains challenging. Different from these works, this paper focuses on the domain adaptive person ReID task, which is more challenging and also more valuable for real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain Adaptive Person Re-Identification</head><p>Recently, many works have been proposed for domain adaptive person ReID task from different aspects. Some works targeting to narrow domain gaps in image space by GANs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b56">57]</ref>. For example, Liu et al. <ref type="bibr" target="#b24">[25]</ref> propose an adaptive transfer network to effectively transfer images from label domain to unlabeled domain. However, transfer in image space cannot guarantee the elimination of domain gaps in feature space, and the quality of generated images largely affects the performance. Several works try to reduce domain gaps in feature space <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>. Ganin et al. <ref type="bibr" target="#b7">[8]</ref> reverse the gradient of domain discriminator to train the feature extractor. However, the scale of gradient is small when discriminator is well-trained, leading to an ineffective learning. Huang et al. <ref type="bibr" target="#b10">[11]</ref> encourage the discriminator to have the same output for features from both labeled and unlabeled domains. However, this will end up with the discriminator outputting a same value for all inputs, instead of narrowing down the gap.Experiments in Sec. 6.3 demonstrate the advantages of proposed DIM against previous methods.</p><p>Some researchers focus on unsupervised optimization on unlabeled dataset. Huang et al. <ref type="bibr" target="#b12">[13]</ref> propose a degradation invariance learning approach with depredated surveillance data. It simultaneously extracts robust features and removes real-world degradations without extra supervision, leading to good robustness and effectiveness. Some works locally predict labels for model training <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b57">58]</ref>. For example, Yu et al. <ref type="bibr" target="#b43">[44]</ref> select positive samples in training batches. However, local label prediction is not precise and will mislead the distance optimization. Based on assigned labels, some works adopt triplet loss for model training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47]</ref>. However, these optimization methods fail to consider the noise in predicted labels on unlabeled dataset. Compared with these methods, proposed GLO method is more robust to noisy label and optimizes distance relationship more effectively.</p><p>Some works adopt additional cues to boost performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref>. For example, Qi et al. <ref type="bibr" target="#b29">[30]</ref> use temporal information and Li et al. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> adopt tracklet information to predict more precise labels on unlabeled datasets. Fu et al. <ref type="bibr" target="#b5">[6]</ref> and Zhang et al. <ref type="bibr" target="#b46">[47]</ref> use local features to improve the performance. Experimental results show that our model outperforms these methods by a clear margin even without additional cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>Given a labeled dataset S and a target unlabeled dataset T , domain adaptive person ReID aims to train the ReID model with both S and T , and guarantee its discriminative power on T . S and T can be denoted as S = { , | = 1... S , ∈ {1, ..., }} and T = { | = 1... T }, respectively. , , S , and are the -th image, its label, the number of images and identities in S, respectively. and T denote the the -th image and the number of images in T . We denote the feature extraction as = Φ( , ) and = Φ( , ) on S and T , respectively. Φ and denote the CNN feature extractor and its parameters, respectively. The target of model training is to make extracted features discriminative for person ReID task on T . We achieve this goal from two aspects: 1) transfer the discriminative cues learned from S to T , and 2) optimize the ReID model on T through unsupervised training. As person ID labels are available in S, cross entropy loss L can be computed on S to learn discriminative features. To bridge the domain gap between T and S, DIM maps T and S into a shared feature space. To achieve unsupervised training, we first predict person ID labels on T , then optimize the ReID model with GLO. Therefore, our ReID model is optimized by three training losses, i.e., the cross entropy loss S, the DIM loss on S and T , as well as the GLO loss on S. The overall loss function can be formulated as:</p><formula xml:id="formula_0">L = L + L + L ,<label>(1)</label></formula><p>where L , L , L denote the three losses discussed above. is loss weights. L is expected to map T and S into a shared feature space, where features from different datasets show similar distribution. This shared feature space hence could be optimized by both T and S, i.e., it can be optimized by L on S and optimized by on T to gain better discriminative power. To learn this shared feature space, DIM first trains a discriminator network DNet, which is able to discriminate the source domain of each sample based on its feature. The CNN feature extractor Φ is updated to confuse DNet. This adversarial training strategy iteratively updates DNet and Φ, finally achieves a discriminative feature space shared by both T and S. Compared with existing two-stage image transfer models, DIM does not involve GANs training and image transfer, thus efficiently bridges the domain gap between T and S. More details of DIM computation could be found in Sec. 4.</p><p>L is computed on T based on predicted labels. Label prediction can be conducted on the entire T and inside each training batch, respectively. Those two predictions lead to the Global Optimization (GO) and Local Optimization (LO), respectively. GO first generates image clusters on T , then introduces a distance threshold to select positive pairs inside each cluster. To further eliminate the negative effects of noisy label prediction, GO involves more samples for loss computation. More specifically, for each anchor image GO generates a group of positive samples and a group of negative samples, then optimizes the group distance, instead of the sample distance. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, this strategy leads to better robustness to label noises than triplet loss. LO is conducted based on label prediction inside each training batch. Each training batch randomly samples a small number of images from the large-scale T . This strategy selects images with different ID labels with high probability. The LO is hence computed to push samples in the same training batch far away from each other.</p><p>We hence denote L as the combination of GO and LO, i.e.,</p><formula xml:id="formula_1">L = L + L ,<label>(2)</label></formula><p>where and are loss weights. L and L are both computed to optimize the ReID model on T . Compared with L , L is more efficient in label prediction and loss computation. As shown in experiments, L substantially accelerates the training convergence. Details of computation to L will be given in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DOMAIN-INVARIANT MAPPING</head><p>Owing to the domain gap between S and T , even model has been well trained on S, the performance often drops a lot on T . To address this issue, we introduce the DIM method to encourage the model to map images from both S and T into a shared domain in feature space. DIM is designed in an adversarial learning manner. Specifically, we use a discriminator network DNet to recognize which domain images come from. DNet takes as input features from both source and target datasets, and output domain recognition scores. We train DNet by Euclidean distance loss with the output target to be 1 for from S and 0 for from T . This objective can be formulated as follows:</p><formula xml:id="formula_2">ℓ = 1 S S ∑︁ =1 (DNet( ) − 1) 2 + 1 T T ∑︁ =1 DNet( ) 2 .<label>(3)</label></formula><p>After training by Eqn. (3), DNet is able to recognize which domain a feature belongs. Then, we use DNet to supervise Φ with the objective that Φ can confuse DNet by extracting features whose domain recognition scores are 0.5. The objective function to train Φ can be formulated as: The training of DNet and Φ is conducted interactively as illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. In each iteration, we first train DNet with Eqn. <ref type="bibr" target="#b2">(3)</ref>. Then, we train Φ with Eqn. (4) to confuse DNet. By interactively training DNet and Φ, Φ is finally able to map images from different domains into a shared feature space.</p><formula xml:id="formula_3">L ( ) = 1 S S ∑︁ =1 (DNet( ) −0.5) 2 + 1 T T ∑︁ =1 (DNet( ) −0.5) 2 . (4)</formula><p>Compared with the method in <ref type="bibr" target="#b33">[34]</ref> that only restricts features from T , L also pulls features in S to approach features in T . As ground truth labels are not provided in T , distribution restriction on T is possible to disturb the procedure of distance optimization on T . While S has ground truth and the distance optimization on S is more robust. Thus, L alleviates the disturbance on distance optimization. Huang et al. <ref type="bibr" target="#b10">[11]</ref> train DNet and Φ with a single objective function to encourange DNet to output a same value with features from both S and T , i.e., only using Eqn. (4) to train both DNet and Φ. This leads to a DNet without domain discriminative ability, i.e., outputting a same value for all inputs. Thus, it is not able to supervise Φ to map samples from S and T to a shared feature space. Experiments in Sec. 6.3 shows the advantage of proposed DIM methods over previous methods in <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GLOBAL-LOCAL OPTIMIZATION 5.1 Label Prediction</head><p>Unsupervised person ReID is challenging mainly owing to lacking annotations on unlabeled datasets. Before applying distance relationship optimization on T , predicting annotations is an important step that offers guidance for distance optimization. A more precise label prediction leads to a more effective optimization.</p><p>In order to predict labels precisely, we first extract features for every image in T by Φ. Features are then L2-normalized, and the set of features on T is denoted as V = { | = 1... T }. Instead of using Euclidean distance to compute distance matrix, we use the -reciprocal encoding <ref type="bibr" target="#b54">[55]</ref> to compute distance among features in V. By additionally considering the distance among neighbors of pairs of features, -reciprocal encoding is able to provide a more precise distance matrix on V.</p><p>Based on the distance matrix, we adopt the hierarchical densitybased cluster algorithm <ref type="bibr" target="#b0">[1]</ref> to perform clustering on V. Clustering parameters are set following previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref>. This separates V into a set of clusters. In previous methods, all the features in the same cluster are considered to having the same label. However, this involves a number of false positive pairs. To improve the precision of generated positive pairs, we introduce a distance threshold to discard positive pairs whose distance is larger than . Then, two images in the same cluster and within the distance of are selected as positive pairs.</p><p>Reducing improves the precision of generated positive pairs, but also reduces the ratio of recall. We find in practice that the ReID performance is insensitive to in the range from 0.4 to 0.7. This is because distance distribution is close to a normal distribution with two peaks. Hence, there are few feature pairs whose distances are between 0.4 to 0.7. A threshold large than 0.7 will involve too many false positive pairs, and a threshold smaller than 0.4 will discard too many true positive pairs. We practically set as 0.5 in experiments. Performance with different values of is compared in Sec. 6.3. Based on generated positive pairs, the annotation matrix can be obtained with , = 1 denoting the pair of and is positive and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Global Optimization</head><p>Based on , Φ can be trained to optimize distance relationships on V. However, as is obtained by unsupervised clustering, it always contains noise. When only several positive/negative pairs are involved in optimization for an anchor in the same time, e.g., triplet loss in previous works, noise will mislead the optimization procedure as illustrated in the left of <ref type="figure" target="#fig_3">Fig. 3</ref>. To chase a more precise and effective optimization, we propose the GO method to consider all positive and negative pairs of an anchor in the same time, i.e., globally optimizing distance for the anchor. By involving more samples in the computation of update direction for anchor samples, GO eliminates the effect of noisy label in and provides precise update direction for anchors as illustrated in the right of <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>For the computation of GO, we propose that GO should satisfy three conditions. 1): GO should enlarge the similarities of positive pairs and also reduce the similarities of negative pairs for an anchor simultaneously. 2): GO should be aware of the density of samples around anchors. Some anchors may have dense neighbors, and positive pairs of them should reach rather high similarities. While some other anchors may have sparse neighbors, and restriction on positive pairs of them can be relaxed. Thus, GO should optimize relative similarity of positive and negative samples for each anchor, instead of absolute similarity for each pair. 3): GO should be aware of the hardness of pairs, i.e., gradients of hard pairs should be larger than gradients of easy pairs. Based on these motivations, we formulate the objective function of GO with respective to the anchor as follows:</p><formula xml:id="formula_4">ℓ ( , ) = 1 || || 1 ∑︁ , , =1 log(1 + ( , )</formula><p>).</p><p>|| || 1 denotes the number of positive pairs of anchor .</p><p>( , ) denotes the similarity between and computed as:</p><formula xml:id="formula_6">( , ) = · / ,<label>(6)</label></formula><p>where is a hyper-parameter to adjust the scale of similarity. denotes the sum of similarities of negative pairs of as = , , =0</p><p>( , ). Note that ( , ) denotes the similarity of a positive pair, and ( , ) denotes the similarity of a negative pair. Positive pair weighting. Previous unsupervised optimization methods treat all image pairs from different cameras on the unlabeled dataset with equal weight. However, some camera pairs have little domain gaps, while others have large domain gaps. In person ReID task, matching positive pairs from cameras with large gaps is more challenging and also practically valuable. Thus, model should pay more attention on pulling together positive pairs from cameras with large gaps. To this end, we further propose a positive pair weighting method based on camera gaps to emphasize positive pairs from cameras with large gaps.</p><p>We use maximum mean discrepancy to evaluate the gaps among cameras on T . Gap for each camera pair is normalized to the range from 0 to 1 by subtracting the minimum gap and then dividing by the maximum value. We propose to weight positive pairs based on camera gaps linearly, i.e., setting the weight of positive pair and as , + . , denotes the gap between cameras where and are from. And denotes the base weight for every positive pairs. is used to enlarge the scale of weights as , is always smaller than 1. Then Eqn. <ref type="formula" target="#formula_5">(5)</ref> is re-formulated as:</p><formula xml:id="formula_7">ℓ ( , ) = 1 || || 1 ∑︁ , , =1 ( , + ) log(1 + ( , )</formula><p>).</p><p>is computed based on statistical data. We first compute the mean gap of every positive pairs over T , which is denoted as . And then is set to 1 − to make the average weight equals to 1. We select all samples in a training batch as anchors. Then the objective function of GO is formulated as:</p><formula xml:id="formula_9">L ( ) = ∑︁ =1 ℓ ( , ),<label>(8)</label></formula><p>where denotes the size of training batch. Discussion. Proposed objective function in Eqn. <ref type="bibr" target="#b6">(7)</ref> satisfies aforementioned three conditions. The gradient of ℓ ( , ) relative to ( , ) and ( , ) can be respectively computed as: Compared with previous methods, L is more effective at updating model against nosy label as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. Moreover, L doesn't need any sample mining algorithm and it automatically focuses on hard pairs. Compared with the objective function in <ref type="bibr" target="#b57">[58]</ref> where enlarging ( , ) is conflicting to enlarging similarities of other positive pairs of , L avoids the conflict and is more effective at model training.</p><formula xml:id="formula_10">ℓ ( , ) · ( , ) = − ( , + ) || || 1 ( ( , ) + ) ,<label>(9)</label></formula><formula xml:id="formula_11">ℓ ( , ) · ( , ) = ( , ) || || 1 ∑︁ , , =1 ( , + ) ( , ) ( , ) + .<label>(10)</label></formula><p>In model training, extracting all the features on T to optimize L in each iteration costs a lot of time. Compromising with it, we only extract features of all images in T at the beginning of training and cache them in a memory bank. In each iteration, features in each training batch are added to corresponding cached features with weight (100 − ℎ)/ ℎ to update the memory bank. Updated features in memory bank are L2-normalized. Memory bank costs little memory compared with CNN model, e.g., only around 130 MB on DukeMTMC-reID.</p><p>Proposed DIM can also be used to bridge gaps between cameras. However, our weighting method shows several advantages: 1) DIM is applied to a pair of domains, thus is expensive to compute for multi-cameras. 2) Weighting method could focus on hard positive samples, thus could better boost feature discriminative power. Experiments have shown the validity of our weighting method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Local Optimization</head><p>Because of the usage of memory bank, GO uses cached features that are not up-to-date. This will reduce the effectiveness of GO. Thus, we need to optimize distance among those up-to-date features in training batches. On the other hand, in the beginning of the model training, we don't have any prior label information of unlabeled data, and label prediction for GO is not precise due to the weak discriminative ability of the initialized model. Therefore, a labelfree training algorithm is needed in the beginning to boost the discriminative ability. Thus, we further propose the LO method performed on up-to-date features in training batches. LO treats all pairs in training batches as negative ones, and the objective function for is formulated as follows:</p><formula xml:id="formula_12">ℓ ( , ) = log(1 + ∑︁ , ≠ ( , )),<label>(11)</label></formula><p>Then the objective function of LO in each training batch is:</p><formula xml:id="formula_13">L ( ) = ∑︁ =1 ℓ ( , ).<label>(12)</label></formula><p>Discussion. The gradient of ℓ relative to ( , ) is:</p><formula xml:id="formula_14">ℓ ( , ) · ( , ) = ( , ) (1 + , ≠ ( , ))</formula><p>.</p><p>Gradient in Eqn. <ref type="bibr" target="#b12">(13)</ref> is always positive and its scale increases with ( , ) increasing. This indicates that ℓ pays more attention on pushing away hard pairs. It can also be observed that L is more efficient to compute compared with L . We also test the method that uses to pull positive pairs in training batches close to each other. However, this method doesn't bring any improvement as shown in Ablation Study in Sec. 6.3. This is because only few positive pairs occur in training batches, and contains many false positive pairs in the beginning of training.</p><p>LO shares certain similarity with Exemplar Invariance (EI) in <ref type="bibr" target="#b57">[58]</ref>, but it is different in two aspects: 1) Different motivations: LO is computed to enhance our training efficiency. EI is computed with memory bank, thus has similar goal with our GO method. 2) Better efficiency: LO is label-free and is more efficient to compute within a training batch. While, EI relies on memory bank and KNN for positive/negative label prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENT 6.1 Dataset</head><p>To evaluate proposed methods, experiments are conducted on three widely used datasets, i.e., Market1501 <ref type="bibr" target="#b51">[52]</ref>, DukeMTMC-reID <ref type="bibr" target="#b52">[53]</ref>, and MSMT17 <ref type="bibr" target="#b37">[38]</ref>. Details of those datasets are given as follows.</p><p>Market1501 contains 32,668 images of 1,501 identities captured from 6 cameras at Tsinghua University. 12,936 images of 751 identities are selected for training and others are used for testing. In testing set, 3,368 images are selected as query images and remaining 19,732 images are used as gallery images.</p><p>DukeMTMC-reID contains 36,411 images of 1,812 identities captured from 8 cameras at Duke University. 16,522 images of 702 identities are selected for training and others are used for testing. 3,368 images from testing set are selected as query images, and remaining 19,732 images are used as gallery images.</p><p>MSMT17 contains 126,441 images of 4,101 identities captured from 15 cameras at Peking University. 32,621 images of 1,041 identities are selected for training and others are selected for testing. In testing set, 11,659 images are selected as query images and remaining 82,161 images are used as gallery images.</p><p>Following previous works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58]</ref>, two evaluation metrics, i.e., Cumulative Matching Characteristics (CMC) and mean Average Precision (mAP), are used to evaluate the performance. Rank1, Rank5 and Rank10 accuracies in CMC are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation Detail</head><p>Proposed model adopts ResNet50 [? ] pre-trained on ImageNet [? ] as backbone. The last fully connected layer of ResNet50 is removed and the stride of the last residual block is set to 1. Input images are resized to 256×128. We use random flipping, random cropping, random erasing <ref type="bibr" target="#b55">[56]</ref>, and CamStyle <ref type="bibr" target="#b59">[60]</ref> for data augmentation. In training procedure, each training batch consists of 32 images from source datasets, and 16 images with 3 additional augmented images per image from target datasets. The Adam optimizer is adopted for training. Learning rate is initialized as 0.00035 and decayed by 0.1 every 20 epochs. Model is totally trained for 60 epochs with costing around 5 hours, faster than SSG <ref type="bibr" target="#b5">[6]</ref> which costs 2,100 epochs and more than one day. GO is used from the 6-th epoch. Label prediction is performed at the beginning of each epoch on memory bank. Distance threshold in label prediction is set to 0.5. DNet consists of two fully connected layers with structure of 2048 × 64 and 64 × 1, and the first fully connected layer is activated by ReLU function. Parameter in Eqn. (6) is set to 0.05. , , and are set as 1, 0.1, and 0.05, respectively. The mean gap of camera pairs on Market1501, DukeMTMC-reID, and MSMT17 is 0.60, 0.50, and 0.74, respectively. Then the base weight for positive pair weighting on corresponding dataset is set as 0.4, 0.5, and 0.26. We use the labeled training set of source domain and the unlabeled training set of target domain to train the model, and use the testing set of target domain to evaluate the model.  In this section, we analysis hyper-parameters on domain adaptive person ReID task. We vary the value of one parameter and keep the others fixed to the optimal values. Experiments show that hyperparameters selected on one dataset can be applied to other datasets. Analysis of distance threshold. Distance threshold affects the accuracy and recall rate of selected positive pairs. Evaluation on the effect of is shown in <ref type="figure">Fig. 4</ref>. It can be observed that performance is insensitive to in an appropriate range from 0.4 to 0.7. And the best performance is achieved when setting to 0.5 on both datasets.</p><p>Analysis of temperature parameter. Temperature parameter affects the computation of L . Evaluation on the effect of is also shown in <ref type="figure">Fig. 4</ref>. It can be observed that the best performance is achieved when is set to 0.05 on both datasets.</p><p>Analysis of base weight. In positive pair weighting, base weight is computed as 1 − . Different value of is evaluated as shown in <ref type="figure">Fig. 5</ref>. It can be observed that setting to 0.4 and 0.5 on Market1501 and DukeMTMC-reID based on 1 − achieves the best performance. This indicates the validity of the proposed method to compute base weight for positive pair weighting.</p><p>Analysis of DIM. Different structures of discriminator and different learning algorithms are compared with proposed DIM method in this section. Experimental results are summarized in <ref type="table" target="#tab_0">Table 1</ref>. It can be observed that, DIM with different structures can always improve the performance on both datasets. This indicates   the validity of DIM to narrow down domain gap in feature space and improve the discriminative ability on the target dataset. DNet with shallow structure is not effective at eliminating domain gap because of lacking domain discriminative ability. And DNet with deep structure provides an unnecessary strong restriction on feature extractor and thus disturbs the distance optimization. Comparison among three structures shows that using a two-layer network achieves the best performance. Different learning algorithms, i.e., ADDA <ref type="bibr" target="#b33">[34]</ref> and OCC <ref type="bibr" target="#b10">[11]</ref>, are also compared in <ref type="table" target="#tab_0">Table 1</ref>. It can be observed that DIM outperforms ADDA and OCC on both datasets, e.g., by 1.9% and 2.8% in mAP on Market-1501.</p><p>Analysis of loss weights. We compare different values of loss weights for , , and . The results are summarized in <ref type="table" target="#tab_1">Table 2</ref>. Compared with setting to 0, setting it larger than 0 largely improves the performance, and the best performance is achieved with as 0.1. This demonstrates that GO is effective at enhancing the discriminative ability on target domain. Compared with setting to 0, setting it to the range from 0.01 to 1 achieves clear improvement and setting it to 1 achieves the best performance. Setting larger than 0 shows improvements against setting it to 0, and setting it to 0.05 achieves the best performance.</p><p>Ablation Study. Each component of our methods is evaluated in this section and experimental results can be observed in <ref type="table" target="#tab_1">Table 2</ref> with setting L , L , and L to 0, respectively. It can be observed that, compared with directly transferring model trained on source datasets to target datasets, DIM+GLO boosts the performance by a large margin, e.g., improving mAP by 42.7% on Market-1501. Compared with the model without L , DIM+GLO largely improves mAP by 33.8% and 26.8% on Market-1501 and DukeMTMC-reID, respectively. This demonstrates that proposed GO can effectively improve the discriminative ability of learned features. It can also be observed that L and L improves the performance clearly, e.g., improves Rank1 accuracy on DukeMTMC-reID by 4.1% and 3.5%, respectively. This indicates the validity of LO and DIM methods. When we use annotation to pull positive pairs close to each other in LO as discussed in Sec. 5.3, the Rank1 accuracy on Market-1501 and DukeMTMC-reID is 0.873 and 0.749, showing no improvement. This indicates LO doesn't need annotation. We compare the training procedures with and without LO in <ref type="figure" target="#fig_7">Fig. 6</ref>. It can be observed that LO largely boosts the performance in the beginning of training, and also improves the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison with State-of-the-art Methods</head><p>In this section, proposed methods are compared with many stateof-the-art methods. The comparison results on Market-1501 and DukeMTMC-reID are summarized in <ref type="table">Table 3</ref>. And the comparison results on MSMT17 are shown in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>Comparison on the unsupervised person ReID. For unsupervised person ReID task, source dataset is not available. Therefore, we only use images from target datasets and L in model training. Comparisons are performed on Market-1501 and DukeMTMC-reID. Compared methods include LOMO <ref type="bibr" target="#b21">[22]</ref>, BOW <ref type="bibr" target="#b51">[52]</ref>, DBC <ref type="bibr" target="#b4">[5]</ref>, and BUC <ref type="bibr" target="#b22">[23]</ref>. It can be observed from <ref type="table">Table 3</ref> that proposed GLO achieves the best performance on both datasets and outperforms state-of-the-art methods by a clear margin. For example, GLO outperforms DBC, currently the best unsupervised person ReID method, by 8.2% and 9% in Rank1 accuracy on Market-1501 and DukeMTMC-reID, respectively. As DBC also predicts labels via unsupervised clustering, these results indicate that proposed GLO method can effectively improve the discriminative ability of learned features. It can also be observed that GLO also outperforms several domain adaptive methods such as PAUL <ref type="bibr" target="#b42">[43]</ref> and DA_2S <ref type="bibr" target="#b11">[12]</ref>. Specially, compared with ECN <ref type="bibr" target="#b57">[58]</ref> that uses a source dataset, GLO still outperforms it by 2.7% in mAP on Market-1501. As ECN uses the same data augmentation method and a similar optimization method, the compared results further demonstrate the advantage of proposed GLO method.</p><p>Comparison on domain adaptive person ReID on Market-1501 and DukeMTMC-reID. Proposed DIM+GLO model is compared with recent state-of-the-art methods including TAUDL † <ref type="bibr" target="#b18">[19]</ref>, UTAL † <ref type="bibr" target="#b19">[20]</ref>, MAR <ref type="bibr" target="#b43">[44]</ref>, PAUL <ref type="bibr" target="#b42">[43]</ref>, CASCL <ref type="bibr" target="#b39">[40]</ref>, UDA <ref type="bibr" target="#b30">[31]</ref>, GPP <ref type="bibr" target="#b58">[59]</ref>, HHL <ref type="bibr" target="#b56">[57]</ref>, ECN <ref type="bibr" target="#b57">[58]</ref>, ATNet <ref type="bibr" target="#b24">[25]</ref>, CR_GAN <ref type="bibr" target="#b3">[4]</ref>, SSG ‡ [6], DA_2S <ref type="bibr" target="#b11">[12]</ref>, CAL * † <ref type="bibr" target="#b29">[30]</ref>, PDA-Net <ref type="bibr" target="#b20">[21]</ref>, and PAST ‡ <ref type="bibr" target="#b46">[47]</ref>. The comparison results are summarized in <ref type="table">Table 3</ref>. It can be observed that proposed DIM+GLO model outperforms state-of-the-art methods on both datasets. Specially, DIM+GLO outperforms SSG ‡ and PAST ‡ that use multi-scale features as enhancement and similar label prediction methods by a clear margin, e.g., 8.3% and 9.9% in Rank1 accuracy on Market-1501. DIM+GLO also outperforms three semi-supervised methods, i.e., TAUDL † , UTAL † , and CAL * † . For instance, CAL * † additionally uses temporal information and DIM+GLO still outperforms it by 14.6% and 12.2% in Rank1 accuracy on Market-1501 and DukeMTMC-reID, respectively. It can also be observed that DIM+GLO achieves comparable performance with supervised baseline models, e.g., only 1.6% and 0.8% lower in Rank1 accuracy on <ref type="table">Table 3</ref>: Performance comparison with state-of-the-art methods on Market1501 and DukeMTMC-reID. "Market" and "Duke" denote Market1501 and DukeMTMC-reID, respectively. " † " denotes that methods are semi-supervised. " * " denotes temporal information is used. And " ‡ " denotes multi-scale features are used.  Marekt-1501 and DukeMTMC-reID, respectively. This indicates that proposed DIM+GLO is powerful to learn discriminative person features without annotation. Comparison on domain adaptive person ReID on MSMT17. MSMT17 is currently the largest person ReID dataset, which is more challenging than DukeMTMC-reID and Market-1501. We compare proposed domain adaptive model DIM+GLO with state-of-the-art methods including ECN <ref type="bibr" target="#b57">[58]</ref>, SSG <ref type="bibr" target="#b5">[6]</ref>, SSG++ <ref type="bibr" target="#b5">[6]</ref>, and GPP <ref type="bibr" target="#b58">[59]</ref>. Performance comparison is shown in <ref type="table" target="#tab_3">Table 4</ref>. It can be observed that proposed DIM+GLO outperforms previous methods by a large margin. For example, DIM+GLO achieves 0.497 and 0.565 in Rank1 accuracy when using Market-1501 and DukeMTMC-reID as the source dataset, respectively. And both of the accuracies outperform the best previous accuracy 0.425 achieve by GPP <ref type="bibr" target="#b58">[59]</ref> with DukeMTMC-reID as the labeled dataset by 7.2% and 14%, respectively. SSG++ is a semi-supervised method and proposed DIM+GLO model also outperforms it by a large margin. This further demonstrates the effectiveness of proposed method on large-scale person ReID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This paper proposes a coupling optimization method for domain adaptive person ReID consisting of the Domain-Invariant Mapping (DIM) method for knowledge transfer and the Global-Local distance Optimization (GLO) method for unsupervised model training. DIM encourages the model to map images from both labeled and unlabeled domains into a shared domain in feature space, enhancing the efficiency of knowledge transfer. GLO involves more samples in distance optimization, enhancing the robustness to noisy label prediction on unlabeled datasets. Experiments on three large-scale datasets show that our methods outperform state-of-the-art methods by a clear margin. Specially, proposed unsupervised training method even outperforms several recent domain adaptive methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, Seattle, WA, USA © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00 https://doi.org/10.1145/3394171.3413904 Labeled Unlabeled Global Local distance Optimization (GLO)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of proposed coupling optimization method consisting of Domain-Invariant Mapping (DIM) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of proposed DIM. Dots in different colors are of different domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of triplet loss and global optimization with noisy labels. Red and greed dots denote negative and positive samples, respectively. Hollow dots denote anchor samples. Red, green, and black arrows denote directions of push, pull and update for anchors, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 ):</head><label>1</label><figDesc>As similarities computed by Eqn. (6) are always larger than 0, gradient in Eqn. (9) always pulls close to positive sample , and gradient in Eqn. (10) always pushes away from negative sample . This means minimizing ℓ leads to enlarging ( , ) and reducing ( , ) simultaneously. 2): ℓ ( , ) is computed based on the ratio of and ( , ), i.e., relative similarity. Thus, ℓ ( , ) is aware of the density of neighbors for . 3): Moreover, the scale of gradient in Eqn. (9) decreases with ( , ) increasing, and the scale of gradient in Eqn. (10) increases with ( , ) increasing. This indicates that ℓ ( , ) pays more attention on positive pairs with small similarities, and negative pairs with large similarities. Thus, ℓ ( , ) is aware of hardness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Evaluation on distance threshold and temperature parameter . Evaluation on base weight .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of training procedures with and without LO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison on different structures of DNet and training algorithms of DIM. "D" and "M" denote DukeMTMC-reID and Market-1501, respectively.</figDesc><table><row><cell>Structure</cell><cell cols="2">D → M Rank1</cell><cell>mAP</cell><cell cols="2">M → D Rank1</cell><cell>mAP</cell></row><row><cell>Without DIM</cell><cell>0.865</cell><cell></cell><cell>0.610</cell><cell>0.727</cell><cell>0.541</cell></row><row><cell>2048 × 1</cell><cell>0.876</cell><cell></cell><cell>0.619</cell><cell>0.73</cell><cell>0.534</cell></row><row><cell>2048 × 64 × 1</cell><cell>0.883</cell><cell cols="2">0.651</cell><cell>0.762</cell><cell>0.583</cell></row><row><cell>2048 × 128 × 64 × 1</cell><cell>0.874</cell><cell></cell><cell>0.611</cell><cell>0.729</cell><cell>0.543</cell></row><row><cell>ADDA [34]</cell><cell>0.872</cell><cell></cell><cell>0.632</cell><cell>0.733</cell><cell>0.551</cell></row><row><cell>OCC [11]</cell><cell>0.866</cell><cell></cell><cell>0.623</cell><cell>0.718</cell><cell>0.530</cell></row><row><cell>DIM</cell><cell>0.883</cell><cell cols="2">0.651</cell><cell>0.762</cell><cell>0.583</cell></row><row><cell cols="2">6.3 Model Analysis.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of loss weights. "D" and "M" denote DukeMTMC-reID and Market-1501, respectively.</figDesc><table><row><cell>Weight</cell><cell>Rank1</cell><cell>D → M</cell><cell>mAP</cell><cell>Rank1</cell><cell>M → D</cell><cell>mAP</cell></row><row><cell>Directly transfer</cell><cell>0.530</cell><cell></cell><cell>0.224</cell><cell>0.349</cell><cell></cell><cell>0.171</cell></row><row><cell>0</cell><cell>0.693</cell><cell></cell><cell>0.313</cell><cell>0.571</cell><cell></cell><cell>0.315</cell></row><row><cell>0.01</cell><cell>0.801</cell><cell></cell><cell>0.617</cell><cell>0.710</cell><cell></cell><cell>0.516</cell></row><row><cell>0.1</cell><cell>0.883</cell><cell></cell><cell>0.651</cell><cell>0.762</cell><cell></cell><cell>0.583</cell></row><row><cell>1</cell><cell>0.857</cell><cell></cell><cell>0.624</cell><cell>0.727</cell><cell></cell><cell>0.544</cell></row><row><cell>5</cell><cell>0.810</cell><cell></cell><cell>0.601</cell><cell>0.712</cell><cell></cell><cell>0.539</cell></row><row><cell>0</cell><cell>0.840</cell><cell></cell><cell>0.612</cell><cell>0.721</cell><cell></cell><cell>0.538</cell></row><row><cell>0.01</cell><cell>0.866</cell><cell></cell><cell>0.632</cell><cell>0.744</cell><cell></cell><cell>0.557</cell></row><row><cell>0.1</cell><cell>0.871</cell><cell></cell><cell>0.649</cell><cell>0.745</cell><cell></cell><cell>0.561</cell></row><row><cell>1</cell><cell>0.883</cell><cell></cell><cell>0.651</cell><cell>0.762</cell><cell></cell><cell>0.583</cell></row><row><cell>5</cell><cell>0.811</cell><cell></cell><cell>0.601</cell><cell>0.718</cell><cell></cell><cell>0.529</cell></row><row><cell>0</cell><cell>0.865</cell><cell></cell><cell>0.610</cell><cell>0.727</cell><cell></cell><cell>0.541</cell></row><row><cell>0.01</cell><cell>0.871</cell><cell></cell><cell>0.621</cell><cell>0.733</cell><cell></cell><cell>0.540</cell></row><row><cell>0.05</cell><cell>0.883</cell><cell></cell><cell>0.651</cell><cell>0.762</cell><cell></cell><cell>0.583</cell></row><row><cell>0.1</cell><cell>0.865</cell><cell></cell><cell>0.581</cell><cell>0.733</cell><cell></cell><cell>0.540</cell></row><row><cell>0.5</cell><cell>0.817</cell><cell></cell><cell>0.479</cell><cell>0.697</cell><cell></cell><cell>0.485</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison with state-of-the-art methods on MSMT17.</figDesc><table><row><cell>Method</cell><cell>Reference</cell><cell>Source</cell><cell cols="2">MSMT17 Rank1 Rank10</cell><cell>mAP</cell></row><row><cell>PTGAN [38]</cell><cell>CVPR'18</cell><cell></cell><cell>0.102</cell><cell>0.244</cell><cell>0.029</cell></row><row><cell>ECN [58]</cell><cell>CVPR'19</cell><cell></cell><cell>0.253</cell><cell>0.421</cell><cell>0.085</cell></row><row><cell>GPP [59]</cell><cell>arXiv</cell><cell>Market-</cell><cell>0.404</cell><cell>0.587</cell><cell>0.152</cell></row><row><cell>SSG [6]</cell><cell>ICCV'19</cell><cell>1501</cell><cell>0.316</cell><cell>0.496</cell><cell>0.132</cell></row><row><cell>SSG++ [6]</cell><cell>ICCV'19</cell><cell></cell><cell>0.376</cell><cell>0.572</cell><cell>0.166</cell></row><row><cell>DIM+GLO</cell><cell>This paper</cell><cell></cell><cell>0.497</cell><cell>0.661</cell><cell>0.207</cell></row><row><cell>PTGAN [38]</cell><cell>CVPR'18</cell><cell></cell><cell>0.118</cell><cell>0.274</cell><cell>0.033</cell></row><row><cell>ECN [58]</cell><cell>CVPR'19</cell><cell></cell><cell>0.302</cell><cell>0.468</cell><cell>0.102</cell></row><row><cell>GPP [59]</cell><cell>arXiv</cell><cell>DukeMT</cell><cell>0.425</cell><cell>0.615</cell><cell>0.160</cell></row><row><cell>SSG [6]</cell><cell>ICCV'19</cell><cell>MC-reID</cell><cell>0.322</cell><cell>0.512</cell><cell>0.133</cell></row><row><cell>SSG++ [6]</cell><cell>ICCV'19</cell><cell></cell><cell>0.416</cell><cell>0.622</cell><cell>0.183</cell></row><row><cell>DIM+GLO</cell><cell>This paper</cell><cell></cell><cell>0.565</cell><cell>0.700</cell><cell>0.244</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Density-based clustering based on hierarchical density estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jgb</forename><surname>Ricardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davoud</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Moulavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander</surname></persName>
		</author>
		<editor>PAKDD. Springer</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixed High-Order Attention Network for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond Triplet Loss: A Deep Quadruplet Network for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Instance-Guided Context Rendering for Cross-Domain Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dispersion based Clustering for Unsupervised Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenmin</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond Human Parts: Dual Part-Aligned Representations for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Domain Adaptive Attention Model for Unsupervised Cross-Domain Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangru</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congyan</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhe</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10529</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SBSGAN: Suppression of Inter-Domain Background Shift for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-world Person Re-Identification via Degradation Invariance Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation by Matching Distributions Based on the Maximum Mean Discrepancy via Unilateral Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsutoshi</forename><surname>Kumagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoharu</forename><surname>Iwata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Globallocal temporal representations for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-Scale Temporal Cues Learning for Video Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4461" to="4473" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pose-Guided Representation Learning for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification by deep learning tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised Tracklet Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-Dataset Person Re-Identification via Unsupervised Pose Disentanglement and Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ci-Siang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Bo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Person reidentification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">View Confusion Feature Learning for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive Transfer Network for Cross-Domain Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ram: a region-aware deep model for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Group-group loss-based global-regional feature learning for vehicle reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2638" to="2652" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-guided hash coding for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Novel Unsupervised Camera-Aware Domain Adaptation Framework for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11334</idno>
		<title level="m">Unsupervised domain adaptive re-identification: Theory and practice</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SCAN: Spatial and channel attention network for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangzhi</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PCM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised Person Re-identification via Multi-label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transferable attention for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">VP-ReID: Vehicle and person re-identification system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised Person Re-Identification by Camera-Aware Similarity Consistency Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cross-Domain Visual Representations via Unsupervised Graph Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Patch-Based Discriminative Feature Learning for Unsupervised Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qize</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised Person Re-identification by Soft Multilabel Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial Attribute-Text Embedding for Person Search With Natural Language Query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1836" to="1846" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning a Discriminative Null Space for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-Training With Progressive Augmentation for Unsupervised Cross-Domain Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Densely Semantically Aligned Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deeply-Learned Part-Aligned Representations for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unlabeled Samples Generated by GAN Improve the Person Re-Identification Baseline in Vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust Partial Matching for Person Search in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingji</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Random erasing data augmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Learning to Adapt Invariance in Memory for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00485</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Point to set similarity based deep feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Yihong Gong, and Nanning Zheng</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
