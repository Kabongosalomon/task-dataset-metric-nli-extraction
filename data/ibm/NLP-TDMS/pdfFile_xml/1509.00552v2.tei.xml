<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAG-Recurrent Neural Networks For Scene Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
							<email>wanggang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DAG-Recurrent Neural Networks For Scene Labeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: With the local representations extracted from Convolutional Neural Networks (CNNs), the 'sand' pixels (in the first image) are likely to be misclassified as 'road', and the 'building' pixels (in the second image) are easy to get confused with 'streetlight'. Our DAG-RNN is able to significantly boost the discriminative power of local representations by modeling their contextual dependencies. As a result, it can produce smoother and more semantically meaningful labeling map. The figure is best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In image labeling, local representations for image units are usually generated from their surrounding image patches, thus long-range contextual information is not effectively encoded. In this paper, we introduce recurrent neural networks (RNNs) to address this issue. Specifically, directed acyclic graph RNNs (DAG-RNNs) are proposed to process DAG-structured images, which enables the network to model long-range semantic dependencies among image units. Our DAG-RNNs are capable of tremendously enhancing the discriminative power of local representations, which significantly benefits the local classification. Meanwhile, we propose a novel class weighting function that attends to rare classes, which phenomenally boosts the recognition accuracy for non-frequent classes. Integrating with convolution and deconvolution layers, our DAG-RNNs achieve new state-of-the-art results on the challenging SiftFlow, CamVid and Barcelona benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene labeling refers to associating one of the semantic classes to each pixel in a scene image. It is usually defined as a multi-class classification problem based on their surrounding image patches. However, some classes may be indistinguishable in a close-up view. As an example in Figure * Equal Contribution 1, the 'sand' and 'road' pixels are hard to be distinguished even for humans with limited context. In contrast, their differentiation becomes conspicuous when they are considered in the global scene. Thus, how to equip local features with a broader view of contextual awareness is a pivotal issue in image labeling.</p><p>In this paper, recurrent neural networks (RNNs) <ref type="bibr" target="#b8">[9]</ref>[16] are introduced to address this issue by modeling the contextual dependencies of local features. Specifically, we adopt undirected cyclic graphs (UCG) to model the interactions among image units. Due to the loopy property of UCGs, RNNs are not directly applicable to UCG-structured images. Thus, we decompose the UCG to several directed acyclic graphs (DAGs, and four DAGs are used in our experiments). In other words, an UCG-structured image is approximated by the combination of several DAG-structured images. Then, we develop the DAG-RNNs, a generalization of RNNs <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b8">[9]</ref>, to process DAG-structured images. Each hidden layer is generated independently through applying DAG-RNNs to the corresponding DAG-structured image, and they are integrated to produce the context-aware feature maps. In this case, the local representations are able to embed the abstract gist of the image, so their discriminative power are enhanced remarkably.</p><p>We integrate the DAG-RNNs with the convolution and deconvolution layers, thus giving rise to an end-to-end trainable full labeling network. Functionally, the convolution layer transforms RGB raw pixels to compact and discriminative representations. Based on them, the proposed DAG-RNNs model the contextual dependencies of local features, and output the improved context-aware representation. The deconvolution layer upsamples the feature maps to match the dimensionality of the desired outputs. Overall, the full labeling network accepts variable-size images and generates the corresponding dense label prediction maps in a single feed-forward network pass. Furthermore, considering that the class frequency distribution is highly imbalanced in natural scene images, we propose a novel class weighting function that attends to rare classes.</p><p>We test the proposed labeling network on three popular and challenging scene labeling benchmarks (SiftFlow <ref type="bibr" target="#b12">[13]</ref>, CamVid <ref type="bibr" target="#b1">[2]</ref> and Barcelona <ref type="bibr" target="#b25">[26]</ref>). On these datasets, we show that our DAG-RNNs are capable of greatly enhancing the discriminative power of local representations, which leads to dramatic performance improvements over baselines (CNNs, even the VGG-verydeep-16 network <ref type="bibr" target="#b22">[23]</ref>). Meanwhile, the proposed class weighting function is able to boost the recognition accuracy for rare classes. Most importantly, our full labeling network significantly outperforms current state-of-the-art methods.</p><p>Next, related work are firstly reviewed, compared and discussed in Section 2. Section 3 elaborates the details of the DAG-RNNs and how they are applied to image labeling. Besides, it presents the details of the full labeling network and the class weighting function. The detailed experimental results and analysis are presented in Section 4. In the end, section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Scene labeling (also termed as scene parsing, semantic segmentation) is one of the most challenging problems in computer vision. It has attracted more and more attention in recent years. Here we would like to highlight and discuss three lines of works that are most relevant to ours.</p><p>The first line of work is to explore the contextual modeling. One attempt is to encode context into local representation. For example, Farabet et al. <ref type="bibr" target="#b6">[7]</ref> stacks surrounding contextual windows from different scales; Pinheiro et al. <ref type="bibr" target="#b16">[17]</ref> increases the size of input windows. Sharma et al. <ref type="bibr" target="#b18">[19]</ref> adopts recursive neural networks to propagate global context to local regions. However, they do not consider any structure for image units, thus their correlations are not effectively captured. In contrast, we interpret the image as an UCG, within which the connections allow the DAG-RNNs to explicitly model the dependencies among image units. Another attempt is to pass context to local classifiers by building probabilistic graphical models (PGM). For example, Shotton et al. <ref type="bibr" target="#b19">[20]</ref> formulates the unary and pairwise features in a 2nd-order Conditional Random Field (CRF). Zhang et al. <ref type="bibr" target="#b33">[34]</ref> and Roy et al. <ref type="bibr" target="#b17">[18]</ref> build a fully connected graph to enforce higher order labeling coherence. Shuai et al. <ref type="bibr" target="#b20">[21]</ref> models the global-order dependencies in a non-parametric framework to disambiguate the local confusions. Our work also differs from them. First, the label dependencies are defined in terms of compatibility functions in PGM, while such dependencies are modeled through a recurrent weight matrix in RNNs. Moreover, the inference of PGM is inefficient as the convergence of local beliefs usually takes many iterations. In contrast, RNNs only need a single forward pass to propagate the local information.</p><p>Some of the previous work exploit 'recurrent' ideas in a different way. They generally refer to applying the identical model recurrently at different iterations (layers). For example, Pinheiro et al. <ref type="bibr" target="#b16">[17]</ref> attachs the RGB raw data with the output of the Convolutional Neural Network (CNN) to produce the input for the same CNN in the next layer. Tu et al. <ref type="bibr" target="#b27">[28]</ref> augments the patch feature with the output of the classifier to be the input for the next iteration, and the classifier parameters are shared across different iterations. Zheng et al. <ref type="bibr" target="#b34">[35]</ref> transforms Conditional Random Fields (CRF) to a neural network, so the inference of CRF equals to applying the same neural network recurrently until some fixed point (convergence) is reached. Our work differs from them significantly. They model the context in the form of intermediate outputs (usually local beliefs), which implicitly encodes the neighborhood information. In contrast, the contextual dependencies are modeled explicitly in DAG-RNNs by propagating information via the recurrent connections.</p><p>Recurrent neural networks (RNNs) have achieved great success in temporal dependency modeling for chainstructured data, such as natural language and speeches. Zuo et al. <ref type="bibr" target="#b36">[37]</ref> applies 1D-RNN to model weak contextual dependencies in image classification. Graves et al. <ref type="bibr" target="#b7">[8]</ref> generalizes 1D-RNN to multi-dimensional RNN (MDRNN) and applies it to offline arabic handwriting recognition. Shuai et al. <ref type="bibr" target="#b21">[22]</ref> also adopts 2D-RNN to real-world image labeling. Recently, Tai et al. <ref type="bibr" target="#b24">[25]</ref> and Zhu et al. <ref type="bibr" target="#b35">[36]</ref> demonstrate that considering tree structure (constituent / parsing trees for sentences) is beneficial for modeling the global representation of sentences. Our proposed DAG-RNN is a generalization of chain-RNNs [1] <ref type="bibr" target="#b9">[10]</ref>, tree-RNNs <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b35">[36]</ref> and 2D-RNNs <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b21">[22]</ref>, and it enables the network to model long-range semantic dependencies for graphical structured images. The most relevant work to ours is <ref type="bibr" target="#b21">[22]</ref>. In comparison with which, (1), we generalize 2D-RNN to DAG-RNN and show benefits in quantitative labeling performance; (2), we integrate the convolution layer, deconvolution layer with our DAG-RNNs to a full labeling network; and (3), we adopt a novel class weighting function to address the extremely imbalanced class distribution issue in natural scene images. To the best of our knowledge, our work is the first attempt to integrate the convolution layers with RNNs in an end-to-end trainable network for real-world image labeling. Moreover, the proposed full network achieves state-of-theart on a variety of scene labeling benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>To densely label an image I, the image is processed by three different functional layers sequentially: (1), Convolution layer produces the corresponding feature map x. Each feature vector in x summarizes the information from a local region in I. <ref type="bibr" target="#b1">(2)</ref>, DAG-RNNs model the contextual dependency among elements in x, and generates the intermediate feature map h, whose element is a feature vector that implicitly embeds the abstract gist of the image. (3), Deconvolution layer <ref type="bibr" target="#b13">[14]</ref> upsamples the feature maps. From which, the dense label prediction maps are derived. We start by introducing the proposed DAG-RNNs, and the details of the full network are elaborated in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">RNNs Revisited</head><p>A recurrent neural network (RNN) is a class of artificial neural network that has recurrent connections, which equip the network with memory. In this paper, we focus on the Elman-type network <ref type="bibr" target="#b5">[6]</ref>. Specifically, the hidden layer h (t) in RNNs at time step t is expressed as a non-linear function over current input x (t) and hidden layer at previous time step h (t−1) . The output layer y (t) is connected to the hidden layer h (t) .</p><p>Mathematically, given a sequence of inputs {x (t) } t=1:T , an Elman-type RNN operates by computing the following hidden and output sequences:</p><formula xml:id="formula_0">h (t) = f (U x (t) + W h (t−1) + b) y (t) = g(V h (t) + c)<label>(1)</label></formula><p>where U, W are weight matrices between the input and hidden layers, and among the hidden units themselves, while V is the output matrix connecting the hidden and output layers; b, c are corresponding bias vectors and f (·), g(·) are element-wise nonlinear activation functions. The initial hidden unit h (0) is usually assumed to be 0. The local information x (t) is progressively stored in the hidden layers by applying Equation 1. In other words, the contextual information (the summarization of past sequence information) is explicitly encoded into local representation h (t) , which improves their representative power dramatically in practice.</p><p>Training a RNN can be achieved by optimizing a discriminative objective with a gradient-based method. Back Propagation through time (BPTT) <ref type="bibr" target="#b29">[30]</ref> is usually used to calculate the gradients. This method is equivalent to unfolding the network in time and using back propagation in a very deep feed-forward network except that the weights across different time steps (layers) are shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">DAG-RNNs</head><p>The aforementioned RNN is designed for chainstructured data (e.g. sentences or speeches), where temporal dependency is modeled. However, interactions among image units are beyond chain. In other words, traditional chain-structured RNNs are not suitable for images. Specifically, we can reshape the feature tensor x ∈ R h×w×d tô x ∈ R (h·w)×d , and generate the chain representation by connecting contiguous elements inx. Such a structure loses spatial relationship of image units, as two adjacent units in image plane may not necessarily be neighbors in the chain. The graphical representations that respect the 2-D neighborhood system are more plausible solutions, and they are pervasively adopted in probabilistic graphical models (PGM). Therefore in this work, undirected cyclic graphs (UCG , an example is shown in <ref type="figure" target="#fig_0">Figure 2</ref>) are used to model the interactions among image units. Due to the loopy structure of UCGs, they are unable to be unrolled to an acyclic processing sequence. Therefore, RNNs are not directly applicable to UCG-structured images. To address this issue, we approximate the topology of UCG by a combination of several directed acyclic graphs (DAGs), each of which is applicable for our proposed DAG-RNNs (one of the induced DAGs is depicted in <ref type="figure" target="#fig_0">Figure 2</ref>). Namely, an UCG-structured image is represented as the combination of a set of DAG-structured images. We now start introducing the detailed mechanism of our DAG-RNNs here, and later elaborate how they are applied to UCG-structured images in the next section.</p><p>We first assume that an image I is represented as a DAG</p><formula xml:id="formula_1">G = {V, E}, where V = {v i } i=1:N is the vertex set and E = {e ij } is the arc set (e ij denotes an arc from v i to v j ).</formula><p>The structure of the hidden layer h follows the same topology as G. Therefore, a forward propagation sequence can be generated by traversing G, on the condition that one node should not be processed until all its predecessors are processed. The hidden layer h (vi) is represented as a nonlinear function over its local input x (vi) and the summarization of hidden representation of its predecessors. The local input x (vi) is obtained by aggregating (e.g. average pooling) from constituent elements in the feature tensor x. In detail, the forward operation of DAG-RNNs is calculated by the following equations:</p><formula xml:id="formula_2">h (vi) = vj ∈P G (vi) h (vj ) h (vi) = f (U x (vi) + Wĥ (vi) + b) o (vi) = g(V h (vi) + c) (2) where x (vi) , h (vi) , o (vi)</formula><p>are the representations of input, hidden and output layers located at v i respectively, P G (v i ) is the direct predecessor set of vertex v i in the graph G,ĥ <ref type="bibr">(vi)</ref> summarizes the information of all the predecessors of v i . Note that the recurrent weight W in Equation 2 is shared across all predecessor vertexes in P G (v i ). We may learn a specific recurrent matrix W for each predecessor when vertexes (except source and sink vertex) in the DAG G have a fixed number of predecessors. In this case, a finer-grained dependency may be captured.</p><p>The derivatives are computed in the backward pass, and each vertex is processed in the reverse order of forward propagation sequence. Specifically, to derive the gradients at v i , we look at equations (besides Equation 2) that involve h (vi) in the forward pass:</p><formula xml:id="formula_3">∀v k ∈ SG(vi) h (v k ) = f (U x (v k ) + W h (v i ) + Wh (v k ) + b) h (v k ) = v j ∈P G (v k )−{v i } h (v j )<label>(3)</label></formula><p>where S G (v i ) is the direct successor set for vertex v i in the graph G. It can be inferred from Equation 2, 3 that the errors backpropagated to the hidden layer (dh (vi) ) at v i have two sources: direct errors from v i ( ∂o (v i ) ∂h (v i ) ), and summation over indirect errors propagated from its successors</p><formula xml:id="formula_4">( v k ∂o (v k ) ∂h (v k ) ∂h (v k ) ∂h (v i )</formula><p>). The derivatives at v i can then be computed by the following equations: 1</p><formula xml:id="formula_5">∆V (v i ) = g (o (v i ) )(h (v i ) ) T dh (v i ) = V T g (o (v i ) ) + v k ∈S G (v i ) W T dh (v k ) • f (h (v k ) ) ∆W (v i ) = v k ∈S G (v i ) dh (v k ) • f (h (v k ) )(h (v i ) ) T ∆U (v i ) = dh (v i ) • f (h (v i ) )(x (v i ) ) T (4) where • denotes the Hadamard product, g (·) = ∂L ∂o(·) ∂o(·) ∂g</formula><p>is the derivative of loss function L with respect to the output function g, and f (·) = ∂h ∂f . It is the second term of dh <ref type="bibr">(vi)</ref> in Equation 4 that enables DAG-RNNs to propagate local information, which behaves similarly to the message passing <ref type="bibr" target="#b31">[32]</ref> in probabilistic graphic models. <ref type="bibr" target="#b0">1</ref> To save space, we omit the expression for ∆b and ∆c here as they can be inferred trivially from Equation 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolution Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAG-Recurrent Neural Network</head><p>Deconvolution Layer <ref type="figure">Figure 3</ref>: The architecture of the full labeling network, which consists of three functional layers: (1), convolution layer: it produces discriminative feature maps; (2), DAG-RNN: it models the contextual dependency among elements in the feature maps; (3), deconvolution layer: it upsamples the feature maps to output the desired sizes of label prediction maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decomposition</head><p>We decompose the UCG U to a set of DAGs G U = {G 1 , . . . , G d , . . .}. Hence, the UCG-structured image is represented as the combination of a set of DAG-structured images. Next, DAG-RNNs are applied independently to each DAG-structured image, and the corresponding hidden layer h d is generated. The aggregation of the independent hidden layers yields the output layer o. These operations can be mathematically expressed as follows:</p><formula xml:id="formula_6">h (v i ) d = f (U d x (v i ) + v j ∈P G d (v i ) W d h (v j ) d + b d ) o (v i ) = g( G d ∈G U V d h (v i ) d + c)<label>(5)</label></formula><p>where U d , W d , V d and b d are weight matrices and bias vec-</p><formula xml:id="formula_7">tor for the DAG G d , P G d (v i ) is the direct predecessor set of vertex v i in G d .</formula><p>This strategy is reminiscent of the treereweighted max-product algorithm (TRW) <ref type="bibr" target="#b28">[29]</ref>, which represents the problem on the loopy graphs as a convex combination of tree-structured problems. We consider the following criterions for the decomposition. Topologically, the combination of DAGs should be equivalent to the UCG U, so any two vertexes can be reachable. Besides, the combination of DAGs should allow the local information to be routed to anywhere in the image. In our experiment, we use the four context propagation directions (southeast, southwest, northwest and northeast) suggested by <ref type="bibr" target="#b7">[8]</ref>[22] to decompose the UCG. One example of the induced DAG of the 8-neighborhood UCG in the southeast direction is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Full Labeling Network</head><p>The skeleton architecture of the full labeling network is illustrated in <ref type="figure">Figure 3</ref>. The network is end-to-end trainable, and it takes input as raw RGB images with any size. It outputs the label prediction maps with the same size of inputs.</p><p>The convolution layer is used to produce compact yet highly discriminative features for local regions. Next, the proposed DAG-RNN is used to model the semantic contextual dependencies of local representations. Finally, the deconvolution layer <ref type="bibr" target="#b13">[14]</ref> is introduced to upsample the feature maps by learning a set of deconvolution filters, and it  <ref type="figure">Figure 4</ref>: Graphical visualization of the class frequencies (left) and weights (right) on the siftFlow datasets <ref type="bibr" target="#b12">[13]</ref>. The classes are sorted in the descending order based on their occurrence frequencies in training images.</p><p>enables the full labeling network to produce the desired size of label prediction maps.</p><p>To train the network, we adopt the average weighted cross entropy loss. It is formally written as:</p><formula xml:id="formula_8">L = − 1 N v i ∈I c j=1 wjlog(o (v i ) j y (v i ) j )<label>(6)</label></formula><p>where N is the number of image units in image I; w is the class weight vector, in which w j stands for the weight for class j; y (vi) is the binary label indicator vector for the image unit located in v i , and o (vi) stands for the corresponding class likelihood vector. The errors propagated from DAG-RNNs to the convolution layer for image unit v i are calculated based on the following equations:</p><formula xml:id="formula_9">∆x (v i ) = G d ∈G U U T d dh (v i ) d • f (h (v i ) d )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Attention to Rare Classes</head><p>In scene images, the class distribution is extremely imbalanced. Namely, very few classes account for large percentage of pixels in images. An example is demonstrated in <ref type="figure">Figure 4</ref>. It's therefore common to put more attention to rare classes, in order to boost their recognition precisions.</p><p>In the patch-based CNN training, Farabet et al. <ref type="bibr" target="#b6">[7]</ref> and Shuai et al. <ref type="bibr" target="#b20">[21]</ref> oversample the rare-class pixels to address this issue. It's however inapplicable to adopt this strategy in our network training, which is a complex structure learning problem. Meanwhile, as the classes are distributed severely unequally in scene images, it's also problematic to weigh classes according to their inverse frequencies. As an example, the frequency ratio between the most frequent (sky) and the most rare class (moon) on the SiftFlow dataset is 3.5 × 10 4 . If the above class weighting criterion is adopted like in <ref type="bibr" target="#b14">[15]</ref>, the frequent classes will be under-attended. Hence, we define the weighting function w as follows:</p><formula xml:id="formula_10">w j = k log10(η/fj )<label>(8)</label></formula><p>where · is the integer ceiling operator, f j is the occurrence frequency of the class j, η denotes the threshold that discriminates the rare classes. Specifically, a class is identified as rare if its frequency is smaller than η, otherwise, it is a frequent class. k is a constant that controls the importance of rare classes (k = 2 in our experiments). The proposed weighting function has the following properties: <ref type="bibr" target="#b0">(1)</ref>, it attends to rare classes by assigning them higher weights; (2), the degree of attention for rare classes grows exponentially based on their ratio magnitudes w.r.t the threshold η; The following criterion is used to determine the value of η: the accumulated frequency of all the non-rare classes is 85%. We call it 85%-15% rule, and [31] uses a similar rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We justify our method on three popular and challenging real-world scene image labeling benchmarks: SiftFlow <ref type="bibr" target="#b12">[13]</ref>, CamVid <ref type="bibr" target="#b1">[2]</ref> and Barcelona <ref type="bibr" target="#b25">[26]</ref>. Two types of scores are reported: the percentage of all correctly classified pixels (Global), and average per-class accuracy (Class).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines</head><p>The convolution neural network (CNN), which jointly learn features and classifiers is used as our first baseline. In this case, the parameters are optimized to maximize the independent prediction accuracy for local patches. Another baseline is the network that shares the same architecture with our DAG-RNNs, while removes the recurrent connections. Mathematically, the W d and b d in Equation 5 are fixed to 0 . In this case, the DAG-recurrent neural network degenerates to an ensemble of four plain two-layer neural networks <ref type="figure">(CNN-ENN)</ref>. The performance disparity between the baselines and DAG-RNNs clearly illuminates the efficacy of our dependency modeling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use the following two networks to be the convolution layers in our experiments:</p><p>• CNN-65: The network consists of five convolutional layers, the kernel sizes of which are 8 × 8 × 3 × 64, 6 × 6 × 64 × 128, 5 × 5 × 128 × 256, 4 × 4 × 256 × 256 and 1×1×256×64 respectively. Each of the first three convolutional layers are followed by a ReLU and nonoverlapping 2 × 2 max pooling layer. The parameters of this network is learned from image patches (65×65) of the target dataset only (Setting 1).</p><p>• VGG-conv5: The network borrows its architecture and parameters from VGG-verydeep-16 net <ref type="bibr" target="#b22">[23]</ref>. In detail, we discard all the layers after the 5 th pooling layer to yield the desired convolution layer. The network is pre-trained on ImageNet dataset and fine-tuned on the target dataset. <ref type="bibr" target="#b4">[5]</ref> (Setting 2).</p><p>In DAG-RNNs, the adopted non-linear functions (refer to Equation 2) are ReLU <ref type="bibr" target="#b10">[11]</ref> for hidden neurons: f (x) = max(0, x) and sof tmax for output layer g. In practice, we apply the function g after the deconvolution layer. The dimensionality of hidden layer h is empirically set to 64 for CNN-65 and 128 for VGG-conv5 respectively. <ref type="bibr" target="#b1">2</ref> In our experiments, we consider two UCGs with 4 and 8 neighborhood systems. Their induced DAGs in the northwestern direction are shown in <ref type="figure">Figure 5</ref>. In comparison with DAG(4), DAG(8) enables information to be propagated in shorter paths, which is critical to prevent the long-range information from vanishing. As exampled in <ref type="figure">Figure 5</ref>, the length of propagation path from v 9 to v 1 in G 8 nw is halved to that in G 4 nw (4 → 2 steps). The full network is trained by stochastic gradient descent with momentum. The parameters are updated after one image finishes its forward and backward passes. The learning rate is initialized to be 10 −3 , and decays exponentially with the rate of 0.9 after 10 epoch. The reported results are based on the model trained in 35 epoches. We tune the parameters and diagnoses the network performance based on CNN-65. We also include the results of VGG-conv5 to see whether our proposed DAG-RNNs are beneficial for the highly discriminative representation from the state-of-the-art VGGverydeep-16 net [23].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">SiftFlow Dataset</head><p>The SiftFlow dataset has 2688 images generally captured from 8 typical outdoor scenes. Every image has 256 × 256 pixels, which belong to one of the 33 semantic classes. We adopt the training/testing split protocol (2488/200 images) provided by <ref type="bibr" target="#b12">[13]</ref> to perform our experiments. Following the 85%-15% criterion, the class frequency threshold η = 0.05. Statistically, out of 33 classes, 27 of them are regarded as infrequent class. The graphical visualization of the weights for different classes are depicted in <ref type="figure">Figure 4</ref>.</p><p>The quantitative results are listed in <ref type="table">Table 1</ref>, within which the upper part presents the performance of methods under setting 1. Our baseline CNN-65 achieves very promising results, which proves the effectiveness of the convolution layer. We also notice that results of CNN-65 fall behind CNN-65-ENN on the average class accuracy. This phenomenon is also observed on the CamVid and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Global Class Byeon et al. <ref type="bibr" target="#b3">[4]</ref> 70.1% 22.6% Liu et al. <ref type="bibr" target="#b12">[13]</ref> 74.8% N/A Farabet et al. <ref type="bibr" target="#b6">[7]</ref> 78.5% 29.4% Pinheiro et al. <ref type="bibr" target="#b16">[17]</ref> 77.7% 29.8% Tighe et al. <ref type="bibr" target="#b26">[27]</ref> 79.2% 39.2% Sharma et al. <ref type="bibr" target="#b18">[19]</ref> 79.6% 33.6% Shuai et al. <ref type="bibr" target="#b20">[21]</ref> 80.1% 39.7% Yang et al. <ref type="bibr" target="#b30">[31]</ref> 79.8% 48.7% CNN-65 76.1% 32.5% CNN-65-ENN 76.1% 37.0% CNN-65-DAG-RNN <ref type="bibr" target="#b3">(4)</ref> 80.5% 42.6% CNN-65-DAG-RNN <ref type="bibr" target="#b7">(8)</ref> 81.1% 48.2%</p><p>Long et al. <ref type="bibr" target="#b13">[14]</ref> 85.2% 51.7% VGG-conv5-ENN 84.0% 48.8% VGG-conv5-DAG-RNN <ref type="bibr" target="#b7">(8)</ref> 85.3% 55.7% <ref type="table">Table 1</ref>: Quantitative performance of our method on the siftFlow dataset. The numbers (in brackets) following the DAG-RNN denote the neighborhood system of the UCG.</p><p>Barcelona benchmarks, as shown by <ref type="table" target="#tab_2">Table 2</ref> and 3 respectively. This result indicates that the proposed class weighting function significantly boosts the recognition accuracy for rare classes. By adding DAG-RNN(8), our full network reaches 81.1% (48.2%) on the global (class) accuracy 3 , which outperforms the baseline (CNN-65-ENN) by 5% (11.2%). Meanwhile, we observe promising accuracy gain (global: 0.6% / class: 5.6% ) by switching DAG-RNN (4) to DAG-RNN <ref type="bibr" target="#b7">(8)</ref>, in which we believe that long-range dependencies are better captured as information propagation paths in DAG <ref type="bibr" target="#b7">(8)</ref> are shorter than those in DAG(4). Such performance benefits can be observed consistently on the CamVid (0.5% / 2.0%) and Barcelona (1.1% / 1.6%) datasets, as evidenced in <ref type="table" target="#tab_2">Table 2 and 3</ref> respectively. Moreover, in comparison with other representation learning nets, which are fed with much richer contextual input (133×133 patch in <ref type="bibr" target="#b16">[17]</ref>, 3-scale 46×46 patches in <ref type="bibr" target="#b6">[7]</ref>), our DAG-RNNs outperform theirs by a large margin. Importantly, our results match the state-of-the-art under this setting. Furthermore, we initialize our convolution layers with VGG-verydeep-16 <ref type="bibr" target="#b22">[23]</ref>, which has been proven to be the state-of-the-art feature extractor. The quantitative results under setting 2 are listed in the lower body of <ref type="table">Table 1</ref>. Our baseline VGG-conv5-ENN surpasses the best performance of methods under setting 1. This result indicates the significance of large-scale data in deep neural network training. Interestingly, our DAG-RNN(8) is still able to further improve the discriminative power of local features by modeling their dependencies, thereby leading to a phenomenal (6.9%) average class accuracy boost. Note that Fully Convolution Networks (FCNs) <ref type="bibr" target="#b13">[14]</ref> uses activations Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global</head><p>Class Tighe et al. <ref type="bibr" target="#b25">[26]</ref> 78.6% 43.8% Sturgess et al. <ref type="bibr" target="#b23">[24]</ref> 83.8% 59.2% Zhang et al. <ref type="bibr" target="#b32">[33]</ref> 82.1% 55.4% Bulo et al. <ref type="bibr" target="#b2">[3]</ref> 82.1% 56.1% Ladicky et al. <ref type="bibr" target="#b11">[12]</ref> 83.8% 62.5% Tighe et al. <ref type="bibr" target="#b26">[27]</ref> 83  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">CamVid Dataset</head><p>The CamVid dataset <ref type="bibr" target="#b1">[2]</ref> contains 701 high-resolution images (960 × 720 pixels) from 4 driving videos at daytime and dusk (3 daytime and 1 dusk video sequence). Images are densely labelled with 32 semantic classes. We follow the usual split protocol <ref type="bibr" target="#b23">[24]</ref> <ref type="bibr" target="#b26">[27]</ref> (468/233) to obtain training/testing images. Similar to other works <ref type="bibr" target="#b1">[2]</ref>[3] <ref type="bibr" target="#b23">[24]</ref>[27], we only report results on the most common 11 categories. According to the 85%-15% rule, 4 classes are identified as rare, and η is 0.1.</p><p>The quantitative results are given in <ref type="table" target="#tab_2">Table 2</ref>. Our baseline networks (CNN-65, CNN-65-ENN) achieve very competitive results. By explicitly modeling contextual dependencies among image units, our CNN-65-DAG-RNN(8) brings phenomenal performance benefit (4.6% and 10.2% for the global and class accuracy respectively). Moreover, in comparison with state-of-the-art methods <ref type="bibr" target="#b2">[3]</ref>[12] <ref type="bibr" target="#b23">[24]</ref> [27], our CNN-65-DAG-RNN(8) outperforms theirs by a large margin (4.8% / 5.8%), demonstrating the profitability of adopting high-level features learned from CNN and context modeling with our DAG-RNNs. Furthermore, the VGG-conv5-ENN alone performs excellently. Even though the performance starts saturating, our DAG-RNN(8) is able to consistently improve the labeling results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Barcelona Dataset</head><p>The barcelona dataset <ref type="bibr" target="#b25">[26]</ref> consists of 14871 training and 279 testing images. The size of the images varies across different instances, and each pixel is labelled as one of the 170</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Global Class Tighe et al. <ref type="bibr" target="#b25">[26]</ref> 66.9% 7.6% Farabet et al. <ref type="bibr" target="#b6">[7]</ref> 46.4% 12.5% Farabet et al. <ref type="bibr" target="#b6">[7]</ref> 67  semantic classes. The training images range from indoor to outdoor scenes, whereas the testing images are only captured from the barcelona street scene. These issues pose Barcelona as a very challenging dataset. Based on the 85%-15% rule, 147 classes are identified as rare classes, and the class frequency threshold η is 0.005. <ref type="table" target="#tab_4">Table 3</ref> presents the quantitative results. From which, we clearly observe that our baseline networks (CNN-65 and CNN-65-ENN) achieve very competitive results, which has already matched the state-of-the-art results. The introduction of DAG-RNN(8) leads to promising performance improvement, therefore the full labeling network clinches the new state-of-the-art under setting 1. More importantly, under setting 2, even though the VGG-conv5-ENN is extraordinarily competitive, the DAG-RNN(8) is still able to enhance its labeling performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Effects of DAG-RNNs to Per-class Accuracy</head><p>In this section, we investigate the effects of our DAG-RNNs for each class. The detailed per-class accuracy for the SiftFlow dataset is listed in <ref type="table" target="#tab_6">Table 4</ref>. Under setting 1, we find that the contextual information encoded through our DAG-RNN(8) is beneficial for almost all classes. In this case, the local representations from CNN-65 are not strong, so their discriminative power can be greatly enhanced by modeling their dependencies. In line with it, we observe remarkable performance benefit (+11.2%) for almost all classes. Under setting 2, the VGG-conv5 net is pre-trained on the ImageNet dataset <ref type="bibr" target="#b4">[5]</ref>, and it recognizes most classes excellently. Even though the local representations are highly discriminative in this situation, our DAG-RNN(8) further tremendously improves their representative power for rare classes. Statistically, we observe a phenomenal 8.6% accuracy gain for rare classes. Under both settings, modeling the dependencies among local features enables the classification to be contextual aware. Therefore, the local ambiguities are mitigated to a large extent. However, we fail to observe commensurate accuracy improvements for extremely-small-size and rare 'object' classes (e.g. bird and bus), we conjecture that the weak local information may have been overwhelmed by context (e.g. a small bird is swallowed by the broad sky in <ref type="figure">Figure 1</ref>).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Discussion of Modeled Dependency</head><p>We show a number of qualitative labeling results in Figure 6. By looking into them, we can have some interesting observations. The DAG-RNNs are capable of (1), enforcing local consistency: neighborhood pixels are likely to be assigned to the same labels. In <ref type="figure" target="#fig_2">Figure 6</ref>, the left-panel examples show that confusing regions are smoothed by using our DAG-RNNs. (2), ensuring semantic coherence: the pixels that are spatially far away are usually given labels that could co-occur in a meaningful scene. For example, the 'desert' and 'mountain' classes are usually not seen together with 'trees' in a 'open country' scene, so they are corrected to 'stone' in the second example of the right panel. More examples of this kind are shown in the right panel. These results illuminate that short-range and long-range contextual dependencies may have been captured by our DAG-RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose DAG-RNNs to process DAGsstructured data, where the interactions among local features are considered in a graphical structure. Our DAG-RNNs are capable of encoding the abstract gist of images into local representations, which tremendously enhance their discriminative power. Furthermore, we propose a novel class weighting function to address the imbalanced class distribution issue, and it is experimentally proved to be effective towards the recognition enhancement for rare classes. Integrating with the convolution and deconvolution layers, our DAG-RNNs achieve state-of-the-art results on three challenging scene labeling benchmarks. We also demonstrate that useful long-range contextual dependencies are captured by our DAG-RNNs, which is helpful for generating smooth and semantically sensible labeling maps in practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An 8-neighborhood UCG and one of its induced DAG in the southeastern (SE) direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>G 8 nwFigure 5 :</head><label>85</label><figDesc>Two UCGs (with 4, 8 neighborhood system) and their induced DAGs in the northwestern (NW) direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative labeling results (best viewed in color). We show input images, local prediction maps (CNN-65-ENN), contextual labeling maps (CNN-65-DAG-RNN(8)) and their ground truth respectively. The numbers outside and inside the parentheses are global and class accuracy respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative performance of our method on the CamVid dataset.</figDesc><table><row><cell>(feature maps) from multiple convolution layers, whereas</cell></row><row><cell>our VGG-conv5-ENN only use feature maps from conv5</cell></row><row><cell>layer. Hence, there is a slight performance gap between</cell></row><row><cell>our VGG-conv5-ENN and FCNs. Nonetheless, our VGG-</cell></row><row><cell>conv5-DAG-RNN(8) still performs comparably with FCNs</cell></row><row><cell>on global accuracy, and significantly outperforms it on the</cell></row><row><cell>class accuracy. Importantly, our full labeling network also</cell></row><row><cell>achieves new state-of-the-art performance under this set-</cell></row><row><cell>ting. The detailed per-class accuracy is listed in Table 4.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative performance of our method on the Barcelona dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>20.2 12.6 12.4 6.93 5.61 3.64 1.64 1.41 1.37 1.33 1.22 1.07 0.89 0.85 0.36 0.26 0.24 0.23 0.18 0.11 0.11 0.06 0.05 0.04 0.03 0.03 0.02 0.01 0.004 --CNN-65-ENN 94.1 84.9 77.6 74.2 80.9 61.1 30.7 71.0 27.7 34.7 21.8 63.5 27.8 47.0 21.0 8.8 35.7 33.7 29.3 6.8 0.37 16.3 1.4 48.1 0 0.43 34.5 6.0 71.4 0 76.1 37.0 CNN-65-DAG-RNN(8) 95.9 87.3 82.5 77.9 85.8 70.2 43.5 80.1 52.9 65.4 37.2 57.8 40.4 59.1 27.6 31.4 51.8 38.0 35.6 50.9 7.8 31.0 5.14 82.8 0 0 54.9 8.59 85.5 0 81.1 48.2 VGG-conv5-ENN 96.0 91.1 84.4 82.9 90.4 83.5 48.8 77.5 63.5 57.6 32.6 60.2 34.9 66.0 25.8 20.0 51.9 44.0 38.6 45.9 26.5 33.7 14.9 50.2 1.1 0 32.7 9.1 99.9 0 84.0 48.8 VGG-conv5-DAG-RNN(8) 96.3 90.8 82.1 85.1 89.2 84.8 55.4 84.2 67.9 75.3 51.5 64.8 45.2 63.5 45.7 37.3 56.8 44.7 36.2 58.7 18.3 40.0 63.3 65.2 18.4 1.4 45.8 5.4 97.9 0 85.3 55.7</figDesc><table><row><cell></cell><cell>sky</cell><cell>building</cell><cell>tree</cell><cell>mountain</cell><cell>road</cell><cell>sea</cell><cell>field</cell><cell>car</cell><cell>sand</cell><cell>river</cell><cell>plant</cell><cell>grass</cell><cell>window</cell><cell>sidewalk</cell><cell>rock</cell><cell>bridge</cell><cell>door</cell><cell>fence</cell><cell>person</cell><cell>staircase</cell><cell>awning</cell><cell>sign</cell><cell>boat</cell><cell>crosswalk</cell><cell>pole</cell><cell>bus</cell><cell>balcony</cell><cell>streetlight</cell><cell>sun</cell><cell>bird</cell><cell>global</cell><cell>class</cell></row><row><cell>Frequency</cell><cell>27.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Per-class accuracy comparison on the SiftFlow dataset. All the numbers are displayed in the percentage scale. The statistics for class frequency is obtained in test images. For reading convenience, the frequent and rare classes are placed in the same block.</figDesc><table><row><cell cols="3">90.8% (77.6%) 97.5% (95.3%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">86.9% (56.1%) 94.8% (72.7%)</cell></row><row><cell cols="3">75.5% (69.7%) 94.9% (91.5%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">55.9% (65.2%) 85.4% (87.7%)</cell></row><row><cell cols="3">74.8% (50.9%) 87.2% (52.4%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">64.0% (57.3%) 83.2% (72.8%)</cell></row><row><cell cols="3">87.9% (75.6%) 94.3% (85.9%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">55.1% (50.8%) 87.2% (42.3%)</cell></row><row><cell>desert</cell><cell>river</cell><cell>field</cell><cell>building</cell><cell>tree</cell><cell>sky</cell><cell>road mountain</cell><cell>rock sidewalk plant</cell><cell>person</cell><cell>unlabled</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Based on our preliminary results, we didn't observe too much performance improvement by using larger h (e.g. 128 in CNN-65, and 256 in VGG-conv5) on the siftFlow dataset. In addition, the networks with larger capacity incur much heavier computation burdens.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">If we disassemble the full labeling network to two disjoint parts -CNN-65 and DAG-RNN(8), and they are optimized independently, the corresponding accuracies are 80.1% and 42.7%. The performance discrepancy indicates the importance of the joint optimization for the full network.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Advances in optimizing recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8624" to="8628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (1)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural decision forests for semantic image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3547" to="3555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Offline arabic handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Guide to OCR for Arabic Scripts</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="297" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What, where and how many? combining object detectors and crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladickỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="424" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonparametric scene parsing: Label transfer via dense scene alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1972" to="1979" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CVPR 2015. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scene labeling using beam search under mutex constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recursive context propagation network for semantic scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2447" to="2455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2006</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integrating parametric and non-parametric models for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CVPR 2015. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quaddirectional 2d-recurrent neural networks for image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Combining appearance and structure from motion features for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="352" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Finding things: Image parsing with regions and per-exemplar detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3001" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new class of upper bounds on the log partition function. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2313" to="2335" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Context driven scene parsing with attention to rare classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3294" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Understanding belief propagation and its generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="236" to="239" />
		</imprint>
	</monogr>
	<note>Exploring artificial intelligence in the new millennium</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic segmentation of urban scenes using dense depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="708" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient inference for fullyconnected crfs with stationarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="582" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03240</idno>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Long short-term memory over tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04881</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks: Learning spatial dependencies for image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
