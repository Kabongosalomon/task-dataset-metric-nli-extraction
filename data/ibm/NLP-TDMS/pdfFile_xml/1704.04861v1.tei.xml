<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
							<email>howarda@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hartwig Adam Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
							<email>menglong@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hartwig Adam Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
							<email>bochen@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hartwig Adam Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
							<email>dkalenichenko@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hartwig Adam Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
							<email>weijunw@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hartwig Adam Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
							<email>weyand@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hartwig Adam Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hartwig Adam Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyperparameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks have become ubiquitous in computer vision ever since AlexNet <ref type="bibr" target="#b18">[19]</ref> popularized deep convolutional neural networks by winning the Ima-geNet Challenge: ILSVRC 2012 <ref type="bibr" target="#b23">[24]</ref>. The general trend has been to make deeper and more complicated networks in order to achieve higher accuracy <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8]</ref>. However, these advances to improve accuracy are not necessarily making networks more efficient with respect to size and speed. In many real world applications such as robotics, self-driving car and augmented reality, the recognition tasks need to be carried out in a timely fashion on a computationally limited platform.</p><p>This paper describes an efficient network architecture and a set of two hyper-parameters in order to build very small, low latency models that can be easily matched to the design requirements for mobile and embedded vision applications. Section 2 reviews prior work in building small models. Section 3 describes the MobileNet architecture and two hyper-parameters width multiplier and resolution multiplier to define smaller and more efficient MobileNets. Section 4 describes experiments on ImageNet as well a variety of different applications and use cases. Section 5 closes with a summary and conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work</head><p>There has been rising interest in building small and efficient neural networks in the recent literature, e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b21">22]</ref>. Many different approaches can be generally categorized into either compressing pretrained networks or training small networks directly. This paper proposes a class of network architectures that allows a model developer to specifically choose a small network that matches the resource restrictions (latency, size) for their application. MobileNets primarily focus on optimizing for latency but also yield small networks. Many papers on small networks focus only on size but do not consider speed.</p><p>MobileNets are built primarily from depthwise separable convolutions initially introduced in <ref type="bibr" target="#b25">[26]</ref> and subsequently used in Inception models <ref type="bibr" target="#b12">[13]</ref> to reduce the computation in the first few layers. Flattened networks <ref type="bibr" target="#b15">[16]</ref> build a network out of fully factorized convolutions and showed the potential of extremely factorized networks. Independent of this current paper, Factorized Networks <ref type="bibr" target="#b33">[34]</ref> introduces a similar factorized convolution as well as the use of topological connections. Subsequently, the Xception network <ref type="bibr" target="#b2">[3]</ref> demonstrated how to scale up depthwise separable filters to out perform Inception V3 networks. Another small network is Squeezenet <ref type="bibr" target="#b11">[12]</ref> which uses a bottleneck approach to design a very small network. Other reduced computation networks include structured transform networks <ref type="bibr" target="#b27">[28]</ref> and deep fried convnets <ref type="bibr" target="#b36">[37]</ref>.</p><p>A different approach for obtaining small networks is shrinking, factorizing or compressing pretrained networks. Compression based on product quantization <ref type="bibr" target="#b35">[36]</ref>, hashing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face Attributes</head><p>Google Doodle by Sarah Harrison <ref type="figure">Figure 1</ref>. MobileNet models can be applied to various recognition tasks for efficient on device intelligence. <ref type="bibr" target="#b1">[2]</ref>, and pruning, vector quantization and Huffman coding <ref type="bibr" target="#b4">[5]</ref> have been proposed in the literature. Additionally various factorizations have been proposed to speed up pretrained networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>. Another method for training small networks is distillation <ref type="bibr" target="#b8">[9]</ref> which uses a larger network to teach a smaller network. It is complementary to our approach and is covered in some of our use cases in section 4. Another emerging approach is low bit networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MobileNet Architecture</head><p>In this section we first describe the core layers that Mo-bileNet is built on which are depthwise separable filters. We then describe the MobileNet network structure and conclude with descriptions of the two model shrinking hyperparameters width multiplier and resolution multiplier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Depthwise Separable Convolution</head><p>The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1 × 1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1 × 1 convolution to combine the outputs the depthwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size. <ref type="figure" target="#fig_0">Figure 2</ref> shows how a standard convolution 2(a) is factorized into a depthwise convolution 2(b) and a 1 × 1 pointwise convolution 2(c).</p><p>A standard convolutional layer takes as input a D F × D F × M feature map F and produces a D F × D F × N feature map G where D F is the spatial width and height of a square input feature map 1 , M is the number of input channels (input depth), D G is the spatial width and height of a square output feature map and N is the number of output channel (output depth). The standard convolutional layer is parameterized by convolution kernel K of size D K ×D K ×M ×N where D K is the spatial dimension of the kernel assumed to be square and M is number of input channels and N is the number of output channels as defined previously.</p><p>The output feature map for standard convolution assuming stride one and padding is computed as:</p><formula xml:id="formula_0">G k,l,n = i,j,m K i,j,m,n · F k+i−1,l+j−1,m<label>(1)</label></formula><p>Standard convolutions have the computational cost of:</p><formula xml:id="formula_1">D K · D K · M · N · D F · D F<label>(2)</label></formula><p>where the computational cost depends multiplicatively on the number of input channels M , the number of output channels N the kernel size D k × D k and the feature map size D F × D F . MobileNet models address each of these terms and their interactions. First it uses depthwise separable convolutions to break the interaction between the number of output channels and the size of the kernel. The standard convolution operation has the effect of filtering features based on the convolutional kernels and combining features in order to produce a new representation. The filtering and combination steps can be split into two steps via the use of factorized convolutions called depthwise separable convolutions for substantial reduction in computational cost.</p><p>Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions. We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, a simple 1×1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use both batchnorm and ReLU nonlinearities for both layers.</p><p>Depthwise convolution with one filter per input channel (input depth) can be written as:</p><formula xml:id="formula_2">G k,l,m = i,jK i,j,m · F k+i−1,l+j−1,m<label>(3)</label></formula><p>whereK is the depthwise convolutional kernel of size D K × D K × M where the m th filter inK is applied to the m th channel in F to produce the m th channel of the filtered output feature mapĜ. Depthwise convolution has a computational cost of:</p><formula xml:id="formula_3">D K · D K · M · D F · D F<label>(4)</label></formula><p>Depthwise convolution is extremely efficient relative to standard convolution. However it only filters input channels, it does not combine them to create new features. So an additional layer that computes a linear combination of the output of depthwise convolution via 1 × 1 convolution is needed in order to generate these new features.</p><p>The combination of depthwise convolution and 1 × 1 (pointwise) convolution is called depthwise separable convolution which was originally introduced in <ref type="bibr" target="#b25">[26]</ref>.</p><p>Depthwise separable convolutions cost:</p><formula xml:id="formula_4">D K · D K · M · D F · D F + M · N · D F · D F<label>(5)</label></formula><p>which is the sum of the depthwise and 1 × 1 pointwise convolutions. By expressing convolution as a two step process of filtering and combining we get a reduction in computation of:</p><formula xml:id="formula_5">D K · D K · M · D F · D F + M · N · D F · D F D K · D K · M · N · D F · D F = 1 N + 1 D 2 K</formula><p>MobileNet uses 3 × 3 depthwise separable convolutions which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy as seen in Section 4.</p><p>Additional factorization in spatial dimension such as in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31]</ref> does not save much additional computation as very little computation is spent in depthwise convolutions. </p><formula xml:id="formula_6">... ... ... M M M D K D K D K D K N N 1 1 1 (a) Standard Convolution Filters ... ... ... M M M D K D K D K D K N N 1 1 1 (b) Depthwise Convolutional Filters ... ... ... M M M D K D K D K D K N N 1 1 1 (c) 1×1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Structure and Training</head><p>The MobileNet structure is built on depthwise separable convolutions as mentioned in the previous section except for the first layer which is a full convolution. By defining the network in such simple terms we are able to easily explore network topologies to find a good network. The MobileNet architecture is defined in <ref type="table" target="#tab_1">Table 1</ref>. All layers are followed by a batchnorm <ref type="bibr" target="#b12">[13]</ref> and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification. <ref type="figure" target="#fig_1">Figure 3</ref> contrasts a layer with regular convolutions, batchnorm and ReLU nonlinearity to the factorized layer with depthwise convolution, 1 × 1 pointwise convolution as well as batchnorm and ReLU after each convolutional layer. Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer. A final average pooling reduces the spatial resolution to 1 before the fully connected layer. Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers.</p><p>It is not enough to simply define networks in terms of a small number of Mult-Adds. It is also important to make sure these operations can be efficiently implementable. For instance unstructured sparse matrix operations are not typically faster than dense matrix operations until a very high level of sparsity. Our model structure puts nearly all of the computation into dense 1 × 1 convolutions. This can be implemented with highly optimized general matrix multiply (GEMM) functions. Often convolutions are implemented by a GEMM but require an initial reordering in memory called im2col in order to map it to a GEMM. For instance, this approach is used in the popular Caffe package <ref type="bibr" target="#b14">[15]</ref>. 1 × 1 convolutions do not require this reordering in memory and can be implemented directly with GEMM which is one of the most optimized numerical linear algebra algorithms. MobileNet spends 95% of it's computation time in 1 × 1 convolutions which also has 75% of the parameters as can be seen in <ref type="table" target="#tab_2">Table 2</ref>. Nearly all of the additional parameters are in the fully connected layer.</p><p>MobileNet models were trained in TensorFlow <ref type="bibr" target="#b0">[1]</ref> using RMSprop <ref type="bibr" target="#b32">[33]</ref> with asynchronous gradient descent similar to Inception V3 <ref type="bibr" target="#b30">[31]</ref>. However, contrary to training large models we use less regularization and data augmentation techniques because small models have less trouble with overfitting. When training MobileNets we do not use side heads or label smoothing and additionally reduce the amount image of distortions by limiting the size of small crops that are used in large Inception training <ref type="bibr" target="#b30">[31]</ref>. Additionally, we found that it was important to put very little or no weight decay (l2 regularization) on the depthwise filters since their are so few parameters in them. For the ImageNet benchmarks in the next section all models were trained with same training parameters regardless of the size of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Width Multiplier: Thinner Models</head><p>Although the base MobileNet architecture is already small and low latency, many times a specific use case or application may require the model to be smaller and faster. In order to construct these smaller and less computationally expensive models we introduce a very simple parameter α called width multiplier. The role of the width multiplier α is to thin a network uniformly at each layer. For a given layer  and width multiplier α, the number of input channels M becomes αM and the number of output channels N becomes αN . The computational cost of a depthwise separable convolution with width multiplier α is:</p><formula xml:id="formula_7">D K · D K · αM · D F · D F + αM · αN · D F · D F (6)</formula><p>where α ∈ (0, 1] with typical settings of 1, 0.75, 0.5 and 0.25. α = 1 is the baseline MobileNet and α &lt; 1 are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly α 2 . Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Resolution Multiplier: Reduced Representation</head><p>The second hyper-parameter to reduce the computational cost of a neural network is a resolution multiplier ρ. We ap- ply this to the input image and the internal representation of every layer is subsequently reduced by the same multiplier.</p><p>In practice we implicitly set ρ by setting the input resolution.</p><p>We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier α and resolution multiplier ρ:</p><formula xml:id="formula_8">D K · D K · αM · ρD F · ρD F + αM · αN · ρD F · ρD F (7)</formula><p>where ρ ∈ (0, 1] which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. ρ = 1 is the baseline MobileNet and ρ &lt; 1 are reduced computation MobileNets. Resolution multiplier has the effect of reducing computational cost by ρ 2 .</p><p>As an example we can look at a typical layer in Mo-bileNet and see how depthwise separable convolutions, width multiplier and resolution multiplier reduce the cost and parameters. <ref type="table" target="#tab_3">Table 3</ref> shows the computation and number of parameters for a layer as architecture shrinking methods are sequentially applied to the layer. The first row shows the Mult-Adds and parameters for a full convolutional layer with an input feature map of size 14 × 14 × 512 with a kernel K of size 3 × 3 × 512 × 512. We will look in detail in the next section at the trade offs between resources and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we first investigate the effects of depthwise convolutions as well as the choice of shrinking by reducing the width of the network rather than the number of layers. We then show the trade offs of reducing the network based on the two hyper-parameters: width multiplier and resolution multiplier and compare results to a number of popular models. We then investigate MobileNets applied to a number of different applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Choices</head><p>First we show results for MobileNet with depthwise separable convolutions compared to a model built with full convolutions. In <ref type="table" target="#tab_4">Table 4</ref> we see that using depthwise separable convolutions compared to full convolutions only reduces accuracy by 1% on ImageNet was saving tremendously on mult-adds and parameters. We next show results comparing thinner models with width multiplier to shallower models using less layers. To make MobileNet shallower, the 5 layers of separable filters with feature size 14 × 14 × 512 in <ref type="table" target="#tab_1">Table 1</ref> are removed. <ref type="table">Table 5</ref> shows that at similar computation and number of parameters, that making MobileNets thinner is 3% better than making them shallower. <ref type="table">Table 6</ref> shows the accuracy, computation and size trade offs of shrinking the MobileNet architecture with the width multiplier α. Accuracy drops off smoothly until the architecture is made too small at α = 0.25. <ref type="table">Table 7</ref> shows the accuracy, computation and size trade offs for different resolution multipliers by training Mo-bileNets with reduced input resolutions. Accuracy drops off smoothly across resolution. <ref type="figure" target="#fig_2">Figure 4</ref> shows the trade off between ImageNet Accuracy and computation for the 16 models made from the cross product of width multiplier α ∈ {1, 0.75, 0.5, 0.25} and resolutions {224, 192, 160, 128}. Results are log linear with a jump when models get very small at α = 0.25.   <ref type="figure" target="#fig_3">Figure 5</ref> shows the trade off between ImageNet Accuracy and number of parameters for the 16 models made from the cross product of width multiplier α ∈ {1, 0.75, 0.5, 0.25} and resolutions {224, 192, 160, 128}. <ref type="table">Table 8</ref> compares full MobileNet to the original GoogleNet <ref type="bibr" target="#b29">[30]</ref> and VGG16 <ref type="bibr" target="#b26">[27]</ref>. MobileNet is nearly as accurate as VGG16 while being 32 times smaller and 27 times less compute intensive. It is more accurate than GoogleNet while being smaller and more than 2.5 times less computation. <ref type="table">Table 9</ref> compares a reduced MobileNet with width multiplier α = 0.5 and reduced resolution 160 × 160. Reduced MobileNet is 4% better than AlexNet <ref type="bibr" target="#b18">[19]</ref> while being 45× smaller and 9.4× less compute than AlexNet. It is also 4% better than Squeezenet <ref type="bibr" target="#b11">[12]</ref> at about the same size and 22× less computation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Shrinking Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fine Grained Recognition</head><p>We train MobileNet for fine grained recognition on the Stanford Dogs dataset <ref type="bibr" target="#b16">[17]</ref>. We extend the approach of <ref type="bibr" target="#b17">[18]</ref> and collect an even larger but noisy training set than <ref type="bibr" target="#b17">[18]</ref> from the web. We use the noisy web data to pretrain a fine grained dog recognition model and then fine tune the model on the Stanford Dogs training set. Results on Stanford Dogs test set are in <ref type="table" target="#tab_1">Table 10</ref>. MobileNet can almost achieve the state of the art results from <ref type="bibr" target="#b17">[18]</ref> at greatly reduced computation and size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Large Scale Geolocalizaton</head><p>PlaNet <ref type="bibr" target="#b34">[35]</ref> casts the task of determining where on earth a photo was taken as a classification problem. The approach divides the earth into a grid of geographic cells that serve as the target classes and trains a convolutional neural network on millions of geo-tagged photos. PlaNet has been shown to successfully localize a large variety of photos and to outperform Im2GPS <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> that addresses the same task.</p><p>We re-train PlaNet using the MobileNet architecture on the same data. While the full PlaNet model based on the Inception V3 architecture <ref type="bibr" target="#b30">[31]</ref> has 52 million parameters and 5.74 billion mult-adds. The MobileNet model has only 13 million parameters with the usual 3 million for the body and 10 million for the final layer and 0.58 Million mult-adds. As shown in Tab. 11, the MobileNet version delivers only slightly decreased performance compared to PlaNet despite being much more compact. Moreover, it still outperforms Im2GPS by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Face Attributes</head><p>Another use-case for MobileNet is compressing large systems with unknown or esoteric training procedures. In a face attribute classification task, we demonstrate a synergistic relationship between MobileNet and distillation <ref type="bibr" target="#b8">[9]</ref>, a knowledge transfer technique for deep networks. We seek to reduce a large face attribute classifier with 75 million parameters and 1600 million Mult-Adds. The classifier is trained on a multi-attribute dataset similar to YFCC100M <ref type="bibr" target="#b31">[32]</ref>.</p><p>We distill a face attribute classifier using the MobileNet architecture. Distillation <ref type="bibr" target="#b8">[9]</ref> works by training the classifier to emulate the outputs of a larger model 2 instead of the ground-truth labels, hence enabling training from large (and potentially infinite) unlabeled datasets. Marrying the scalability of distillation training and the parsimonious parameterization of MobileNet, the end system not only requires no regularization (e.g. weight-decay and early-stopping), but also demonstrates enhanced performances. It is evident from Tab. 12 that the MobileNet-based classifier is resilient to aggressive model shrinking: it achieves a similar mean average precision across attributes (mean AP) as the in-house while consuming only 1% the Multi-Adds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Object Detection</head><p>MobileNet can also be deployed as an effective base network in modern object detection systems. We report results for MobileNet trained for object detection on COCO data based on the recent work that won the 2016 COCO challenge <ref type="bibr" target="#b9">[10]</ref>. In table 13, MobileNet is compared to VGG and Inception V2 <ref type="bibr" target="#b12">[13]</ref> under both Faster-RCNN <ref type="bibr" target="#b22">[23]</ref> and SSD <ref type="bibr" target="#b20">[21]</ref> framework. In our experiments, SSD is evaluated with 300 input resolution (SSD 300) and Faster-RCNN is compared with both 300 and 600 input resolution (Faster-RCNN 300, Faster-RCNN 600). The Faster-RCNN model evaluates 300 RPN proposal boxes per image. The models are trained on COCO train+val excluding 8k minival images  and evaluated on minival. For both frameworks, MobileNet achieves comparable results to other networks with only a fraction of computational complexity and model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Face Embeddings</head><p>The FaceNet model is a state of the art face recognition model <ref type="bibr" target="#b24">[25]</ref>. It builds face embeddings based on the triplet loss. To build a mobile FaceNet model we use distillation to train by minimizing the squared differences of the output </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a new model architecture called Mo-bileNets based on depthwise separable convolutions. We investigated some of the important design decisions leading to an efficient model. We then demonstrated how to build smaller and faster MobileNets using width multiplier and resolution multiplier by trading off a reasonable amount of accuracy to reduce size and latency. We then compared different MobileNets to popular models demonstrating superior size, speed and accuracy characteristics. We concluded by demonstrating MobileNet's effectiveness when applied to a wide variety of tasks. As a next step to help adoption and exploration of MobileNets, we plan on releasing models in Tensor Flow.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Convolutional Filters called Pointwise Convolution in the context of Depthwise Separable Convolution The standard convolutional filters in (a) are replaced by two layers: depthwise convolution in (b) and pointwise convolution in (c) to build a depthwise separable filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Left: Standard convolutional layer with batchnorm and ReLU. Right: Depthwise Separable convolutions with Depthwise and Pointwise layers followed by batchnorm and ReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>This figure shows the trade off between computation (Mult-Adds) and accuracy on the ImageNet benchmark. Note the log linear dependence between accuracy and computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>This figure shows the trade off between the number of parameters and accuracy on the ImageNet benchmark. The colors encode input resolutions. The number of parameters do not vary based on the input resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Example objection detection results using MobileNet SSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">MobileNet Body Architecture</cell></row><row><cell cols="2">Type / Stride</cell><cell>Filter Shape</cell><cell>Input Size</cell></row><row><cell cols="2">Conv / s2</cell><cell>3 × 3 × 3 × 32</cell><cell>224 × 224 × 3</cell></row><row><cell cols="2">Conv dw / s1</cell><cell>3 × 3 × 32 dw</cell><cell>112 × 112 × 32</cell></row><row><cell cols="2">Conv / s1</cell><cell>1 × 1 × 32 × 64</cell><cell>112 × 112 × 32</cell></row><row><cell cols="2">Conv dw / s2</cell><cell>3 × 3 × 64 dw</cell><cell>112 × 112 × 64</cell></row><row><cell cols="2">Conv / s1</cell><cell>1 × 1 × 64 × 128</cell><cell>56 × 56 × 64</cell></row><row><cell cols="2">Conv dw / s1</cell><cell>3 × 3 × 128 dw</cell><cell>56 × 56 × 128</cell></row><row><cell cols="2">Conv / s1</cell><cell>1 × 1 × 128 × 128</cell><cell>56 × 56 × 128</cell></row><row><cell cols="2">Conv dw / s2</cell><cell>3 × 3 × 128 dw</cell><cell>56 × 56 × 128</cell></row><row><cell cols="2">Conv / s1</cell><cell>1 × 1 × 128 × 256</cell><cell>28 × 28 × 128</cell></row><row><cell cols="2">Conv dw / s1</cell><cell>3 × 3 × 256 dw</cell><cell>28 × 28 × 256</cell></row><row><cell cols="2">Conv / s1</cell><cell>1 × 1 × 256 × 256</cell><cell>28 × 28 × 256</cell></row><row><cell cols="2">Conv dw / s2</cell><cell>3 × 3 × 256 dw</cell><cell>28 × 28 × 256</cell></row><row><cell cols="2">Conv / s1</cell><cell>1 × 1 × 256 × 512</cell><cell>14 × 14 × 256</cell></row><row><cell>5×</cell><cell cols="2">Conv dw / s1 3 × 3 × 512 dw Conv / s1 1 × 1 × 512 × 512</cell><cell>14 × 14 × 512 14 × 14 × 512</cell></row><row><cell cols="2">Conv dw / s2</cell><cell>3 × 3 × 512 dw</cell><cell>14 × 14 × 512</cell></row><row><cell cols="2">Conv / s1</cell><cell>1 × 1 × 512 × 1024</cell><cell>7 × 7 × 512</cell></row><row><cell cols="2">Conv dw / s2</cell><cell>3 × 3 × 1024 dw</cell><cell>7 × 7 × 1024</cell></row><row><cell cols="2">Conv / s1</cell><cell cols="2">1 × 1 × 1024 × 1024 7 × 7 × 1024</cell></row><row><cell cols="2">Avg Pool / s1</cell><cell>Pool 7 × 7</cell><cell>7 × 7 × 1024</cell></row><row><cell cols="2">FC / s1</cell><cell>1024 × 1000</cell><cell>1 × 1 × 1024</cell></row><row><cell cols="2">Softmax / s1</cell><cell>Classifier</cell><cell>1 × 1 × 1000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Resource Per Layer Type</figDesc><table><row><cell>Type</cell><cell cols="2">Mult-Adds Parameters</cell></row><row><cell>Conv 1 × 1</cell><cell>94.86%</cell><cell>74.59%</cell></row><row><cell cols="2">Conv DW 3 × 3 3.06%</cell><cell>1.06%</cell></row><row><cell>Conv 3 × 3</cell><cell>1.19%</cell><cell>0.02%</cell></row><row><cell cols="2">Fully Connected 0.18%</cell><cell>24.33%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Resource usage for modifications to standard convolution. Note that each row is a cumulative effect adding on top of the previous row. This example is for an internal MobileNet layer with DK = 3, M = 512, N = 512, DF = 14.</figDesc><table><row><cell>Layer/Modification</cell><cell>Million</cell><cell>Million</cell></row><row><cell></cell><cell cols="2">Mult-Adds Parameters</cell></row><row><cell>Convolution</cell><cell>462</cell><cell>2.36</cell></row><row><cell>Depthwise Separable Conv</cell><cell>52.3</cell><cell>0.27</cell></row><row><cell>α = 0.75</cell><cell>29.6</cell><cell>0.15</cell></row><row><cell>ρ = 0.714</cell><cell>15.1</cell><cell>0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Depthwise Separable vs Full Convolution MobileNet</figDesc><table><row><cell>Model</cell><cell>ImageNet</cell><cell>Million</cell><cell>Million</cell></row><row><cell></cell><cell cols="3">Accuracy Mult-Adds Parameters</cell></row><row><cell>Conv MobileNet</cell><cell>71.7%</cell><cell>4866</cell><cell>29.3</cell></row><row><cell>MobileNet</cell><cell>70.6%</cell><cell>569</cell><cell>4.2</cell></row><row><cell cols="3">Table 5. Narrow vs Shallow MobileNet</cell><cell></cell></row><row><cell>Model</cell><cell>ImageNet</cell><cell>Million</cell><cell>Million</cell></row><row><cell></cell><cell cols="3">Accuracy Mult-Adds Parameters</cell></row><row><cell>0.75 MobileNet</cell><cell>68.4%</cell><cell>325</cell><cell>2.6</cell></row><row><cell>Shallow MobileNet</cell><cell>65.3%</cell><cell>307</cell><cell>2.9</cell></row><row><cell cols="3">Table 6. MobileNet Width Multiplier</cell><cell></cell></row><row><cell>Width Multiplier</cell><cell>ImageNet</cell><cell>Million</cell><cell>Million</cell></row><row><cell></cell><cell cols="3">Accuracy Mult-Adds Parameters</cell></row><row><cell>1.0 MobileNet-224</cell><cell>70.6%</cell><cell>569</cell><cell>4.2</cell></row><row><cell>0.75 MobileNet-224</cell><cell>68.4%</cell><cell>325</cell><cell>2.6</cell></row><row><cell>0.5 MobileNet-224</cell><cell>63.7%</cell><cell>149</cell><cell>1.3</cell></row><row><cell>0.25 MobileNet-224</cell><cell>50.6%</cell><cell>41</cell><cell>0.5</cell></row><row><cell cols="3">Table 7. MobileNet Resolution</cell><cell></cell></row><row><cell>Resolution</cell><cell>ImageNet</cell><cell>Million</cell><cell>Million</cell></row><row><cell></cell><cell cols="3">Accuracy Mult-Adds Parameters</cell></row><row><cell>1.0 MobileNet-224</cell><cell>70.6%</cell><cell>569</cell><cell>4.2</cell></row><row><cell>1.0 MobileNet-192</cell><cell>69.1%</cell><cell>418</cell><cell>4.2</cell></row><row><cell>1.0 MobileNet-160</cell><cell>67.2%</cell><cell>290</cell><cell>4.2</cell></row><row><cell>1.0 MobileNet-128</cell><cell>64.4%</cell><cell>186</cell><cell>4.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .Table 11 .</head><label>811</label><figDesc>MobileNet Comparison to Popular Models Performance of PlaNet using the MobileNet architecture. Percentages are the fraction of the Im2GPS test dataset that were localized within a certain distance from the ground truth. The numbers for the original PlaNet model are based on an updated version that has an improved architecture and training dataset.</figDesc><table><row><cell>Model</cell><cell>ImageNet</cell><cell>Million</cell><cell>Million</cell></row><row><cell></cell><cell cols="3">Accuracy Mult-Adds Parameters</cell></row><row><cell>1.0 MobileNet-224</cell><cell>70.6%</cell><cell>569</cell><cell>4.2</cell></row><row><cell>GoogleNet</cell><cell>69.8%</cell><cell>1550</cell><cell>6.8</cell></row><row><cell>VGG 16</cell><cell>71.5%</cell><cell>15300</cell><cell>138</cell></row><row><cell cols="4">Table 9. Smaller MobileNet Comparison to Popular Models</cell></row><row><cell>Model</cell><cell>ImageNet</cell><cell>Million</cell><cell>Million</cell></row><row><cell></cell><cell cols="3">Accuracy Mult-Adds Parameters</cell></row><row><cell>0.50 MobileNet-160</cell><cell>60.2%</cell><cell>76</cell><cell>1.32</cell></row><row><cell>Squeezenet</cell><cell>57.5%</cell><cell>1700</cell><cell>1.25</cell></row><row><cell>AlexNet</cell><cell>57.2%</cell><cell>720</cell><cell>60</cell></row><row><cell cols="3">Table 10. MobileNet for Stanford Dogs</cell><cell></cell></row><row><cell>Model</cell><cell>Top-1</cell><cell>Million</cell><cell>Million</cell></row><row><cell></cell><cell cols="3">Accuracy Mult-Adds Parameters</cell></row><row><cell>Inception V3 [18]</cell><cell>84%</cell><cell>5000</cell><cell>23.2</cell></row><row><cell>1.0 MobileNet-224</cell><cell>83.3%</cell><cell>569</cell><cell>3.3</cell></row><row><cell>0.75 MobileNet-224</cell><cell>81.9%</cell><cell>325</cell><cell>1.9</cell></row><row><cell>1.0 MobileNet-192</cell><cell>81.9%</cell><cell>418</cell><cell>3.3</cell></row><row><cell>0.75 MobileNet-192</cell><cell>80.5%</cell><cell>239</cell><cell>1.9</cell></row><row><cell>Scale</cell><cell cols="3">Im2GPS [7] PlaNet [35] PlaNet</cell></row><row><cell></cell><cell></cell><cell cols="2">MobileNet</cell></row><row><cell>Continent (2500 km)</cell><cell>51.9%</cell><cell>77.6%</cell><cell>79.3%</cell></row><row><cell>Country (750 km)</cell><cell>35.4%</cell><cell>64.0%</cell><cell>60.3%</cell></row><row><cell>Region (200 km)</cell><cell>32.1%</cell><cell>51.1%</cell><cell>45.2%</cell></row><row><cell>City (25 km)</cell><cell>21.9%</cell><cell>31.7%</cell><cell>31.7%</cell></row><row><cell>Street (1 km)</cell><cell>2.5%</cell><cell>11.0%</cell><cell>11.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 12 .</head><label>12</label><figDesc>Face attribute classification using the MobileNet architecture. Each row corresponds to a different hyper-parameter setting (width multiplier α and image resolution).</figDesc><table><row><cell cols="5">Width Multiplier / Mean Million</cell><cell>Million</cell></row><row><cell cols="2">Resolution</cell><cell></cell><cell cols="3">AP Mult-Adds Parameters</cell></row><row><cell cols="4">1.0 MobileNet-224 88.7%</cell><cell>568</cell><cell>3.2</cell></row><row><cell cols="4">0.5 MobileNet-224 88.1%</cell><cell>149</cell><cell>0.8</cell></row><row><cell cols="4">0.25 MobileNet-224 87.2%</cell><cell>45</cell><cell>0.2</cell></row><row><cell cols="4">1.0 MobileNet-128 88.1%</cell><cell>185</cell><cell>3.2</cell></row><row><cell cols="4">0.5 MobileNet-128 87.7%</cell><cell>48</cell><cell>0.8</cell></row><row><cell cols="4">0.25 MobileNet-128 86.4%</cell><cell>15</cell><cell>0.2</cell></row><row><cell cols="2">Baseline</cell><cell cols="2">86.9%</cell><cell>1600</cell><cell>7.5</cell></row><row><cell cols="6">Table 13. COCO object detection results comparison using differ-</cell></row><row><cell cols="6">ent frameworks and network architectures. mAP is reported with</cell></row><row><cell cols="6">COCO primary challenge metric (AP at IoU=0.50:0.05:0.95)</cell></row><row><cell>Framework</cell><cell cols="2">Model</cell><cell>mAP</cell><cell>Billion</cell><cell>Million</cell></row><row><cell>Resolution</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mult-Adds Parameters</cell></row><row><cell></cell><cell cols="3">deeplab-VGG 21.1%</cell><cell>34.9</cell><cell>33.1</cell></row><row><cell>SSD 300</cell><cell cols="3">Inception V2 22.0%</cell><cell>3.8</cell><cell>13.7</cell></row><row><cell></cell><cell cols="3">MobileNet 19.3%</cell><cell>1.2</cell><cell>6.8</cell></row><row><cell>Faster-RCNN</cell><cell>VGG</cell><cell></cell><cell>22.9%</cell><cell>64.3</cell><cell>138.5</cell></row><row><cell>300</cell><cell cols="3">Inception V2 15.4%</cell><cell>118.2</cell><cell>13.3</cell></row><row><cell></cell><cell cols="3">MobileNet 16.4%</cell><cell>25.2</cell><cell>6.1</cell></row><row><cell>Faster-RCNN</cell><cell>VGG</cell><cell></cell><cell>25.7%</cell><cell>149.6</cell><cell>138.5</cell></row><row><cell>600</cell><cell cols="3">Inception V2 21.9%</cell><cell>129.6</cell><cell>13.3</cell></row><row><cell></cell><cell cols="3">Mobilenet 19.8%</cell><cell>30.5</cell><cell>6.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 14 .</head><label>14</label><figDesc>MobileNet Distilled from FaceNet MobileNet on the training data. Results for very small MobileNet models can be found in table 14.</figDesc><table><row><cell>Model</cell><cell>1e-4</cell><cell>Million</cell><cell>Million</cell></row><row><cell></cell><cell cols="3">Accuracy Mult-Adds Parameters</cell></row><row><cell>FaceNet [25]</cell><cell>83%</cell><cell>1600</cell><cell>7.5</cell></row><row><cell>1.0 MobileNet-160</cell><cell>79.4%</cell><cell>286</cell><cell>4.9</cell></row><row><cell>1.0 MobileNet-128</cell><cell>78.3%</cell><cell>185</cell><cell>5.5</cell></row><row><cell>0.75 MobileNet-128</cell><cell>75.2%</cell><cell>166</cell><cell>3.4</cell></row><row><cell>0.75 MobileNet-128</cell><cell>72.5%</cell><cell>108</cell><cell>3.8</cell></row><row><cell>of FaceNet and</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We assume that the output feature map has the same spatial dimensions as the input and both feature maps are square. Our model shrinking results generalize to feature maps with arbitrary sizes and aspect ratios.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The emulation quality is measured by averaging the per-attribute cross-entropy over all attributes.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow. org, 1, 2015. 4</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1504.04788</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02357v2</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Training deep neural networks with low precision multiplications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7024</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>abs/1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">IM2GPS: estimating geographic information from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-Scale Image Geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal Location Estimation of Videos and Images</title>
		<editor>J. Choi and G. Friedland</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10012</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07061</idno>
		<title level="m">Quantized neural networks: Training neural networks with low precision weights and activations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 1mb model size</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<title level="m">Speeding up convolutional neural networks with low rank expansions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Flattened convolutional neural networks for feedforward acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5474</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06789</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Speeding-up convolutional neural networks using fine-tuned cp-decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6553</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02325</idno>
		<title level="m">Ssd: Single shot multibox detector</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05279</idno>
		<title level="m">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structured transforms for small-footprint deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3088" to="3096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>2012. 4</idno>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04337</idno>
		<title level="m">Factorized convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PlaNet -Photo Geolocation with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.06473</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep fried convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1476" to="1483" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
