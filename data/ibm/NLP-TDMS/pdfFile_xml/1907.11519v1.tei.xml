<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Aware Multipath Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumindu</forename><surname>Tissera</surname></persName>
							<email>dumindutissera@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Telecommunication Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CODEGEN QBITS Lab University of Moratuwa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumara</forename><surname>Kahatapitiya</surname></persName>
							<email>kumara0093@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Telecommunication Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rukshan</forename><surname>Wijesinghe</surname></persName>
							<email>rukshandw@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Telecommunication Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CODEGEN QBITS Lab University of Moratuwa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subha</forename><surname>Fernando</surname></persName>
							<email>subhaf@uom.lk</email>
							<affiliation key="aff1">
								<orgName type="institution">CODEGEN QBITS Lab University of Moratuwa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranga</forename><surname>Rodrigo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Telecommunication Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CODEGEN QBITS Lab University of Moratuwa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Aware Multipath Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Making a single network effectively address diverse contexts-learning the variations within a dataset or multiple datasets-is an intriguing step towards achieving generalized intelligence. Existing approaches of deepening, widening, and assembling networks are not cost effective in general. In view of this, networks which can allocate resources according to the context of the input and regulate flow of information across the network are effective.</p><p>In this paper, we present Context-Aware Multipath Network (CAMNet), a multi-path neural network with datadependant routing between parallel tensors. We show that our model performs as a generalized model capturing variations in individual datasets and multiple different datasets, both simultaneously and sequentially. CAMNet surpasses the performance of classification and pixel-labeling tasks in comparison with the equivalent single-path, multi-path, and deeper single-path networks, considering datasets individually, sequentially, and in combination. The data-dependent routing between tensors in CAMNet enables the model to control the flow of information end-to-end, deciding which resources to be common or domain-specific.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Training a single model to fit data originating from different distributions, or even different datasets altogether, is an interesting problem. This requires the model to adapt based on the context of data. In other words, a model requires to extract different types of features to perform well with different datasets. Thus, it is difficult for a single model, which merely increases the number of parameters with its depth, to accommodate such variation in feature extraction and perform well in multiple contexts.</p><p>To this end, it is interesting to investigate the idea of a multipath neural network with data dependent soft path assignment, as such an approach can effectively utilize a large network while sharing resources. Such a single network can learn from varying contexts and even extract richer combination of features to improve performance of a single dataset.  <ref type="figure" target="#fig_0">Figure 1b</ref> shows the utilization of this network for a given input in MNIST dataset. Due to learnable gates, CAMNet has data-dependent routing, shown by the color and intensity of routes and tensors (activation paths).</p><p>A model which adapts to variations in the context of the input can provide practical advantages over conventional networks. For instance, (1) the ability to perform well in multiple different datasets of varying contexts concurrently and, (2) the ability to adapt to a novel domain while preserving the performance in previously learned domains are a few compelling application scenarios to explore.</p><p>Approaches towards generalized, intelligent networks that do not merely go deeper receive praise in current literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. In addition, learning the mix of shared resources and domain-specific resources is important. Adapting to a dataset while preserving already learned information <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> is also an emerging area of research. Hence, a single network which can generalize to variations in an individual dataset and multiple datasets-in terms of allocating resources in a data-dependant manneris important.</p><p>In this paper, we propose CAMNet, a multi-path convolutional neural network, which employs a data-dependent routing mechanism to route among parallel tensors. Such a learnable routing mechanism helps regulate the flow of information resulting in rich feature extraction and better utilization of resources at runtime <ref type="figure" target="#fig_0">(Fig. 1)</ref>. Thus, the novelty of CAMNet is twofold:</p><p>• Multiple parallel tensors in each layer which extract different sets of features, accommodating better feature utilization for different domains. • A novel learnable, data-dependant routing algorithm which controls the data flow among parallel tensors in subsequent layers. This allows the network to select the data path based on the context of the input image, resulting in a single model which performs well in different contexts.</p><p>In the proposed routing algorithm of CAMNet, each tensor among the parallel set of tensors in each layer predicts each tensor in the succeeding layer, which is made data-dependent with the introduction of learnable parameters in this prediction. Our intuition is that extracted features should be dependent upon the context of the input, in the case of learning from diverse data.</p><p>To evaluate the effectiveness of CAMNet, we first show the performance gain in individual classification datasets by CAMNet, followed by jointly optimizing CAMNet on a combined dataset of multiple domains, which shows variation in activation paths and better overall performance. Then, we evaluate how well CAMNet adapts to new domains, preserving the performance in previouslylearned contexts, considering the Learning without Forgetting (LwF) technique <ref type="bibr" target="#b18">[19]</ref>. We show the variation of path combinations taken by CAMNet for different domains. Finally, we empirically show that different parallel paths have learned different weight distributions.</p><p>The rest of the paper is organized as follows. We present related literature in Section 2 and formulate the network architecture along with the routing algorithm in Section 3. In Section 4, we show the performance of CAMNet on individual and joint datasets as well as lifelong learning. Finally, we discuss the outcomes and potential further extensions of CAMNet in Section 5 and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Having deep networks for generalizing to wide-range datasets is a common practice . Although the depth influences the performance of a neural network, training such networks is difficult and the general intelligence of this kind of a network is questionable <ref type="bibr" target="#b32">[33]</ref>. Hence, more attention is paid towards generally intelligent neural networks.</p><p>One such approach is to harvest more information within a layer in a neural network. Capsule networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref> involves extracting information about pose and orientation where, instead of convolutional scalars, there are vectors and matrices as layer outputs.</p><p>An alternate approach towards general intelligence is to build more flexible networks where the network is supposed to learn more than the weights. Hypernetworks <ref type="bibr" target="#b8">[9]</ref> includes a smaller network embedded inside the large network where the small network is predicting the weights of the large network. Squeeze-and-excitation networks <ref type="bibr" target="#b12">[13]</ref> introduced learnable weighting of each convolutional channel in order to build a generalized model across different datasets. Highway networks <ref type="bibr" target="#b32">[33]</ref> proposed the use of gates which learn to regulate the flow of information across the network which enabled deep models to be trained effectively. However, these approaches use a single-path network. Although the weights might vary, each training or testing input is fed through a single path. In contrast, our model is able to benefit from parallel paths with different weights on each path.</p><p>The term multitask learning is defined in several ways depending on the nature of the application. Sharing information between similar tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>, performing multiple tasks on a single input <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>, and learning from multiple domains <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref> can be considered as few such applications where the multitasking is defined in different forms. However, our work highlights the capability of CAMNet to perform context awareness in individual datasets and to learn from multiple domains either sequentially or simultaneously.</p><p>The common multitasking (MTL) in computer vision refers to processing multiple different tasks in a single input (e.g., semantic segmentation and surface-normal prediction). Conventional approaches to MTL includes using shared layers to some extent along with task-specific layers. Choosing the number of task-specific layers and shared layers is task dependant. However, recent approaches solve the problem of choosing from possible combinations in this context by letting the model to learn the use of shared and task-specific layers according to the task. Cross-stitch networks <ref type="bibr" target="#b23">[24]</ref> and sluice networks <ref type="bibr" target="#b30">[31]</ref> introduce sharing resources between parallel networks where communication between parallel layers is done through learning a linear combination of parallel tensors. NDDR-CNN <ref type="bibr" target="#b6">[7]</ref> use discriminative dimensionality reduction to fuse features from parallel tensors.</p><p>However, our intention in this work is to build an intelligent model which can handle a diversity in a dataset, especially in input domain, to an extent where the model can even handle multiple datasets at a time. The main difference between CAMNet and these approaches is that our model learns the routing between parallel tensors in a datadependant manner, starting from the input. Also, all the parallel paths share the same single input and output making CAMNet a single-input-single-output model. A routing layer learns from data while forwarding the incoming flow to particular set of tensors in the next layer.</p><p>Lifelong learning involves learning from multiple datasets one after the other. Conventional approaches include fine tuning <ref type="bibr" target="#b7">[8]</ref> and feature extraction <ref type="bibr" target="#b4">[5]</ref> which suffer from catastrophic forgetting. Rebuffi et al. <ref type="bibr" target="#b27">[28]</ref> introduced incremental classification as opposed to batch training in order to overcome catastrophic forgetting. Learning without Forgetting (LwF) <ref type="bibr" target="#b18">[19]</ref> and Elastic Weight Consolidation (EWC) <ref type="bibr" target="#b15">[16]</ref> are also two approaches introduced to overcome this issue in terms of modifying the objective function. In contrast, PackNet <ref type="bibr" target="#b21">[22]</ref> and Piggyback <ref type="bibr" target="#b20">[21]</ref> methods use binary masking on dense weight filters once trained in a dataset in order to free up the least used weights to learn from the next dataset. However, these approaches need larger filters depending on the number of datasets to be trained sequentially.</p><p>Approaches that can gradually build customized networks according to the input are also inspiring for our research. ConvNet-AIG <ref type="bibr" target="#b34">[35]</ref> and BlockDrop <ref type="bibr" target="#b37">[38]</ref> are two approaches introduced for data dependant choosing of residual blocks in a deep network as alternatives to conventional Residual Networks <ref type="bibr" target="#b9">[10]</ref>. These approaches learn which residual block to keep according to the nature of the input.</p><p>The idea of ensembles is quite popular where multiple neural networks are stacked together to enhance the cumulative performance on a single dataset. However, deploying a complex ensemble model for learning from diverse data would be challenging and computationally expensive in the presence of complex deeper networks as individual models <ref type="bibr" target="#b0">[1]</ref>. There is also the computational redundancy where multiple networks learning similar patterns and weights. Also, it has been shown that the knowledge gained by an ensemble model can be compressed in to a single model which is much simpler <ref type="bibr" target="#b10">[11]</ref>. Our approach differs from ensembles in the network architecture itself where our network is not a stack of different paths. Our network is able to learn from different datasets simultaneously while sharing intermediate tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture and Method</head><p>As the intuition behind CAMNet is to design a contextaware network which can differentiate subtle variations within a dataset, or even between multiple different datasets and learn sequentially with minimal changes to previously learned features, we consider a model with parallel paths. In contrast to conventional networks with a single path of data flow (series of tensors) subjected to convolution and fully connected operations, CAMNet consists of multiple parallel data paths as such. The proposed data-dependent and learnable routing algorithm delegates the data flow from the input among these parallel paths and recombine to produce the single output of the network. It routes between sub-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv(1x1)</head><p>Dense (Softmax) Flatten n Gates n Predictions <ref type="figure">Figure 2</ref>: Operations carried out by a 3-dimensional tensor i in layer l (T l i ). 1 × 1 convolution limits the amount of parameters fed into the dense gate operation. sequent layers based on two main operations: the prediction and the construction. The prediction operation involves each tensor in a certain layer predicting each tensor in the subsequent layer and the construction operation computes each tensor in the subsequent layer utilizing the predictions made by all the tensors from the previous layer for that particular tensor.</p><formula xml:id="formula_0">(G i ) T l i t i2 t in t i1 Conv</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Routing between Tensors</head><p>Suppose we have m tensors in a particular layer (l), and n tensors in the subsequent layer (l+1). We denote tensor k of layer l as T l k . Each tensor in layer l performs a prediction for each tensor in the layer l + 1. The prediction t ij made by tensor i in layer l to tensor j in layer l + 1 is given by,</p><formula xml:id="formula_1">t ij = W ij T l i + b ij ,<label>(1)</label></formula><p>where W ij and b ij correspond to weight and bias terms, respectively. In implementation if T l i is a 3-dimensional tensor, this operation is carried out via a standard convolution ( <ref type="figure">Fig 2)</ref>. Moreover, each tensor in layer l predicts n dimensional vector of gate values: G i , which represent the probabilities of the corresponding tensor i being routed to each tensor in layer l + 1.</p><formula xml:id="formula_2">G i = [g i1 , g i2 , . . . , g in ]</formula><p>Here, g ij corresponds to the scalar gate value connecting i th tensor in l th layer (T l i ) to j th tensor in layer l + 1 (T l+1 j ). G i is given by,</p><formula xml:id="formula_3">G i = softmax(W Gi T l i + b Gi ),<label>(2)</label></formula><p>where W Gi and b Gi correspond to weight and bias terms. In practice, if T l i is a 3-dimensional tensor, we first run a 1× 1 convolution before applying the dense operation in Eq. 2 to reduce the number of parameters. The activation function softmax(.) limits the utilization of multiple parallel tensors for a certain context so that each tensor is more likely to be allocated to a single tensor in the subsequent layer. <ref type="figure">Figure 2</ref> shows the operations carried out by a 3-dimensional tensor in layer l in the prediction phase. The tensors in layer l + 1 are calculated based on the predictions and gate values provided by layer l. The predictions are weighted by the corresponding gate and a nonlinear activation is applied to produce each tensor in layer l + 1. Eq. 3 shows how the of tensor j in layer l + 1 (T l+1 j ) is calculated using the predictions made by tensors in layer l (t ij , i = 1, 2, . . . , m) and the gates connecting corresponding tensors in layer l to the particular tensor in layer l + 1 (g ij , i = 1, 2 . . . , m).</p><formula xml:id="formula_4">T l+1 j = f n i=1 (g ij × t ij ) .<label>(3)</label></formula><p>Here, f stands for the non-linear activation function applied to the weighted prediction. We use ReLU(.) in the intermediate layers and either softmax(.) for classification or tanh(.) for image-to-image translation in the final layer. <ref type="figure" target="#fig_1">Figure 3</ref> shows the construction of layer l + 1 from layer l predictions and gates. Algorithm 1 demonstrates the routing algorithm between subsequent layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Forward Layers</head><p>We add layers which do not perform routing, to deepen the network without increasing the computational overhead due to additional routing layers. We refer to such layers as forward layers, which can be either convolutional or fullyconnected layers (one or several following each tensor) as depicted in <ref type="figure" target="#fig_0">Fig. 1a</ref> (e.g., C32 in orange). Forward layers help make CAMNet deeper and isolate the learning process of individual paths for a single step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>Since the routing layers incur convolutional or dense predictions, initializing CAMNet with pretrained single path neural networks is impractical. Moreover, stitching pretrained models trained on different tasks does not align with Algorithm 1 Prediction and routing algorithm between layer l and l + 1</p><p>Input: T l : layer l, {T l = [T l i for i = 1, 2, . . . , m]} m: number of tensors in layer l n: number of tensors in layer l + 1 Predictions from current layer:</p><formula xml:id="formula_5">for i = 1 to m do for j = 1 to n do t ij ← W ij T l i + b ij end for G i ← softmax(W Gi T l i + b Gi ) end for Construction of next layer: for j = 1 to n do T l+1 j ← f ( n i=1 (g ij × t ij )) end for Return: T l+1 :layerl+1, {T l+1 =[T l+1 j for j = 1, 2, . . . , n]}</formula><p>the intuition of this work. Thus, all the training is done from scratch. We perform lifelong learning on different datasets with the use of Learning without Forgetting (LwF) algorithm <ref type="bibr" target="#b18">[19]</ref>. LwF is based on the principle of assigning new taskspecific parameters (θ n ) before training on a subsequent dataset, keeping the previous task-specific layer intact (θ o ). The loss function is a weighted loss of the cross-entropy for the new task and the knowledge distillation loss <ref type="bibr" target="#b10">[11]</ref> for the old task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct various experiments to show the ability of our model to generalize in multiple datasets separately, accommodating the variation within a dataset and in multiple different datasets combined. Moreover, we illustrate the effectiveness of the data-dependant routing in contrast to the equivalent multi-path network without connections between parallel tensors. We vary the number of parallel tensors to investigate the effect of the width of CAMNet on the performance. For these evaluations, we consider classification and pixel-labeling as test cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>For classification, we use MNIST <ref type="bibr" target="#b17">[18]</ref>, Fashion MNIST <ref type="bibr" target="#b38">[39]</ref>, KMNIST <ref type="bibr" target="#b2">[3]</ref>, NotMNIST <ref type="bibr" target="#b1">[2]</ref>, CIFAR10 <ref type="bibr" target="#b16">[17]</ref> and SVHN <ref type="bibr" target="#b24">[25]</ref> datasets. For pixel-labeling, we use a few existing datasets on image-to-image translation and semantic segmentation. Furthermore, we consider the CMP facades dataset <ref type="bibr" target="#b33">[34]</ref>, satellite to aerial map dataset extracted from Google maps and used in pix2pix <ref type="bibr" target="#b13">[14]</ref>, Cityscapes dataset <ref type="bibr" target="#b3">[4]</ref> and KITTI road segmentation dataset <ref type="bibr" target="#b5">[6]</ref>. In addition to these datasets, we present our own dataset of drone-image to road-topology translation, which contains around 300 annotated image pairs. These images are sampled from several videos of drones following roads, hence, contain a considerable variety in the context of the input. Here, we only annotate the road and the background, disregarding other objects, in contrast to road-segmentation datasets. <ref type="table" target="#tab_0">Table 1</ref> illustrates the baseline networks and the different versions of CAMNet architectures we compared for classification. BaseCNN is a basic Convolutional Neural Network with 6 convolutional layers followed by 3 fully-connected layers. BaseCNN2 is a deeper network with 10 convolutional layers which carries a similar amount of parameters as CAMNet3 in total (2.0m). MultiCNNX stands for X number of parallel BaseCNNs sharing the same input and the output (Averaging). In particular, we used 3 such BaseCNNs (MultiCNN3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Compared Architectures</head><p>CAMNet architectures are designed such that an equivalent single path network would be a BaseCNN. We compared 3 versions of CAMNet by varying the number of parallel tensors in a layer (CAMNet2, CAMNet3, CAMNet4 where X in CAMNetX stands for number of parallel tensors in each layer). tinyCAMNet uses half the number of CNN filters in each layer, dropping two hidden fully-connected layers which has fewer parameters than BaseCNN. Figure1 illustrates CAMNet3 architecture.</p><p>In addition, we build an SENet <ref type="bibr" target="#b12">[13]</ref> equivalent to the BaseCNN, with the addition of gated weighting operations in each channel after convolutions as in the original paper. We also build a cross-stitch network (Cr-Stitch2) <ref type="bibr" target="#b23">[24]</ref> with two parallel paths, each being equal to a BaseCNN, with stitching operations wherever CAMNet2 has a routing layer instead.</p><p>For image-to-image translation, we compare a shallow and a deep version of CAMNet. The shallow version has only routing layers, but no forward layers. In the deep version, a forward layer of similar size exists after each routing layer. We compare these versions of CAMNet with UNet architecture <ref type="bibr" target="#b28">[29]</ref> variant used in pix2pix implementation <ref type="bibr" target="#b13">[14]</ref>. This Unet variant carries 13 million parameters which is similar to the deep version of CAMNet. The shallow version has only 6 million parameters in contrast. We consider CAMNet versions with two parallel tensors in each layer in this evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Individual and Joint Classification</head><p>We evaluate our model with the MNIST dataset where we record an error of 0.22% with CAMNet3 having 2.0 million parameters. We train with images which are augmented by pixel shift, rotation and scaling with no padding. Wan et al. <ref type="bibr" target="#b35">[36]</ref> achieve an error of 0.21% with data augmentation and Sobour et al. <ref type="bibr" target="#b31">[32]</ref>, an error of 0.25% error with pixel shift and reconstruction loss. <ref type="table" target="#tab_1">Table 2</ref> further shows the error with other datasets where CAMNet outperforms other architectures. CAMNet surpasses the equivalent SENet because CAMNet can benefit from multiple parallel paths for a certain input, in contrast to a single path instance in SENet. CAMNet2 also surpasses the performance of Cross-Stitch Network (Cr-Stitch2), showing that data-dependent gating is superior than independently learned stitching, specially for data with varying context.</p><p>In addition we jointly train our network with the aforementioned MNIST classification datasets ( <ref type="table" target="#tab_1">Table 2</ref>, 5 th row), where CAMNet4 performs the best in combination, achieving an overall error of 2.52%. We also compare the joint performance with Rebuffi et al. <ref type="bibr" target="#b26">[27]</ref> (1.4M), which consists separate task-specific softmax layers at the output for each dataset. Rebuffi et al. shows joint error of 2.7% which is similar to CAMNet2, but surpassed by CAMNet3 and CAMNet4.</p><p>One important observation is tinyCAMNet3, managing to surpass MultiCNN3 in joint training even though its performance in individual datasets is similar ( <ref type="table" target="#tab_1">Table 2</ref>). This effect proves that CAMNets are particularly suitable for learning from multiple domains. We repeat individual and joint training without augmentation for CAMNet3 and Multi-CNN3. Here, CAMNet3 shows the above observation profoundly ( <ref type="table" target="#tab_1">Table 2</ref> No Aug.* columns). This effect is being dampened when CAMNets are trained with data augmentation, showing that CAMNets capture the variation introduced by augmentation better than baselines to improve performance even in individual datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Lifelong Learning</head><p>We adopt the Learning without Forgetting (LwF) technique (see Section 3.3) to train our model with the four  classification datasets considered, sequentially. In this experiment, we feed the datasets in the order of MNIST, Fashion MNIST, KMNIST, and NotMNIST. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates the change of the test accuracy on previous datasets after trained on a new dataset. CAMNet3 simply outperforms its equivalent single path CNN (BaseCNN), equivalent multipath CNN (MultiCNN3) and deeper BaseCNN2, whereas CAMNet4 shows the best results. In addition, CAMNet architectures shows a smooth adaptation to the Fashion MNIST dataset once trained on the MNIST dataset. (See the curves in <ref type="figure" target="#fig_4">Figure 5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Image-to-image Translation</head><p>We develop the combined dataset for joint training with all the pixel labelling datasets (see Section 4.1 for datasets) by stacking each dataset, 5% at a time. The models are trained as an image-to-image translation with mean squared error (L2 loss) on this joint dataset, despite some datasets being segmentation datasets. All the data from different datasets are re-scaled to a common resolution of 256 × 256. We further evaluate the model with satellite-to-aerial (Drones) dataset and drone image-to-road topology (Maps) dataset, individually.   bined datasets. We report the changes in test loss compared to UNet. <ref type="figure" target="#fig_5">Figure 6</ref> shows the visual comparison of results between UNet-pix2pix and deep CAMNet trained on the combined dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visualization of Routing</head><p>We visualize the tensor activations and the gate values of each layer (see <ref type="figure" target="#fig_0">Fig. 1b</ref> and <ref type="figure" target="#fig_6">Fig. 7</ref>) by representing the average activation of each tensor (red) and gate value (blue) by their color intensities. The color intensity of tensor j in layer l represents the strength of prediction of T l j normalized by the summation of the values of all tensors in the corresponding layer. Gates computed as softmax probabilities are mapped with the color intensity based on the corresponding probabilities. Blue colored lines between two adjacent layers illustrate the data flow from a particular tensor to the following layer and their width represents the weight associated with the data flow g ij .</p><p>CAMNet being able to take different path combinations for inputs originating from different datasets is impressive. <ref type="figure" target="#fig_6">Figure 7</ref> shows the different combinations of paths taken in CAMNet, along with the combination of activated gates for each image when the model is trained on the combined dataset. Taking different combinations of routes to build a network can be interpreted as an adaptive single model which switches between slightly different sub-modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Weight Histograms of Parallel Convolutions</head><p>Here, we analyze the difference between convolutional weights learned across the parallel convolutions of particular layers. For this purpose, we compare the encoderdecoder version of CAMNet with a similar multi-path network and no routing in-between. <ref type="figure">Figure 8</ref> shows the weight histograms of the forward layers (see Sec. 3.2 for definition of forward layers) after 2 nd and 9 th routing layers. Since the CAMNet we test has two parallel tensors in each routing layer, we have two weight histograms of parallel forward convolutions per each layer. The figure illustrates that the histograms of parallel convolutions of an equivalent multipath network with no routing are similar whereas the histograms of parallel convolutions of CAMNet with learnable routing are distinct. Such observations confirm that the parallel paths have learned different portions of information. work, and even an equivalent deeper single-path network with similar number of parameters. The experiments further indicate that the improvement of CAMNet is not merely due to the number of parallel tensors, but due to the datadependant routing algorithm which effectively uses parallel resources. Increasing the width of CAMNet shows an improvement. However, for a given dataset, at some point increasing the width of CAMNet does not further improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effect of Routing</head><p>We prefer CAMNet over multi-path CNN with no routing (MultiCNN) since it better utilizes a multi-path network. It creates a specific path for each input based on the underlying context related to the domain. Moreover, the network shares the common features of different contexts while allowing the network to learn more domain-specific features within the remaining tensors. <ref type="figure" target="#fig_8">Figure 9</ref> indicates the difference between information flow in CAMNet with a similar multi-path network without routing.</p><p>One benefit of data-dependant routing is that the parallel paths are able to extract relevant features for a particular input while not learning redundant features. The learned weights of parallel convolutions being distinct (Sec. 4.7) indicates that our network demotivates similar information being learned by parallel tensors. We can further observe the variation of learned information in parallel tensors by the network visualization for different input-output combinations in <ref type="figure" target="#fig_6">Figure 7</ref>. <ref type="figure" target="#fig_8">Figure 9</ref> further illustrates the effect of routing between tensors where a similar multi-path network with no routing does not have the regulation of flow and allocation of resources depending on the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Number of Parallel Tensors</head><p>The number of parallel tensors included in each layer depends on the diversity of the domain and task. For a given dataset or a set of datasets, the performance of CAMNet in- creases with the number of parallel tensors and saturates at some point. The performance in individual datasets seems to saturate at CAMNet3; CAMNet4 does not provide better results than CAMNet3 in most individual datasets ( <ref type="table" target="#tab_1">Table  2)</ref>. Although CAMNet4 shows the best results even in lifelong learning and joint training, the increment of accuracy is very low compared to the increment from CAMNet2 to CAMNet3. <ref type="table" target="#tab_1">(Table 2</ref> and <ref type="figure" target="#fig_2">Figure 4</ref>)</p><p>It is further possible to have different number of parallel tensors in each routing layer. We do not investigate this fact in this paper. However, depending on the degree of similarity between datasets or within a dataset, and the difference between intended outputs, the exact number of parallel tensors in each layer will vary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we presented CAMNet, a multi-path neural network with data-dependant routing for context awareness. We evaluated classification version of CAMNet architecture with classification datasets and encoder-decoder version of CAMNet with pixel labeling. We show that CAM-Net outperforms equivalent baseline single path and multipath networks and even deeper single-path networks when tested with both individual datasets and dataset combinations. In addition, CAMNet surpasses baseline networks in lifelong learning, preserving already learned information better. CAMNet can address a wider context awareness which enable it to learn from even multiple different contexts better than comparable deep networks. The datadependant routing enables the model to wisely allocate resources, deciding between domain-specific and shared tensors, learning the information flow end-to-end. We believe that the idea of partially utilizing a network with the help of learnable routing is an impressive solution to contextawareness and multi-domain learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of data-dependant routing: Figure 1a shows CAMNet3 classification architecture: C (convolution) and F (dense) stand for forward layers. RC and RF indicate the presence of routing in these layers. The superscript indicates intended number of tensors in the next layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Constructing layer l + 1 based on the predictions and gates computed by layer l See Eq. 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Accuracy change when trained on multiple subsequent datasets: CAMNet architectures preserve the learned features better, in comparison with BaseCNN, MultiCNN, and even a deeper CNN (BaseCNN2). The lifelong leraning results are averaged over 3 trials</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Accuracy change when trained on a subsequent dataset. CAMNet architectures show a smoother adaptation to the second dataset while preserving the accuracy on the first.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Outputs of Unet-pix2pix and deep CAMNet after joint training in the combined dataset. Deep CAMNet shows better outputs, closer to the target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Route Visualization in image-to-image translation. Different path combinations considered for each dataset after joint training. The dark blue connections and dark red tensors show the paths taken with frequently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5 (d) F orw 9 2 Figure 8 :</head><label>528</label><figDesc>Weights Histograms of forward convolutions after 2 nd routing layer and 9 th routing layer. Top row shows CAMNet and bottom row shows similar multi-path network without gates. F orw l i denotes the forward convolution carried by tensor i at the output of routing layer l. Differences in histograms show that parallel paths have learned different portions of information in CAMNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Top figure is CAMNet allocating resources based on the input, whereas the bottom figure shows a similar network without learnable routing, in which, there is no context-aware resource allocation. Hence, it shows a redundancy in parallel layers where similar features have been learned across the parallel tensors in the latter case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Compared Networks: C denotes a convolutional layer and F stands for a fully connected layer. Suffix r stands for a routing layer. CAMNetX stands for the CAM-Net version carrying X number of tensors in each layer. C16 C16 r C32 C32 r C64 C64 r F 10</figDesc><table><row><cell>Network</cell><cell>Structure</cell></row><row><cell>BaseCNN</cell><cell>C32 C32 C64 C64 C128 C128 F 32 F 32 F 10</cell></row><row><cell>BaseCNN2</cell><cell>C32 C32 C64 C64 C128 C128 C128</cell></row><row><cell></cell><cell>C256 C256 C256 F 32 F 32 F 10</cell></row><row><cell>MultiCNNX</cell><cell>X number of parallel BaseCNNs</cell></row><row><cell>CAMNetX</cell><cell>r C32 C32 r C64 C64 r C128 C128 F 32 r F 32 r F 10</cell></row><row><cell></cell><cell>(Equivalent single path BaseCNN)</cell></row><row><cell>tinyCAMNetX</cell><cell></cell></row><row><cell>SENet</cell><cell>Equi. to BaseCNN, All convolutions with SE operation</cell></row><row><cell>Cr-Stitch2</cell><cell>Equi. to CAMNet2 with stitching replaced routing</cell></row></table><note>r</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification errors in individual and joint datasets: Row 5 shows the joint performance of all MNIST datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Network</cell><cell cols="11">BaseCNN MultiCNN3 CAMNet2 CAMNet3 tinyCAMNet3 CAMNet4 SENet Cr-Stitch2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Params(M)</cell><cell></cell><cell cols="2">0.5</cell><cell></cell><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell><cell>1.2</cell><cell>2.0</cell><cell>0.47</cell><cell>3.0</cell><cell>0.5</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">No Aug.</cell><cell></cell><cell></cell><cell></cell><cell>No Aug.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MNIST</cell><cell></cell><cell></cell><cell cols="2">0.3</cell><cell></cell><cell cols="2">0.48</cell><cell cols="3">0.3</cell><cell>0.25</cell><cell>0.53 0.22</cell><cell>0.32</cell><cell>0.26</cell><cell>0.25</cell><cell>0.33</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Fashion</cell><cell></cell><cell></cell><cell cols="2">6.37</cell><cell></cell><cell cols="5">6.53 6.02</cell><cell>5.8</cell><cell>7.0</cell><cell>5.66</cell><cell>5.98</cell><cell>5.69</cell><cell>5.92</cell><cell>5.88</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">KMNIST</cell><cell></cell><cell></cell><cell cols="2">1.38</cell><cell></cell><cell cols="5">2.73 1.14</cell><cell>1.07</cell><cell>2.52 0.95</cell><cell>1.26</cell><cell>1.06</cell><cell>1.15</cell><cell>1.16</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">NotMNIST</cell><cell></cell><cell cols="2">2.37</cell><cell></cell><cell cols="2">3.1</cell><cell cols="3">2.54</cell><cell>2.39</cell><cell>3.38 2.18</cell><cell>2.64</cell><cell>2.15</cell><cell>2.47</cell><cell>2.47</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Joint</cell><cell></cell><cell></cell><cell></cell><cell cols="2">2.93</cell><cell></cell><cell cols="5">4.17 3.03</cell><cell>2.66</cell><cell>3.84 2.57</cell><cell>2.82</cell><cell>2.52</cell><cell>2.88</cell><cell>2.98</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR10</cell><cell></cell><cell></cell><cell cols="2">8.3</cell><cell></cell><cell></cell><cell></cell><cell cols="3">9.71</cell><cell>7.59</cell><cell>7.02</cell><cell>8.88</cell><cell>7.06</cell><cell>8.71</cell><cell>8.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SVHN</cell><cell></cell><cell></cell><cell cols="2">3.86</cell><cell></cell><cell></cell><cell></cell><cell cols="3">3.23</cell><cell>3.54</cell><cell>3.28</cell><cell>3.38</cell><cell>3.20</cell><cell>3.42</cell><cell>3.45</cell></row><row><cell></cell><cell>90 100</cell><cell>99.7</cell><cell>91.28</cell><cell>95.78</cell><cell>95.84</cell><cell></cell><cell>90 100</cell><cell>99.7</cell><cell>91.87</cell><cell>96.16</cell><cell>95.88</cell><cell></cell><cell>90 100</cell><cell>99.7</cell><cell>93.38</cell><cell>97.66</cell><cell>92.66 96.7</cell></row><row><cell>Accuracy %</cell><cell>20 30 40 50 60 70 80</cell><cell cols="4">MNIST +FasMNIST +KMNIST +NotMNIST Dataset 65.4 31.65 53.81 65.12 12.42 15.65 MNIST Fashion MNIST KMNIST notMNIST</cell><cell>Accuracy %</cell><cell>20 30 40 50 60 70 80</cell><cell cols="4">MNIST +FasMNIST +KMNIST +NotMNIST Dataset 76.45 36.93 53.44 35.36 73.79 19.32 MNIST Fashion MNIST KMNIST notMNIST</cell><cell>Accuracy %</cell><cell>80 20 30 40 50 60 70</cell><cell cols="2">Dataset MNIST +FasMNIST +KMNIST +NotMNIST 60.09 60.92 45.84 39.46 21.08 MNIST Fashion MNIST KMNIST notMNIST</cell></row><row><cell></cell><cell></cell><cell cols="4">(a) BaseCNN</cell><cell></cell><cell cols="5">(b) MultiCNN3</cell><cell></cell><cell></cell><cell cols="2">(c) BaseCNN2</cell></row><row><cell>Accuracy %</cell><cell>20 30 40 50 60 70 80 90 100</cell><cell cols="4">MNIST +FasMNIST +KMNIST +NotMNIST Dataset 99.75 97.22 82.36 46.46 91.96 59.8 35.33 96.27 86.43 96.3 MNIST Fashion MNIST KMNIST notMNIST</cell><cell>Accuracy %</cell><cell>20 30 40 50 60 70 80 90 100</cell><cell cols="4">MNIST +FasMNIST +KMNIST +NotMNIST Dataset 99.78 99.23 98.29 89.8 92.29 69.94 41.43 96.55 96.08 88.84 MNIST Fashion MNIST KMNIST notMNIST</cell><cell>Accuracy %</cell><cell>90 100 20 30 40 50 60 70 80</cell><cell cols="2">99.72 MNIST +FasMNIST +KMNIST +NotMNIST 98.24 96.17 91.69 92.88 96.58 96.44 92.45 Dataset 76.21 43.42 MNIST Fashion MNIST KMNIST notMNIST</cell></row><row><cell></cell><cell></cell><cell cols="4">(d) CAMNet2</cell><cell></cell><cell></cell><cell cols="4">(e) CAMNet3</cell><cell></cell><cell></cell><cell cols="2">(f) CAMNet4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Test loss comparison of image-to-image translation</cell></row><row><cell cols="4">with satellite-to-aerial (Drones) dataset and drone image-</cell></row><row><cell cols="4">to-road topology (Maps) dataset individually, and the com-</cell></row><row><cell cols="4">bined dataset (Drones, Maps, Facades, Cityscapes, KITTI).</cell></row><row><cell cols="4">DeepCAMNet reduces the error significantly compared to</cell></row><row><cell>UNet-pix2pix.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Drones</cell><cell>Maps</cell><cell>Combined</cell></row><row><cell>UNet-pix2pix</cell><cell>0.0785</cell><cell>0.0133</cell><cell>0.0798</cell></row><row><cell>CAMNet</cell><cell cols="2">-0.51% -6.02%</cell><cell>-7.27%</cell></row><row><cell cols="3">DeepCAMNet -14.65% -18.8%</cell><cell>-14.16%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>illustrates the versions of CAMNet (Sec. 4.2)</cell></row><row><cell>outperforming the UNet architecture in individual and com-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. on Knowl. Discovery and Mata Mining</title>
		<meeting>ACM SIGKDD Int. Conf. on Knowl. Discovery and Mata Mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<ptr target="http://yaroslavvb.blogspot.it/2011/09/notmnist-dataset.html" />
		<title level="m">Notmnist dataset. Google (Books/OCR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01718</idno>
		<title level="m">Deep learning for classical Japanese literature</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A new performance measure and evaluation benchmark for road detection algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fritsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kuehnl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. on Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="page" from="1693" to="1700" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix capsules with EM routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning with whom to share in multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the Nat. Academy of Sci</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5334" to="5343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7765" to="7773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond shared hierarchies: Deep multitask learning through soft layer ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crossstitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Curriculum learning of multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5492" to="5500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Medical Image Comput. and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Routing networks: Adaptive selection of non-linear functions for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Latent multi-task architecture learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI 2019</title>
		<meeting>of AAAI 2019</meeting>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatial pattern templates for recognition of objects with regular structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tyleček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Šára</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conf. on Pattern Recognit</title>
		<meeting><address><addrLine>Saarbrucken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="364" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive inference graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Le</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Blockdrop: Dynamic inference paths in residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8817" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pad-net: Multitasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
