<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Facebook AI &amp; Bar-Ilan University Lior Wolf Facebook AI &amp; Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Facebook AI &amp; Bar-Ilan University Lior Wolf Facebook AI &amp; Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Facebook AI &amp; Bar-Ilan University Lior Wolf Facebook AI &amp; Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel, real-time, semantic segmentation network in which the encoder both encodes and generates the parameters (weights) of the decoder. Furthermore, to allow maximal adaptivity, the weights at each decoder block vary spatially. For this purpose, we design a new type of hypernetwork, composed of a nested U-Net for drawing higher level context features, a multi-headed weight generating module which generates the weights of each block in the decoder immediately before they are consumed, for efficient memory utilization, and a primary network that is composed of novel dynamic patch-wise convolutions. Despite the usage of less-conventional blocks, our architecture obtains real-time performance. In terms of the runtime vs. accuracy trade-off, we surpass state of the art (SotA) results on popular semantic segmentation benchmarks: PASCAL VOC 2012 (val. set) and real-time semantic segmentation on Cityscapes, and CamVid. The code is available: https://nirkin.com/hyperseg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation plays a crucial role in scene understanding, whether the scene is microscopic, telescopic, captured by a moving vehicle, or viewed through an AR device. New mobile applications go beyond seeking accurate semantic segmentation, and also requiring real-time processing, spurring research into real-time semantic segmentation. This domain has since become a leading testbed for new architectures and training methods, with the goals of improving both accuracy and speed. Recent work added capacity <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and attention mechanisms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b50">50]</ref> to improve performance. When runtime is not a concern, the image is often processed multiple times by the model and the results are accumulated. In this paper, we attempt to improve the performance in a different way: by providing the network with additional adaptivity.</p><p>We add this adaptivity using a meta-learning tech-* Performed this work while an intern at Facebook. nique, often referred to as dynamic networks or hypernetworks <ref type="bibr" target="#b12">[13]</ref>. These networks are used for tasks ranging from text analysis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b51">51]</ref> to 3D modeling <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">43]</ref>, but rarely for generating image-like maps. The reason is that the hypernetworks, as suggested by previous methods, do not fully capture the signals of high resolution images. Semantic segmentation map are an especially interesting case. They are generated by a coarse to fine pyramid, where each level of the process can benefit from adaptation, since these effects accumulate from one block to the next. Moreover, since every part of the image may contain a different object, such adaptation is best done locally.</p><p>We thus offer a novel encoder-decoder approach, in which the encoder's backbone is based on recent advances in the field. The encoded signal is mapped to dynamic network weights using an internal U-Net, while the decoder consists of dynamic blocks with spatially varying weights.</p><p>The proposed architecture achieves SotA accuracy vs. runtime trade-off on the most widely used benchmarks for this task: PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref>, CityScapes <ref type="bibr" target="#b7">[8]</ref>, and CamVid <ref type="bibr" target="#b1">[2]</ref>. For CityScapes and CamVid, the SotA accuracy result is obtained under the real-time conditions. Despite using an unconventional architecture that employs locally connected layers with dynamic weights, our method is very efficient (See <ref type="figure" target="#fig_0">Fig. 1</ref> for our run-time / accuracy tradeoff relative to other methods.)</p><p>To summarize, our contributions are:</p><p>• A new hypernetwork architecture that employs a U-Net within a U-Net. • Novel dynamic patch-wise convolution with weights that vary both per input and per spatial location. • SotA accuracy vs. runtime trade-off on the major benchmarks of the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Hypernetworks. Hypernetwork <ref type="bibr" target="#b12">[13]</ref>, are networks that generate weight values for other networks (often referred to as primary networks). Hypernetworks are useful as a modeling tool, e.g., as implicit functions for image-to-image translation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>, 3D scene representation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">43]</ref>, and also for avoiding compute and data heavy training cycles during neural architecture search (NAS) <ref type="bibr" target="#b56">[56]</ref> and continual learning <ref type="bibr" target="#b49">[49]</ref>. To our knowledge, however, hypernetworks were never proposed for semantic segmentation, as we propose to do here.</p><p>Locally connected layers. Connectivity in locally connected layers follows a spatial pattern that is similar to conventional convolutional layers but without weight sharing. Such layers played an important role in the early days of deep learning, mostly due to computational reasons <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b48">48]</ref>.</p><p>Locally connected layers were introduced as an accuracy enhancing component in the context of face recognition, where these were motivated by the need to model each part of the face in a different way <ref type="bibr" target="#b44">[44]</ref>. However, subsequent face recognition methods tend to use conventional convolutions, e.g., <ref type="bibr" target="#b42">[42]</ref>. Partial sharing of weights, in which convolutions are shared within image patches, was proposed for the analysis of facial actions <ref type="bibr" target="#b59">[59]</ref>.</p><p>As far as we can ascertain, we are the first to propose locally connected layers in combination with hypernetworks or in the context of semantic segmentation or, more generally, in image-to-image mapping.</p><p>Semantic segmentation. Early semantic segmentation methods used feature engineering and often relied on data driven approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">47]</ref>. To our knowledge, Long et al. <ref type="bibr" target="#b27">[28]</ref> were the first to show end-to-end training of convolutional neural networks (CNN) for semantic segmentation. Their fully convolutional network (FCN) output dense, per-pixel predictions of variable resolutions, based on a classification network backbone. They incorporated skip connections between the early and final layers, for combining coarse and fine information. Subsequent methods added a post processing step based on conditional random fields (CRF) to further refine the segmentation masks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr">60]</ref>.</p><p>Nirkin et al. overcame limited, scarce segmentation labels by utilizing motion from videos <ref type="bibr" target="#b31">[31]</ref>. U-Nets <ref type="bibr" target="#b39">[39]</ref> used encoder-decoder pairs, concatenating the last feature maps of the encoder, in each stride, with corresponding upsampled feature maps from the decoders.</p><p>Some proposed replacing strided convolutions with dilated convolutions, a.k.a. atrous convolutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b55">55]</ref>. This approach produced more detailed segmentations by enlarging the receptive field of the logits but also drastically increased computational costs. Another approach for expanding the receptive field is called spatial pyramid pooling (SPP) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b58">58]</ref>, in which features from different strides are average pooled and concatenated together, after which the information is fused by subsequent convolution layers. Subsequent work combined atrous convolutions with SPP (ASPP), achieving improved accuracy, yet with even higher computational cost <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. To further improve accuracy, some proposed inference strategies of applying networks multiple times on multi-scale and horizontally flipped versions of the input image, and combining the results using average pooling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Recently, Tao et al. <ref type="bibr" target="#b46">[46]</ref> utilized attention to better combine the inference strategy predictions, taking scale into account. Finally, others proposed axial attention, performing attention along the height and width axes separately, to better model long range dependencies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b50">50]</ref> .</p><p>Real-time segmentation. The goal of some methods is to achieve the best trade-off between accuracy and computations, with an emphasis on maintaining real-time performance. Real-time methods typically adopt an architecture composed of an encoder based on an efficient backbone and a relatively small decoder.</p><p>One early example of inexpensive semantic segmentation is the SegNet <ref type="bibr" target="#b0">[1]</ref>, which uses an encoder-decoder architecture with skip connections and transposed convolutions for upsampling. ENet <ref type="bibr" target="#b34">[34]</ref> proposed an architecture based on the ResNet's bottleneck block <ref type="bibr" target="#b18">[19]</ref>, achieving a high frames per second (FPS) rate but sacrificing considerable accuracy. ICNet <ref type="bibr" target="#b57">[57]</ref> utilizes fused features extracted from image pyramids, reporting better accuracy than previous methods. GUNet <ref type="bibr" target="#b28">[29]</ref> performed upsampling guided by the fused feature maps from the encoder, extracted from multiscale input images. SwiftNet <ref type="bibr" target="#b32">[32]</ref> suggested an encoderdecoder with SPP and 1 × 1 convolutions for reducing the number of dimensions before each skip connection.</p><p>Subsequent methods benefited from progress in efficient network architecture <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b52">52]</ref>, such as depth-wise separable convolutions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref> and inverted residual blocks <ref type="bibr" target="#b41">[41]</ref> which we also use in our work. BiSeNet <ref type="bibr" target="#b53">[53]</ref> proposed an additional, coarser downsampling path that is fused with the finer resolution main network before upsampling. BiSeNetV2 <ref type="bibr" target="#b52">[52]</ref> extends BiSeNet by offering a more elaborate fusion of the two branches and additional prediction heads from intermediate layers for boosting the training. Finally, TDNet <ref type="bibr" target="#b21">[22]</ref> proposed a network for video semantic segmentation, by circularly distributing sub-networks over sequential frames, leveraging temporal continuity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Overview. Our proposed hypernetwork encoder-decoder approach is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref> and <ref type="figure">Fig. 3</ref>. Similarly to U-Net-based methods <ref type="bibr" target="#b39">[39]</ref>, we employ skip connections between corresponding layers of the encoder and the decoder. Our network, however, uses encoder and subsequent blocks which we refer to as the context head and weight mapper, in the spirit of hypernetwork design. Skip connections, therefore, connect different encoder levels with the levels of a hierarchical primary network, which serves as our decoder. Moreover, our decoder weights vary between patches at each stride level.</p><p>Our proposed model involves three sub-networks: the backbone, b (shown in blue, in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>, the context head, h (orange box in <ref type="figure" target="#fig_1">Fig. 2</ref>(a), also detailed in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>), and the primary network, acting as the decoder, d ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>). In addition, the decoder consists of multiple meta blocks, visualized in detail in <ref type="figure">Fig. 3</ref>(a). Each meta block, i = 0 . . . n, includes an additional weight mapping network component, w i , represented as orange boxes in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>.</p><p>Information flow. The weights of the three networks: θ b , θ h , and θ w , are fixed during inference and learned during the training process, while θ mi , the weights of the decoder meta block, m i , are predicted dynamically at inference time. The encoder's backbone, b, maps an input image, I ∈ R 3×H×W , to a set of feature maps, F i ∈ <ref type="bibr">, 5]</ref>, of different resolutions, where H and W are the number of pixels along the height and width of the image correspondingly.</p><formula xml:id="formula_0">R Ci× H 2 i × W 2 i , i ∈ [1</formula><p>The context head, h :</p><formula xml:id="formula_1">R Cn× H 2 n × W 2 n → R Cn× H 2 n × W</formula><p>2 n , maps the last feature map from b to a signal, φ. This signal is then fed to w :</p><formula xml:id="formula_2">R Cn× H 2 n × W 2 n → R ( i |θ m i |)× H 2 n × W 2 n</formula><p>which generates the weights for meta blocks of the primary network, d. Note that these weights vary from one spatial location to the next. We define a fixed positional encod-</p><formula xml:id="formula_3">ing, P H,W ∈ R 2×H×W , such that in each position (i, j), P H,W i,j = ( 2i−H+1 H−1 , 2j−W +1 W −1 ), i ∈ [0, H), j ∈ [0, W ).</formula><p>Finally, given the input image and the feature maps, F 1 , . . . , F n , their corresponding positional encodings of the same resolutions, P 0 , . . . , P n , and the weights, θ d , the decoder, d, outputs the segmentation prediction, S ∈ R C×H×W , where C is the number of classes in the semantic segmentation task.</p><p>Our entire network is, therefore, defined by the following <ref type="figure">Figure 3</ref>. (a) The meta block based on the inverted residual block <ref type="bibr" target="#b41">[41]</ref>. Each purple layer represents a dynamic patch-wise convolution with weights generated by the orange layer, wi. (b) Visualization of the dynamic patch-wise convolution operation. Each color represents weights corresponding to a specific patch and '*' is the convolution operation. Please see Sec. 3.3 for more details. set of equations:</p><formula xml:id="formula_4">F 1 , . . . , F n = b(I|θ b ),<label>(1)</label></formula><formula xml:id="formula_5">φ = h(F n |θ h ), (2) θ mi = w i (φ|θ w ), i = 0, . . . , n, (3) S = d({F i }, {P i }|{θ mi }) ,<label>(4)</label></formula><p>where the weights of each network are specified explicitly after the separator sign.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The encoder and the hypernetwork</head><p>The first component of the hypernetwork is the backbone network, b (blue box in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>). It is based on the Effi-cientNet <ref type="bibr" target="#b45">[45]</ref> family of models. Sec. 4 provides details of this network. In our work, the head of the backbone architecture is replaced with our context head, h. The backbone outputs the feature map, F i , in each stride. In order to decrease the size of the decoder, we augment it with additional 1 × 1 convolutions that reduce the number of channels of each F i by a factor r i . The exact values for r i are detailed in Appendix B.</p><p>The last feature map of b, the backbone network, is of size H 2 n × W 2 n . Each pixel in this feature map encodes a patch in the input image. These patches have little overlap, and the limited receptive field can lead to poor results in case of large objects that span multiple patches. The context head, h, therefore, combines information from multiple patches.</p><p>We detail the structure of h in <ref type="figure" target="#fig_1">Fig. 2</ref>(c). Network h uses the nested U-Net structure introduced by Xuebin et al. <ref type="bibr" target="#b35">[35]</ref>. In our implementation, we employ 2×2 convolutions with a stride of two, that output half the number of channels of the input. Such convolutions are computationally cheaper than 3 × 3 convolutions, which require padding for the low resolution feature maps processed by h, and this padding can significantly increase the spatial resolution. The bottommost feature map is average pooled to extract the highest level context, and then upsampled to the previous resolution using nearest neighbor interpolation. Finally, in the upsampling path of h, at each level, we concatenate the feature map with its corresponding upsampled feature map, followed with a fully connected layer.</p><p>While the weight mapping network, w = [w 0 , . . . , w n ], is a key part of our hypernetwork, it is more efficient in our hierarchical network to divide w into parts and attach these parts to primary network blocks ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>). Hence, instead of directly following h, the layers, w 0 , . . . , w n , of the weight mapping network are embedded in each of the meta blocks of d. The rationale is that the mapping from context to weights incurs a large expansion in memory, which can become a performance bottleneck. Instead, the weights are generated right before they are consumed, minimizing the maximum memory consumption and better utilizing the memory cache. Each w i is a 1 × 1 convolution with channel groups, g wi , and is detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The decoder (the primary network)</head><p>The decoder, d, shown in <ref type="figure" target="#fig_1">Fig. 2</ref>(b), consists of n + 1 meta blocks, m 0 , . . . , m n , illustrated in <ref type="figure">Fig. 3(a)</ref>. Block, m 0 , corresponds to the input image, and each of the blocks, m i , i = 1..n, corresponds to the feature map, F i , of the encoder. Each block is followed by bilinear upsampling and concatenation with the next finer resolution feature map.</p><p>Unlike conventional schemes, by employing a hypernetwork, the weights of the decoder, d, depend on the input image. Moreover, the weights of d are not only conditioned on the input image but also vary between different regions of the image. With this approach, we can efficiently combine low level information from the stem of the network with high level information from the bottom layers. This allows our method to achieve higher accuracy using a much smaller decoder, thus enabling real-time performance. The hypernetwork can be seen as a type of attention, and similar to some attention-based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">33]</ref>  <ref type="table">Table 2</ref>. Results on the PASCAL VOC 2012, val. set <ref type="bibr" target="#b10">[11]</ref>. ' ', represents metrics that were computed by us using open source (listed in Appendix D).</p><p>knowing the position information of the pixels. For this reason, we augment the input image and the encoder's feature maps with additional positional encoding.</p><p>The design of m 0 , . . . , m n is based on the inverted residual block of MobileNetV2 <ref type="bibr" target="#b41">[41]</ref>: a point-wise convolution, pw 1 , followed by depth-wise convolution, dw, and another point-wise convolution, pw 2 , without an activation function. Instead of regular convolutions, our network employs dynamic, patch-wise convolutions, described in the next section. For very small patches -smaller than 4 × 4 in our large model; smaller than 8 × 8 in our smaller models -the meta block includes only pw 1 . The total meta parameters, θ mi ∈ R (|θ pw 1 |+|θ dw |+|θ pw 1 |)× H 2 n × W 2 n , required by each m i is the combined meta parameters of all dynamic convolutions in m i : θ mi = θ pw1 ∪θ dw ∪θ pw2 . The weights, θ mi , are generated by the w i layer, embedded in m i , given the signal,</p><formula xml:id="formula_6">φ i ∈ R C φ i × H 2 n × W 2 n .</formula><p>At inference, the batch-normalization layers of m i are fused with w i ; more details are provided in Appendix E.</p><p>Employing the full signal in each m i is inefficient both computationally and in the number of trainable parameters, because φ is directly mapped into a large number of weights. We thus instead divide the channels of φ into parts, C φ0 , . . . , C φn , which are relative in size to the required number of weights of each meta block. The division of the channels is defined using the following procedure: C φ0 , . . . , C φn = divide channels( C n , max(g w0 , . . . , g wn ), |θ m0 |, . . . , |θ mn |), <ref type="bibr" target="#b4">(5)</ref> where divide channels(·) is detailed in Appendix A. This routine ensures that each part is proportional to its allocated signal channels, is divisible by max(g w0 , . . . , g wn ) for the grouped convolutions in w, and is allocated a minimal number of channels.</p><p>The number of groups, g wi , is an important hyperparameter, since it controls the amount of computations and trainable parameters invested in producing the weights for m i . Increasing g wi reduces the computations and trainable parameters in direct proportions, as can be seen from the following equations:</p><formula xml:id="formula_7">|θ wi | = |θ mi | · C φi g wi ,<label>(6)</label></formula><formula xml:id="formula_8">FLOPs wi = |θ mi | · C φi · H 2 n · W 2 n g wi .<label>(7)</label></formula><p>The effect of different values of g wi is studied in Sec. 4.3.</p><p>The exact values of g wi used in our tests are reported in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic patch-wise convolution</head><p>We illustrate the operation of the dynamic patchwise convolution (DPWConv), the layers, pw 1 , dw, and pw 2 of m i , in <ref type="figure">Fig. 3(b)</ref>.</p><p>Given an input feature map, X ∈ R Cin×H×W , and a grid of weights, θ ∈</p><formula xml:id="formula_9">R Cout× C in G ×K h ×Kw×N h ×Nw ,</formula><p>where C in and C out are the channel numbers for the input and output, G is the number of channel groups, H and W are the input's height and width, K h and K w are the height and width of the kernel, and N h and N w are the number of patches along the height and width axes, we define output patches as follows:</p><formula xml:id="formula_10">O i,j = X i,j * θ i,j ,<label>(8)</label></formula><p>where * is the convolution operation, i ∈ [0, N h ) and j ∈ [0, N w ) are the patch indices, X i,j is a patch of X in the grid location (i, j), and θ i,j are the corresponding weights from the weights grid. We first apply padding to the entire input feature map, X, and then at each patch, X i,j , we wrap the adjacent pixels from neighboring patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>We experiment on three popular benchmarks: PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref>, Cityscapes <ref type="bibr" target="#b7">[8]</ref>, and CamVid <ref type="bibr" target="#b1">[2]</ref>. We report results using the following standard measures: class mean intersection over union (mIoU), frames per second (FPS), billion floating point operations (GFLOP), and number of trainable parameters.</p><p>FPS is measured using established protocols <ref type="bibr" target="#b32">[32]</ref>: We record FPS for the elapsed time between data upload to GPU through to prediction download. Our model is implemented in PyTorch without specific optimizations. Finally, we use a batch size of 1 to simulate real-time inference. Similar to most previous methods, we measure FPS on a NVIDIA GeForce GTX 1080TI GPU (i7-5820k CPU and 32GB DDR4 RAM). GFLOPs and trainable parameters are calculated using the pytorch-OpCounter library [61], also used by others <ref type="bibr" target="#b32">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Resolution  <ref type="table">Table 3</ref>. Real-time semantic segmentation results on Cityscapes <ref type="bibr" target="#b7">[8]</ref>. '-' Implies that the metric was not reported. ' ', denotes that the Our models (in orange) attain a significantly better trade-off than previous methods.</p><p>We experiment with large, medium, and small variants of our model, HyperSeg-L, HyperSeg-M, and HyperSeg-S, respectively. The models share the same template and are named according to their size as reflected by their parameter numbers. Both HyperSeg-M and HyperSeg-S omit the finest resolution level of d; we bilinearly upsample predictions to the input resolution from their previous level. In HyperSeg-S the channels of the layers in m i are halved, relative to those of the largest model. We provide model backbone and resolution details, separately, for each experiment. For other hyperparameter values, see Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training details</head><p>We initialize our network using weights pretrained on ImageNet <ref type="bibr" target="#b40">[40]</ref> for θ b , and using random values sampled from the normal distribution for θ h and θ w . The Adam optimizer <ref type="bibr" target="#b22">[23]</ref> was used for training, with β 1 = 0.5 and β 2 = 0.999. Following others <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, we use a polynomial learning rate scheduling that decays the initial learning rate, lr 0 , after i iterations by a factor of (1 − i t ) p , where t is the total number of iterations and p is a scalar constant. The exact values for each dataset are listed in Tab. 1.</p><p>For the Cityscapes and Camvid benchmarks, we apply the following image augmentations: random resize with scale range [0.5, 2.0], crop, and horizontal flipping with probability 0.5. For PASCAL VOC we use a similar horizontal flip, and random resize with scale range of [0.25, 1.0]. We further randomly rotate the images, in the range of −30 • to 30 • , jitter colors to manipulate brightness, contrast, saturation, and hue, and finally pad the images to a resolution of 512 × 512. We train all our models on two Volta V100 32GB GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">PASCAL VOC 2012 benchmark tests</head><p>PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref> contains images of varying resolutions, up to 500 × 500, representing 21 classes (including a class for the background). This set originally contained 1,464 training, 1,449 validation, and 1,456 test images. Its training set was later extended by others to a total of 10,582 images <ref type="bibr" target="#b13">[14]</ref>. This set is not typically used to evaluate real-time segmentation methods but the low resolution images allow for quick experimentation. For this reason, we chose this benchmark for our initial tests.</p><p>Tab. 2 reports accuracy, FLOPs, and number of trainable parameters for our model and those of existing work. We chose methods that reported results on the PASCAL VOC validation set, without inference strategies (e.g., without horizontal mirroring and multi-scale testing). Besides in- creasing inference times by several factors, these techniques can blur the contributions of the underlying methods. As evident from the results, compared with previous work, our methods achieve the best mIoU, with lower GFLOPs and a smaller number of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Cityscapes benchmark tests</head><p>Cityscapes <ref type="bibr" target="#b7">[8]</ref> provides 5k images of urban street scenes, labeled with 19 classes. Image resolutions are 2048 × 1024 pixels, and are typically downsampled or cropped during training time. The images are partitioned into 2,975 train-ing, 500 validation, and 1,525 test images.</p><p>Tab. 3 compares variants of our approach using the EfficientNet-B1 backbone <ref type="bibr" target="#b45">[45]</ref> operating on different resolutions, with previous methods. We only show previous work considered to be fast: Methods that run at 10 FPS or faster and report test set mIoU. Our models achieve the best accuracy on both validation and test sets, as well as the best trade-off between accuracy and run-time performance. The trade-off comparison can be best seen in <ref type="figure" target="#fig_0">Fig. 1</ref>. Importantly, our model incurs a large run-time penalty, due to the unoptimized operations of DPWConv. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the trade-off between GFLOPs and accuracy of our method relative to previous work. Evidently, our method achieves a significantly better trade-off than other methods. While GFLOPs does not directly correlate with FPS, it does suggest the potential run-time performance, once all our functions are optimized. <ref type="figure" target="#fig_3">Fig. 5</ref> provides qualitative results of our HyperSeg-S model on Cityscapes validation images. Our model produces high quality segmentations without any apparent artifacts, due to the partitioning of images into patches. The last two rows of <ref type="figure" target="#fig_3">Fig. 5</ref> offer sample failure cases. In the second row from the bottom, our model confuses a truck with a car. In the last row, our model fails to segment the poles and mistakenly labels pixels as wall or sidewalk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">CamVid benchmark tests</head><p>CamVid offers 701 images of driving scenes, similar to those of Cityscapes, labeled for 11 classes <ref type="bibr" target="#b1">[2]</ref>. All images share the same resolution, 960 × 720. The images are partitioned into 367 training, 101 validation, and 233 test images. Following the training protocol used by all of our baselines, we train on both the training and validation sets.</p><p>Tab. 4 compares our approach with previous real-time methods pretrained on ImageNet <ref type="bibr" target="#b40">[40]</ref>. For a fair comparison, we exclude methods that use additional outside data, other than ImageNet and CamVid. We test two variations of our model, both using the EfficientNet-B1 backbone <ref type="bibr" target="#b45">[45]</ref>. HyperSeg-S operates on resolutions of 768×576 and HyperSeg-L on 1024 × 768. Both models achieve SotA mIoU by a margin relative to that reported by the previous SotA, with HyperSeg-S running at 38 FPS.</p><p>Even without outside data, our method outperforms SotA results reported by methods that use Cityscapes as additional training data: The best results using Cityscapes was reported by BiSeNetV2-L <ref type="bibr" target="#b52">[52]</ref>, which improves its performance from 73.2% mIoU, when trained without additional data, to 78.5% with this data. This is still lower, by a margin, than our method: 79.1% with 16.6FPS. In fact, their result is almost identical to our 38FPS network. Both variants of our method do not use any additional training data.</p><p>Ablation Study. We performed ablation studies on the CamVid dataset <ref type="bibr" target="#b1">[2]</ref>, to show the contribution of our metalearning approach and the effect of using different backbones. The results are reported in Tab. 4.</p><p>In the first six middle experiments, for each model configuration we replace the EfficientNet-B1 backbone with different backbones: ResNet18, PSPNet18, and PSPNet50. We have explicitly chosen backbones that were used by previous methods. In our implementation, a fully connected layer transforms the last feature map before it is fed to the context head, to 1280 channels for the ResNet18 and PSPNet18 backbones, and 2048 channels for the PSPNet50 backbone. In the PSPNet backbones we do not use dilations  <ref type="table">Table 4</ref>. Real-time semantic segmentation results on CamVid <ref type="bibr" target="#b1">[2]</ref> (test set; no outside data). Top: previous methods. Middle: ablation study. The first six rows are variants of our models with different backbones. In comparison to the baselines, we improve accuracy and increase runtime. In the "w/o DPWConv" variants, we replace the dynamic depth-wise convolutions with regular convolutions. Those variants achieve lower accuracy compared to our full method. Bottom: Our full method. Computed by us using open source.</p><p>in any of the convolutions. In the experiments marked with "w/o DPWConv", we replace all the dynamic patch-wise convolutions with regular convolutions, effectively eliminating all the meta-learning elements of our method.</p><p>The results clearly show that the EfficientNet-B1 backbone is superior to the ResNet18, PSPNet18, and PSPNet50 backbones, yet our method still outperforms previous methods with those backbones. Finally, removing meta-learning from our method causes a 1.1% drop in accuracy for the HyperSeg-S configuration, and a reduction of 0.7% in accuracy for the HyperSeg-L configuration, with only a slight improvement in FPS, showing that meta-learning is an integral part of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose to marry autoencoders with hypernetworks for the task of semantic segmentation. In our scheme, the hypernetowrk is a composition of three networks: the backbone of the semantic segmentation encoder, b, a context head, h, in the form of an internal U-Net, and multiple weight mapping heads, w i . The decoder is a multi-block decoder, where each block, d i , implements locally connected layers. The outcome is a new type of U-Net that is able to dynamically and locally adapt to the input, thus holding the potential to better tailor the segmentation process to the input image. As our experiments show, our method outperforms the SotA methods, in this very competitive field, across multiple benchmarks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature division algorithm</head><p>Algorithm. 1 describes the signal channel division algorithm referred to in Sec. 3.2. The algorithm starts by dividing the channels, C, into units of size s u . In our experiments s u = max(g w0 , . . . , g wn ), which assures that the divided channels will be divisible by their corresponding g wi , which are all power of 2.</p><p>Each weight is first allocated a single unit in order to make sure minimal channel allocation for each of the weights. The units are divided by their total number relative to the sum of the weights, starting from the large weights. This gives priority to the smaller weights, which receive the remainder of the allocations that were rounded down.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model details</head><p>The hyperparameters of each of our five models are laid out in Tab. 5. Variables r i are the reduction factors, corresponding to each of the features maps, F i , from b, defined in Sec. 3.1. g wi , also defined in Sec. 3.1, equal 16 across all levels for our earliest model, HyperSeg-L (PASCAL VOC), and their values were experimentally adapted for the newer models trained on CamVid and Cityscapes.</p><p>The last rows detail the number of feature map channels in each m i . A single arrow, C in → C out , denotes a single 1×1 convolution pw 1 :</p><formula xml:id="formula_11">R Cin× H 2 i × W 2 i → R Cout× H 2 i × W 2 i ,</formula><p>and two arrows, C in → C hidden → C out , specify the channels of the full meta block (described in Sec. 3.2):</p><formula xml:id="formula_12">pw 1 : R Cin× H 2 i × W 2 i → R C hidden × H 2 i × W 2 i ,<label>(9)</label></formula><p>dw :</p><formula xml:id="formula_13">R C hidden × H 2 i × W 2 i → R C hidden × H 2 i × W 2 i ,<label>(10)</label></formula><formula xml:id="formula_14">pw 2 : R C hidden × H 2 i × W 2 i → R Cout× H 2 i × W 2 i<label>(11)</label></formula><p>Our PASCAL VOC model was trained using the cross entropy loss; all other models were trained using bootstrapped cross entropy loss <ref type="bibr" target="#b37">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional ablation studies</head><p>Ablation study on PASCAL VOC. We tested multiple variants of our method, to show the effects of employing spatially varying convolutions and to evaluate the contribution of the positional encoding. We describe these variants using the following terminology: 1 × 1 denotes evaluating the entire image as a single patch. That is, we only generate a single set of weights per input image. For this variant, we completely remove network h. 16 × 16 is the original number of patches used for reporting our results on PASCAL VOC in Tab. 2.</p><p>Our ablation results are reported in Tab. C. Evidently, the larger the grid size, the better our accuracy. The contribution of the positional encoding is significant, given that the gap between the two best previous methods is 0.1%, as can be seen in Tab. 2. As evident from the reported FPS column, the 1 × 1 variant is the fastest because it does not require unoptimized operations used for the DPWConv and because the network h is absent.</p><p>Ablation study on Cityscapes. We test the effect of varying g wi , i ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> in our HyperSeg-M model on resolutions of 1536 × 768 and report results in Tab. 7. We fix the ratio between the groups relative to |θ mi | while maintaining multiples of 2. We start by setting g w1 = 1 for the test in the first row and then double the group number in each subsequent test. The number of parameters and flops of w i decreases as the number of groups increases, according to Eq. 6 and Eq. 7. Surprisingly, the experiment in the middle row provides the best GFLOPs / accuracy trade-off, achieving the best accuracy with fewer GFLOPs and parameters than the first two tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Open source repositories</head><p>The open source repositories used in <ref type="table">Tables 2, 3</ref>, and 4, to report information about previous methods, which was not available otherwise, are listed in Tab. 8. The protocol for computing the FPS, GFLOPs, and trainable parameters, is described in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Convolution and batch normalization fusion</head><p>Following others <ref type="bibr" target="#b32">[32]</ref>, we fuse the convolution and batch normalization operations in the inference stage for improving the runtime performance. For regular convolutions, the batch normalization operation is fused with its prior convolution. The batch normalization operations following DP-WConv are fused with the corresponding weight mapping layer. We next provide more details on these steps.</p><p>The batch normalization operation can be described in the form of matrix multiplication,</p><formula xml:id="formula_15">θ BN · x + b BN , for which θ BN i,i = γi √ σ 2 i +</formula><p>on the diagonal and zero everywhere else,</p><formula xml:id="formula_16">and b BN i = β i − γ i µi √ σ 2 i + for i ∈ [0, C),</formula><p>where C is the number of feature channels, γ is the scaling factor, β is the shifting factor, µ and σ are the mean and standard deviation, respectively, and is a scalar constant used for numeric stabilization.</p><p>The convolution operation can also be written as a matrix multiplication by reshaping its weights, θ * , and input fea-Algorithm 1 Divides the channels, C, in unit size, s u , into chunks relative to the weights, w 0 , . . . , w n .   <ref type="table">Table 6</ref>. Ablation study on PASCAL VOC 2012, val. set <ref type="bibr" target="#b10">[11]</ref>. Each row represents a different model, trained with the specified grid size, with or without positional encoding.</p><p>ture map, x, such that θ * ∈ R C×Cin×k 2 andx ∈ R Cin×k 2 , wherex i,j is the k ×k neighborhood in location (i, j) of the original feature, x. The combined convolution and batch normalization operations can then be written as:  <ref type="table">Table 7</ref>. Ablation study on Cityscapes, val. set <ref type="bibr" target="#b7">[8]</ref>. The Groups column represents: gw 1 , . . . , gw n .</p><formula xml:id="formula_17">O i,j = θ BN · (θ * ·x i,j + b * ) + b BN .<label>(12)</label></formula><p>The fused convolution operation will then have the weights θ * = θ BN · θ * and biasb * = θ BN · b * + b BN . Similarly, the DPWConv operation on a patch location (m, n) followed by batch normalization is: where θ w are the weights of the weight mapping layer and φ m,n is the signal corresponding to the patch in location (m, n). The batch normalization operation can then be fused into the weight mapping layer using the adjusted weights,θ w = θ BN · θ w , and bias,b w = θ BN · b w + b BN .  <ref type="bibr" target="#b1">[2]</ref>. The first four rows display predictions on different scenes and the last two rows demonstrate failures of our model: in the first row a bicyclist is partly segmented as a pedestrian, and in the last row our model fails to detect a sign. <ref type="figure" target="#fig_4">Figure 6</ref>. Qualitative results on PASCAL VOC 2012 validation set <ref type="bibr" target="#b10">[11]</ref>. First and 4th columns: input image, 2nd and 5th columns: our predictions, and 3rd and final column: ground truth. The first four rows demonstrate how our model performs on different classes. The last two rows present failure cases of our model. <ref type="figure">Figure 7</ref>. Qualitative results on CamVid test set <ref type="bibr" target="#b1">[2]</ref>. The columns represent: input (left), prediction (center), and ground truth (right). The first four rows provide samples from different scenes, and the last two rows demonstrate failure cases.</p><formula xml:id="formula_18">O i,j = θ BN · [(θ w · φ m,n ) ·x i,j + b w ] + b BN ,<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional qualitative results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Run-time / accuracy trade-off comparison on the Cityscapes [8] test set. Our models (in orange) achieve the best accuracy and the best run-time vs. accuracy trade-off relative to all previous real-time methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Method overview. (a) The hypernetwork encoder based on an EfficientNet [45] backbone, b, with its final layer replaced by the context head, h. (b) The primary network decoder, d, and layers wi of the weight mapping network, w, embedded in each meta block. The input to the decoder, d, are the input image and the features, Fi, concatenated with positional embedding, Pi. Its weights are determined dynamically for each patch in the image. Gray arrows represent skip connections, ×2 blocks are bilinear upsampling, and the blue '+' signs are concatenations. (c) The context head is designed as a nested U-Net. Please see Sec. 3 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>specific metric was computed by us using available open source (listed in Appendix D). 1 Reported using horizontal mirroring and multiscale (confirmed by open source). GFLOPs vs. accuracy trade-off on Cityscapes [8].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results on Cityscapes<ref type="bibr" target="#b7">[8]</ref> validation set images. Left to right: input, our result, and ground truth. The first four rows showcase our model's performance in diverse scenes. The last two rows provide sample failures. Please note that the reflective car hood region is ignored in evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>provides qualitative results on PASCAL VOC 2012 val. set<ref type="bibr" target="#b10">[11]</ref>. In the first four rows we have specifically chosen samples with different classes to best demonstrate the performance of our model.The last two rows offer failure cases, top left: boat classified as a chair; bottom left: the model failed to detect the bottles from a top view; top right: dog classified as a cat; and bottom right: sheep classified as a dog. Fig 7 shows qualitative results of our HyperSeg-L model on the CamVid dataset test set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Training hyperparameters for each benchmark.</figDesc><table><row><cell>, d benefits from</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>[ 60 ]</head><label>60</label><figDesc>Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip HS Torr. Conditional random fields as recurrent neural networks. In Int. Conf. Comput. Vis.</figDesc><table><row><cell></cell><cell></cell><cell>, pages</cell></row><row><cell>1529-1537, 2015. 2</cell><cell></cell><cell></cell></row><row><cell>[61] Ligeng Zhu.</cell><cell>pytorch-OpCounter.</cell><cell>Available on-</cell></row><row><cell cols="3">line: https://github.com/Lyken17/pytorch-</cell></row><row><cell>OpCounter. 5</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1: procedure DIVIDE CHANNELS(C, s u , w 0 , . . . , w n ) ← sort(w 0 , . . . , w n ) , . . . , r5  1  /4, 1 /4, 1 /4, 1 /4, 1 /4 -, 2 /5, 1 /4, 1 /5, 1 /6 1 /4, 1 /4, 1 /4, 1 /4, 1 /4 1 /4, 1 /4, 1 /4, 1 /4, 1 /4 1 /4, 1 /4, 1 /4, 1 /4, 1 /4 g w0 , . . . , g w516, 16, 16, 16, 16, 16 -, 4, 16, 8, 16, 32   </figDesc><table><row><cell>2:</cell><cell cols="2">total units ← C su</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3:</cell><cell cols="6">w Descending order</cell></row><row><cell>4:</cell><cell cols="2">r ← total units n i=0 wi</cell><cell></cell><cell></cell><cell></cell><cell>Units to weights ratio</cell></row><row><cell>5:</cell><cell cols="2">out ← {s u | for each w i ∈ w}</cell><cell></cell><cell cols="3">Each weight group should be allocated with at least one unit</cell></row><row><cell>6:</cell><cell cols="3">total units ← total units − |out|</cell><cell></cell><cell></cell></row><row><cell>7:</cell><cell>i ← 0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8:</cell><cell cols="2">while total units = 0 do</cell><cell></cell><cell></cell><cell></cell></row><row><cell>9:</cell><cell cols="2">if i = n or w i · r ≤ 1 then</cell><cell></cell><cell></cell><cell></cell></row><row><cell>10:</cell><cell cols="3">curr units ← total units</cell><cell></cell><cell></cell></row><row><cell>11:</cell><cell>else</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>12:</cell><cell cols="3">curr units ← w i · r − 1</cell><cell></cell><cell></cell></row><row><cell>13:</cell><cell cols="3">out i ← out i + curr units · s u</cell><cell></cell><cell></cell></row><row><cell>14:</cell><cell cols="3">total units ← total units − curr units</cell><cell></cell><cell></cell></row><row><cell>15:</cell><cell cols="2">i ← i + 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>16:</cell><cell cols="2">return out</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Params</cell><cell></cell><cell>HyperSeg-L (PASCAL VOC)</cell><cell>HyperSeg-S (Cityscapes)</cell><cell>HyperSeg-M (Cityscapes)</cell><cell>HyperSeg-S (CamVid)</cell><cell>HyperSeg-L (CamVid)</cell></row><row><cell cols="2">Backbone</cell><cell>EfficientNet-B3</cell><cell>EfficientNet-B1</cell><cell>EfficientNet-B1</cell><cell>EfficientNet-B1</cell><cell>EfficientNet-B1</cell></row><row><cell cols="2">Resolution</cell><cell>512 × 512</cell><cell>1536 × 768</cell><cell>1024 × 512</cell><cell>768 × 576</cell><cell>1024 × 768</cell></row><row><cell cols="5">r 1 -, 4, 16, 8, 16, 32</cell><cell cols="2">-, 8, 16, 32, 32, 64 8, 8, 16, 32, 32, 64</cell></row><row><cell cols="2">m 5 channels</cell><cell>98 − → 96</cell><cell>130 − → 32</cell><cell>82 − → 64</cell><cell>82 − → 64</cell><cell>82 − → 64</cell></row><row><cell cols="2">m 4 channels</cell><cell>132 − → 34</cell><cell>62 − → 16</cell><cell>94 − → 32</cell><cell>94 − → 32</cell><cell>94 − → 32</cell></row><row><cell cols="2">m 3 channels</cell><cell>48 − → 96 − → 12</cell><cell>26 − → 8</cell><cell>44 − → 16</cell><cell>44 − → 16</cell><cell>44 − → 16</cell></row><row><cell cols="2">m 2 channels</cell><cell>22 − → 44 − → 8</cell><cell>14 − → 28 − → 8</cell><cell>24 − → 48 − → 16</cell><cell>24 − → 48 − → 16</cell><cell>24 − → 48 − → 16</cell></row><row><cell cols="2">m 1 channels</cell><cell>16 − → 32 − → 6</cell><cell>26 − → 52 − → 19</cell><cell>34 − → 68 − → 19</cell><cell>22 − → 44 − → 12</cell><cell>22 − → 44 − → 16</cell></row><row><cell cols="2">m 0 channels</cell><cell>11 − → 22 − → 21</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>21 − → 42 − → 12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Model details. Each row represents a different model hyperparameter and each column a different model, where "HyperSeg-&lt;size&gt; (dataset)" is the model's template name (see Sec. 4) and the dataset on which it was trained on. "-" denotes that the decoder level corresponding to the specific hyperparameter was omitted.</figDesc><table><row><cell cols="3">Grid size Positional encoding mIoU (%) FPS</cell></row><row><cell>1 × 1</cell><cell>77.56</cell><cell>46.8</cell></row><row><cell>4 × 4</cell><cell>78.92</cell><cell>22.4</cell></row><row><cell>8 × 8</cell><cell>80.23</cell><cell>26.9</cell></row><row><cell>16 × 16</cell><cell>80.33</cell><cell>28.2</cell></row><row><cell>16 × 16</cell><cell>80.61</cell><cell>26.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>List of open source repositories used for comparing to previous methods.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09673</idno>
		<title level="m">Dynamic filter networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1223" to="1231" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stacked deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sifting through scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Filosof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viki</forename><surname>Mayzels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1431" to="1443" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dense Image Correspondences for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On sifts and their scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viki</forename><surname>Mayzels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1522" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporally distributed networks for fast video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="8818" to="8827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hypernetwork functional image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Sylwester Klocek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Maziarka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Wołczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marekśmieja</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="496" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9522" to="9531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep meta functionals for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gidi</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1824" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Guided upsampling network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Mazzini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07466</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<title level="m">Proceedings of the european conference on computer vision (ECCV)</title>
		<meeting>the european conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On face segmentation, face swapping, and face perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tran</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="98" to="105" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="68" to="80" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">U2-net: Going deeper with nested u-structure for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Osmar R Zaiane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">107404</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Largescale deep unsupervised learning using graphics processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Machine. Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Medical image comput. and comput. assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">W</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dense correspondences across scenes and scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moria</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="875" to="888" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Large-scale object recognition with cuda-accelerated hierarchical neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Uetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. conf. on intelligent comput. and intelligent sys</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="536" to="541" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Johannes Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin F Grewe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00695</idno>
		<title level="m">Continual learning with hypernetworks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07853</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10430</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02147</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Graph hypernetworks for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05749</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep region and multi-label learning for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3391" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Url Auto-Deeplab-L</forename><surname>Method</surname></persName>
		</author>
		<ptr target="https://github.com/MenghaoGuo/AutoDeeplab" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
