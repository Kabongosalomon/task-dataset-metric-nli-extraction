<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2020 DIRECTIONAL MESSAGE PASSING FOR MOLECULAR GRAPHS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
							<email>klicpera@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janek</forename><surname>Groß</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
							<email>guennemann@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2020 DIRECTIONAL MESSAGE PASSING FOR MOLECULAR GRAPHS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1 /4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76 % on MD17 and by 31 % on QM9. Our implementation is available online. 1 Published as a conference paper at ICLR 2020 embeddings are equivariant with respect to the above transformations since the directions move with the molecule. Hence, they preserve the relative directional information between neighboring atoms. We propose to let message embeddings interact based on the distance between atoms and the angle between directions. Both distances and angles are invariant to translation, rotation, and inversion of the molecule, as required. Additionally, we show that the distance and angle can be jointly represented in a principled and effective manner by using spherical Bessel functions and spherical harmonics. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet can learn both molecular properties and atomic forces. It is twice continuously differentiable and solely based on the atom types and coordinates, which are essential properties for performing molecular dynamics simulations. DimeNet outperforms previous GNNs on average by 76 % on MD17 and by 31 % on QM9. Our paper's main contributions are:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years scientists have started leveraging machine learning to reduce the computation time required for predicting molecular properties from a matter of hours and days to mere milliseconds. With the advent of graph neural networks (GNNs) this approach has recently experienced a small revolution, since they do not require any form of manual feature engineering and significantly outperform previous models . GNNs model the complex interactions between atoms by embedding each atom in a high-dimensional space and updating these embeddings by passing messages between atoms. By predicting the potential energy these models effectively learn an empirical potential function. Classically, these functions have been modeled as the sum of four parts: <ref type="bibr" target="#b27">(Leach, 2001</ref>)</p><formula xml:id="formula_0">E = E bonds + E angle + E torsion + E non-bonded ,<label>(1)</label></formula><p>where E bonds models the dependency on bond lengths, E angle on the angles between bonds, E torsion on bond rotations, i.e. the dihedral angle between two planes defined by pairs of bonds, and E non-bonded models interactions between unconnected atoms, e.g. via electrostatic or van der Waals interactions. The update messages in GNNs, however, only depend on the previous atom embeddings and the pairwise distances between atoms -not on directional information such as bond angles and rotations. Thus, GNNs lack the second and third terms of this equation and can only model them via complex higher-order interactions of messages. Extending GNNs to model them directly is not straightforward since GNNs solely rely on pairwise distances, which ensures their invariance to translation, rotation, and inversion of the molecule, which are important physical requirements.</p><p>In this paper, we propose to resolve this restriction by using embeddings associated with the directions to neighboring atoms, i.e. by embedding atoms as a set of messages. These directional message 1. Directional message passing, which allows GNNs to incorporate directional information by connecting recent advances in the fields of equivariance and graph neural networks as well as ideas from belief propagation and empirical potential functions such as Eq. 1. 2. Theoretically principled orthogonal basis representations based on spherical Bessel functions and spherical harmonics. Bessel functions achieve better performance than Gaussian radial basis functions while reducing the radial basis dimensionality by 4x or more. 3. The Directional Message Passing Neural Network (DimeNet): A novel GNN that leverages these innovations to set the new state of the art for molecular predictions and is suitable both for predicting molecular properties and for molecular dynamics simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>ML for molecules. The classical way of using machine learning for predicting molecular properties is combining an expressive, hand-crafted representation of the atomic neighborhood <ref type="bibr" target="#b2">(Bartók et al., 2013)</ref> with Gaussian processes <ref type="bibr" target="#b1">(Bartók et al., 2010;</ref> or neural networks <ref type="bibr" target="#b5">(Behler &amp; Parrinello, 2007)</ref>. Recently, these methods have largely been superseded by graph neural networks, which do not require any hand-crafted features but learn representations solely based on the atom types and coordinates molecules <ref type="bibr" target="#b15">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b21">Hy et al., 2018;</ref><ref type="bibr" target="#b40">Unke &amp; Meuwly, 2019)</ref>. Our proposed message embeddings can also be interpreted as directed edge embeddings or embeddings on the line graph <ref type="bibr" target="#b7">(Chen et al., 2019b)</ref>. (Undirected) edge embeddings have already been used in previous GNNs for molecules <ref type="bibr" target="#b23">(Jørgensen et al., 2018;</ref><ref type="bibr" target="#b6">Chen et al., 2019a)</ref>. However, these GNNs use both node and edge embeddings and do not leverage any directional information.</p><p>Graph neural networks. GNNs were first proposed in the 90s <ref type="bibr" target="#b4">(Baskin et al., 1997;</ref><ref type="bibr" target="#b38">Sperduti &amp; Starita, 1997)</ref> and 00s <ref type="bibr" target="#b18">(Gori et al., 2005;</ref><ref type="bibr" target="#b36">Scarselli et al., 2009</ref>). General GNNs have been largely inspired by their application to molecular graphs and have started to achieve breakthrough performance in various tasks at around the same time the molecular variants did <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b25">Klicpera et al., 2019;</ref><ref type="bibr" target="#b45">Zambaldi et al., 2019)</ref>. Some recent progress has been focused on GNNs that are more powerful than the 1-Weisfeiler-Lehman test of isomorphism <ref type="bibr" target="#b29">(Morris et al., 2019;</ref><ref type="bibr" target="#b28">Maron et al., 2019)</ref>. However, for molecular predictions these models are significantly outperformed by GNNs focused on molecules (see Sec. 7). Some recent GNNs have incorporated directional information by considering the change in local coordinate systems per atom <ref type="bibr" target="#b22">(Ingraham et al., 2019)</ref>. However, this approach breaks permutation invariance and is therefore only applicable to chain-like molecules (e.g. proteins).</p><p>Equivariant neural networks. Group equivariance as a principle of modern machine learning was first proposed by <ref type="bibr" target="#b11">Cohen &amp; Welling (2016)</ref>. Following work has generalized this principle to spheres , molecules <ref type="bibr" target="#b39">(Thomas et al., 2018)</ref>, volumetric data <ref type="bibr" target="#b41">(Weiler et al., 2018)</ref>, and general manifolds <ref type="bibr" target="#b12">(Cohen et al., 2019)</ref>. Equivariance with respect to continuous rotations has been achieved so far by switching back and forth between Fourier and coordinate space in each layer  or by using a fully Fourier space model <ref type="bibr" target="#b0">Anderson et al., 2019)</ref>. The former introduces major computational overhead and the latter imposes significant constraints on model construction, such as the inability of using non-linearities. Our proposed solution does not suffer from either of those limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REQUIREMENTS FOR MOLECULAR PREDICTIONS</head><p>In recent years machine learning has been used to predict a wide variety of molecular properties, both low-level quantum mechanical properties such as potential energy, energy of the highest occupied molecular orbital (HOMO), and the dipole moment and high-level properties such as toxicity, permeability, and adverse drug reactions <ref type="bibr" target="#b42">(Wu et al., 2018)</ref>. In this work we will focus on scalar regression targets, i.e. targets t ∈ R. A molecule is uniquely defined by the atomic numbers z = {z 1 , . . . , z N } and positions X = {x 1 , . . . , x N }. Some models additionally use auxiliary information Θ such as bond types or electronegativity of the atoms. We do not include auxiliary features in this work since they are hand-engineered and non-essential. In summary, we define an ML model for molecular prediction with parameters θ via f θ : {X, z} → R.</p><p>Symmetries and invariances. All molecular predictions must obey some basic laws of physics, either explicitly or implicitly. One important example of such are the fundamental symmetries of physics and their associated invariances. In principle, these invariances can be learned by any neural network via corresponding weight matrix symmetries <ref type="bibr" target="#b34">(Ravanbakhsh et al., 2017)</ref>. However, not explicitly incorporating them into the model introduces duplicate weights and increases training time and complexity. The most essential symmetries are translational and rotational invariance (follows from homogeneity and isotropy), permutation invariance (follows from the indistinguishability of particles), and symmetry under parity, i.e. under sign flips of single spatial coordinates.</p><p>Molecular dynamics. Additional requirements arise when the model should be suitable for molecular dynamics (MD) simulations and predict the forces F i acting on each atom. The force field is a conservative vector field since it must satisfy conservation of energy (the necessity of which follows from homogeneity of time <ref type="bibr" target="#b30">(Noether, 1918)</ref>). The easiest way of defining a conservative vector field is via the gradient of a potential function. We can leverage this fact by predicting a potential instead of the forces and then obtaining the forces via backpropagation to the atom coordinates, i.e. F i (X, z) = − ∂ ∂xi f θ (X, z). We can even directly incorporate the forces in the training loss and directly train a model for MD simulations <ref type="bibr" target="#b31">(Pukrittayakamee et al., 2009)</ref>:</p><formula xml:id="formula_1">L MD (X, z) = f θ (X, z) −t(X, z) + ρ 3N N i=1 3 α=1 − ∂f θ (X, z) ∂x iα −F iα (X, z) ,<label>(2)</label></formula><p>where the targett =Ê is the ground-truth energy (usually available as well),F are the ground-truth forces, and the hyperparameter ρ sets the forces' loss weight. For stable simulations F i must be continuously differentiable and the model f θ itself therefore twice continuously differentiable. We hence cannot use discontinuous transformations such as ReLU non-linearities. Furthermore, since the atom positions X can change arbitrarily we cannot use pre-computed auxiliary information Θ such as bond types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DIRECTIONAL MESSAGE PASSING</head><p>Graph neural networks. Graph neural networks treat the molecule as a graph, in which the nodes are atoms and edges are defined either via a predefined molecular graph or simply by connecting atoms that lie within a cutoff distance c. Each edge is associated with a pairwise distance between atoms d ij = x i − x j 2 . GNNs implement all of the above physical invariances by construction since they only use pairwise distances and not the full atom coordinates. However, note that a predefined molecular graph or a step function-like cutoff cannot be used for MD simulations since this would introduce discontinuities in the energy landscape. GNNs represent each atom i via an atom embedding h i ∈ R H . The atom embeddings are updated in each layer by passing messages along the molecular edges. Messages are usually transformed based on an edge embedding e (ij) ∈ R He and summed over the atom's neighbors N i , i.e. the embeddings are updated in layer l via</p><formula xml:id="formula_2">h (l+1) i = f update (h (l) i , j∈Ni f int (h (l) j , e (l) (ij) )),<label>(3)</label></formula><p>with the update function f update and the interaction function f int , which are both commonly implemented using neural networks. The edge embeddings e (l) (ij) usually only depend on the interatomic distances, but can also incorporate additional bond information  or be recursively updated in each layer using the neighboring atom embeddings <ref type="bibr" target="#b23">(Jørgensen et al., 2018)</ref>.</p><p>Directionality. In principle, the pairwise distance matrix contains the full geometrical information of the molecule. However, GNNs do not use the full distance matrix since this would mean passing messages globally between all pairs of atoms, which increases computational complexity and can lead to overfitting. Instead, they usually use a cutoff distance c, which means they cannot distinguish between certain molecules <ref type="bibr" target="#b43">(Xu et al., 2019)</ref>. E.g. at a cutoff of roughly 2 Å a regular GNN would not be able to distinguish between a hexagonal (e.g. Cyclohexane) and two triangular molecules (e.g. Cyclopropane) with the same bond lengths since the neighborhoods of each atom are exactly the same for both (see Appendix, <ref type="figure">Fig. 6</ref>). This problem can be solved by modeling the directions to neighboring atoms instead of just their distances. A principled way of doing so while staying invariant to a transformation group G (such as described in Sec. 3) is via group-equivariance <ref type="bibr" target="#b11">(Cohen &amp; Welling, 2016)</ref>.</p><formula xml:id="formula_3">A function f : X → Y is defined as being equivariant if f (ϕ X g (x)) = ϕ Y g (f (x))</formula><p>, with the group action in the input and output space ϕ X g and ϕ Y g . However, equivariant CNNs only achieve equivariance with respect to a discrete set of rotations <ref type="bibr" target="#b11">(Cohen &amp; Welling, 2016)</ref>. For a precise prediction of molecular properties we need continuous equivariance with respect to rotations, i.e. to the SO(3) group.</p><p>Directional embeddings. We solve this problem by noting that an atom by itself is rotationally invariant. This invariance is only broken by neighboring atoms that interact with it, i.e. those inside the cutoff c. Since each neighbor breaks up to one rotational invariance they also introduce additional degrees of freedom, which we need to represent in our model. We can do so by generating a separate embedding m ji for each atom i and neighbor j by applying the same learned filter in the direction of each neighboring atom (in contrast to equivariant CNNs, which apply filters in fixed, global directions). These directional embeddings are equivariant with respect to global rotations since the associated directions rotate with the molecule and hence conserve the relative directional information between neighbors.</p><p>Representation via joint 2D basis. We use the directional information associated with each embedding by leveraging the angle α (kj,ji) = ∠x k x j x i when aggregating the neighboring embeddings m kj of m ji . We combine the angle with the interatomic distance d kj associated with the incoming message m kj and jointly represent both in a (kj,ji) SBF ∈ R NSHBF·NSRBF using a 2D representation based on spherical Bessel functions and spherical harmonics, as explained in Sec. 5. We empirically found that this basis representation provides a better inductive bias than the raw angle alone. Note that by only using interatomic distances and angles our model becomes invariant to rotations. Message embeddings. The directional embedding m ji associated with the atom pair ji can be thought of as a message being sent from atom j to atom i. Hence, in analogy to belief propagation, we embed each atom i using a set of incoming messages m ji , i.e. h i = j∈Ni m ji , and update the message m ji based on the incoming messages m kj <ref type="bibr" target="#b44">(Yedidia et al., 2003)</ref>. Hence, as illustrated in <ref type="figure">Fig. 1</ref>, we define the update function and aggregation scheme for message embeddings as</p><formula xml:id="formula_4">m (l+1) ji = f update (m (l) ji , k∈Nj \{i} f int (m (l) kj , e (ji) RBF , a (kj,ji) SBF )),<label>(4)</label></formula><p>where e (ji) RBF denotes the radial basis function representation of the interatomic distance d ji , which will be discussed in Sec. 5. We found this aggregation scheme to not only have a nice analogy to belief propagation, but also to empirically perform better than alternatives.</p><p>Note that since f int now incorporates the angle between atom pairs, or bonds, we have enabled our model to directly learn the angular potential E angle , the second term in Eq. 1. Moreover, the message embeddings are essentially embeddings of atom pairs, as used by the provably more powerful GNNs based on higher-order Weisfeiler-Lehman tests of isomorphism. Our model can therefore provably distinguish molecules that a regular GNN cannot (e.g. the previous example of a hexagonal and two triangular molecules) <ref type="bibr" target="#b29">(Morris et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PHYSICALLY BASED REPRESENTATIONS</head><p>Representing distances and angles. For the interaction function f int in Eq. 4 we use a joint representation a (kj,ji) SBF of the angles α (kj,ji) between message embeddings and the interatomic distances d kj = x k − x j 2 , as well as a representation e (ji) RBF of the distances d ji . Earlier works have used a set of Gaussian radial basis functions to represent interatomic distances, with tightly spaced means that are distributed e.g. uniformly  or exponentially <ref type="bibr" target="#b40">(Unke &amp; Meuwly, 2019)</ref>. Similar in spirit to the functional bases used by steerable CNNs <ref type="bibr" target="#b13">(Cohen &amp; Welling, 2017;</ref><ref type="bibr" target="#b8">Cheng et al., 2019)</ref> we propose to use an orthogonal basis instead, which reduces redundancy and thus improves parameter efficiency. Furthermore, a basis chosen according to the properties of the modeled system can even provide a helpful inductive bias. We therefore derive a proper basis representation for quantum systems next.  with the spherical Bessel functions of the first and second kind j l and y l and the spherical harmonics Y m l . As common in physics we only use the regular solutions, i.e. those that do not approach −∞ at the origin, and hence set b lm = 0. Recall that our first goal is to construct a joint 2D basis for d kj and α (kj,ji) , i.e. a function that depends on d and a single angle α. To achieve this we set m = 0 and obtain Ψ SBF (d, α) = l a l j l (kd)Y 0 l (α). The boundary conditions are satisfied by setting k = z ln c , where z ln is the n-th root of the l-order Bessel function, which are precomputed numerically. Normalizing Ψ SBF inside the cutoff distance c yields the 2D spherical Fourier-Bessel basisã (kj,ji) SBF ∈ R NSHBF·NSRBF , which is illustrated in <ref type="figure">Fig. 2</ref> and defined byã <ref type="bibr">SBF,ln</ref> </p><formula xml:id="formula_5">Ψ(d, α, ϕ) = ∞ l=0 l m=−l (a lm j l (kd) + b lm y l (kd))Y m l (α, ϕ),<label>(5)</label></formula><formula xml:id="formula_6">(d, α) = 2 c 3 j 2 l+1 (z ln ) j l ( z ln c d)Y 0 l (α),<label>(6)</label></formula><p>with l ∈ [0 . . N SHBF − 1] and n ∈ [1 . . N SRBF ]. Our second goal is constructing a radial basis for d ji , i.e. a function that solely depends on d and not on the angles α and ϕ. We achieve this by setting l = m = 0 and obtain Ψ RBF (d) = aj 0 ( z0,n c d), with roots at z 0,n = nπ. Normalizing this function on [0, c] and using j 0 (d) = sin(d)/d gives the radial basisẽ RBF ∈ R NRBF , as shown in <ref type="figure" target="#fig_2">Fig. 3 and defined</ref> bỹ</p><formula xml:id="formula_7">e RBF,n (d) = 2 c sin( nπ c d) d ,<label>(7)</label></formula><p>with n ∈ [1 . . N RBF ]. Both of these bases are purely real-valued and orthogonal in the domain of interest. They furthermore enable us to bound the highest-frequency components by ω α ≤ NSHBF 2π , ω d kj ≤ NSRBF c , and ω dji ≤ NRBF c . This restriction is an effective way of regularizing the model and ensures that predictions are stable to small perturbations. We found N SRBF = 6 and N RBF = 16 radial basis functions to be more than sufficient. Note that N RBF is 4x lower than PhysNet's 64 (Unke &amp; Meuwly, 2019) and 20x lower than SchNet's 300 radial basis functions .  u(d)ẽ RBF (d) and their first and second derivatives to go to 0 at the cutoff. We achieve this with the polynomial</p><formula xml:id="formula_8">h (0) j h (0) i σ(W +b) Output t (1) i m (1) ji Directional message passing e (ji) RBF a (kj,ji) SBF m (l−1) kj k ∈ Nj \{i} m (l−1) ji σ(W +b) σ(W +b) W T W W k + Residual σ(W +b) + Residual Residual Output t (l) i m (l) ji e (ji) RBF m (l) ji j ∈ Ni W j σ(W +b) σ(W +b) σ(W +b) W t (l) i mji σ(W +b)</formula><formula xml:id="formula_9">u(d) = 1 − (p + 1)(p + 2) 2 d p + p(p + 2)d p+1 − p(p + 1) 2 d p+2 ,<label>(8)</label></formula><p>where p ∈ N 0 . We did not find the model to be sensitive to different choices of envelope functions and choose p = 6. Note that using an envelope function causes the bases to lose their orthonormality, which we did not find to be a problem in practice. We furthermore fine-tune the Bessel wave numbers k n = nπ c used inẽ RBF ∈ R NRBF via backpropagation after initializing them to these values, which we found to give a small boost in prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DIRECTIONAL MESSAGE PASSING NEURAL NETWORK (DIMENET)</head><p>The Directional Message Passing Neural Network's (DimeNet) design is based on a streamlined version of the PhysNet architecture <ref type="bibr" target="#b40">(Unke &amp; Meuwly, 2019)</ref>, in which we have integrated directional message passing and spherical Fourier-Bessel representations. DimeNet generates predictions that are invariant to atom permutations and translation, rotation and inversion of the molecule. DimeNet is suitable both for the prediction of various molecular properties and for molecular dynamics (MD) simulations. It is twice continuously differentiable and able to learn and predict atomic forces via backpropagation, as described in Sec. 3. The predicted forces fulfill energy conservation by construction and are equivariant with respect to permutation and rotation. Model differentiability in combination with basis representations that have bounded maximum frequencies furthermore guarantees smooth predictions that are stable to small deformations. <ref type="figure" target="#fig_4">Fig. 4</ref> gives an overview of the architecture.</p><p>Embedding block. Atomic numbers are represented by learnable, randomly initialized atom type embeddings h (0) i ∈ R F that are shared across molecules. The first layer generates message embeddings from these and the distance between atoms via m (1)</p><formula xml:id="formula_10">ji = σ([h (0) j h (0) i e (ji) RBF ]W + b),<label>(9)</label></formula><p>where denotes concatenation and the weight matrix W and bias b are learnable. Interaction block. The embedding block is followed by multiple stacked interaction blocks. This block implements f int and f update of Eq. 4 as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. Note that the 2D representation a (kj,ji) SBF is first transformed into an N bilinear -dimensional representation via a linear layer. The main purpose of this is to make the dimensionality of a (kj,ji) SBF independent of the subsequent bilinear layer, which uses a comparatively large N bilinear × F × F -dimensional weight tensor. We have also experimented with using a bilinear layer for the radial basis representation, but found that the element-wise multiplication e (ji) RBF W m kj performs better, which suggests that the 2D representations require more complex transformations than radial information alone. The interaction block transforms each message embedding m ji using multiple residual blocks, which are inspired by ResNet <ref type="bibr" target="#b20">(He et al., 2016)</ref> and consist of two stacked dense layers and a skip connection.</p><p>Output block. The message embeddings after each block (including the embedding block) are passed to an output block. The output block transforms each message embedding m ji using the radial basis e (ji) RBF , which ensures continuous differentiability and slightly improves performance. Afterwards the incoming messages are summed up per atom i to obtain h i = j m ji , which is then transformed using multiple dense layers to generate the atom-wise output t (l) i . These outputs are then summed up to obtain the final prediction t = i l t (l) i . Continuous differentiability. Multiple model choices were necessary to achieve twice continuous model differentiability. First, DimeNet uses the self-gated Swish activation function σ(x) = x · sigmoid(x) <ref type="bibr" target="#b32">(Ramachandran et al., 2018)</ref> instead of a regular ReLU activation function. Second, we multiply the radial basis functionsẽ RBF (d) with an envelope function u(d) that has a root of multiplicity 3 at the cutoff c. Finally, DimeNet does not use any auxiliary data but relies on atom types and positions alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head><p>Models. For hyperparameter choices and training setup see Appendix B. We use 6 state-of-theart models for comparison: SchNet , PhysNet (results based on the reference implementation) (Unke &amp; Meuwly, 2019), provably powerful graph networks (PPGN, results provided by the original authors) (Maron et al., 2019), MEGNet-simple (without auxiliary information) <ref type="bibr" target="#b6">(Chen et al., 2019a</ref><ref type="bibr">), Cormorant (Anderson et al., 2019</ref>, and symmetrized gradient-domain machine learning (sGDML) <ref type="bibr" target="#b10">(Chmiela et al., 2018)</ref>. Note that sGDML cannot be used for QM9 since it can only be trained on a single molecule.</p><p>QM9. We test DimeNet's performance for predicting molecular properties using the common QM9 benchmark <ref type="bibr" target="#b33">(Ramakrishnan et al., 2014)</ref>. It consists of roughly 130 000 molecules in equilibrium   with up to 9 heavy C, O, N, and F atoms. We use 110 000 molecules in the training, 10 000 in the validation and 10 831 in the test set. We only use the atomization energy for U 0 , U , H, and G, i.e. subtract the atomic reference energies, which are constant per atom type, and perform the training using eV. In <ref type="table" target="#tab_0">Table 1</ref> we report the mean absolute error (MAE) of each target and the overall mean standardized MAE (std. MAE) and mean standardized logMAE (for details see Appendix C). We predict ∆ simply by taking LUMO − HOMO , since it is calculated in exactly this way by DFT calculations. We train a separate model for each target, which significantly improves results compared to training a single shared model for all targets (see App. E). DimeNet sets the new state of the art on 11 out of 12 targets and decreases mean std. MAE by 31 % and mean logMAE by 0.22 compared to the second-best model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MD17</head><p>. We use MD17  to test model performance in molecular dynamics simulations. The goal of this benchmark is predicting both the energy and atomic forces of eight small organic molecules, given the atom coordinates of the thermalized (i.e. non-equilibrium, slightly moving) system. The ground truth data is computed via molecular dynamics simulations using DFT. A separate model is trained for each molecule, with the goal of providing highly accurate individual predictions. This dataset is commonly used with 50 000 training and 10 000 validation and test samples. We found that DimeNet can match state-of-the-art performance in this setup. E.g. for Benzene, depending on the force weight ρ, DimeNet achieves 0.035 kcal mol −1 MAE for the energy or 0.07 kcal mol −1 and 0.17 kcal mol −1 Å −1 for energy and forces, matching the results reported by <ref type="bibr" target="#b0">Anderson et al. (2019)</ref> and <ref type="bibr" target="#b40">Unke &amp; Meuwly (2019)</ref>. However, this accuracy is two orders of magnitude below the DFT calculation's accuracy (approx. 2.3 kcal mol −1 for energy <ref type="bibr" target="#b16">(Faber et al., 2017)</ref>), so any remaining difference to real-world data is almost exclusively due to errors in the DFT simulation. Truly reaching better accuracy can therefore only be achieved with more precise ground-truth data, which requires far more expensive methods (e.g. CCSD(T)) and thus ML models that are more sample-efficient <ref type="bibr" target="#b10">(Chmiela et al., 2018)</ref>. We therefore instead test our model on the harder task of using only 1000 training samples. As shown in <ref type="table" target="#tab_1">Table 2</ref> DimeNet outperforms SchNet by a large margin and performs roughly on par with sGDML. However, sGDML uses hand-engineered descriptors that provide a strong advantage for small datasets, can only be trained on a single molecule (a fixed set of atoms), and does not scale well with the number of atoms or training samples.</p><p>Ablation studies. To test whether directional message passing and the Fourier-Bessel basis are the actual reason for DimeNet's improved performance, we ablate them individually and compare the mean standardized MAE and logMAE for multi-task learning on QM9. <ref type="table" target="#tab_2">Table 3</ref> shows that both of our contributions have a significant impact on the model's performance. Using 64 Gaussian RBFs instead of 16 and 6 Bessel basis functions to represent d ji and d kj increases the error by 10 %, which shows that this basis does not only reduce the number of parameters but additionally provides a helpful inductive bias. DimeNet's error increases by around 26 % when we ignore the angles between messages by setting N SHBF = 1, showing that directly incorporating directional information does indeed improve performance. Using node embeddings instead of message embeddings (and hence also ignoring directional information) has the largest impact and increases MAE by 68 %, at which point DimeNet performs worse than SchNet. Furthermore, <ref type="figure" target="#fig_5">Fig. 5</ref> shows that the filters exhibit a structurally meaningful dependence on both the distance and angle. For example, some of these filters are clearly being activated by benzene rings (120 • angle, 1.39 Å distance). This further demonstrates that the model learns to leverage directional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this work we have introduced directional message passing, a more powerful and expressive interaction scheme for molecular predictions. Directional message passing enables graph neural networks to leverage directional information in addition to the interatomic distances that are used by normal GNNs. We have shown that interatomic distances can be represented in a principled and effective manner using spherical Bessel functions. We have furthermore shown that this representation can be extended to directional information by leveraging 2D spherical Fourier-Bessel basis functions. We have leveraged these innovations to construct DimeNet, a GNN suitable both for predicting molecular properties and for use in molecular dynamics simulations. We have demonstrated DimeNet's performance on QM9 and MD17 and shown that our contributions are the essential ingredients that enable DimeNet's state-of-the-art performance. DimeNet directly models the first two terms in Eq. 1, which are known as the important "hard" degrees of freedom in molecules <ref type="bibr" target="#b27">(Leach, 2001)</ref>. Future work should aim at also incorporating the third and fourth terms of this equation. This could improve predictions even further and enable the application to molecules much larger than those used in common benchmarks like QM9. <ref type="figure">Figure 6</ref>: A standard non-directional GNN cannot distinguish between a hexagonal (left) and two triangular molecules (right) with the same bond lengths, since the neighborhood of each atom is exactly the same. An example of this would be Cyclohexane and two Cyclopropane molecules with slightly stretched bonds, when the GNN either uses the molecular graph or a cutoff distance of c ≤ 2.5 Å. Directional message passing solves this problem by considering the direction of each bond.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A INDISTINGUISHABLE MOLECULES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTAL SETUP</head><p>The model architecture and hyperparameters were optimized using the QM9 validation set. We use 6 stacked interaction blocks and embeddings of size F = 128 throughout the model. For the basis functions we choose N SHBF = 7 and N SRBF = N RBF = 6. For the weight tensor in the interaction block we use N bilinear = 8. We did not find the model to be very sensitive to these values as long as they were large enough (i.e. at least 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DIMENET FILTERS</head><p>To illustrate the filters learned by DimeNet we separate the spatial dependency in the interaction function f int via f int (m, d ji , d kj , α (kj,ji) ) = n [σ(W m + b)] n f filter1,n (d ji )f filter2,n (d kj , α <ref type="bibr">(kj,ji)</ref> ).</p><p>The filters f filter1,n : R + → R and f filter2,n : R + × [0, 2π] → R F are given by f filter1,n (d) = (W RBF e RBF (d)) n ,</p><p>f filter2,n (d, α) = (W SBF a SBF (d, α)) T W n ,</p><p>where W RBF , W SBF , and W are learned weight matrices/tensors, e RBF (d) is the radial basis representation, and a SBF <ref type="figure">(d, α)</ref> is the 2D spherical Fourier-Bessel representation. <ref type="figure" target="#fig_5">Fig. 5</ref> shows how the first 15 elements of f filter2,n (d, α) vary with d and α when choosing the tensor slice n = 1 (with α = 0 at the top of the figure). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MULTI-TARGET RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Aggregation scheme for message embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>From</head><label></label><figDesc>Schrödinger to Fourier-Bessel. To construct a basis representation in a principled manner we first consider the space of possible solutions. Our model aims at approximating results of density functional theory (DFT) calculations, i.e. results given by an electron density Ψ(d)|Ψ(d) , with the electron wave function Ψ(d) andd = x k − x j . The solution space of Ψ(d) is defined by the time-independent Schrödinger equation − 2 2m ∇ 2 + V (d) Ψ(d) = EΨ(d), with constant mass m and energy E. We do not know the potential V (d) and so choose it in an uninformative way by simply setting it to 0 inside the cutoff distance c (up to which we pass messages between atoms) and to ∞ outside. Hence, we arrive at the Helmholtz equation (∇ 2 + k 2 )Ψ(d) = 0, with the wave number k = √ 2mE and the boundary condition Ψ(c) = 0 at the cutoff c. Separation of variables in polar coordinates (d, α, ϕ) yields the solution<ref type="bibr" target="#b19">(Griffiths &amp; Schroeter, 2018)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Radial Bessel basis for N RBF = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Continuous cutoff.ã (kj,ji) SBF andẽ RBF (d) are not twice continuously differentiable due to the step function cutoff at c. To alleviate this problem we introduce an envelope function u(d) that has a root of multiplicity 3 at d = c, causing the final functions a RBF (d) = u(d)ã RBF (d) and e RBF (d) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The DimeNet architecture. denotes the layer's input and denotes concatenation. The distances d ji are represented using spherical Bessel functions and the distances d kj and angles α(kj,ji)   are jointly represented using a 2D spherical Fourier-Bessel basis. An embedding block generates the inital message embeddings m ji . These embeddings are updated in multiple interaction blocks via directional message passing, which uses the neighboring messages m kj , k ∈ N j \ {i}, the 2D representations a (kj,ji) SBF , and the distance representations e (ji) RBF . Each block passes the resulting embeddings to an output block, which transforms them using the radial basis e (ji) RBF and sums them up per atom. Finally, the outputs of all layers are summed up to generate the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Examples of DimeNet filters. They exhibit a clear 2D structure. For details see Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MAE on QM9. DimeNet sets the state of the art on 11 targets, outperforming the second-best model on average by 31 % (mean std. MAE).</figDesc><table><row><cell>Target</cell><cell cols="2">Unit</cell><cell cols="6">PPGN SchNet PhysNet MEGNet-s Cormorant DimeNet</cell></row><row><cell>µ</cell><cell>D</cell><cell></cell><cell>0.047</cell><cell>0.033</cell><cell>0.0529</cell><cell>0.05</cell><cell>0.13</cell><cell>0.0286</cell></row><row><cell>α</cell><cell>a 0</cell><cell>3</cell><cell>0.131</cell><cell>0.235</cell><cell>0.0615</cell><cell>0.081</cell><cell>0.092</cell><cell>0.0469</cell></row><row><cell>HOMO</cell><cell cols="2">meV</cell><cell>40.3</cell><cell>41</cell><cell>32.9</cell><cell>43</cell><cell>36</cell><cell>27.8</cell></row><row><cell>LUMO</cell><cell cols="2">meV</cell><cell>32.7</cell><cell>34</cell><cell>24.7</cell><cell>44</cell><cell>36</cell><cell>19.7</cell></row><row><cell>∆</cell><cell cols="2">meV</cell><cell>60.0</cell><cell>63</cell><cell>42.5</cell><cell>66</cell><cell>60</cell><cell>34.8</cell></row><row><cell>R 2</cell><cell>a 0</cell><cell>2</cell><cell>0.592</cell><cell>0.073</cell><cell>0.765</cell><cell>0.302</cell><cell>0.673</cell><cell>0.331</cell></row><row><cell>ZPVE</cell><cell cols="2">meV</cell><cell>3.12</cell><cell>1.7</cell><cell>1.39</cell><cell>1.43</cell><cell>1.98</cell><cell>1.29</cell></row><row><cell>U 0</cell><cell cols="2">meV</cell><cell>36.8</cell><cell>14</cell><cell>8.15</cell><cell>12</cell><cell>28</cell><cell>8.02</cell></row><row><cell>U</cell><cell cols="2">meV</cell><cell>36.8</cell><cell>19</cell><cell>8.34</cell><cell>13</cell><cell>-</cell><cell>7.89</cell></row><row><cell>H</cell><cell cols="2">meV</cell><cell>36.3</cell><cell>14</cell><cell>8.42</cell><cell>12</cell><cell>-</cell><cell>8.11</cell></row><row><cell>G</cell><cell cols="2">meV</cell><cell>36.4</cell><cell>14</cell><cell>9.40</cell><cell>12</cell><cell>-</cell><cell>8.98</cell></row><row><cell>c v</cell><cell cols="2">cal mol K</cell><cell>0.055</cell><cell>0.033</cell><cell>0.0280</cell><cell>0.029</cell><cell>0.031</cell><cell>0.0249</cell></row><row><cell cols="2">std. MAE %</cell><cell></cell><cell>1.84</cell><cell>1.76</cell><cell>1.37</cell><cell>1.80</cell><cell>2.14</cell><cell>1.05</cell></row><row><cell>logMAE</cell><cell>-</cell><cell></cell><cell cols="2">−4.64 −5.17</cell><cell>−5.35</cell><cell>−5.17</cell><cell>−4.75</cell><cell>−5.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>MAE on MD17 using 1000 training samples (energies in kcal mol , forces in kcal mol Å ). DimeNet outperforms SchNet by a large margin and performs roughly on par with sGDML.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">sGDML SchNet DimeNet</cell></row><row><cell>Aspirin</cell><cell>Energy Forces</cell><cell>0.19 0.68</cell><cell>0.37 1.35</cell><cell>0.204 0.499</cell></row><row><cell>Benzene</cell><cell>Energy Forces</cell><cell>0.10 0.06</cell><cell>0.08 0.31</cell><cell>0.078 0.187</cell></row><row><cell>Ethanol</cell><cell>Energy Forces</cell><cell>0.07 0.33</cell><cell>0.08 0.39</cell><cell>0.064 0.230</cell></row><row><cell>Malonaldehyde</cell><cell>Energy Forces</cell><cell>0.10 0.41</cell><cell>0.13 0.66</cell><cell>0.104 0.383</cell></row><row><cell>Naphthalene</cell><cell>Energy Forces</cell><cell>0.12 0.11</cell><cell>0.16 0.58</cell><cell>0.122 0.215</cell></row><row><cell>Salicylic acid</cell><cell>Energy Forces</cell><cell>0.12 0.28</cell><cell>0.20 0.85</cell><cell>0.134 0.374</cell></row><row><cell>Toluene</cell><cell>Energy Forces</cell><cell>0.10 0.14</cell><cell>0.12 0.57</cell><cell>0.102 0.216</cell></row><row><cell>Uracil</cell><cell>Energy Forces</cell><cell>0.11 0.24</cell><cell>0.14 0.56</cell><cell>0.115 0.301</cell></row><row><cell>std. MAE (%)</cell><cell>Energy Forces</cell><cell>2.53 1.01</cell><cell>3.32 2.38</cell><cell>2.49 1.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">: Ablation studies using multi-task</cell></row><row><cell cols="3">learning on QM9. All of our contributions</cell></row><row><cell cols="3">have a significant impact on performance.</cell></row><row><cell>Variation</cell><cell>MAE MAE DimeNet</cell><cell>∆logMAE</cell></row><row><cell>Gaussian RBF</cell><cell>110 %</cell><cell>0.10</cell></row><row><cell>N SHBF = 1</cell><cell>126 %</cell><cell>0.11</cell></row><row><cell>Node embeddings</cell><cell>168 %</cell><cell>0.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>MAE on QM9 with multi-target learning. Single-target learning significantly improves performance on all targets. Using a separate output block per target slightly reduces this difference with little impact on training time.</figDesc><table><row><cell>Target</cell><cell cols="2">Unit</cell><cell cols="3">Multi-target Sep. output blocks Single-target</cell></row><row><cell>µ</cell><cell>D</cell><cell></cell><cell>0.0775</cell><cell>0.0815</cell><cell>0.0286</cell></row><row><cell>α</cell><cell>a 0</cell><cell>3</cell><cell>0.0649</cell><cell>0.0616</cell><cell>0.0469</cell></row><row><cell>HOMO</cell><cell cols="2">meV</cell><cell>45.1</cell><cell>45.5</cell><cell>27.8</cell></row><row><cell>LUMO</cell><cell cols="2">meV</cell><cell>41.1</cell><cell>33.9</cell><cell>19.7</cell></row><row><cell>∆</cell><cell cols="2">meV</cell><cell>59.2</cell><cell>63.6</cell><cell>34.8</cell></row><row><cell>R 2</cell><cell>a 0</cell><cell>2</cell><cell>0.345</cell><cell>0.348</cell><cell>0.331</cell></row><row><cell>ZPVE</cell><cell cols="2">meV</cell><cell>2.87</cell><cell>1.44</cell><cell>1.29</cell></row><row><cell>U 0</cell><cell cols="2">meV</cell><cell>12.9</cell><cell>10.6</cell><cell>8.02</cell></row><row><cell>U</cell><cell cols="2">meV</cell><cell>13.0</cell><cell>10.5</cell><cell>7.89</cell></row><row><cell>H</cell><cell cols="2">meV</cell><cell>13.0</cell><cell>10.4</cell><cell>8.11</cell></row><row><cell>G</cell><cell cols="2">meV</cell><cell>13.8</cell><cell>10.8</cell><cell>8.98</cell></row><row><cell>c v</cell><cell cols="2">cal mol K</cell><cell>0.0309</cell><cell>0.0283</cell><cell>0.0249</cell></row><row><cell cols="2">std. MAE %</cell><cell></cell><cell>1.92</cell><cell>1.90</cell><cell>1.05</cell></row><row><cell>logMAE</cell><cell>-</cell><cell></cell><cell>−5.07</cell><cell>−5.21</cell><cell>−5.57</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported by the German Federal Ministry of Education and Research (BMBF), grant no. 01IS18036B, and by the Deutsche Forschungsgemeinschaft (DFG) through the Emmy Noether grant GU 1409/2-1 and the TUM International Graduate School of Science and Engineering (IGSSE), GSC 81. The authors of this work take full responsibilities for its content.</p><p>We found the cutoff c = 5 Å and the learning rate 1 × 10 −3 to be rather important hyperparameters. We optimized the model using AMSGrad <ref type="bibr" target="#b35">(Reddi et al., 2018)</ref> with 32 molecules per mini-batch. We use a linear learning rate warm-up over 3000 steps and an exponential decay with ratio 0.1 every 2 000 000 steps. The model weights for validation and test were obtained using an exponential moving average (EMA) with decay rate 0.999. For MD17 we use the loss function from Eq. 2 with force weight ρ = 100, like previous models . Note that ρ presents a trade-off between energy and force accuracy. It should be chosen rather high since the forces determine the dynamics of the chemical system <ref type="bibr" target="#b40">(Unke &amp; Meuwly, 2019)</ref>. We use early stopping on the validation loss. On QM9 we train for at most 3 000 000 and on MD17 for at most 100 000 steps.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C SUMMARY STATISTICS</head><p>We summarize the results across different targets using the mean standardized MAE</p><p>and the mean standardized logMAE</p><p>with target index m, number of targets M = 12, dataset size N , ground truth valuest (m) , model f (m) θ , inputs X i and z i , and standard deviation σ m oft <ref type="bibr">(m)</ref> . Std. MAE reflects the average error compared to the standard deviation of each target. Since this error is dominated by a few difficult targets (e.g. HOMO ) we also report logMAE, which reflects every relative improvement equally but is sensitive to outliers, such as SchNet's result on R 2 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cormorant: Covariant Molecular Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Truong-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondor</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gaussian Approximation Potentials: The Accuracy of Quantum Mechanics, without the Electrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">P</forename><surname>Bartók</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Csányi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">136403</biblScope>
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On representing chemical environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">P</forename><surname>Bartók</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Csányi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review B</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">184115</biblScope>
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Machine learning unifies the modeling of materials and molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">P</forename><surname>Bartók</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandip</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Poelking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Kermode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Csányi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Ceriotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1701816</biblScope>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Neural Device for Searching Direct Correlations between Structures and Properties of Chemical Compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><forename type="middle">I</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">A</forename><surname>Palyulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><forename type="middle">S</forename><surname>Zefirov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="715" to="721" />
			<date type="published" when="1997-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Behler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Parrinello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">146401</biblScope>
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weike</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxing</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyue Ping</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemistry of Materials</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3564" to="3572" />
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised Community Detection with Line Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Robert</forename><surname>Calderbank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine learning of accurate energy-conserving molecular force fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huziel</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Poltavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1603015</biblScope>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards exact molecular dynamics simulations with machine-learned force fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Huziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Group Equivariant Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gauge Equivariant Convolutional Networks and the Icosahedral CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkay</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Steerable CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spherical CNNs. In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Machine learning prediction errors better than DFT accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Anatole Von Lilienfeld</surname></persName>
		</author>
		<idno>1702.05532</idno>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2005-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><forename type="middle">F</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schroeter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
	<note>Introduction to Quantum Mechanics. 3 edition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting molecular properties with covariant compositional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Truong Son Hy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">241745</biblScope>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative Models for Graph-Based Protein Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural Message Passing with Edge Updates for Predicting Properties of Molecules and Materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">Wedel</forename><surname>Peter Bjørn Jørgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikkel</forename><forename type="middle">N</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
		<idno>1806.03146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks Meet Personalized PageRank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Leach</surname></persName>
		</author>
		<title level="m">Molecular Modelling: Principles and Applications</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Provably Powerful Graph Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Invariante Variationsprobleme. Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmy</forename><surname>Noether</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematisch-Physikalische Klasse</title>
		<imprint>
			<biblScope unit="page" from="235" to="257" />
			<date type="published" when="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simultaneous fitting of a potential-energy surface and its corresponding force fields using feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pukrittayakamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malshe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Raff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Narulkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bukkapatnum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Komanduri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">134101</biblScope>
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Searching for Activation Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anatole Von Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Equivariance Through Parameter-Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the Convergence of Adam and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SchNet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huziel Enoc Sauceda</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Supervised neural networks for the classification of structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="714" to="735" />
			<date type="published" when="1997-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Tensor Field Networks: Rotation-and Translation-Equivariant Neural Networks for 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno>1802.08219</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">PhysNet: A Neural Network for Predicting Energies, Forces, Dipole Moments, and Partial Charges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meuwly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Theory and Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3678" to="3693" />
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">MoleculeNet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneesh</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Understanding belief propagation and its generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Exploring artificial intelligence in the new millennium</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="236" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with relational inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vinícius Flores Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
