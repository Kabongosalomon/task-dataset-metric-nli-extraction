<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UPANets: Learning from the Universal Pixel Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hsun</forename><surname>Tseng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Manchester</orgName>
								<address>
									<settlement>Manchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin-Jye</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Management of Technology</orgName>
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<settlement>Hinschu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Nan</forename><surname>Feng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<settlement>Kunming</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengzhong</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Manchester</orgName>
								<address>
									<settlement>Manchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ping</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Manchester</orgName>
								<address>
									<settlement>Manchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Yu</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Manchester</orgName>
								<address>
									<settlement>Manchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mou-Chung</forename><surname>Tseng</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">College of Management</orgName>
								<orgName type="institution">National Taipei University of Technology</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Manchester</orgName>
								<address>
									<settlement>Manchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UPANets: Learning from the Universal Pixel Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Corresponding author: Shin-Jye Lee</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>INDEX TERMS Computer vision</term>
					<term>Attention</term>
					<term>Image classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Among image classification, skip and densely-connection-based networks have dominated most leaderboards. Recently, from the successful development of multi-head attention in natural language processing, it is sure that now is a time of either using a Transformer-like model or hybrid CNNs with attention. However, the former need a tremendous resource to train, and the latter is in the perfect balance in this direction. In this work, to make CNNs handle global and local information, we proposed UPANets, which equips channel-wise attention with a hybrid skip-densely-connection structure. Also, the extreme-connection structure makes UPANets robust with a smoother loss landscape. In experiments, UPANets surpassed most well-known and widely-used SOTAs with an accuracy of 96.47% in Cifar-10, 80.29% in Cifar-100, and 67.67% in Tiny Imagenet. Most importantly, these performances have high parameters efficiency and only trained in one customer-based GPU. We share implementing code of UPANets in https://github.com/hanktseng131415go/UPANets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The field of Computer vision has experienced a range of trends in a decade. Except for fundamental machine learning methods <ref type="bibr" target="#b0">[1]</ref> and deep fully-connected convolutional neural networks <ref type="bibr" target="#b1">[2]</ref>, the introducing models of <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b5">[6]</ref> [7] in Imagnet competition has boomed the image classification. A variety of CNN-based model with residual, also known as skip-connection, networks <ref type="bibr">[8]</ref><ref type="bibr">[9]</ref><ref type="bibr">[10]</ref><ref type="bibr">[11]</ref><ref type="bibr">[12]</ref><ref type="bibr">[13]</ref><ref type="bibr">[14]</ref><ref type="bibr">[15]</ref> has conquered Cifar-10, Cifar-100, and Imagenet. Although some discussions and works, such as <ref type="bibr">[17]</ref>, mentioned convolutional layer could capture local characteristic and global profile if CNNs were in deep structure, the authors of <ref type="bibr">[18]</ref> have argued the duty to capture global pattern is contributed with an attention mechanism. Also, because <ref type="bibr">[18]</ref> has opened a path of applying pure multi-head attention from Transformer to image classification, some works, such as <ref type="bibr">[19,</ref><ref type="bibr">20]</ref>, started to apply pure attention in computer vision. Not only toward computer vision, [21] utilized a sparse attention mechanism to make time-series forecasting more efficient. Therefore, the usage of attention does popularize in many categories nowadays. However, we have also noticed that most attention-based methods need powerful GPUs with large exclusive CUDA memory because generating the query, key, and value needs at least three times more resource than simply using one multi-layer perceptron. If we are facing computer vision with high resolution and many channels, the needed resource is unprecedented. In this regard, we want to endow the already excellent and efficient CNN-based networks to capture global information with learnable parameter and lesser resource than multi-head attention, so we proposed Channel-wise Pixel Attention CPA to make global pattern learning as <ref type="figure" target="#fig_0">Figure 1</ref>. Also, as residual neural networks have shined in image classification, denselyconnection CNNs <ref type="bibr" target="#b6">[22]</ref> also occupy the aforementioned wellknown image datasets leaderboard. With the observation in <ref type="bibr">[16]</ref>, we improve performance by proposing another hybrid skip-densely-connection structure similar to dual-path networks <ref type="bibr" target="#b7">[23]</ref>. By integrating proposed methods into a networks, our UPANets can additionally process universal pixels with CNNs and CPA, reuse feature maps by denselyconnection, residual learning with skip-connection, and create a smooth learning landscape toward spatial pixel attention with extreme connection.</p><p>We first discuss an essential background and current trend toward image classification with merits and flaws in I. INTRODUCTION in this work. The contributions which have been brought by proposed methods are also listed in here. Then, in II. RELTEDWORK, the well-known and vital observation toward image classification and this work were mentioned with a critical analysis. Then a range of the proposed methods and the structure about UPANets were in III. UPANets. Moreover, comparing performance in terms of every proposed method in well-known datasets can be seen in IV. EXPERIMENT and V. CONCLUSION. Lastly, extra findings about UPANets experiments were in Appendix. The contributions from this work are:</p><p>• Channel pixel attention, which helps form complex features even in shallow depth with fewer parameters. • Spatial pixel attention, which helps to learn spatial information. • Hybrid skip-densely connection, which makes CNNs reuse feature with a deep structure. • Extreme connection, which can generate a smooth loss landscape. • A competitive image classification model surpassed well-known, also widely-used SOTAs in Cifar-10, Cifar-100, and Tiny Imagenet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELTEDWORK</head><p>Since the introducing of skip connection of ResNets <ref type="bibr" target="#b5">[6]</ref>, we have witnessed a surge in computer vision toward creating a smooth loss landscape. The skip connection has offered a great path to let deep learning fulfil the true meaning of dee. Most importantly, it prevents overfitting. The visualization of loss landscape <ref type="bibr">[16]</ref> has proven one of the reasons that why simply applying skip connection can boost accuracy. Also, DenseNets <ref type="bibr" target="#b6">[22]</ref> has shown another method to connect original and outputting information.</p><p>[16] also has shown that using densely-connection makes the loss landscape smoother than ResNets. Following that, dual path networks <ref type="bibr" target="#b7">[23]</ref> combining the merit of adding residual as ResNets and the inheriting input information as DenseNets. Not only that, the Deep layer aggregation model <ref type="bibr" target="#b8">[24]</ref> similarly used dense connectivity to build a tree-based structure toward fusing images and image detection. Among the development of creating a smooth loss landscape, SAM <ref type="bibr" target="#b9">[25]</ref> shows that dividing every gradient parameter with L2-norm to update will create a smooth path to possible optimum. Then, SAM restores the updated grad in the first step so the model can learn how to follow the same path to avoid harsh landscapes. Finally, the parameters were updated by the original gradient in the second step. With this operation, SAM has made a series of either residual networks or densely connective networks, such as EffNet-L2 <ref type="bibr" target="#b10">[26]</ref> and PyramidNet <ref type="bibr" target="#b11">[27]</ref>, to gain the state-of-the-art performance in Imagenet, Cifar-10, and Cifar-100 classification benchmark.</p><p>Utilizing the attention mechanism in computer vision is also a norm. We have observed CBAM <ref type="bibr" target="#b12">[28]</ref> used max pooling and average pooling to let convolutions capture different angles information to apply the pooling method. Among utilizing average pooling, SENets [7] used global average pooling to squeeze the spatial information into one value, and then it uses a simple multi-layer perceptron with a ReLU and another MLP layer with a Softmax to make channel attention. By embedding characteristic of SENet, the work showed an improvement toward embedding a SE-block after a convolutional layer in VGG <ref type="bibr" target="#b2">[3]</ref>, Inception Net <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, and ResNeXt <ref type="bibr" target="#b13">[29]</ref>. After, EfficientNet <ref type="bibr" target="#b10">[26]</ref> proposed a general formula to help build a decent CNN-based structure and utilized similar SENets but with Swish <ref type="bibr" target="#b14">[30]</ref> to obtain the stateof-the-art performance in that time. On the other hand, natural language processing has also seen a successful development with attention, especially the introduction of Transformer in <ref type="bibr" target="#b15">[31]</ref>. Furthermore, ViT [18] arbitrary used the same multihead attention in the Transformer to classify the Imagenet-1k picture. The same notion can also be seen in DeiT-B <ref type="bibr" target="#b16">[32]</ref>, which used attention to transfer the pre-trained parameter on image classification. In the work of BiT [8], we also can see that transferring parameters from a massive model has been another trend either in computer vision or natural language processing.</p><p>Except for EfficientNet and PyramidNet in finding a general convolutional structure formula, Wide ResNet <ref type="bibr" target="#b17">[33]</ref> has revealed that expanding the width of a CNN layer can offer an efficient performance with increasing performance. Comparing different combinations of kernel size in two or three layers in a block, two layers give a robust performance in their experiments. Also, the order of stacking a batch normalisation, activation function, and convolution is a vital element in CNNs. PreAct ResNet <ref type="bibr" target="#b18">[34]</ref> has proven to place batch normalisation and activation before the convolution can perform relatively well in most cases. Additionally, applying a bottleneck block is a popular method in big CNNs. Res2net <ref type="bibr" target="#b19">[35]</ref> has proposed a different type of bottleneck to boost object detection performance. With the bottleneck structure in CNNs, the image model can reduce the parameters and maintain a deep structure. Sharing with the same notion, ShuffleNets <ref type="bibr" target="#b20">[36]</ref> and Shufflenets v2 <ref type="bibr" target="#b22">[37]</ref> used a channel shuffle operation after grouping convolutional layers to keep the same performance as the original CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Critical analysis</head><p>By ResNets and DenseNets, skip and densely-connection play significant roles in building deep structure in the field of computer vision. Attention mechanism has also been a trend. However, applying multi-head attention as ViT is inefficient to make attention global. The combination of kernels in CNNs is also a vital aspect. Learning from the Wide ResNets, wide CNNs can benefit more, so we designed a similar structure as the basic block in ResNets but in a wide version. Lastly, we are surprised by how efficiencies were ShuffleNets v1 and v2 used relative fewer parameters than ResNets, but they still maintained the performance as much as possible. Nonetheless, as the shuffle operation might mess up the memory location in the process of back-propagation, the saving time in computation was offset by grouping CNNs and re-building corresponding gradient direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. UPANets</head><p>In this section, the proposed methods are listed. The attention methods for channel-wise and pixel-wise are revealed, firstly. Then, the UPA block is shown after the attention. Combining the skip and densely-connection in the UPA block, an explanation of UPA layers shows how they work together in UPANets. The structure of UPANets is shown after the proposed methods of extreme connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel pixel attention</head><p>A convolutional kernel is good at capturing local information with learning weight in a kernel. Although a convolutional neural network can form a complex pattern by stacking deep enough layers, so it makes lower hidden layer process local information and deeper hidden layer capture global patterns, the process is not direct. Nonetheless, applying a network to learn the essential pixels from channel to channel in width might bring a positive effect and help CNNs consider global information directly. Therefore, we propose channel pixel attention, CPA, which applies a one-layer multi-layer perceptron (MLPs) to pay attention to the pixel in the same position across channels. The method can be presented as:</p><formula xml:id="formula_0">= # ! " ! # + $ !%&amp;<label>(1)</label></formula><p>where indicates the channel th , ∈ ℝ '×)×*×" , ! " ∈ ℝ '×*×+×) , which is reshaped to do a dot product with ! # .  Among the CPA samples in <ref type="figure" target="#fig_0">Figure 1</ref>, the outputted feature maps from CPA are combining the original feature itself and helpful information from others. These combining feature show CPA can help a feature map fuse a more complex feature map without losing original features. Compared with deep structure, CPA helps a shallow network form complex pattern easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial pixel attention</head><p>Global average pooling is widely applied in the image classification model. We agree that applied global average pooling before a final hidden layer can easily help the model learn which channel is vital for accuracy by weighing the representative value of a feature map. Most importantly, this operation does not require extra computational resource. However, we are wondering whether a learnable global pooling method could improve performance. To improve accuracy by important information in the spatial direction, we propose spatial pixel attention, SPA, which uses a onelayer perceptron. The method can be defined as the following formula:</p><formula xml:id="formula_1">= # ! " ! # + $ !%&amp;<label>(2)</label></formula><p>where indicates the channel th , ∈ ℝ '×)×&amp; , ! " ∈ ℝ '×)×, , = × , and ! # ∈ ℝ '×,×&amp; . <ref type="figure">Figure 3</ref>. Spatial pixel attention. To demonstrate, we take a × feature map, in (a), with = as an example. Then, the process from (a) to (b) is reshaping the convolutional image. The (b) to (c) is applying spatial pixel attention, which is the same notion as the global average pooling.</p><p>In <ref type="figure">Figure 3</ref>, the process from (b) to (c) is implemented by a fully-connected neural network with a bias. By weighting a learnable matrix, SPA can decide to pay how much attention to essential pixels and then squeeze the whole pixel into one pixel by doing dot product instead of arbitrary pooling with average. In classifying Cifar-10 and Cifar-100, with 32 × 32 dimension per image, the maximum adding parameters is 1024 with no bias per feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverted triangular shape CNN layer with 3x3 kernels</head><p>Growing width in convolution is another helpful direction to improve performance. Also, the combination of two 3 × 3 convolutions is experimentally robust in most image classification. In UPANets, every first layer of CNN uses twice times channels of 3 × 3 kernel than the one. Thus, this shape can be viewed as an inverted triangle shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UPA blocks</head><p>UPA blocks follow the findings in Wide ResNet which indicated the combination of two 3 × 3 convolutional layers could offer the most robust accuracy. The order of the convolution, batch normalization, and activation function follows the typical structure of CNNs. Meanwhile, CPA is applied parallelly, so the CPA input is the same as the CNN. Then, both outputs are simply added with layer-normalized afterwards. The structure can be seen in <ref type="figure" target="#fig_3">Figure 4</ref>. From <ref type="figure" target="#fig_3">Figure 4</ref>, the differences between the stride one and stride two versions are applying to concatenate operation or not. The operation is densely connectivity. On the other hand, the residual connection is used in CPA to determine whether to output the current learned information or the information from the last block. Lastly, a 2 × 2 kernel average pooling is applied to down-sample; please referring <ref type="figure" target="#fig_2">Figure 2</ref>. By <ref type="figure" target="#fig_3">Figure 4</ref>, CPA can be embedded every CNNs-based models as SENets <ref type="bibr">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UPA layers</head><p>In DenseNets, reusing features has been proved with a series of benefit, including reducing parameters, speeding up the computing process, and forming complex feature maps. This work uses densely-connection, but we modified it into a different UPA block structure, as <ref type="figure" target="#fig_4">Figure 5</ref>. The root information is preserved by the concatenating process until the last stride one UPA block. In the stride two UPA block, applying a 2 × 2 average pool means no stride two convolutions to down-sample. Except for the stride two operations in block 0 in every layer, each block follows the stride one operation. Nonetheless, the width of every stride one block is smaller than its input shape that can be referred to as the following equation:</p><formula xml:id="formula_2">-= . /<label>(3)</label></formula><p>where = 1 ⋯ , . indicates the summation of adding width of this layer, -indicates the output width of this block, and / equals to two times width of the last layer because the original input is remained and the processed information is appended after that. For example, if the width of the layer 1 is set to 16, the outputted width of the layer 1 would be 32 because of densely-connection. Therefore, the block 0 width in the layer 2 is 32, / = 32. Then, when the number of blocks in layer 2 is 4, = 4, the width in every block is 8, -= 8 because / = 32 and 01 2 = 8. In this case, the outputted width from this block of this layer will be 40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extreme connectivity</head><p>Applying skip connection in a deep neural network has been a norm since ResNet introduced. Further, the dense connectivity in DenseNets has shown a different but more efficient way than before to connect the dense information. From the landscape of using skip connection, the surface is smoother, and thus this landscape raises the chance to reach a better optimum with a lower risk in overfitting. Based on this observation, to create an even smoother loss landscape, we introduce extreme connection; we will use exc in the following discussion across the whole model. It is only applied between each block and the last hidden layer. <ref type="figure">Figure  6</ref> eveals applied exc with SPA and global average pooling, GAP. This operation can be represented as the following:</p><formula xml:id="formula_3">= [ &amp; ( &amp; " ), 1 ( 1 " ), ⋯ , -( - " )]<label>(4)</label></formula><p>where ∈ ℝ '×) , which is the output from the flattenconcatenate . is the data number and represents the number of channels. Also, means the block th in a network. Different from the common image neural networks, which apply global average pooling before the final fully connected layer, we add the operation which combines SPA with GAP, as In <ref type="figure">Figure 6</ref>, exc builds the relationship from the final hidden layer to the output of each block. GAP servers the place of determining which convolution plays a vital role toward the label. SPA determines which pixel should be paid more attention to the class. By combining both operations with a layer normalization, both side information can be scaled to the same level to learn. <ref type="table">Table 1</ref>, referring to the narrative in UPA layers the detail transferring of size, width, and the proposing attention in the Cifar-10, is presented. The proposed CPA is applied in each UPA block. Also, exc is used in every UPA layer with the proposed SPA and GPA. <ref type="table">Table 1</ref>. The UPANets structure for the Cifar-10. represents the data number, indicates the filters number, are blocks, means the depth multiplier, is the number of the block, and is the convolutional width. UPA Block 0 and the others Blocks follow the stride 2 and stride 1 UPA block, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UPANets structure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment environment and setting</head><p>We implemented UPANets compared with CNN-based SOTAs for comparison. Although we do not reveal the costing time, it is better to unveil the experiment environment in a customer-based GPU, RTX Titan with 24GB, and an eight-core CPU, intel i9-9900KF, with 32GB RAM. As the limitation of the hardware, we mainly compared UPANets and others in Cifar-10, Cifar-100, and tiny Imagenet datasets. Every training process was implemented in a cosine annealing learning schedule with a half cycle. Similarly, every training optimizer was stochastic gradient descent with an initial learning rate of 0.1, momentum 0.9, and weight decay 0.0005. A simple combination of data argumentation was applied with random crop in padding 4, random horizontal flip, normalization, and input shape in Cifar and input shape in tiny Imagenet, respectively. As we conducted a series of experiments with different epochs, the specific used epochs number is revealed before in each sub-section experiment comparison. Lastly, the batch size was set to 100 in every training processes.</p><p>On the other hand, we used efficiency to examine the turnover rate between the parameters and accuracy throughout our experiments. Although the most crucial index is still the accuracy, also known as a top-1 error, we still hope the efficiency of the parameter should be considered during comparing models. The efficiency can be revealed as the following simple equation:</p><formula xml:id="formula_4">= /<label>(5)</label></formula><p>where represents the efficiency, means the size of used parameters, and is the abbreviation of the accuracy. By this equation, we can learn whether this structure or setting could convert the parameters into performance efficiently. The meaning of the equation can also be understood as the ratio of accuracy and parameters. For example, if a 100% accuracy is brought by two parameters, = 0.5. Also, if another 100% accuracy is contributed by four parameters, = 0.25 . By these two examples, the 0.5 is greater than 0.25 with the meaning of higher efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance exploring in UPANets</head><p>In this sub-section, we implemented a series of performan comparisons toward different components among UPANets. The performance of UPANets with = 16 in Cifar-10 and Cifar-100 are revealed in the following comparisons, please see the meaning of in <ref type="table">Table 1</ref>. Each performance was recorded in testing stage with the highest accuracy. The total epochs number in this sub-section was set to 100, and the experiment setting was also following the aforementioned experiment description in Experiment environment and setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">LEARNABLE EX-CONNECTION</head><p>In the sub-section of Extreme connectivity, one of the reasons for ushering the connection is creating a smooth loss landscape to raise the chance to reach an optimum. Another reason is connecting a shallow layer with the final layer, and thus the model can be deep without facing overfitting. In the following table, we implemented UPANets16 in a series of variants. The variants were different in the connection structure. UPANets16 final GAP owns the typical CNNbased structure, which is only equipped with a GAP layer before the output layer. UPANets16 final SPA used SPA to replace the only GAP layer in typical CNNs. UPANets16 exc GAP follows the proposed exc structure with GAP layers. UPANets16 exc SPA shares the same structure as UPANets16 exc GAP but applied SPA layers instead. Lastly, UPANets16 (exc SPA &amp; GAP) used layer normalizations to combine SPA and GAP layers with exc structure. The performance and efficiency of forenamed models are listed in <ref type="table" target="#tab_0">Table 2</ref>.  <ref type="table" target="#tab_0">Table 2</ref>, comparing the performances between UPANets16 final GAP and UPANets16 final SPA, shows that a learnable global average pooling by applying a fully-connected layer can improve the performance either in Cifar-10 and Cifar-100. The same trend is shown in the aspect of efficiency. However, when we ushered exc into UPANets16, UPANets16 exc GAP outperformed UPANets 16 exc SPA with better efficiency. As a result, we tried to apply layer normalization to combine both operations and then witnessed an improvement in Cifar-10 and Cifar-100. Also, efficiency became better. The evidence reveals that either GAP or SPA offers a specific contribution to improvement. The GAP can help to decide which combination of the channels is essential. Moreover, the combination of the pixels is essential among SPA. By combining both operations can supplement each other. The performance comparison toward whether using a fully-connected layer of CNN layer in SPA can be seen in A. CNN &amp; Fullyconnected layer comparison in the Appendix.</p><p>We compared performance toward the accuracy, but we also followed the method in [16] with a slight modification to visualize different loss landscape in the same scale toward the loss of classifying Cifar-10. We used min-max scaling to convert different loss range into [0:1], which can be seen in <ref type="figure" target="#fig_6">Figure 8</ref>. Also, the top-1 error landscape is shown in <ref type="figure" target="#fig_7">Figure  9</ref>. As [16] explained, the landscape can only be regarded as the possible landscape for the visualization because it is produced by random sampling in a visual dimension. Regarding using min-max scaling for the loss landscape, an in-depth discussion is explained in C. Landscape toward UPANets and Others among Appendix.  The loss landscape in <ref type="figure" target="#fig_6">Figure 8</ref> and the top-1 error map in <ref type="figure" target="#fig_7">Figure 9</ref> illustrate that applying extreme connection did make the landscape smooth, so the chance of reaching minimum and preventing overfitting is rising. The difference between the original and normalized landscape becomes evident in the top-1 error landscape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FUSION OF CHANNEL PIXEL ATTENTION</head><p>Based on the description of Channel pixel attention, we expect that this operation can help CNNs to consider global information as the widely used multi-head attention in the Transformer <ref type="bibr" target="#b15">[31]</ref> but only needs one-third of parameters in attention by only using one fully-connected layer to do a weighted sum, instead of creating a query, key, and the value for attention. <ref type="figure" target="#fig_8">By Figure 7</ref>, we sampled the first 32nd feature maps from the convolution and the CPA layer of UPA Block 0 in UPA layer 2. The outputted feature maps are the information before using add and layer normalization, so the respective scale and output are remaining origin. We can see that the output of the CNN only detected a specific pattern toward the kernel. Also, some kernels only detected background information. Further, if the kernel could not detect a feature, a feature map remained dim. On the side of CPA outputs, every feature map covered the learned information from the others. Instead of simply extracting whole feature maps, each pixel considered the same position pixel from the others by learnable weights. Thus, the CPA can decide which pixel helps consider and vice versa. Before applying layer normalization, the samples of Conv + CPA own the detected pattern from the convolutional layer, local information, and concludes the global feature from other feature maps. The in-depth exploration of learned pattern in CNN and CPA can be seen in D. Samples Pattern of the CNN and CPA in UPA block of Appendix. In the bellowing <ref type="table" target="#tab_1">Table 3</ref>, the improvement, which CPA brought, is discussed. In <ref type="table" target="#tab_1">Table 3</ref>, UPANets16 w/o CPA reveals an obvious decease in both datasets so that CPA can boost the classification performance. On the other part, we also implemented a series of comparison among applying CPA and shuffle operation in ShuffleNets v1 and v2, as we realize CPA can offer the same effect of connecting independent CNNs. In that case, we want to validate whether the CPA can also maintain the same performance with fewer parameters. We placed the shuffle operation in the same place as ShuffleNets, which means there is a shuffle between two CNN layers with the first CNN in groups. In this experiment, CPA offered a better performance compared with shuffled UPANets. As the number of groups escalating, the performance difference between CPA and shuffle increases. While we agree that shuffle operation has very efficient parameters utilization, CPA can offer better performance with a minor resource trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results comparison with SOTAs</head><p>UPANets was not only implemented in F=16, 32, and 64, a series of SOTAs were also reimplemented for comparison in Cifar-10 and Cifar100. The structure of reimplemented SOTAs followed the work in the link 1 . Every model was trained in 200 epochs and followed the experiment setting in Experiment environment and setting.</p><p>1 https://github.com/kuangliu/pytorch-cifar</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">CIFAR-10</head><p>In this comparison, the performance of each model was recorded in accuracy toward testing data, parameters size in million and efficiency, as equation <ref type="bibr" target="#b4">(5)</ref>. Because there are three performance indexes in <ref type="table" target="#tab_2">Table 4</ref>, we presented the information in a scatter plot as <ref type="figure" target="#fig_0">Figure 10</ref>, which contains accuracy in the y-axis and efficiency in the x-axis. The size of the circle toward each model represents a relative parameter size in a million compared with others. Besides, the specific used value for plotting and comparing can be seen in <ref type="table" target="#tab_2">Table 4</ref>.  From <ref type="figure" target="#fig_0">Figure 10</ref> and <ref type="table" target="#tab_2">Table 4</ref>, UPANets64 has the best accuracy. What is more, UPANets have an outstanding performance in balancing efficiency and accuracy in the scatter plot. We also observed that models claimed in the lite structure are located in the bottom right area, but they lost certain accuracy. Nonetheless, UPANets16 and DenseNets located in the upper right corner, indicating our proposed model and DenseNets have similar high efficiency. In terms of only viewing accuracy, UPANets64 is the only model reaching over 96% accuracy without needing too many parameters, especially compared with ResNets101 and DenseNets201.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CIFAR-100</head><p>We applied the same experimental setting with 1. CIFAR-10 in this Cifar-100 comparison. Similarly, please observe the result in <ref type="figure" target="#fig_0">Figure 11</ref>, corresponding with values in <ref type="table" target="#tab_3">Table  5</ref>.  By <ref type="figure" target="#fig_0">Figure 11</ref> and <ref type="table" target="#tab_3">Table 5</ref>, UPANets64 also has the most excellent classification performance. Also, UPAnets variants had a decent performance as they surpassed most of SOTAs. The overall performance pattern is similar to <ref type="figure" target="#fig_0">Figure 10</ref>. So, we believe our UPANets has a competitive performance among classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TINY IMAGENET</head><p>Although we compare a series of SOTAs with UPANets in Cifar-10 and Cifar100, the difficulty of datasets is relatively small comparing with Tiny Imagenet as it needs to classify two times more labels. Besides, the image size is also two times larger than Cifar-series datasets, so we only test UPANets64 in 100 epochs with the same experiment setting as comparison above. We compared with some SOTAs who also were tested on Tiny Imagenet in their works under below: Although it is still rare for comparing classification in Tiny Imagenet, we can know UPANets has not only excellent capability in simple datasets but also great ability in complex datasets like Tiny Imagenet. Our UPANets performance could be one of the state-of-the-art models in the Tiny Imagenet benchmark. Especially, a model which was trained end-to-end in a machine equipped with a customer-based GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We proposed a new pixel-attention operation, CPA, which can capture global information and offer the same effect of Shuffle Nets with shallow depth and better accuracy. By ushering learnable global average pooling, SPA, and extreme connection, the smooth loss landscape can raise the chance of reaching minima. Integrating proposed methods into UPANets and comparing with a series of SOTAs in Cifar10, Cifar-100, and Tiny Imagenet, UPANets surpassed most SOTAs and can offer competitive performance in image classification. These evidence shows that learning universal pixels with proposed attention methods can profoundly improve computer vision ability.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Width in UPANets</head><p>From <ref type="table" target="#tab_7">Table 8</ref>, the effect of width did bring positive performance, especially in a more difficult task as Cifar-100, though the efficiency decreased as the width going wider. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Landscape toward UPANets and Others</head><p>The introducing of the visualizing loss landscape method in [16] helps researchers understand the possible training landscape among the parameters of a model. By the description of the actual implementing source code <ref type="bibr" target="#b7">23</ref> , the primary usage is setting a random sampling range from -1 to 1 with a specific sampling number, and the default number is 50. However, using this strategy, as this sampling method is similar to the sensitivity analysis in determining feature importance, only proper sampling can produce a calculatable loss. This dilemma becomes even worse when we try to visualize a sensitive model, such as DenseNets, because a little adding noise might cause the loss to Nan. Therefore, how to define a good sampling range is a challenge. On the other hand, although the filter normalization has been introduced in [16] for comparing loss landscapes from different models, we found that different range of loss is still hardly comparing with others. An enormous total range of a loss will make most landscape smother because an outlier will break the harmony of the loss map. We used a grid search for finding a visualizable range carefully without modifying the original visualization method to address the previous barriers. On the ground of making two landscape comparable, we also used min-max scaling for every loss landscape. A series of before and after scaled landscapes are shown in the following figures. For demonstrating, we endto-end trained a DenseNets and our models for Cifar-10 version based on the code in this project 4 and applied the method mentioned above in <ref type="figure" target="#fig_0">Figure 12</ref> and following comparisons. What the visualizable sample range was [−0.0375: 0.0375] with 50 samples. The largest loss broke the harmony of the original loss landscape on the left. The relative more minor loss owns the majority number, but it is hard to see the fluctuation of the landscape from the relative more minor loss because of the outlier. Therefore, we only see a flatten space on the left. Min-max scaled loss landscape shows a much different view on the right. Although the centre of the map is still flat, the surrounding loss stands erect on edge. Not only the scaled landscape can reveal a much reasonable profile, but scaling can also make different landscapes comparable. However, apart from the sampling range of DenseNets, the sample range among each UPANets variants was the same default range in <ref type="bibr">[16]</ref>, which is [−1: 1]. We offered UPANets16 loss and error landscapes, which are with and without scaled in the range [−0.0375: 0.0375] in <ref type="figure" target="#fig_0">Figure 13</ref>. Please compare the original loss landscape in UPANets16 final GAP and UPANets16 in <ref type="figure" target="#fig_6">Figure 8</ref> and Apart from the loss landscape in UPANets16 final GAP and UPANets16, the loss landscape of the remaining models in  By observing the scale bar on the right side of each plot, the ranges are different from landscape to landscape. Nonetheless, the min-max scaling makes every landscape comparable to the same level. From this series of scaled landscapes, we can further make sure that extreme connectivity offers a smother landscape compared with the landscapes of UPANets16 final GAP and SPA.  <ref type="figure" target="#fig_2">Figure 20</ref>, to compare in the same environment, contains the error landscape in the same range as <ref type="figure" target="#fig_0">Figure 12</ref> and <ref type="figure" target="#fig_0">Figure  13</ref>. We can observe that UPANets16 has the same smooth landscape as DenseNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Samples Pattern of the CNN and CPA in UPA block</head><p>Following the same method in 2. fusion of channel pixel attention, we sampled the feature maps with random noise, which follows the standard normal distribution. Thus, we can observe the actual convolution patterns and the forming complex CPA patterns in <ref type="figure" target="#fig_0">Figure 19</ref>. Without losing global information, the combination of convolution and CPA outputs also own detected local information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 19.</head><p>Samples of fusion feature maps in UPANets with using noise input.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Channel pixel attention process and samples. The image on the top is an original sampled image from Cifar-10. The feature maps in the middle line are the outputs from the CNNs before CPA. On the bottom line are the samples from CPA. The red square is the weighted pixel sum from each orange square pixel in the same position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>!#</head><label></label><figDesc>∈ ℝ '×)×) . After the pixel attention by one-layer MLP, batch normalization and layer normalization with residual connection are applied. The workflow of the CPA can be demonstrated in Figure 2. Moreover, the sample feature maps with demonstration are in Figure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Channel pixel attention structure in stride one and stride two sets. In the orange region, CPA can make channel-wise pixel attention and downsample image by avgpool2d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>UPA blocks structure in the stride one and stride two sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>UPA layers with densely-connection. In the UPA block 0, a stride two UPA block uses the residual connection with × kernel average pooling is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 6 .</head><label>66</label><figDesc>Extreme connection structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Normalizing loss landscape between UPANets16 final GAP and UPANets16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Top-1 error landscape toward UPANets16 final GAP and UPANets16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Samples of fusion feature maps in UPANets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Scatter plot of UPANets performance with SOTAs in Cifar-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Scatter plot of UPANets performance with SOTAs in Cifar-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>The loss landscape of un-scaled, left, and the scaled, right, of DenseNets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 .Figure 14 .</head><label>1314</label><figDesc>The UPANets16 loss landscape in the range The original loss landscape of UPANets16 final GAP and UPANets16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>2 Figure 15 .Figure 16 .Figure 17 .</head><label>2151617</label><figDesc>are shown in the following figures. Then come with the figures of top-1 error landscape in Figure 18. Original Scaled The original and scaled loss landscape of UPANets16 final SPA. Original Scaled The original and scaled loss landscape of UPANets16 GAP. Original Scaled The original and scaled loss landscape of UPANets16 SPA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 18 .Figure 20 .Figure 18 and</head><label>182018</label><figDesc>The top-1 error landscape of UPA 16 final SPA, UPA16 GAP, and UPA16 SPA.UPANets16 DenseNets The top-1 error landscape of Cifar-10 version UPANets16 and DenseNets in the range [− . : . ]. Figure 20 show many different trends. No matter what version of UPANets16 variants in Figure 18, the top-1 error maps still present in a deep pattern. In contrast, the top-1 error map in UPANets16 and DenseNets show a smooth pattern, which is consistent with the observation in [16] and might be contributed by the dense connectivity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>The performance comparison table among UPANets16 variants in Cifar-10 and Cifar-100.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>The performance comparison table among UPA16 CPA</figDesc><table /><note>variants toward Cifar-10 and Cifar-100.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The table of UPANets performance with SOTAs in Cifar-10.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Table of UPANets performance with SOTAs in Cifar-100.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Table of UPANets performance with SOTAs in Tiny Imagenet.</figDesc><table><row><cell>Model</cell><cell>Test Avg</cell><cell>Size (M)</cell><cell>Efficiency</cell></row><row><cell></cell><cell>Accuracy ↑</cell><cell></cell><cell></cell></row><row><cell>DenseNets +</cell><cell>60.00</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Residual Networks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[38]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PreActResNets18</cell><cell>63.48</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>[39]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UPANets64</cell><cell>67.67</cell><cell>24.40</cell><cell>2.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 ,</head><label>7</label><figDesc>UPANets16 (CNN) applied CNNs to replace all fully-connected layers in both CPA and SPA of UPANets16. While CNN can share weight in the spatial dimension, the benefit brought a side effect on performance. Similarly, the efficiency did not be dimmed by the extra parameters in the FC-UPANets16.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>The comparison of using CNN and Fully-connected layer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>The comparison of using different width CNNs in UPANets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tomgoldstein/loss-landscape 3 https://github.com/JoelNiklaus/loss_landscape</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/kuangliu/pytorch-cifar</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the IEEE conference on computer vision and pattern recognition</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07436</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>in Forecasting</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01629</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sharpness-Aware Minimization for Efficiently Improving Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01412</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5927" to="5935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Res2net: A new multiscale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shuffle net: An application of generalized perfect shuffles to multihop lightwave networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Hluchyj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Karol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Lightwave Technology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1386" to="1397" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">DenseNet Models for Tiny ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Abai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajmalwar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10429</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Puzzle mix: Exploiting saliency and local statistics for optimal mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5275" to="5285" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
