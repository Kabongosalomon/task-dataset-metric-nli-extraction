<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technical University of Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technical University of Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technical University of Darmstadt</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning sentence embeddings often requires large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-theart unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1% of the performance of in-domain supervised approaches. Further, we show that TSDAE is a strong pre-training method for learning sentence embeddings, significantly outperforming other approaches like Masked Language Model. 1 A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TS-DAE and other recent approaches on four different datasets from heterogeneous domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence embedding techniques encode sentences into a fixed-sized, dense vector space such that semantically similar sentences are close. The most successful previous approaches like InferSent <ref type="bibr" target="#b10">(Conneau et al., 2017)</ref>, Universial Sentence Encoder (USE) <ref type="bibr" target="#b8">(Cer et al., 2018)</ref> and SBERT <ref type="bibr" target="#b26">(Reimers and Gurevych, 2019)</ref> heavily relied on labeled data to train sentence embedding models. However, for most tasks and domains, labeled data is not available and data annotation is expensive. To overcome this limitation, unsupervised approaches have been proposed which learn to embed sentences just using an unlabeled corpus for training.</p><p>We propose a new approach: Transformer-based Sequential Denoising Auto-Encoder (TSDAE). It significantly outperforms previous methods via an encoder-decoder architecture. During training, TS-DAE encodes corrupted sentences into fixed-sized vectors and requires the decoder to reconstruct the original sentences from this sentence embedding. For good reconstruction quality, the semantics must be captured well in the sentence embedding from the encoder. Later, at inference, we only use the encoder for creating sentence embeddings.</p><p>A crucial shortcoming of previous unsupervised approaches is the evaluation. Often, approaches are only evaluated on the Semantic Textual Similarity (STS) task from SemEval <ref type="bibr" target="#b20">(Li et al., 2020;</ref><ref type="bibr">Carlsson et al., 2021;</ref><ref type="bibr" target="#b11">Giorgi et al., 2020)</ref>. As we argue in Section 8, we perceive this as an insufficient evaluation. The STS datasets do not include sentences with domain specific knowledge. Further, unsupervised sentence embeddings approaches achieve rather inferior results on this tasks compared to methods like USE or SBERT that use labeled data from other tasks like NLI. It remains unclear, how well unsupervised sentence embedding methods will perform on domain specific tasks.</p><p>To answer this question, we compare TSDAE with previous unsupervised sentence embeddings approaches on three different tasks (Information Retrieval, Re-Ranking and Paraphrase Identification), for heterogeneous domains and different text styles. We show that TSDAE can outperform other state-of-the-art unsupervised approaches by up to 6.4 points. TSDAE is able to perform on-par or even outperform existent supervised models like USE-large, which had been trained with a lot of labeled data from various datasets.</p><p>Further, we demonstrate in Section 7 that TS-DAE works well as a pre-training task for sentence embedding models. We observe a significant performance improvement compared to other arXiv:2104.06979v1 [cs.CL] 14 Apr 2021 pre-training tasks like Masked Language Model (MLM) or using the pretrained BERT without domain specific pre-training.</p><p>Our contributions are three-fold:</p><p>• We propose a novel unsupervised method, TS-DAE based on denoising auto-encoders. We show that it outperforms the previous best approach by up to 6.4 points on diverse datasets.</p><p>• To the best of our knowledge, we are the first to compare the unsupervised sentence embedding methods based on pre-trained Transformers for various tasks on heterogeneous domains.</p><p>• TSDAE outperforms other methods including MLM by a large margin as a pre-training method for sentence embedding learning on multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this work, we view sentence embedding methods as supervised if the training process requires labels for sentence pairs and otherwise as unsupervised ones. Supervised sentence embeddings utilize training signals for sentence pairs which provide the information about the relation between the sentences. Since sentence embeddings are usually applied to measure the similarity of a sentence pair, the most direct way is to label this similarity for supervised training <ref type="bibr">(Henderson et al., 2017)</ref>. Many studies also find that natural language inference (NLI), question answering and conversational context datasets can successfully be used to train sentence embeddings <ref type="bibr" target="#b10">(Conneau et al., 2017;</ref><ref type="bibr" target="#b8">Cer et al., 2018)</ref>. The recently proposed Sentence-Transformers <ref type="bibr" target="#b26">(Reimers and Gurevych, 2019)</ref> introduce pre-trained Transformers to the field of sentence embeddings with the above-mentioned supervision like NLI labels <ref type="bibr" target="#b5">(Bowman et al., 2015;</ref><ref type="bibr" target="#b31">Williams et al., 2018)</ref>. Although high-quality sentence embeddings can be derived via supervised training, the labeling cost is a major obstacle for practical usage, especially for specialized domains.</p><p>Unsupervised sentence embeddings utilize only unlabeled corpus during training. Recent work combines pre-trained Transformers with different training objectives to achieve state-of-theart results. Among them, Contrastive Tension (CT) <ref type="bibr" target="#b11">(Giorgi et al., 2020)</ref> simply views the iden-tical and different sentences as positive and negative examples, resp. and train two independent encoders; BERT-flow <ref type="bibr" target="#b20">(Li et al., 2020)</ref> trains model via debiasing embedding distribution towards Gaussian. For more details, please refer to Section 5. Both of them requires only independent sentences. By contrast, DeCLUTR <ref type="bibr" target="#b11">(Giorgi et al., 2020)</ref> utilizes sentence-level contexts and requires long documents (2048 tokens at least) for training. This requirement is hardly met for many cases, e.g. Tweets or dialogues. Thus, in this work we only consider methods which uses only single sentences during training.</p><p>Most previous work mainly evaluate only on Semantic Textual Similarity (STS) from the SemEval shared tasks. As we show in Section 8, the unsupervised approaches perform much worse than the out-of-the-box supervised pre-trained models even though they were not specifically trained for STS. Further, a good performance on STS does not necessarily correlate with the performance on down-stream tasks <ref type="bibr" target="#b25">(Reimers et al., 2016)</ref>. It remains unclear how these methods perform on specific tasks and domains. To answer this, we compare three powerful unsupervised methods based on pre-trained Transformers including CT, BERTflow and our proposed TSDAE on different tasks of heterogeneous domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequential Denoising Auto-Encoder</head><p>Although Sequential Denoising Auto-Encoder (SDAE) <ref type="bibr" target="#b30">(Vincent et al., 2010;</ref><ref type="bibr" target="#b12">Goodfellow et al., 2016;</ref><ref type="bibr" target="#b15">Hill et al., 2016)</ref> is a popular unsupervised method in machine learning, how to combine it with pre-trained Transformers remains unclear. In this section, we first introduce the training objective of TSDAE and then give the optimal configuration of TSDAE. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the architecture of TSDAE. TS-DAE train sentence embeddings by adding a certain type of noise (e.g. deleting or swapping words) to input sentences, encoding the damaged sentences into fixed-sized vectors and then reconstructing the vectors into the original input. Formally, the train-  ing objective is:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Objective</head><formula xml:id="formula_0">J SDAE (θ) = E x∼D [log P θ (x|x)] = E x∼D [ l t=1 log P θ (x t |x)] = E x∼D [ l t=1 log exp(h T t e t ) N i=1 exp(h T t e i ) ]</formula><p>where D is the training corpus, x = x 1 x 2 · · · x l is the input sentence with l tokens,x is the corresponding damaged sentence, e t is the word embedding of x t , N is the vocabulary size and h t is the hidden state at decoding step t.</p><p>An important difference to original transformer encoder-decoder setup presented in <ref type="bibr" target="#b29">Vaswani et al. (2017)</ref> is the information available to the decoder: Our decoder decodes only from a fixed-size sentence representation produced by the encoder. It does not have access to all contextualized word embeddings from the encoder. This modification introduces a bottleneck, that should force the encoder to produce a meaningful sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TSDAE</head><p>The model architecture of TSDAE is a modified encoder-decoder Transformer where the key and value of the cross-attention are both confined to the sentence embedding only. Formally, the formulation of the modified cross-attention is:</p><formula xml:id="formula_1">H (k) = Attention(H (k−1) , [s T ], [s T ]) Attention(Q, K, V ) = softmax QK T √ d V</formula><p>where H (k) ∈ R t×d is the decoder hidden states within t decoding steps at the k-th layer, d is the size of the sentence embedding, [s T ] ∈ R 1×d is a one-row matrix including the sentence embedding vector and Q, K and V are the query, key and value, respectively. By exploring different configurations on the Semantic Textual Similarity (STS) datasets <ref type="bibr" target="#b7">(Cer et al., 2017)</ref>, we discover that the best combination is: (1) adopting deletion as the input noise and setting the deletion ratio to 0.6, (2) using the output of the [CLS] token as fixed-sized sentence representation (3) tying the encoder and decoder parameters during training. For the detailed tuning process, please refer to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets and Evaluation</head><p>In most of the previous work, the sentence embedding methods are mainly evaluated on the STS task from the SemEval shared tasks, which is a rather generic task. How the methods perform on specific tasks and domains remains unclear. To fill this gap, different tasks from heterogeneous domains are used for evaluation. The tasks include Re-Ranking (RR), Information Retrieval (IR) and Paraphrase Identification (PI). In detail, the datasets used are as follows: AskUbuntu (RR task) is a collection of user posts from the technical forum AskUbuntu <ref type="bibr" target="#b19">(Lei et al., 2016)</ref>. Models are required to re-rank 20 candidate questions according to the similarity given an input post. The candidates are obtained via BM25 term-matching <ref type="bibr" target="#b27">(Robertson et al., 1994)</ref>. The evaluation metric is Mean Average Precision (MAP).</p><p>CQADupStack (IR task) is a question retrieval dataset of forum posts on various topics from Stack-Exchange <ref type="bibr" target="#b16">(Hoogeveen et al., 2015)</ref>. In detail, it has 12 forums including Android, English, gaming, geographic information system, Mathematica, physics, programmers, statistics, Tex, Unix, Webmasters and WordPress. Models are required to retrieve duplicate questions from a large candidate pool. The metric is MAP@100.</p><p>TwitterPara (PI task) consists of two similar datasets: the Twitter Paraphrase Corpus (PIT-2015) <ref type="bibr" target="#b32">(Xu et al., 2015)</ref> and the Twitter News URL Corpus (noted as TURL) <ref type="bibr" target="#b18">(Lan et al., 2017)</ref>. The dataset consists of pairs of tweets together with a crowd-annotated score if the pair is a paraphrase. The evaluation metric is Average Precision (AP) over the gold confidence scores and the similarity scores from the models.</p><p>SciDocs (RR task) is a benchmark consisting of multiple tasks about scientific papers <ref type="bibr" target="#b9">(Cohan et al., 2020)</ref>. In our experiments, we use the tasks of Cite: Given a paper title, identify the titles the paper is citing; Co-Cite (CC), Co-Read (CR), and Co-View (CV), for which we must find papers that are frequently co-cited/-read/-viewed for a given paper title. For all these tasks, given one query paper title, models are required to identify up to 5 relevant papers titles from up to 30 candidates. The negative examples were selected randomly. The evaluation metric is MAP. For evaluation, sentences are first encoded into fixed-sized vectors and then the cosine similarity of the vectors is adopted as the sentence similarity. Since sentence-level embeddings are the target, for AskUbuntu, CQADupStack and SciDocs, we remove the body texts and only keep the titles for evaluation. For the datasets with sub-datasets or sub-tasks including CQADupStack, TwitterPara and SciDocs, the final score is derived by averaging the scores from each sub-dataset or sub-task.</p><p>For unsupervised training, we use all sentences from the training split without any labels. For indomain supervised training, we extract all the relevant pairs as the training set from the original training splits for IR and RR tasks. For the PI task, we keep the original labeling scheme, i.e., the confidence scores. The statistics for each dataset are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we compare our proposed TSDAE with other unsupervised counterparts and out-ofthe-box supervised pre-trained models on the above mentioned tasks. We include two recent state-ofthe-art unsupervised approaches, CT and BERTflow, in the comparison. We use the proposed hyper-parameters from the respective paper. Without other specification, BERT-base-uncased is used as the base Transformer model. To eliminate the influence of randomness, we report the scores averaged over 5 random seeds for all the in-domain models. For other details, please refer to Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline Methods</head><p>For the baselines of shallow methods, we include avg. GloVe embeddings <ref type="bibr" target="#b24">(Pennington et al., 2014)</ref> and Sent2Vec <ref type="bibr" target="#b23">(Pagliardini et al., 2018)</ref>. The former generates sentence embeddings by averaging word embeddings trained on a large corpus from the general domain; the latter is also a bag-of-words model but trained on the in-domain unlabeled corpus. For deep models, we report the scores of two Sentence-Transformer models including the one trained on the supervised NLI data from the BERTbase-uncased 2 checkpoint (noted as SBERT-base-NLI) and the one trained on large-scale supervised paraphrase data from the DistilRoBERTa checkpoint (noted as SDRoBERTa-para). We also report the scores of the Google's Universial Sentence Embedding (USE)  which is trained on multiple supervised datasets including NLI and community question answering. Since there are IR and RR tasks, we additionally include the popular baseline BM25 for comparison.</p><p>To better understand the relative performance of these unsupervised methods, we also train SBERT models in an in-domain supervised manner and view their scores as the upper bound. </p><formula xml:id="formula_2">(i) , y (i) } M i=1</formula><p>, MNRL views the labeled pairs as positive and the other in-batch combinations as negative. Formally, the training objective for each batch is:</p><formula xml:id="formula_3">J MNRL (θ) = 1 M M i=1 log exp σ(f θ (x (i) ), f θ (y (i) )) M j=1 exp σ(f θ (x (i) ), f θ (y (j) ))</formula><p>where σ is a certain similarity function for vectors and f θ is the sentence encoder that embeds sentences. For TwitterPara, whose relevant scores are labeled, the Mean Square Error (MSE) loss is adopted to train the in-domain models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Contrastive Tension (CT)</head><p>CT <ref type="bibr">(Carlsson et al., 2021)</ref> finetunes pre-trained Transformers in a contrastive-learning fashion. For each sentence, it construct a binary cross-entropy loss by viewing the same sentence as the relevant and samples K random sentences as the irrelevant. To make the training process stable, for each sentence pair (a, b), CT uses two independent encoders f θ 1 and f θ 2 from the same initial parameter point to encode the sentence a and b, respectively. Formally, the learning objective is:</p><formula xml:id="formula_4">J CT (θ 1 , θ 2 ) = E (a,b)∼D [y log σ(f θ 1 (a) T f θ 2 (b)) + (1 − y) log(1 − σ(f θ 1 (a) T f θ 2 (b))]</formula><p>where y ∈ {0, 1} represents whether sentence a is identical to sentence b and σ is the Logistic function. Despite its simplicity, CT achieves state-ofthe-art unsupervised performance on the Semantic Textual Similarity (STS) datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">BERT-flow</head><p>In stead of fine-tuning the parameters of the pretrained Transformers, BERT-flow <ref type="bibr" target="#b20">(Li et al., 2020)</ref> aims at fully exploiting the semantic information encoded by these pre-trained models themselves via distribution debiasing. </p><formula xml:id="formula_5">J BERT-flow (φ) = E x∼D [log p U (u)] (1) = E u [log(p Z (f −1 φ (u))| det ∂f −1 φ (u) ∂u |)] (2)</formula><p>where u is the biased embedding of sentence x and z = f −1 φ (u) is the debiased sentence embedding which follows a standard Gaussian distribution. Equation 2 is derived by applying the changeof-variables theorem to Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. Among the unsupervised methods, TSDAE outperforms the previous best approach (CT) by up-to 6.4 points and 2.6 points on average over all tasks.</p><p>Another feasible option for a task without training data is to use an out-of-the-box pre-trained model (SBERT-base-NLI, SDRoBERTa-para, USElarge). <ref type="bibr">3</ref> We note that out-of-the-box models achieve quite strong performances and outperform most unsupervised approaches. TSDAE is able to outperform SDRoBERTa-para on all tasks and outperforms USE-large on the AskUbuntu and SciDocs tasks. We observe that TSDAE yields the largest performance increase for SciDocs (3.8 points vs. USE and 6.8 points vs. SDRoBERTapara), the dataset that is the most distinct from the training datasets of USE / SDRoBERTa-para. Training with in-domain labeled data can, as expected, further improve the performance. TSDAE is on average 6.6 points behind the in-domain supervised system. This indicates substantial room for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>We further analyze unsupervised sentence embedding learning methods. In particular, we analyze the required size of the unlabeled corpus and using the methods as pre-training task for fine-tuning with labeled data.</p><p>For all the datasets except TwitterPara, the analysis is carried out on the development set. For TwitterPara, the test set is used, as it has no development split released by the original paper. All the hyper-parameters are chosen up-front without tuning to a particular dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Influence of Corpus Size</head><p>In certain domains, getting a sufficiently high number of (unlabeled) sentences can be challenging. Hence, data efficiency and deriving good sentence embeddings even with little unlabeled training data can be important.</p><p>In order to study this, we train the unsupervised approaches with different corpus sizes: Between 128 and 65,536 sentences. For each experiment, we The results are shown in <ref type="figure" target="#fig_3">Figure 2</ref>. We observe that TSDAE is outperforming previous unsupervised learning methods often with as little as 1000 unlabeled sentences. With 10k unlabeled sentences, the downstream performance usually stagnates for all tested unsupervised sentence embedding methods. The only exception where more training data is helpful is for the CQADupStack task. This is expected, as the CQADupStack consists of 12 vastly different StackExchange forums, hence, requiring more unlabeled data to represent all domains well.</p><p>We conclude that comparatively little unlabeled data of ∼10k sentences is needed to tune pretrained transformers to a specific domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Relevant Content Words</head><p>Not all word types play an equal role in determining the semantics of a sentence. Often, nouns are the critical content words in a sentence, while e.g. prepositions are less important and can be add / removed from a sentences without changing the content too much.</p><p>In this section, we investigate which word types are the most relevant for the different sentence embedding methods, i.e., which words (part-of-speech tags) mainly influence if a sentence pair is perceived as similar or not. We are especially interested if we observe differences between in-domain supervised approaches (SBERT-sup.), out-of-thebox pre-trained approaches (USE, SDRoBERTa), and unsupervised approaches (TSDAE, CT, BERTflow).</p><p>To measure this, we select a sentence pair (a, b) that is labeled as relevant and find the word that maximally reduces the cosine-similarity score for  the pair (a, b):</p><formula xml:id="formula_6">w =argmax w (cossim(a, b)− min(cossim(a \ w, b), cossim(a, b \ w)))</formula><p>among all words w that appear in either a or b. Then, we record the POS tag forŵ and compute the distribution of POS tags across all sentence pairs. POS-tags are determined using CoreNLP . The result averaged over the four datasets is shown in <ref type="figure" target="#fig_4">Figure 3</ref>. For the result on each dataset, please refer to Appendix H. We find that nouns (NN) are by far the most relevant content words in a sentence for all methods. We do not perceive significant differences between the three types of approaches. This is good news for the unsupervised methods (TSDAE, CT, BERT-flow) and show that these can learn which words types are critical in a sentence without having labeled data. On the down side, unsupervised approaches might have issues for tasks where nouns are not the most critical content words.</p><p>When analyzing the differences between the approaches in more detail, we notice that unsupervised approaches have challenges to pick-up fine nuances for specific tasks. For example, for the AskUbuntu task, version numbers of e.g. the operating system plays a critical role but is missed by the unsupervised and supervised out-of-box approaches. We think it will be hard for approaches to learn such fine nuances without having labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Equivalent Labeling Work</head><p>The goal of unsupervised sentence embedding learning methods is to eliminate the need of labeled training data, which can be expensive in the creation. However, as shown in Section 6, approaches with sufficient in-domain labeled data significantly outperform unsupervised approaches.</p><p>As far as we know, previous work did not study the point of intersection between unsupervised and supervised approaches: If you only need few labeled examples to outperform unsupervised ap- proaches, annotating those might be the more viable solution.</p><p>To find this intersection point, we train the indomain supervised SBERT approach with varying size of labeled training data. Results are shown in <ref type="figure" target="#fig_5">Figure 4</ref>. To estimate the intersection with more precision, we apply binary search. We set the search precision to the standard deviation of the target score over 5 random seeds.</p><p>The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. To match the performance of TSDAE, 140 -6k annotated examples are required. CQADupStack and the Twitter-Paraphrase corpus, which compromise various domains, require more labeled data than AskUbuntu (1 domain). Surprisingly, SciDocs, which includes data from all type of scientific domains, the indomain supervised approach outperforms unsupervised approaches with just 464 labeled examples. This dataset appears to be especially challenging for unsupervised approaches, as we observe a large performance gap between in-domain supervised and unsupervised approaches.</p><p>In an annotation experiment on the Twitter dataset, we measured that annotating 100 Tweet pairs takes about 20 minutes for an (experienced) annotator. Hence, the state-of-the-art unsupervised TSDAE approach achieves the same performance as a supervised approach with 0.5 -20 hours of annotation work for one annotator (2.5h -100h for 5 crowd annotators).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Usage for Pre-Training</head><p>Another important application of unsupervised methods is the usage as pre-training method before in-domain supervised training. Previous work <ref type="bibr" target="#b13">(Gururangan et al., 2020)</ref> showed that MLM pretraining on the in-domain data can significantly improve the performance for classification tasks. This lead to the question if these unsupervised approaches can be used as pre-training methods for later in-domain supervised sentence embedding training?</p><p>We analyze the pre-training effect of TSDAE and CT 4 and compare it with MLM pre-training. All approaches are trained on the full unlabeled corpus. The results are shown in <ref type="figure" target="#fig_5">Figure 4</ref>. TSDAE outperforms MLM by a significant margin for all datasets except for AskUbuntu. There, MLM works slightly better. For the other datasets, TSDAE shows a clear out-performance to other pre-training strategies. The difference is quite consistent also for larger labeled training sets. We conclude, that TSDAE works well as pre-training method and can significantly improve the performance for later supervised training even for larger training datasets.</p><p>For CT, we observe mixed results: For Twit-terPara, we observe a strong improvement, for CQaDupStack a weak improvement and for AskUbuntu &amp; SciDocs it actually weakens the performance compared to no-domain specific pretraining. Results for CT show that a good unsupervised approach must not necessarily be a good pre-training approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Semantic Textual Similarity</head><p>Previous unsupervised sentence embedding learning approaches <ref type="bibr" target="#b11">(Giorgi et al., 2020;</ref><ref type="bibr">Carlsson et al., 2021;</ref><ref type="bibr" target="#b20">Li et al., 2020;</ref><ref type="bibr" target="#b28">Su et al., 2021</ref>) primarily evaluated on the task of Semantic Textual Similarity (STS) with data from SemEval <ref type="bibr" target="#b3">(Agirre et al., 2012</ref><ref type="bibr" target="#b4">(Agirre et al., , 2013</ref><ref type="bibr" target="#b1">(Agirre et al., , 2014</ref><ref type="bibr" target="#b0">(Agirre et al., , 2015</ref><ref type="bibr" target="#b2">(Agirre et al., , 2016</ref>, the STS benchmark <ref type="bibr" target="#b7">(Cer et al., 2017)</ref>, and from the SICK-Relatedness dataset <ref type="bibr" target="#b22">(Marelli et al., 2014)</ref> using Pearson or Spearman's rank correlation.</p><p>We find the (sole) evaluation on STS problematic for several reasons. First, the STS datasets consists of sentences which don't requiring domain specific knowledge, they are primarily from news and image captions. Pre-trained models like USE and SBERT, which use labeled data from other tasks, significantly outperform unsupervised approaches on this general domain. It remains unclear if the proposed unsupervised approach will work better for specific domains. Second, the STS datasets have an artificial score distribution that dissimilar and similar pairs appear roughly equally. For most real-word tasks, there is an extreme skew and only a tiny fraction of pairs are considered similar. Third, to perform well on the STS datasets, a method must rank dissimilar pairs and similar pairs equally well. A method that identifies perfectly similar pairs, but has issues to differentiate between various types of dissimilar pairs, would score badly on the STS datasets. In contrast, most real-world tasks, like duplicate questions detection, related paper finding, or paraphrase mining, only require to identify the few similar pairs out of a pool of millions of possible combinations. Overall, we think that the performance on the STS datasets does not correlate well with downstream task performance. This has also been previously shown in <ref type="bibr" target="#b25">Reimers et al. (2016)</ref> for various unsupervised approaches.</p><p>In order to make our work still comparable to previous work, we conduct experiments on the STS datasets. We sample sentences from Wikipedia as done by <ref type="bibr">Carlsson et al. (2021)</ref> and train unsupervised approaches based on pre-trained transformers on it. Following previous work, we use Spearman's rank correlation. As mentioned, correlation has the downside that it emphasizes the whole score range of 0 to 5 equally, but most tasks are only interested in the highly similar pairs. To address this, we also report Average Precision (AP) and view the labeled scores larger than 4 as positive and the other as negative.</p><p>The results 5 are shown in <ref type="table" target="#tab_5">Table 3</ref>. The AP score of our proposed TSDAE method is on par with CT and falls behind it for Spearman correlation. This indicates TSDAE works well on finding similar sentence pairs, while ignoring how dissimilar the negative pairs are.</p><p>Unsupervised approaches perform much worse than the out-of-the-box supervised pre-trained models (SBERT, SDRoBERTa and USE), even though those did not use any STS specific training data. Thus, we argue that more emphasis of the evaluation should be put on specific tasks and domains as we do in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work, we study unsupervised sentence embeddings based on pre-trained Transformers on different tasks from heterogeneous domains. Our proposed TSDAE approach significantly outperforms other, state-of-the-art unsupervised methods by upto 6.4 points (2.6 points on average across all tasks). Further, we show that TSDAE works well as pretraining method, significantly outperforming the popular MLM on multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Optimal Configuration of TSDAE</head><p>To obtain the optimal configuration, we compare TSDAE models trained and evaluated on the general domain without bias towards any specific domain. The greedy search is applied by sequentially finding the best (1) noise type and ratio (2) pooling method and (3) weight tying scheme. Similar to the choice of CT and BERT-flow, we train the models on the combination of SNLI and MultiNLI without labels and evaluate the models on the STS Benchmark (STS-B) with the metric of Spearman rank correlation. The maximum number of training steps is 30K and the models are evaluated every 1.5K training steps, reporting the best validation performance. Scores are obtained by calculating the average over 5 random seeds.</p><p>We first compare the scores of different noise types, fixing the noise ratio as 0.3 (i.e. 30% tokens are influenced) and the pooling method as CLS pooling. The results are show in <ref type="table" target="#tab_7">Table 4</ref>. This indicates deletion is the best noise type. We then tune the noise ratio of the deletion noise and the results are shown in <ref type="table" target="#tab_8">Table 5</ref>. This indicates 0.6 is the best noise ratio.   We then compare different pooling methods with the best setting so far. The results are shown in <ref type="table" target="#tab_9">Table 6</ref>. Since there is little difference between CLS and mean pooling and mean pooling loses the position information, the CLS pooling is chosen. Finally, we find that tying the encoder and the decoder can further improve the validation score to 79.15. <ref type="bibr">CLS Mean Max 78.77 78.84 78.17</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Data Statistics</head><p>The statistics of the four datasets used in this work are shown in <ref type="table" target="#tab_11">Table 7</ref>. Multiple sub-datasets are included in CQADupStack, SciDocs and TwitterPara. CQADupStack has one sub-dataset for each of the 12 forums. The avg. #relevant, avg. #candidates and avg. length in <ref type="table" target="#tab_11">Table 7</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiment Settings</head><p>We implement TSDAE, CT and BERT-flow based on Pytorch and Huggingface's Transformers 6 . For these three unsupervised methods, following the original papers, the number of training steps is 100K; the batch size is 8; the optimizers are AdamW, RMSProp and AdamW, respectively; the initial learning rates are 3e-5, 1e-5 and 1e-6, resp. The weight decay for BERT-flow is 0.01. The learning rate for CT follows a segmented-constant scheduling scheme: 1e-5 for step 1 to 500; 8e-6 for step 501 to 1000; 6e-6 for step 1001 to 1500; 4e-6 for step 1501 to 2000; 2e-6 for others. The pooling method for CT and BERT-flow is both mean pooling. Since CT trains two independent encoders and we find the second encoder has better performance, we use the second encoder for evaluation.</p><p>We use the repository of sentence-transformers 7 to train the in-domain supervised models. For them, the number of training epochs is 10; the maximum number of training steps is 20K; the batch size is 64; the similarity function σ is set to cosine similarity; early-stopping is applied by checking the validation performance. To eliminate the influence of randomness, we report the scores averaged over 5 random seeds for all the in-domain unsupervised and supervised models. All the pre-trained checkpoints used are listed in <ref type="table" target="#tab_13">Table 8</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Results of Other Checkpoints</head><p>The results of other checkpoints besides BERT-base-uncased are shown in <ref type="table" target="#tab_14">Table 9</ref>. For all the methods, better results are achieved by using BERT checkpoints, which also makes TSDAE significantly outperforms others. We suppose this advantage comes from the additional pre-training task, next sentence prediction of the BERT models, which guides the model to learn from sentence-level contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Influence of Number of Training Steps</head><p>The influence of the number of training steps for AskUbuntu, CQADupStack and TwitterPara is shown in <ref type="figure" target="#fig_6">Figure 5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Influence of Corpus Size</head><p>The influence of corpus size for AskUbuntu, CQADupStack and TwitterPara is shown in <ref type="figure">Figure 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Influence of Different POS Tags</head><p>The influence of different POS tags on the output similarity scores for AskUbuntu, CQADupStack and TwitterPara is shown in <ref type="figure" target="#fig_8">Figure 8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Detailed Results of Semantic Textual Similarity</head><p>The detailed results of STS on each dataset are shown in <ref type="table" target="#tab_0">Table 10</ref> and <ref type="table" target="#tab_0">Table 11</ref> with the evaluation metric of Spearman's rank correlation and average precision, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of TSDAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For AskUbuntu, CQADupStack and SciDocs, where the relevant sentence pairs are labeled, the in-domain SBERT models are trained with the Multiple-Negative Ranking Loss (MNRL) (Henderson et al., 2017) from the BERT-base-uncased Transformer checkpoint. For a batch of relevant sentences pairs {x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The influence of the number of training sentences (in thousands) on the model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>POS tag for the most relevant content word in a sentence, i.e. the word that mostly influences if a sentence pair is considered as similar.train a BERT-base-uncased model with 10 epochs up to 100k training steps. The models are evaluated at the end of each epoch and the best score on the development set is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of different pre-training approaches (TSDAE/MLM/CT+SBERT) with increasing sizes of labeled training data (in thousands). SBERT: Training from the standard bert-base-uncased checkpoint. TSDAE: Unsupervised baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>The influence of the number of training steps on the model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>The influence of the number of training sentences on the model performance.G Usage for Pre-TrainingThe pre-training performance on AskUbuntu, CQADupStack and TwitterPara is shown inFigure 7. The influence of the number of training sentences on the model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>The influence of different POS tags on the output similarity scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation using average precision. Results are averaged over 5 random seeds. The best results excluding the upper bound are bold. USE-large was trained with in-domain training data for AskUbuntu and CQADupStack (scores in italic). Our proposed TSDAE significantly outperforms both other unsupervised and supervised out-ofthe-box approaches.</figDesc><table><row><cell>Method</cell><cell cols="2">AskU. CQADup.</cell><cell></cell><cell>TwitterP.</cell><cell>SciDocs</cell><cell>Avg.</cell></row><row><cell>Sub-task/-dataset</cell><cell></cell><cell cols="3">TURL PIT Avg. Cite CC</cell><cell>CR CV Avg.</cell></row><row><cell cols="2">Proposed unsupervised method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TSDAE</cell><cell>59.4</cell><cell>14.5</cell><cell>76.8</cell><cell cols="2">69.2 73.0 71.4 73.9 75.0 75.6 74.0 55.0</cell></row><row><cell cols="5">Previous unsupervised approaches based on BERT-base</cell></row><row><cell>CT</cell><cell>56.3</cell><cell>13.3</cell><cell>74.6</cell><cell cols="2">70.4 72.5 63.4 67.1 70.1 69.7 67.6 52.4</cell></row><row><cell>BERT-flow</cell><cell>53.7</cell><cell>9.2</cell><cell>72.8</cell><cell cols="2">65.7 69.2 61.3 62.8 66.7 67.1 64.5 49.1</cell></row><row><cell cols="3">Other previous unsupervised approaches</cell><cell></cell><cell></cell></row><row><cell>BM25</cell><cell>53.4</cell><cell>9.7</cell><cell>65.2</cell><cell cols="2">63.2 64.2 47.4 49.7 56.1 55.4 52.1 44.9</cell></row><row><cell>Avg. GloVe</cell><cell>51.0</cell><cell>10.0</cell><cell>70.1</cell><cell cols="2">52.1 61.1 58.8 60.6 64.2 65.4 62.2 46.1</cell></row><row><cell>Sent2Vec</cell><cell>49.0</cell><cell>3.2</cell><cell>47.5</cell><cell cols="2">39.9 43.7 61.6 66.0 66.1 66.7 65.1 40.2</cell></row><row><cell cols="4">Out-of-the-box supervised pre-trained models</cell><cell></cell></row><row><cell>SBERT-base-NLI</cell><cell>52.5</cell><cell>8.7</cell><cell>71.5</cell><cell cols="2">60.7 66.1 60.7 62.5 64.2 65.7 63.3 47.6</cell></row><row><cell>SDRoBERTa-para</cell><cell>56.5</cell><cell>12.9</cell><cell>74.8</cell><cell cols="2">69.1 72.2 64.5 66.2 68.7 69.5 67.2 52.2</cell></row><row><cell>USE-large</cell><cell>59.3</cell><cell>15.9</cell><cell>77.1</cell><cell cols="2">69.8 73.5 67.1 69.5 71.4 72.6 70.2 54.7</cell></row><row><cell cols="3">In-domain supervised training (upper bound)</cell><cell></cell><cell></cell></row><row><cell>SBERT-supervised</cell><cell>63.8</cell><cell>16.3</cell><cell>81.6</cell><cell cols="2">75.8 78.7 90.4 91.2 86.2 83.6 87.9 61.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The paper of BERT-flow claims that the BERT word embeddings are highly relevant to the word frequency, which in turn influences the hidden states via the Masked Language Modeling (MLM) pre-training. This finally leads to biased sentence embeddings generated by the pooling over these hidden states. To solve this problem, BERT-flow inputs the biased sentence embedding into a trainable flow network f</figDesc><table /><note>φ (Kingma and Dhariwal, 2018) for debiasing via fitting a standard Gaussian distribution, while keeping the parame- ters of the BERT model unchanged. Formally, the training objective is:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Intersection point (number of labeled sentence pairs) between unsupervised TSDAE and in-domain supervised SBERT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on the STS datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">: Results with different noise types</cell><cell></cell><cell></cell></row><row><cell>Ratio</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell></row><row><cell cols="10">Score 77.81 77.70 77.75 78.02 78.25 78.77 78.19 77.69 75.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results with different noise ratio.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results with different pooling methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>are all general statistics without distinguishing the sub-datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Task #queries</cell><cell>Avg. #relevant</cell><cell cols="2">Avg. #candidates</cell><cell>Avg. length</cell><cell>Size of unsupervised training set</cell><cell>Size of supervised training set</cell></row><row><cell>AskUbuntu</cell><cell>RR</cell><cell>200</cell><cell>5.9/5.4</cell><cell>20</cell><cell></cell><cell>9.2</cell><cell>165K</cell><cell>23K</cell></row><row><cell>CQADupStack</cell><cell>IR</cell><cell>3K</cell><cell>1.1/1.1</cell><cell>39K</cell><cell></cell><cell>8.6</cell><cell>44K</cell><cell>13K</cell></row><row><cell>SciDocs</cell><cell>RR</cell><cell>4K</cell><cell>5</cell><cell>30</cell><cell></cell><cell>12.5</cell><cell>312K</cell><cell>380K</cell></row><row><cell>Dataset</cell><cell cols="4">Task #paraphrase #non-paraphrase</cell><cell cols="2">Avg. length</cell><cell>Size of unsupervised training set</cell><cell>Size of supervised training set</cell></row><row><cell>TwitterPara</cell><cell>PI</cell><cell>-/2K</cell><cell cols="2">-/9K</cell><cell></cell><cell>13.9</cell><cell>53K</cell><cell>23K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Dataset statistics. The slash symbol '/' separates the numbers for development and test.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Model checkpoints used in this work.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Evaluation of different checkpoints using average precision. '+/-' separates the mean value and standard deviation over scores of 5 random seeds.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell></row><row><cell></cell><cell>54</cell><cell></cell><cell></cell><cell></cell><cell>14</cell><cell></cell><cell></cell></row><row><cell>MAP score</cell><cell>44 46 48 50 52</cell><cell></cell><cell>CT BERTflow tSDAE</cell><cell>MAP@100 score</cell><cell>4 6 8 10 12</cell><cell></cell><cell>CT BERTflow tSDAE</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>20 Number of training steps (thousands) 40 60 80</cell><cell></cell><cell></cell><cell>0</cell><cell>20 Number of training steps (thousands) 40 60 80</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a) AskUbuntu</cell><cell></cell><cell></cell><cell></cell><cell>(b) CQADupStack</cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AP score</cell><cell>40 45 50 55 60 70 65</cell><cell>0</cell><cell>20 Number of training steps (thousands) 40 60 80 CT BERTflow tSDAE</cell><cell>MAP score</cell><cell>70 50 60 65 55</cell><cell>0</cell><cell>20 Number of training steps (thousands) 40 60 80 CT BERTflow tSDAE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(c) TwitterPara</cell><cell></cell><cell></cell><cell></cell><cell>(d) SciDocs</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at: https://github.com/ UKPLab/sentence-transformers/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Results for other checkpoints is reported in Appendix D</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">USE-large was trained using question-answer pairs from StackExchange, i.e. it used in-domain training data for AskUbuntu and CQADupStack, likely resulting in inflated scores for these tasks. SBERT and SDRoBERTa was trained on data which is completely different from our evaluation datasets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We don't include BERT-flow as it does not update the Transformer parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Averaged scores over all the datasets are reported. For detailed results on each STS dataset, please refer to Appendix I.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/huggingface/transformers 7 https://github.com/UKPLab/sentence-transformers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1 and grant GU 798/17-1) and has been funded by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/s15-2045</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015</title>
		<meeting>the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-04" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/s14-2010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>SemEval@COLING; Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/s16-1081</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-16" />
			<biblScope unit="page" from="497" to="511" />
		</imprint>
		<respStmt>
			<orgName>The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2012</title>
		<meeting>the 6th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2012<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06-07" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
		<respStmt>
			<orgName>The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">*sem 2013 shared task: Semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM 2013</title>
		<meeting>the Second Joint Conference on Lexical and Computational Semantics, *SEM 2013<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06-13" />
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Amaru Cuba Gyllensten, and Erik Ylipää Hellqvist. 2021. Semantic re-tuning with contrastive tension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelia</forename><surname>Gogoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal sentence encoder for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-2029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SPECTER: document-level representation learning using citation-informed transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.207</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2270" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osvald</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">D</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03659</idno>
		<title level="m">DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">László</forename><surname>Lukács</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00652</idno>
		<title level="m">Balint Miklos, and Ray Kurzweil. 2017. Efficient Natural Language Response Suggestion for Smart Reply</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cqadupstack: A benchmark data set for community question-answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doris</forename><surname>Hoogeveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><forename type="middle">M</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2838931.2838934</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Australasian Document Computing Symposium</title>
		<meeting>the 20th Australasian Document Computing Symposium<address><addrLine>Parramatta, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-12-08" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
			<biblScope unit="page" from="10236" to="10245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A continuously growing dataset of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1126</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="1224" to="1234" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised question retrieval with gated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1153</idno>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="1279" to="1289" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.733</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9119" to="9130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-5010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>Baltimore</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, Reykjavik</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation, LREC 2014, Reykjavik<address><addrLine>Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05-26" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1049</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="528" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Task-oriented intrinsic evaluation of semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sentencebert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="3980" to="3990" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Okapi at TREC-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third Text REtrieval Conference</title>
		<meeting>The Third Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11-02" />
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyiwen</forename><surname>Ou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15316</idno>
		<title level="m">Whitening Sentence Representations for Better Semantics and Faster Retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Bowman. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 1: Paraphrase and semantic similarity in twitter (PIT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/s15-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015</title>
		<meeting>the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-04" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multilingual universal sentence encoder for semantic retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jax</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">Hernándeź</forename><surname>Abrego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Method AskUbuntu CQADupStack TwitterPara SciDocs Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<title level="m">Method STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Out-of-the-box supervised pre-trained models SBERT-base-NLI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Evaluation on the task of STS using Spearman&apos;s rank correlation. Method STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Out-of-the-box supervised pre-trained models SBERT-base-NLI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Evaluation on the task of STS using average precision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
