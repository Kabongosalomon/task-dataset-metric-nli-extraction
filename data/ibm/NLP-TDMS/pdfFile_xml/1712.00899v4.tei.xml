<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Realistic Face Photo-Sketch Synthesis via Composition-Aided GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jun</forename><forename type="middle">Yu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Gao</forename><forename type="middle">†</forename><surname>Fei</surname></persName>
							<email>gaofei@hdu.edu.cn.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Meng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au.qingminghuangiswiththe</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<email>qmhuang@ucas.ac.cn.†correspondingauthor:feigao</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
							<email>yujun@hdu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">are with the Key Laboratory of Complex Systems Modeling and Simulation</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<postCode>310018</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">and the State Key Laboratory of Integrated Services Networks</orgName>
								<orgName type="department" key="dep3">School of Electronic Engineering</orgName>
								<orgName type="laboratory">Fei Gao is with the Key Laboratory of Complex Systems Modeling and Simulation</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<postCode>310018</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an 710071</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<postCode>230009</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<addrLine>6 Cleveland St</addrLine>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">School of Computer Science and Technology, University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Realistic Face Photo-Sketch Synthesis via Composition-Aided GANs</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON CYBERNETICS</title>
						<imprint>
							<biblScope unit="volume">2020</biblScope>
							<biblScope unit="issue">X</biblScope>
							<biblScope unit="page">1</biblScope>
						</imprint>
					</monogr>
					<note>Meng Wang is with the</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Face photo-sketch synthesis</term>
					<term>image-to-image translation</term>
					<term>generative adversarial network</term>
					<term>deep learning</term>
					<term>face parsing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face photo-sketch synthesis aims at generating a facial sketch/photo conditioned on a given photo/sketch. It is of wide applications including digital entertainment and law enforcement. Precisely depicting face photos/sketches remains challenging due to the restrictions on structural realism and textural consistency. While existing methods achieve compelling results, they mostly yield blurred effects and great deformation over various facial components, leading to the unrealistic feeling of synthesized images. To tackle this challenge, in this work, we propose to use the facial composition information to help the synthesis of face sketch/photo. Specially, we propose a novel composition-aided generative adversarial network (CA-GAN) for face photo-sketch synthesis. In CA-GAN, we utilize paired inputs including a face photo/sketch and the corresponding pixel-wise face labels for generating a sketch/photo. Next, to focus training on hard-generated components and delicate facial structures, we propose a compositional reconstruction loss. In addition, we employ a perceptual loss function to encourage the synthesized image and real image to be perceptually similar. Finally, we use stacked CA-GANs (SCA-GAN) to further rectify defects and add compelling details. Experimental results show that our method is capable of generating both visually comfortable and identity-preserving face sketches/photos over a wide range of challenging data. In addition, our method significantly decrease the best previous Fréchet Inception distance (FID) from 36.2 to 26.2 for sketch synthesis, and from 60.9 to 30.5 for photo synthesis. Besides, we demonstrate that the proposed method is of considerable generalization ability. We have made our code and results publicly available: https://fei-hdu.github.io/ca-gan/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>F ACE photo-sketch synthesis refers synthesizing a face sketch (or photo) given one input face photo (or sketch). It has a wide range of applications such as digital entertainment and law enforcement. For example, face sketch-synthesis is essential for drawing robots, which draw portraits for human. Besides, face photo-sketch synthesis can significantly boost the accuracy and efficiency of identity verificaiton, in cases where only sketches of criminal suspects are in access <ref type="bibr" target="#b0">[1]</ref>. Ideally, the synthesized photo or sketch portrait should be identitypreserved and appearance-realistic, so that it will yield both high identification accuracy and excellent perceptual quality.</p><p>So far, tremendous efforts have been made to develop facial sketch synthesis methods, both shallow-learning based and deep-learning based <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Especially, inspired by the great success of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b4">[5]</ref> in various image-to-image translation tasks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, researchers recently extend GANs for face photo-sketch synthesis <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. While these methods achieve compelling results, precisely depicting face photos/sketches remains challenging due to the restrictions on structural realism and textural consistency. By carefully examining the synthesized images from existing methods, we observe serious deformations and aliasing defects over the mouth and hair regions. Besides, the synthesized photos/sketches are typically unpleasantly blurred. Such artifacts lead to the unrealistic feeling of synthesized sketches.</p><p>To tackle this challenge, we propose to use the facial composition information to help face photo-sketch synthesis. Specially, we propose to use pixel-wise face labelling masks to character the facial composition. This is motivated by the following two observations. First, pixel-wise face labelling masks are capable of representing the strong geometric constrain and complicated structural details of faces. Second, it is easy to access pixel-wise facial labels due to recent development on face parsing techniques <ref type="bibr" target="#b10">[11]</ref>, thus avoiding heavy human annotations and feasible for practical applications.</p><p>Additionally, we propose a composition-adaptive reconstruction loss to focus training on hard-generated components and prevents the large components from overwhelming the generator during training <ref type="bibr" target="#b11">[12]</ref>. In typical image generation methods, the reconstruction loss is uniformly calculated across the whole image as (part of) the objective <ref type="bibr" target="#b5">[6]</ref>. Thus large components that comprise a vast number of pixels dominate the training procedure, obstructing the model to generate delicate facial structures. However, for face photos/sketches, large arXiv:1712.00899v4 [cs.CV] 9 Jan 2020 components are typically unimportant for recognition (e.g. background) or easy to generate (e.g. facial skin). In contrast, small components (e.g. eyes) typically comprise complicated structures, and thus difficult to generate. To eliminate this barrier, we introduce a weighting factor for the distinct pixel loss of each component, which down-weights the loss assigned to large components.</p><p>We refer to the resulted model as Composition-Aided Generative Adversarial Network (CA-GAN). In CA-GAN, we utilize paired inputs including a face photo/sketch and the corresponding pixel-wise face labelling masks for generating the portrait, and use the improved reconstruction loss for training. Moreover, we use a perceptual loss <ref type="bibr" target="#b12">[13]</ref> based on a pre-trained face recognition network, to further boost the realism of synthesed photos/sketches. Finally, we use stacked CA-GANs (SCA-GAN) for refinement, which proves to be capable of rectifying defects and adding compelling details <ref type="bibr" target="#b13">[14]</ref>. As the proposed framework jointly exploits the image appearance space and structural composition space, it is capable of generating natural face photos and sketches. Experimental results show that our methods significantly outperform existing methods in terms of perceptual quality, and obtain better or comparable face recognition accuracies. We also verify the excellent generalization ability of our new model on faces in the wild.</p><p>In summary, we have made the following contributions:</p><p>• First, we propose to use the facial composition information to help the synthesis of face sketch/photo, and design a novel generator architecture accordingly. To the best of our knowledge, this is the first work to employ facial composition information in the loop of learning a face photo-sketch synthesis model. • Second, we propose a novel compositional loss to focus training on hard-generated components and delicate facial structures, which proves to improve the realism of synthesized photos/sketches. • Third, we employ a perceptual loss to enforce highfrequency and identity constraints on the synthesized images, which encourages the synthesized photos/sketches to be perceptually realistic with preserved identity. • Incrementally, we use a stack of our models to further rectify defects and add compelling details, and train it in an end-to-end manner; • Our model significantly decrease previous state-of-theart Fréchet Inception distance (FID) from 36.2 to 26.2 for sketch synthesis, and from 60.9 to 30.5 for photo synthesis. Besides, we demonstrate that the proposed method is of considerable generalization ability. We have made our code and results publicly available: https:// github.com/fei-hdu/ca-gan.</p><p>The rest of this paper is organized as follows. Section II introduces related works. Section III details the proposed method. Experimental results and analysis are presented in section IV. Section V concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Face Photo-Sketch Synthesis</head><p>Tremendous efforts have been made to develop facial photosketch synthesis methods, which can be broadly classified into two groups: data-driven methods and model-driven methods <ref type="bibr" target="#b14">[15]</ref>. The former refers to methods that try to synthesize a photo/sketch by using a linear combination of similar training photo/sketch patches <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. These methods have two main parts: similar photo/sketch patch searching and linear combination weight computation. The similar photo/sketch searching process heavily increases the time consuming for test. Model-driven refers to methods that learn a mathematical function offline to map a photo to a sketch or inversely <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Traditionally, researchers pay great efforts to explore hand-crafted features, neighbour searching strategies, and learning techniques. However, these methods typically yield serious blurred effects and great deformation in synthesized face photos and sketches.</p><p>Recently, a number of trials are made to learn deep learning <ref type="bibr" target="#b25">[26]</ref> based face sketch synthesis models. For example, Zhang et al. <ref type="bibr" target="#b26">[27]</ref> propose to use branched fully convolutional network (BFCN) for generating structural and textural representations, respectively, and then use face parsing results to fuse them together. However, the resulted sketches exists heavily blurred and ring effects. More recently, inspired by the great success achieved by conditional Ganerative Network (cGAN) <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b28">[29]</ref> in various image-to-image translation tasks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b29">[30]</ref>, researchers extend GANs for face photo-sketch synthesis <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. To name a few, Wang et al. <ref type="bibr" target="#b7">[8]</ref> propose to first generate a sketch using the vanilla cGANs <ref type="bibr" target="#b5">[6]</ref> and then refine it by using a post-processing approach termed back projection. Di et al. combine the Convolutional Variational Autoncoder and cGANs for attribute-aware face sketch synthesis <ref type="bibr" target="#b8">[9]</ref>. However, there are also great deformation in various facial parts. Wang et al. follow the ideas of Pix2Pix <ref type="bibr" target="#b5">[6]</ref> and CycleGAN <ref type="bibr" target="#b30">[31]</ref>, and use multi-scale discriminators for generating high-quality photos/sketches <ref type="bibr" target="#b31">[32]</ref>.</p><p>For now, a number of works have been proposed to further boost the performance. To name a few, Zhang et al. <ref type="bibr" target="#b9">[10]</ref> embed photo priors into cGANs and design a parametric sigmoid activation function for compensating illumination variations. Peng et al. <ref type="bibr" target="#b32">[33]</ref> use a Siamese network to extract deep patch representation and combine it with a probabilistic graphical model for robust face sketch synthesis. Zhang et al. <ref type="bibr" target="#b33">[34]</ref> propose a dual-transfer method to improve the face recognition performance. Zhu et al. <ref type="bibr" target="#b34">[35]</ref> and Zhang et al. <ref type="bibr" target="#b35">[36]</ref> propose to map photos and sketches to a common space, so as to add a consistency constraint to the mappings between paired photosketches. Zhang et al. <ref type="bibr" target="#b36">[37]</ref> propose to use three modules for producing high-quality sketches. Specially they use a U-Net to produce a coarse result, a traditional method to produce fine details for important face components, and a CNN to produce the high-frequency band.</p><p>Few exiting methods use the composition information to guide the generation of the face sketch <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b37">[38]</ref> in a heuristic manner. In particular, they try to learn a specific generator for each component and then combine them together to form the entire face. Similar ideas have also been proposed for face image hallucination <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. In contrast, we propose to employ facial composition information in the loop of learning to boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Image-to-image Translation</head><p>Our work is highly related to image-to-image translation, which has achieved significant progress with the development of generative adversarial networks (GANs) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b40">[41]</ref> and variational auto-encoders (VAEs) <ref type="bibr" target="#b41">[42]</ref>. Among them, conditional generative adversarial networks (cGAN) <ref type="bibr" target="#b5">[6]</ref> attracts growing attentions because there are many interesting works based on it, including conditional face generation <ref type="bibr" target="#b42">[43]</ref>, text to image synthesis <ref type="bibr" target="#b13">[14]</ref>, and image style transfer <ref type="bibr" target="#b43">[44]</ref>. Inspired by these observations, we are interested in generating sketchrealistic portraits by using stacked cGAN. However, we found the vanilla cGAN <ref type="bibr" target="#b5">[6]</ref> insufficient for this task, thus propose to boost the performance by both developing the network architecture and modifying the objective.</p><p>Among existing works, stacked networks have achieved great success in various directions, such as unsupervised image generation <ref type="bibr" target="#b44">[45]</ref>, unsupervised image-to-image generation <ref type="bibr" target="#b45">[46]</ref>, and text-to-image generation <ref type="bibr" target="#b13">[14]</ref>. All of them obtained amazing results. Our stacked GAN is similar to these works, but has the following differences: 1) Previous works only use the noise vector, source image, or text vector as input. In contrast, we use a source image as well as its composition information as input; Correspondingly, our generators in both stage-I and stage-II contain two encoders for extracting composition and appearance representations, respectively. In contrast, generators in previous stacked GANs contain one single encoder; 2) Previous stacked GANs use global L1 loss and adversarial loss for training. We additionally use compositional L1 loss and perception loss, which significantly improve the quality of generated images; and 3) In previous stacked GANs, Stage-I GAN is first trained and then fixed while training Stage-II GAN. In our stacked GAN, both Stage-I GAN and Stage-II GAN are trained jointly in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD A. Preliminaries</head><p>The proposed method is capable of handling both sketch synthesis and photo synthesis, because these two procedures are symmetric. In this section, we take face sketch synthesis as an example to introduce our method.</p><p>Our problem is defined as follows. Given a face photo X, we would like to generate a sketch portrait Y that shares the same identity with sketch-realistic appearance. Our key idea is using the face composition information to help the generation of sketch portrait. The first step is to obtain the structural composition of a face. Face parsing can assign a compositional label for each pixel in a facial image. We thus employ the face parsing result (i.e. pixel-wise labelling masks) M as prior knowledge for the facial composition. The remaining problem is to generate the sketch portrait based on the face photo and composition masks: {X, M} → Y. Here, we propose a composition-aided GAN (CA-GAN) for this purpose. We further employ stacked CA-GANs (SCA-GAN) to refine the generated sketch portraits. Details will be introduced next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Face Decomposition</head><p>Assume that the given face photo is X ∈ R m×n×d , where m, n, and d are the height, width, and number of channels, respectively. We decompose the input photo into C components (e.g. hair, nose, mouth, etc.) by employing the face parsing method proposed by Liu et al. <ref type="bibr" target="#b10">[11]</ref> due to its excellent performance. For notational convenience, we refer to this model as P-Net. By using P-Net, we get the pixel-wise labels related to 8 components, i.e. two eyes, two eyebrows, nose, upper and lower lips, inner mouth, facial skin, hair, and background <ref type="bibr" target="#b10">[11]</ref>.</p><p>Let M = {M <ref type="bibr" target="#b0">(1)</ref> , · · · , M (C) } ∈ R m×n×C denote the pixel-wise face labelling masks. Here, M</p><formula xml:id="formula_0">(c) i,j ∈ [0, 1], s.t. c M (c)</formula><p>i,j = 1 denotes the probability pixel X i,j belongs to the c-th component, predicted by P-Net, c = 1, · · · , C with C = 8. We use soft labels (probabilistic outputs) in this paper. In the preliminary implementation, we also tested our model while using hard labels (binary outputs), i.e. each value M (c) i,j denotes whether X i,j belongs to the c-th component. Because it is almost impossible to get absolutely precise pix-wise face labels, using hard labels occasionally yields deformation in the border area between adjacent components.</p><p>Notes: We note that an existing face parsing model [11] is adopted here, as this paper is mainly to explore how to use facial composition information to boost the performance of photo-sketch synthesis. Specially, we here apply the P-Net <ref type="bibr" target="#b10">[11]</ref> pre-trained for face photos to both photos and sketches.</p><p>Although P-Net is not specially designed or learned for sketches, fortunately, we obtain fairly good parsing results. This might due to the fact that P-Net is capable of extracting high-semantic features from a face sketch. Besides, P-Net involves the consistency of two adjacent pixels and Conditional Random Filed (CRF) inference <ref type="bibr" target="#b10">[11]</ref>, both of which characterize local-dependencies inside an image (photo/sketch) and boost the robustness of P-Net.</p><p>Some examples are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, where facial components are distinguished in colors. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>  works fine for most faces, but may fail in cases. Specially, it is hard for P-Net to precisely detect the hair regions for most face photos/sketches. Besides, P-Net may fail to detect eyes (e.g. the example in the forth row, first column) or part of facial skins (e.g. the example in the bottom row, first column). Note that face decomposition could more or less introduce errors, however we make no manual intervention or selection to guarantee the claimed overall accuracy. Besides, replacing P-Net with some more advanced face parsing method, e.g. MaskGAN <ref type="bibr" target="#b46">[47]</ref>, does improve the synthesis performance. Related code and results have been released on the project page of this work: https://github.com/fei-hdu/ca-gan. In addition, in our CA-GAN and SCA-GAN, the input photo/sketch and its corresponding parsing masks are complementary to each other. Even if the parsing masks are not precise enough, the generator is expected to produce a high-quality face sketch/photo. As will be shown in Section IV, the apparent performance improvement of our methods over cGAN further reflects the effectiveness of the parsing results. We expect that an advanced face parsing model will be complementary to our approach, but it is slightly beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Composition-aided GAN (CA-GAN)</head><p>In the proposed framework, we first utilize paired inputs including a face photo and the corresponding pixel-wise face labels for generating the portrait. Second, we propose an compositional reconstruction loss, to focus training on hardgenerated components and delicate facial structures. Next, we employ a perceptual loss function to encourage the synthesized image and real image to be perceptually similar.</p><p>1) Generator Architecture: The architecture of the generator in CA-GAN is presented in <ref type="figure" target="#fig_1">Fig. 2</ref>. For clarity, we illustrate sizes of feature maps in the format of width × height×the number of channels. In our case, the generator needs to translate two inputs (i.e., the face photo X and the face labelling masks M) into a single output Y. Because X and M are of different modalities, we propose to use distinct encoders to model them. The corresponding encoders are referred to as Appearance Encoder and Composition Encoder, respectively. The outputs of these two encoders are concatenated at the bottleneck layer for the decoder <ref type="bibr" target="#b47">[48]</ref>. In this way, the information of both facial details and composition can be well modelled respectively. This architecture is different from previous works, where generator typically includes an encoder and a decoder <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>The architectures of the encoder, decoder, and discriminator are exactly the same as those used in <ref type="bibr" target="#b5">[6]</ref> but without dropout, following the shape of a "U-Net". Specifically, we concatenate all channels at layer i in both encoders with those at layer n−i in the decoder. Details of the network will be found in Part III-E. We note that we use the network in <ref type="bibr" target="#b5">[6]</ref> here because it is a milestone in the image-to-image translation community and has shown appealing results in various tasks. Nevertheless, our proposed techniques are complementary for arbitrary cGANs frameworks.</p><p>In the preliminary experiment, we test the network with one single encoder that takes the cascade of X and M, i.e. [X, M <ref type="bibr" target="#b0">(1)</ref> , · · · , M (C) ] ∈ R m×n×(d+C) , as the input. This network is the most straightforward solution for simultaneously encoding the face photo and the composition masks. Experimental results show that using this structure decreases the face sketch recognition accuracy by about 2 percent and yield slightly blurred effects in the area of hair.</p><p>2) Compositional Loss: Previous approaches of cGANs have found it beneficial to mix the GAN objective with a pixelwise reconstruction loss for various tasks, e.g. image translation <ref type="bibr" target="#b5">[6]</ref> and super-resolution reconstruction <ref type="bibr" target="#b12">[13]</ref>. Besides, using the normalized L 1 distance encourage less blurring than the L 2 distance. We therefore use the normalized L 1 distance between the generated sketch Y and the target Y in the computation of reconstruction loss. We introduce the compositional reconstruction loss starting from the standard reconstruction loss for image generation. a) Global Reconstruction Loss: In previous works about cGANs, the pixel-wise reconstruction loss is calculated over the whole image. For distinction, we refer to it as global reconstruction loss in this paper. Suppose both Y and Y have shape m × n. The global reconstruction loss is expressed as:</p><formula xml:id="formula_1">L L1,global (Y, Y) = 1 mn Y − Y 1 .<label>(1)</label></formula><p>In the global pixel loss, the L 1 loss related to the c th component, c = 1, 2, · · · , C, can be expressed as:</p><formula xml:id="formula_2">L (c) L1,global = 1 mn Y M (c) − Y M (c) 1 ,<label>(2)</label></formula><formula xml:id="formula_3">with L L1,global = c L (c) L1,global .</formula><p>Here, denotes the pixelwise product operation. As all the pixels are treated equally in the global reconstruction loss, large components (e.g. background and facial skin) contribute more to learn the generator than small components (e.g. eyes and mouth).</p><p>b) Compositional Reconstruction Loss: In this paper, we introduce a weighting factor, γ c , to balance the distinct reconstruction loss of each component. Specially, inspired by the idea of balanced cross-entropy loss <ref type="bibr" target="#b11">[12]</ref>, we set γ c by inverse component frequency. Let 1 be a m×n matrix of ones. When we adopt the soft facial labels, M (c) ⊗1 is the sum of the possibilities every pixel belonging to the c th component. Here, ⊗ denotes the convolutional operation. If we adopt the hard facial labels, it becomes the number of pixels belonging to the c th component. The component frequency is thus</p><formula xml:id="formula_4">M (c) ⊗1 mn . So we set γ c = mn M (c) ⊗1 and multiply it with L (c)</formula><p>L1,global , resulting in the balanced L 1 loss:</p><formula xml:id="formula_5">L (c) L1,cmp = 1 M (c) ⊗ 1 Y M (c) − Y M (c) 1 (3)</formula><p>Obviously, the balanced L 1 loss is exactly the normalized L 1 loss across the related compositional region.</p><p>The compositional reconstruction loss is defined as,</p><formula xml:id="formula_6">L L1,cmp (Y, Y) = C c=1 L (c) L1,cmp .<label>(4)</label></formula><p>As γ c is broadly in inverse proportion to the component size, it reduces the loss contribution from large components. From the other aspect, it high-weights the losses assigned to small and hard-generated components. Thus the compositional loss focus training on hard components with tiny details, and prevents the vast number of pixels of unimportant component (e.g. background) or easy component (e.g. facial skin) from overwhelming the generator during training. c) Compositional Loss: In practice, we use a weighted average of the global reconstruction loss and compositional reconstruction loss:</p><formula xml:id="formula_7">L cmp (Y, Y) = αL L1,cmp + (1 − α)L L1,global ,<label>(5)</label></formula><p>where α ∈ [0, 1] is used to balance the global reconstruction loss and the compositional pixel loss. We adopt this form in our experiments and set α = 0.7, as it yields slightly improved perceptual comfortability over the compositional loss. In the following, we refer to the weighted reconstruction loss as compositional loss.</p><p>3) Perceptual Loss: In addition, the synthesized image and target image should have similar high-frequency representations and the same identity, which are critical in human perceived quality <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. To this end, we encourage them to have similar feature representations <ref type="bibr" target="#b12">[13]</ref> as computed by a pre-trained face recognition network, VGGFace <ref type="bibr" target="#b50">[51]</ref>. The perceptual loss is expressed as:</p><formula xml:id="formula_8">L vggf ace = 1 |S| l∈S ψ l (y) − ψ l (G(x)) 2 ,<label>(6)</label></formula><p>where ψ(·) denotes the inference process of VGGFace, ψ l (·) denotes the outputs of the l-th layer in the VGGFace; S is the set of selected layers; |S| is the number of selected layers.</p><p>Features in different layers of VGGFace contain both highfrequency and identity information. Minimizing the perceptual loss for early layers tends to produce images that contain textures indistinguishable from y. In contrast, using the loss for higher layers preserves face identity and overall spatial structure <ref type="bibr" target="#b12">[13]</ref>. We therefore select both early and high layers, including the conv1-1, conv5-1, and conv5-3 layers, in our experiments. Using a perceptual loss encourages the output image to be perceptually similar to the target image with the same identity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Objective:</head><p>We express the adversarial loss of CA-GAN as <ref type="bibr" target="#b5">[6]</ref>: <ref type="figure">M)</ref>))].</p><formula xml:id="formula_9">L adv (G, D) = E X,M,Y∼p data (X,M,Y) [log D(X, M, Y)] + E X,M∼p data (X,M) [log(1 − D(X, M, G(X,</formula><p>(7) Similar to the settings in <ref type="bibr" target="#b5">[6]</ref>, we do not add a Gaussian noise z as the input.</p><p>Finally, we use a combination of the adversarial loss, the compositional loss, and the perceptual loss to learn the generator. We aim to solve:</p><formula xml:id="formula_10">(G * , D * ) = arg min G max D L adv + λL cmp + γL vggf ace ,<label>(8)</label></formula><p>where λ and γ are weighting factors. We set them to be 10 and 5, respectively, in the implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Stacked Refinement Network</head><p>Finally, we use stacked CA-GAN (SCA-GAN) to further boost the quality of the generated sketch portrait <ref type="bibr" target="#b13">[14]</ref>. The architecture of SCA-GAN is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>SCA-GAN includes two-stage GANs, each comprises a generator and a discriminator, which are sequentially denoted by G (1) , D <ref type="bibr" target="#b0">(1)</ref> , G <ref type="bibr" target="#b1">(2)</ref> , D <ref type="bibr" target="#b1">(2)</ref> . Stage-I GAN yields an initial portrait, Y <ref type="bibr" target="#b0">(1)</ref> , based on the given face photo X and pix-wise label masks M. Afterwards, Stage-II GAN takes {X, Y <ref type="bibr" target="#b0">(1)</ref> , M} as inputs to rectify defects and add compelling details, yielding a refined sketch portrait, Y <ref type="bibr" target="#b1">(2)</ref> . Here, X and Y (1) are concatenated and input into the appearance encoder of G <ref type="bibr" target="#b1">(2)</ref> .</p><p>Note that stacking the cGAN also be of benefit. Besides, stacking more than two CA-GANs would further boost the performance. However, using a stack of GANs monotonically increase both the computational complexity and model size, we therefore use a stack of two CA-GANs in the rest of this work, unless otherwise specified. Corresponding analysis will be presented in Part IV-B5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Network Architectures</head><p>In this work, every photo or sketch is represented in the RGB color space. Following cGAN <ref type="bibr" target="#b5">[6]</ref>, let Ci/j denote a Convolution-InstanceNorm-LeakyReLU layer with i input channels and j output channels. TCi/j denotes a TransposedConvolution-InstanceNorm-ReLU. All convolutions are 4 × 4 spatial filters applied with stride 2. Assume that the generator G (k) involves an appearance encoder E   <ref type="bibr" target="#b0">(1)</ref> and D <ref type="bibr" target="#b1">(2)</ref> . The last layer in the decoder is followed by a Tanh function, and the last layer of the discriminator is followed by a Sigmoid function. Besides, InstanceNorm is not applied to the first layer in the encoder. We use leaky ReLUs with slope 0.2 in all the encoders and discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Optimization</head><p>To optimize our networks, we alternate between one gradient descent step on D, then one step on G. We use minibatch SGD and apply the Adam solver. For clarity, we illustrate the optimization procedure of SCA-GAN in Algorithm 1. We trained our models on a single Pascal Titan Xp GPU. When we used a training set of 500 samples, it took about 3 hours to train CA-GAN and 6 hours to train SCA-GAN.</p><p>Algorithm 1 Optimization procedure of SCA-GAN (for sketch synthesis).</p><p>Input: a set of training instances, in form of triplet:</p><p>{a face photo X, pix-wise label masks M, a target sketch Y }; iteration time t = 0, max iteration T ; Output: optimal G (1) , D <ref type="bibr" target="#b0">(1)</ref> , G <ref type="bibr" target="#b1">(2)</ref> , D <ref type="bibr" target="#b1">(2)</ref> ; initial G (1) , D <ref type="bibr" target="#b0">(1)</ref> , G <ref type="bibr" target="#b1">(2)</ref> , D <ref type="bibr" target="#b1">(2)</ref> ; for t = 1 to T do 1. Randomly select one training instance: { a face photo X, pix-wise label masks M, a target sketch Y. } 2. Estimate the initial sketch portrait:</p><formula xml:id="formula_11">Y (1) = G (1) (X, M) 3.</formula><p>Estimate the refined sketch portrait:</p><formula xml:id="formula_12">Y (2) = G (2) (X, M, Y (1) ) 4. Update D (1) : D (1) * = arg min D (1) L adv (G (1) , D (1) ) 5. Update D (2) : D (2) * = arg min D (2) L adv (G (2) , D (2) ) 6. Update G (1) : G (1) * = arg max G (1) L adv (G (1) , D (1) ) + λL L 1 (Y, Y (1) ) 7. Update G (2) : G (2) * = arg max G (2) L adv (G (2) , D (2) ) + λL L 1 (Y, Y (2) ) end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we will first introduce the experimental settings and then present a series of empirical results to verify the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Datasets:</head><p>We conducted experiments on two widely used and public available datasets: the CUHK Face Sketch (CUFS) dataset <ref type="bibr" target="#b51">[52]</ref> and the CUFSF dataset <ref type="bibr" target="#b52">[53]</ref>. The composition of these datasets are briefly introduced below.</p><p>• The CUFS dataset consists of 606 face photos from three datasets: the CUHK student dataset <ref type="bibr" target="#b53">[54]</ref> (188 persons), the AR dataset <ref type="bibr" target="#b54">[55]</ref> (123 persons), and the XM2VTS dataset <ref type="bibr" target="#b55">[56]</ref> (295 persons). For each person, there are one face photo and one face sketch drawn by the artist. • The CUFSF dataset includes 1194 persons <ref type="bibr" target="#b56">[57]</ref>. In the CUFSF dataset, there are lighting variation in face photos and shape exaggeration in sketches. Thus the CUFSF dataset is very challenging. For each person, there are one face photo and one face sketch drawn by the artist. Dataset partition. There are great divergences in the experimental settings among existing works. In this paper, we follow the most used settings presented in <ref type="bibr" target="#b1">[2]</ref>, and split the dataset in the following ways. For the CUFS dataset, 268 face photo-sketch pairs (including 88 pairs from the CUHK student dataset, 80 pairs from the AR dataset, and 100 pairs from the XM2VTS dataset) are selected for training, and the rest are for testing. For the CUFSF dataset, 250 pairs are selected for training, and the rest 944 pairs are for testing.</p><p>Preprocessing. Following existing methods <ref type="bibr" target="#b1">[2]</ref>, all these face images (photos and sketches) are geometrically aligned relying on three points: two eye centers and the mouth center. The aligned images are cropped to the size of 250 × 200.</p><p>In the CUFS and CUFSF datasets, landmarks of each face photo/sketch are released. For the face images beyond these datasets (Part IV-E), we use MTCNN <ref type="bibr" target="#b57">[58]</ref> for landmark detection. MTCNN is pre-trained and released by the corresponding authors, and has been widely used in face-related tasks.</p><p>In the proposed method, the input image should be of fixed size, e.g. 256 × 256. In the default setting of <ref type="bibr" target="#b5">[6]</ref>, the input image is resized from an arbitrary size to 256×256. However, we observed that resizing the input face photo will yield serious blurred effects and great deformation in the generated sketch <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b31">[32]</ref>. In contrast, by padding the input image to the target size, we can obtain considerable performance improvement (as will be seen in Part IV-B1). We therefore use zero-padding for cGAN, CA-GAN, SCA-GAN, as well as their model variants across all the experiments.</p><p>2) Criteria: In this work, we choose the Fréchet Inception distance (FID) to evaluate the realism and variation of synthesized photos and sketches <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>. FID measures the Earth-Mover Distance (EMD) between the distribution of generated samples and that of the ground-truth samples, in the feature space. In this paper, the 2048-dimensional feature of the Inception-v3 network pre-trained on ImageNet is used <ref type="bibr" target="#b60">[61]</ref>. Lower FID values mean closer distances between synthetic and real data distributions. FID has been widely used in image generation tasks and shown highly consistency with human perception. In our experiments, we use all the test samples to compute the FID.</p><p>We additionally adopt the Feature Similarity Index Metric (FSIM) <ref type="bibr" target="#b61">[62]</ref> between a synthesized image and the corresponding ground-truth image to objectively assess the quality of the synthesized image. In FSIM, the phase congruency (PC) and the image gradient magnitude (GM) is employed as features, and the feature similarity between a test image and its corresponding reference image is employed as the quality index. Notably, although FSIM works well for evaluating quality of natural images and has become a prevalent metric in the face photo-sketch synthesis community, it is of low consistency with human perception for synthesized face photos and sketches <ref type="bibr" target="#b62">[63]</ref>.</p><p>Finally, we statistically evaluate the face recognition accuracy while using the ground-truth photo/sketche as the probe image and synthesized photos/sketches as the images in the gallery. Null-space Linear Discriminant Analysis (NLDA) <ref type="bibr" target="#b63">[64]</ref> is employed to conduct the face recognition experiments. NLDA is a face recognition method and is derived from Linear discriminant analysis (LDA) for solving the small sample size problem. In the experiment section, we use NLDA to denote the face recognition accuracy by using NLDA. We repeat each face recognition experiment 20 times by randomly splitting the data and report the average accuracy.</p><p>We use the proposed architecture for both sketch synthesis and photo synthesis. In the following context, we present a series of experiments:</p><p>• First, we perform ablation study on face photo-sketch synthesis on the CUFS dataset (see Part IV-B); • Second, we perform face photo-sketch synthesis on the CUFS and CUFSF datasets and compare with existing advanced methods (see Part IV-C and Part IV-D); • Third, we conduct experiments on faces in the wild to verify whether the proposed method is robust to lighting and pose variations (see Part IV-E); and • Finally, we verify that the proposed techniques speed up and stabilize the training procedure (see Part IV-F). Our code and results are publicly available at: https://github. com/fei-hdu/ca-gan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>We first evaluate the effectiveness of our design choices, including (i) using zero-padding instead of resizing in preprocessing, (ii) using face composition (pixel-wise label masks M) as auxiliary input, (iii) the compositional loss L cmp , (iv) the perceptual loss L vggf ace , and (v) stacked refinement (stack). To this end, we construct several model variants and separately conduct photo synthesis and sketch synthesis experiments on the CUFS dataset. Results are shown in <ref type="table" target="#tab_2">Table  I</ref> and discussed below. 1) Preprocessing: We have used zero-padding and resizing in cGAN, respectively. As shown in <ref type="table" target="#tab_2">Table I</ref>, using zeropadding instead of resizing dramatically improve the performance for both sketch synthesis and photo synthesis, in terms of all the three indices. Such considerable performance improvement demonstrates the significance of keeping image ratios. As a result, we use zero-padding for all the models across all the experiments.</p><p>2) Face composition masks: <ref type="table" target="#tab_2">Table I</ref> shows that using the compositional masks as auxiliary input significantly improve the realism of the synthesized face images. Specially, it decreases FID by 2.7 (43.2 → 40.5) for sketch synthesis and by 36.5 (117.6 → 81.1) for photo synthesis. Besides, this improves the face recognition accuracy from 89.0 to 98.8 for photo synthesis, suggesting that compositional information is essential for photo-based face recognition. 3) Compositional loss: <ref type="table" target="#tab_2">Table I</ref> shows that using compositional loss decreases FID by 0.8 (40.5 → 39.7) for sketch synthesis, while both the image and the compositional masks are used as input. We conduct additional experiments without using the compositional masks as input. Corresponding results show that compositional loss decreases FID by 3.0 (43.2 → 39.7) for sketch synthesis and by 34.5 (117.6 → 81.1) for photo synthesis. This highlights our motivation that focusing training on hard components is key. 4) Perceptual loss: Using the perceptual loss L vggf ace significantly reduces the FID values for both sketch synthesis and photo synthesis. Specially, it decreases the FID by 3.6 (39.7 → 36.1) for sketch synthesis and 39.8 (81.1 → 41.3) for photo sketch. Such comparison results demonstrate that the perceptual loss significantly improve the realism of synthesized face sketches and photos. Besides, the perceptual loss significantly improve the face recognition accuracy by about 20 (79 → 99) for photo synthesis, on the CUFSF dataset. 5) Stacked refinement: As shown in <ref type="table" target="#tab_2">Table I</ref>, stacked cGANs dramatically decrease the FID of cGAN from 43.2 to 36.6 for sketch synthesis, and from 117.6 to 104.3 for photo synthesis. This suggests that stacked refinement is effective for improving the realism of synthesized images. Likewise, compared to CA-GAN, SCA-GAN further decreases FID by 3.5 (39.7 → 36.2) for sketch synthesis and by 1.0 (41.3 → 40.3) for photo synthesis. Besides, SCA-GAN achieves better results than stacked cGANs.</p><p>In addition, we have evaluated the performance of stacking different number of cGANs or our CA-GANs, respectively. Experiments are conducted on face sketch synthesis on the CUFS database. The corresponding results are shown in <ref type="table" target="#tab_2">Table  III</ref> and visualized in <ref type="figure" target="#fig_4">Fig. 4</ref>. Here, one stage denotes no stacking, and stack-k denotes stacking k cGANs or CA-GANs, with k = 2, 3, 4. Obviously, the variants of CA-GAN consistently outperform those of cGAN. Specially, by stacking more CA-GANs, we generally generate better sketches, with decreasing FID, slightly increasing NLDA, and nearly invariable FSIM. In contrast, as we stack more cGANs, the FID decreases initially and begins to increase when four cGANs are used. In addition, stacking more than two GANs effects the performance slightly, which is possibly due to the limited number of training examples. However, using a stack of GANs monotonically increase both the computational complexity and model size. We therefore use a stack of two CA-GANs in the rest of this work, unless otherwise specified.</p><p>To conclude, the proposed approaches significantly improve the realism of synthesized face photos/sketches, with comparative or improved FSIM and NLDA scores. Besides, the performance gains shown in <ref type="table" target="#tab_2">Table I</ref> indicate that the effect of the proposed approaches is at least partly additive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Face Sketch Synthesis</head><p>In this section, we compare the proposed methods with a number of existing advanced methods, including MRF <ref type="bibr" target="#b20">[21]</ref>, MWF <ref type="bibr" target="#b64">[65]</ref>, SSD <ref type="bibr" target="#b18">[19]</ref>, MrFSPS <ref type="bibr" target="#b21">[22]</ref>, RSLCR <ref type="bibr" target="#b1">[2]</ref>, MRNF <ref type="bibr" target="#b2">[3]</ref>, BFCN <ref type="bibr" target="#b65">[66]</ref>, DGFL <ref type="bibr" target="#b66">[67]</ref>, BP-GAN <ref type="bibr" target="#b7">[8]</ref>, and cGAN <ref type="bibr" target="#b5">[6]</ref>. Synthesized images of existing methods are released by corresponding authors at: http://www.ihitworld.com/. All these methods and ours follow the same experimental settings. We don't compare with some recently published works, e.g. <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, because experimental settings in these works are specially designed according to their motivations and different from ours. <ref type="table" target="#tab_2">Table II</ref> show the results, where "avg." denotes the average value of each criterion across the CUFS dataset and CUFSF dataset. Obviously, our final model, SCA-GAN, significantly decreases the previous state-of-the-art FID by a large margin across both datasets. Besides, CA-GAN obtains the second best FID values. This demonstrates that our methods dramatically improve the realism of the synthesized sketches, compared to existing methods.</p><p>In addition, our methods are highly comparable with previous methods, in terms of FSIM and NLDA. Note that FSIM is designed for evaluating the quality degradation of photos caused by blurring, noise, or compression, and is not suitable for evaluating visual quality of sketches <ref type="bibr" target="#b62">[63]</ref>. Besides, Y. Blau and T. Michaeli prove mathematically that distortion and perceptual quality are at odds with each other <ref type="bibr" target="#b67">[68]</ref>. In other words, the FSIM value increases, the perceptual quality must be worse. They also show that GANs approaches the perception-distortion bound. Thus the inferiority in the FISM value doesn't mean worse perceptual quality. We report FSIMs here just because it has been widely used in existing works.</p><p>Moreover, the face sketch recognition accuracy is strongly correlated with FSIM, or the fidelity of a synthesized sketch <ref type="bibr" target="#b68">[69]</ref>. A lower FSIM score generally corresponds to lower face sketch recognition accuracy. Thus, CA-GAN and SCA-GAN also show slight inferiority in NLDA. Note that our VGGFace loss network is pre-trained on photos. The perceptual loss is expected to add high-frequency constraints on synthesized sketches and has led to better FID scores (as shown in <ref type="table" target="#tab_2">Table  II</ref>). Besides, as will be seen in <ref type="table" target="#tab_2">Table IV</ref>, the perceptual loss significantly improves the performance of face photo synthesis with respect to all the three indices. It is promising to boost   <ref type="bibr" target="#b53">[54]</ref>, the AR dataset <ref type="bibr" target="#b54">[55]</ref>, the XM2VTS dataset <ref type="bibr" target="#b55">[56]</ref>, and the CUFSF dataset <ref type="bibr" target="#b56">[57]</ref>, sequentially.</p><formula xml:id="formula_13">(a) (b) (d) (e) (f) (c) (g) (h) (i) (a)</formula><p>the face sketch recognition performance by using a specifically designed loss network. <ref type="figure" target="#fig_6">Fig. 5</ref> presents some synthesized face sketches from different methods on the CUFS dataset and the CUFSF dataset. Due to space limitation, we only compare to several advanced methods here. Obviously, MrFSPS, RSLCR, and BFCN yield serious blurred effects and great deformation in various facial parts. In contrast, GANs based methods can generate sketchlike textures (e.g. hair region) and shadows. However, BP-GAN yields over-smooth sketch portraits, and cGAN yields deformations in synthesized sketches, especially in the mouth region. Notably, CA-GAN alleviates such artifacts, and SCA-GAN almost eliminates them.</p><p>To conclude, both the qualitative and quantitative evaluations shown in <ref type="table" target="#tab_2">Table II</ref> and <ref type="figure" target="#fig_6">Fig. 5</ref> demonstrate that both CA-GAN and SCA-GAN are capable of generating quality sketches. Specially, our methods achieve significantly gain in realism of synthesized sketches over previous state-ofthe-art methods. Besides, our methods perform on par with previous state-of-art methods in term of FSIM and face sketch recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Face Photo Synthesis</head><p>We exchange the roles of the sketch and photo in the proposed model, and evaluate the face photo synthesis performance. To our best knowledge, only few methods have been proposed for face photo synthesis. Here we compare the proposed method with one advanced method: MrFSPS <ref type="bibr" target="#b21">[22]</ref>. As shown in <ref type="table" target="#tab_2">Table IV</ref>, both CA-GAN and SCA-GAN significantly outperform existing methods in general, according to all these criteria. In other words, SCA-GAN and CA-GAN approach the perception-distortion bound. Specially, CA-GAN reduces previous best (average) FID from 60.9 to 32.7 for photo synthesis; and SCA-GAN achieves better performance than CA-GAN. In addition, both CA-GAN and  <ref type="bibr" target="#b53">[54]</ref>, the AR dataset <ref type="bibr" target="#b54">[55]</ref>, the XM2VTS dataset <ref type="bibr" target="#b55">[56]</ref>, and the CUFSF dataset <ref type="bibr" target="#b56">[57]</ref>, sequentially. SCA-GAN considerably improve the face recognition accuracy by about 5 and 20 percent on the CUFS dataset and CUFSF dataset, respectively. Such considerable performance improvement demonstrates that our method can produce both perceptually realistic and identity-preserving face photos. <ref type="figure" target="#fig_7">Fig.6</ref> shows examples of synthesized photos. Obviously, results of MrFSPS are heavily blurred. Besides, there is serious degradation in the synthesized photos by using cGAN. In contrast, the photos generated by CA-GAN or SCA-GAN consistently show considerable improvement in perceptual quality. Results of CA-GAN and SCA-GAN express more natural colors and details. Recall the quantitative evaluations shown in <ref type="table" target="#tab_2">Table IV</ref>, we can safely draw the conclusion that our methods are capable of generating natural face photos while preserving the identity of the input sketch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robustness Evaluation</head><p>To verify the generalization ability of the learned model, we apply the model trained on the CUFS training dataset to faces in the wild.  </p><formula xml:id="formula_14">(a) (b) (d) (c) (a) (b) (d) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Lighting and Pose Variations:</head><p>We first apply the learned models to a set of face photos with lighting variation and pose variation. <ref type="figure" target="#fig_8">Fig. 7</ref> shows some synthesized results from cGAN, CA-GAN, and SCA-GAN. Clearly, results of cGAN exists blurring and inky artifacts over dark regions. In contrast, the photos produced by CA-GAN and SCA-GAN show less artifacts and express improved details over the eye and mouth regions. Besides, results of SCA-GAN show the best quality.</p><p>2) Face photo-sketch synthesis of national celebrities: We further test the learned models on the photos and sketches of national celebrities. All these photos and sketches are downloaded from the web and adopted as input. These images contain different lighting conditions and backgrounds compared with the images in the training set. <ref type="figure" target="#fig_9">Fig. 8</ref> shows the synthesized sketches and photos. Obviously, our results express more natural textures and details than cGAN, for both sketch synthesis and photo synthesis. Both CA-GAN and SCA-GAN show outstanding generalization ability in the sketch synthesis task. The synthesized photos here are dissatisfactory. This might be due to the great divergence between the input sketches in terms of textures and styles. It is necessary to further improve the generalization ability of the photo synthesis models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Analysis of the Training Procedure</head><p>We finally analyse the training procedure of cGAN <ref type="bibr" target="#b5">[6]</ref>, CA-GAN, and SCA-GAN. To this end, we conduct sketch syn- <ref type="figure">Fig. 9</ref>. Reconstruction errors about sketch synthesis and photo synthesis during the training process, on the CUFS dataset. train denotes the reconstruction error on the training set, and test that on the testing set. thesis and photo synthesis experiments on the CUFS dataset, respectively. During training, after every epoch, we apply the learned model to the training/testing subset and calculate the corresponding reconstruction errors (Global L1 loss) on each subset. <ref type="figure">Fig. 9</ref> shows the reconstruction errors, where train denotes the reconstruction error on the training set, and test that on the testing set. For clarity, these curves are plot on semilog coordinate.</p><p>Obviously, there are relatively larger fluctuations in loss curves of cGAN, in contrast to those of CA-GAN and SCA-GAN. Besides, the reconstruction errors of both CA-GAN and SCA-GAN drop faster initially and are lower than that of cGAN. Finally, SCA-GAN shows slight superiority over CA-GAN in terms of the reconstruction errors. These observations demonstrate that the proposed approaches considerably speed the training up and stabilize it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Summary</head><p>In this part, we briefly summary the conclusions we could draw from the experimental results.</p><p>• First, our model is capable of generating both visually realistic and identity-preserving face sketches/photos over a wide range of challenging data. Specially, our model significantly decrease previous state-of-the-art FID from 36.2 to 26.2 for sketch synthesis, and from 60.9 to 30.5 for photo synthesis; • Second, both using face composition as supplemental input and training with the compositional loss increase the realism of synthesized sketches/photos; • Third, the perceptual loss significantly improves the quality of synthesized photos, but contributes little to face sketch synthesis. It is promising to using a face sketch recognition network to boost the quality of synthesized sketches; • Forth, stacking CA-GANs generally improve the quality of synthesized face photos/sketches; • Fifth, the proposed approaches speed the training up and stabilize it; and</p><p>• Finally, both CA-GAN and SCA-GAN are of significantly improved generalization ability, especially for face sketch synthesis. It is challenging to produce perceptually comfortable photos from face sketches in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a novel composition-aided generative adversarial network (CA-GAN) for face photo-sketch synthesis. Our approach dramatically improves the realism of the synthesized face photos and sketches over previous state-of-the-art methods. We hope that the presented approach can support applications of other image generation problems. Besides, it is essential to develop models that can handle photos/sketches with great variations in head poses, lighting conditions, and styles. To this end, exploring hierarchical deep features <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref> and using multi-task learning <ref type="bibr" target="#b71">[72]</ref> might be promising solutions. Exciting work remains to be done to qualitatively evaluate the quality of synthesized sketches and photos. Here the reliability of FID is indeterminate, because the dimension of deep features is dramatically higher than the number of photo/sketh samples. Using dimension reduction <ref type="bibr" target="#b72">[73]</ref> before computating FID might be a solution. Finally, it is meaningful to apply face photo-sketch synthesis algorithms to practices, such as image privacy protection <ref type="bibr" target="#b73">[74]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of parsing results produced by P-Net<ref type="bibr" target="#b10">[11]</ref>. For each sample, from left to right are photo, parsing result of photo, sketch, and parsing result of the sketch, sequentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Generator architecture of the proposed composition-aided generative adversarial network (CA-GAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Pipeline of the proposed stacked composition-aided generative adversarial network (SCA-GAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>c</head><label></label><figDesc>, and a decoder Dec (k) , with k = 1, 2. SCA-GAN composes of:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Results of face sketch synthesis while stacking different number of cGANs or CA-GANs, respectively, on the CUFS database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Photo, (b) MrFSPS [3], (c) RSLCR[2], (d) FCN [1], (e) BP-GAN [8], (f) cGAN [4], (g) CA-GAN, (h)stack-CA-GAN, and (i) Sketch drawn by artist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Examples of synthesized face sketches on the the CUFS dataset and the CUFSF dataset. (a) Photo, (b) MrFSPS [22], (c) RSLCR[2], (d) BFCN [66], (e) BP-GAN [8], (f) cGAN [6], (g) CA-GAN, (h) SCA-GAN, and (i) Sketch drawn by artist. From top to bottom, the examples are selected from the CUHK student dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Examples of synthesized face photos. (a) Sketch drawn by artist, (b) MrFSPS [22], (c) cGAN, (d) CA-GAN, (e) SCA-GAN, and (f) ground-truth photo. From top to bottom, the examples are selected from the CUHK student dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Robustness to lighting and pose variations. (a) Photo, (b) cGAN, (c) CA-GAN, and (d) SCA-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Face photo-sketch synthesis results of national celebrities. (a) Input sketch/photo, (b) cGAN, (c) CA-GAN, (d) SCA-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>:</head><label></label><figDesc>Cc/64 → C64/128 → C128/256 → C256/512 → C512/512 → C512/512 → C512/512 → C512/512. c is sequentially 3, 8, 6, and 8 for E TC1024/512 → TC1536/512 → TC1536/512 → TC1536/512 → TC1536/256 → TC768/128 → TC384/64 → TC192/3; and • Discriminator: Cl/64 → C64/128 → C128/256 → C256/512 → C512/1. l is sequentially 14(= 3 + 3 + 8) and 17(= 3 + 3 + 8 + 3) for D</figDesc><table><row><cell>(1) a , E c , E (1) a , E (2)</cell><cell>(2)</cell></row></table><note>• Encoderc ;• Decoder:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDY ON THE CUFS DATASET. OUR BASELINE IS CGAN WITH A FACE PHOTO/SKETCH AS INPUT. THE BEST INDEX IN EACH COLUMN IS SHOWN IN BOLDFACE. ↓ INDICATES LOWER IS BETTER, WHILE ↑ HIGHER IS BETTER. IN MODEL VARIANTS, X DENOTES THE INPUT FACE PHOTO, M THE FACE COMPOSITION, Lcmp THE COMPOSITIONAL LOSS, L vggf ace THE PERCEPTUAL LOSS L vggf ace , AND stack THE STACKED REFINEMENT.TABLE II PERFORMANCE ON FACE SKETCH-SYNTHESIS ON THE CUFS DATASET AND CUFSF DATASET. THE BEST AND SECOND BEST INDICES IN EACH LINE ARE SHOWN IN BOLDFACE AND UNDERLINE FORMAT, RESPECTIVELY. ↓ INDICATES LOWER IS BETTER, WHILE ↑ HIGHER IS BETTER. HERE, WE COMPARE THE</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Model Variants</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Sketch Synthesis</cell><cell>Photo Synthesis</cell></row><row><cell>Remarks</cell><cell></cell><cell cols="4">Preprocessing Input X</cell><cell cols="3">Input M Lcmp</cell><cell cols="2">L vggf ace</cell><cell>Stack</cell><cell>FID↓</cell><cell cols="2">FSIM↑ NLDA↑</cell><cell>FID↓</cell><cell>FSIM↑ NLDA↑</cell></row><row><cell></cell><cell></cell><cell cols="2">Resizing</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>90.8</cell><cell></cell><cell>62.1</cell><cell>48.2</cell><cell>132.4</cell><cell>65.6</cell><cell>16.9</cell></row><row><cell>cGAN</cell><cell></cell><cell cols="2">Zero-Padding</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>43.2</cell><cell></cell><cell>71.1</cell><cell>95.5</cell><cell>117.6</cell><cell>74.8</cell><cell>89.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Zero-Padding</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>43.8</cell><cell></cell><cell>69.2</cell><cell>85.5</cell><cell>103.3</cell><cell>77.0</cell><cell>94.1</cell></row><row><cell></cell><cell></cell><cell cols="2">Zero-Padding</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>40.5</cell><cell></cell><cell>71.3</cell><cell>95.2</cell><cell>81.1</cell><cell>78.0</cell><cell>98.8</cell></row><row><cell></cell><cell></cell><cell cols="2">Zero-Padding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>39.7</cell><cell></cell><cell>71.2</cell><cell>95.6</cell><cell>81.1</cell><cell>78.6</cell><cell>98.6</cell></row><row><cell>CA-GAN</cell><cell></cell><cell cols="2">Zero-Padding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>36.1</cell><cell></cell><cell>71.3</cell><cell>95.8</cell><cell>41.3</cell><cell>78.6</cell><cell>98.5</cell></row><row><cell cols="2">stack-cGAN</cell><cell cols="2">Zero-Padding</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>36.6</cell><cell></cell><cell>71.2</cell><cell>95.3</cell><cell>104.3</cell><cell>75.5</cell><cell>88.0</cell></row><row><cell>SCA-GAN</cell><cell></cell><cell cols="2">Zero-Padding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.2</cell><cell></cell><cell>71.6</cell><cell>95.7</cell><cell>40.3</cell><cell>79.5</cell><cell>99.4</cell></row><row><cell cols="15">PROPOSED METHODS WITH A NUMBER OF EXISTING ADVANCED METHODS, INCLUDING MRF [21], MWF [65], SSD [19], MRFSPS [22], RSLCR[2],</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="11">BFCN [66], MRNF [3], DGFL [67], BP-GAN [8], AND CGAN [6].</cell></row><row><cell>Criterion</cell><cell cols="2">Dataset</cell><cell></cell><cell cols="4">Traditional methods</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Deep methods</cell><cell></cell><cell>(Deep) GANs based methods</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">MRF MWF SSD MrFSPS RSLCR</cell><cell cols="4">BFCN MRNF DGFL</cell><cell>BP-GAN cGAN CA-GAN SCA-GAN</cell></row><row><cell></cell><cell></cell><cell>CUFS</cell><cell>68.2</cell><cell>87.0</cell><cell cols="2">97.2</cell><cell>105.5</cell><cell cols="2">106.9</cell><cell>99.7</cell><cell>84.5</cell><cell>94.4</cell><cell></cell><cell>86.1</cell><cell>43.2</cell><cell>36.1</cell><cell>34.2</cell></row><row><cell>FID↓</cell><cell cols="2">CUFSF</cell><cell>70.7</cell><cell>86.9</cell><cell cols="2">75.9</cell><cell>87.2</cell><cell cols="2">126.4</cell><cell>123.9</cell><cell>-</cell><cell>-</cell><cell></cell><cell>42.9</cell><cell>29.2</cell><cell>19.6</cell><cell>18.2</cell></row><row><cell></cell><cell></cell><cell>avg.</cell><cell>69.5</cell><cell>87.0</cell><cell cols="2">86.6</cell><cell>96.8</cell><cell cols="2">116.6</cell><cell>111.8</cell><cell>-</cell><cell>-</cell><cell></cell><cell>64.5</cell><cell>36.2</cell><cell>27.8</cell><cell>26.2</cell></row><row><cell></cell><cell></cell><cell>CUFS</cell><cell>70.4</cell><cell>71.4</cell><cell cols="2">69.6</cell><cell>73.4</cell><cell cols="2">69.6</cell><cell>69.3</cell><cell>71.4</cell><cell>70.6</cell><cell></cell><cell>69.1</cell><cell>71.1</cell><cell>71.3</cell><cell>71.6</cell></row><row><cell>FSIM↑</cell><cell cols="2">CUFSF</cell><cell>69.6</cell><cell>70.3</cell><cell cols="2">68.2</cell><cell>68.9</cell><cell cols="2">66.5</cell><cell>66.2</cell><cell>-</cell><cell>-</cell><cell></cell><cell>68.2</cell><cell>72.8</cell><cell>72.7</cell><cell>72.9</cell></row><row><cell></cell><cell></cell><cell>avg.</cell><cell>70.0</cell><cell>70.9</cell><cell cols="2">68.9</cell><cell>71.2</cell><cell cols="2">68.1</cell><cell>67.8</cell><cell>-</cell><cell>-</cell><cell></cell><cell>68.7</cell><cell>72.0</cell><cell>72.0</cell><cell>72.3</cell></row><row><cell></cell><cell></cell><cell>CUFS</cell><cell>88.4</cell><cell>92.3</cell><cell cols="2">91.1</cell><cell>97.7</cell><cell cols="2">98.0</cell><cell>92.1</cell><cell>96.9</cell><cell>98.7</cell><cell></cell><cell>93.1</cell><cell>95.5</cell><cell>95.8</cell><cell>95.7</cell></row><row><cell>NLDA↑</cell><cell cols="2">CUFSF</cell><cell>45.6</cell><cell>73.8</cell><cell cols="2">70.6</cell><cell>75.4</cell><cell cols="2">75.9</cell><cell>69.8</cell><cell>-</cell><cell>-</cell><cell></cell><cell>67.5</cell><cell>80.9</cell><cell>78.1</cell><cell>78.0</cell></row><row><cell></cell><cell></cell><cell>avg.</cell><cell>67.0</cell><cell>83.2</cell><cell cols="2">80.9</cell><cell>86.6</cell><cell cols="2">87.0</cell><cell>81.0</cell><cell>-</cell><cell>-</cell><cell></cell><cell>85.3</cell><cell>88.2</cell><cell>86.9</cell><cell>86.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">PERFORMANCE OF FACE SKETCH SYNTHESIS WHILE STACKING</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">DIFFERENT NUMBER OF CGANS OR CA-GANS, RESPECTIVELY.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">one stage stack-2 stack-3 stack-4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FID↓</cell><cell></cell><cell>cGAN CA-GAN</cell><cell cols="2">43.2 39.7</cell><cell>36.6 34.2</cell><cell></cell><cell>35.6 32.8</cell><cell>36.6 32.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FSIM↑</cell><cell></cell><cell>cGAN CA-GAN</cell><cell cols="2">71.1 71.2</cell><cell>71.2 71.6</cell><cell></cell><cell>71.1 71.6</cell><cell>71.2 71.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NLDA↑</cell><cell></cell><cell>cGAN CA-GAN</cell><cell cols="2">95.5 95.6</cell><cell>95.3 95.7</cell><cell></cell><cell>95.5 95.9</cell><cell>95.2 95.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>ON FACE PHOTO SYNTHESIS ON THE CUFS AND CUFSF DATASETS. THE BEST AND SECOND BEST RESULTS IN EACH ROW ARE SHOWN IN BOLDFACE AND UNDERLINE FORMAT, RESPECTIVELY. ↓ INDICATES LOWER IS BETTER, WHILE ↑ HIGHER IS BETTER.</figDesc><table><row><cell>Criterion</cell><cell>Dataset</cell><cell cols="4">MrFSPS cGAN CA-GAN SCA-GAN</cell></row><row><cell></cell><cell>CUFS</cell><cell>92.0</cell><cell>88.7</cell><cell>41.3</cell><cell>40.3</cell></row><row><cell>FID↓</cell><cell>CUFSF</cell><cell>95.6</cell><cell>33.1</cell><cell>24.4</cell><cell>20.6</cell></row><row><cell></cell><cell>avg.</cell><cell>93.8</cell><cell>60.9</cell><cell>32.7</cell><cell>30.5</cell></row><row><cell></cell><cell>CUFS</cell><cell>80.3</cell><cell>76.2</cell><cell>78.6</cell><cell>79.5</cell></row><row><cell>FSIM↑</cell><cell>CUFSF</cell><cell>79.3</cell><cell>79.5</cell><cell>83.7</cell><cell>84.5</cell></row><row><cell></cell><cell>avg.</cell><cell>79.8</cell><cell>77.8</cell><cell>81.1</cell><cell>82.0</cell></row><row><cell></cell><cell>CUFS</cell><cell>96.7</cell><cell>94.8</cell><cell>98.5</cell><cell>99.4</cell></row><row><cell>NLDA↑</cell><cell>CUFSF</cell><cell>59.4</cell><cell>77.5</cell><cell>99.8</cell><cell>99.9</cell></row><row><cell></cell><cell>avg.</cell><cell>78.2</cell><cell>86.1</cell><cell>99.2</cell><cell>99.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comprehensive survey to face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="30" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random sampling for fast face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="215" to="227" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Markov random neural fields for face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xinbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1142" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural probabilistic graphical model for face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of image synthesis and editing with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Science and Technology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="660" to="674" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Back projection: an effective postprocessing method for gan-based face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Face synthesis from visual attributes via sketch using conditional VAEs and GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00077</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust face sketch synthesis via generative adversarial fusion of priors and parametric sigmoid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chia-Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1163" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-objective convolutional learning for face labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3451" to="3459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="2999" to="3007" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">StackGAN++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1947" to="1962" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data-driven vs. modeldriven: Fast face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="page" from="214" to="221" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast preprocessing for robust face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artifical Intelligence</title>
		<meeting>International Joint Conference on Artifical Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4530" to="4536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stylizing face images via multiple exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="135" to="145" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Face sketchphoto synthesis and retrieval using sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1213" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time exemplar-based face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="800" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2216" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face photo-sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1955" to="67" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple representations-based face sketch-photo synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2201" to="2215" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face sketch synthesis via sparse representation-based greedy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2466" to="77" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust face sketch style synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">220</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transductive face sketchphoto synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1364" to="1376" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multimodal distance metric learning using click constraints for image ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiaokang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dacheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4014" to="4024" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contentadaptive sketch portrait generation by decompositional representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Izquierdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning meets game theory: Bregman-based algorithms for interactive deep generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tembine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context-aware semantic inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4398" to="4411" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High-quality facial photo-sketch synthesis using multi-adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
	<note>13th IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face sketch synthesis in the wild via deep patch representation-based probabilistic graphical model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="172" to="183" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dual-transfer face sketchphoto synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="642" to="657" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A deep collaborative framework for face photosketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3096" to="3108" />
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face sketch synthesis by multidomain adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1419" to="1428" />
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bionic face sketch generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Compositional model-based sketch generator in facial entertainment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="904" to="915" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to hallucinate face images via component generation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artifical Intelligence</title>
		<meeting>International Joint Conference on Artifical Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4537" to="4543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2458" to="2467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
	<note>neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Stylebank: An explicit representation for neural image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5077" to="5086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unsupervised imageto-image translation with stacked cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">MaskGAN: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11922</idno>
		<ptr target="https://github.com/switchablenorms/CelebAMask-HQ" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Skeleton-aided articulated motion generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Biologically inspired image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="210" to="219" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Blind image quality prediction by exploiting multi-level deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="432" to="442" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Face photo-sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1955" to="1967" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Coupled information-theoretic encoding for face photo-sketch recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Face photo recognition using sketch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="257" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The AR face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVC</title>
		<imprint>
			<date type="published" when="1998-06" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. 24</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">XM2VTSDB: the extended M2VTS database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Audio-and Video-Based Biometric Person Authentication</title>
		<meeting>the International Conference on Audio-and Video-Based Biometric Person Authentication</meeting>
		<imprint>
			<date type="published" when="1999-04" />
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The FERET evaluation methodology for face recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rizvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Are GANs created equal? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10337</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision,&quot; computer vision and pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">FSIM: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2378</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Evaluation on synthesized face sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="991" to="1000" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A new lda-based face recognition system which can solve the small sample size problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1713" to="1726" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Markov weight fields for face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1091" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">End-to-end photosketch generation via fully convolutional representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 5th ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="627" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep graphical feature learning for face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3574" to="3580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Training-free synthesized face sketch recognition using image quality assessment metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Hierarchical deep click feature prediction for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hongyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dacheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Spatial pyramidenhanced NetVLAD with and weighted triplet loss for place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multitask autoencoder model for recovering human poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chaoqun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dacheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="5060" to="5068" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Discriminative low-rank preserving projection for dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">105768</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Image privacy protection by identifying sensitive objects via deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhenzhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Baopeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jianping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1005" to="1016" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
