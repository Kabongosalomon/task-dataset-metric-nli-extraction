<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent semantic segmentation methods exploit encoderdecoder architectures to produce the desired pixel-wise segmentation prediction. The last layer of the decoders is typically a bilinear upsampling procedure to recover the final pixel-wise prediction. We empirically show that this oversimple and data-independent bilinear upsampling may lead to sub-optimal results.</p><p>In this work, we propose a data-dependent upsampling (DUpsampling) to replace bilinear, which takes advantages of the redundancy in the label space of semantic segmentation and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. The main advantage of the new upsampling layer lies in that with a relatively lowerresolution feature map such as 1 16 or 1 32 of the input size, we can achieve even better segmentation accuracy, significantly reducing computation complexity. This is made possible by 1) the new upsampling layer's much improved reconstruction capability; and more importantly 2) the DUpsampling based decoder's flexibility in leveraging almost arbitrary combinations of the CNN encoders' features. Experiments demonstrate that our proposed decoder outperforms the state-of-the-art decoder, with only ∼20% of computation. Finally, without any post-processing, the framework equipped with our proposed decoder achieves new state-of-the-art performance on two datasets: 88.1% mIOU on PASCAL VOC with 30% computation of the previously best model; and 52.5% mIOU on PASCAL Context.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fully convolutional networks (FCNs) <ref type="bibr" target="#b20">[21]</ref> have achieved tremendous success in dense pixel prediction applications such as semantic segmentation, for which the algorithm is asked to predict a variable for each pixel of an input image and is a fundamental problem in computer vision. The great achievement of FCNs results from powerful features Root, stride 2 257x257</p><p>Block1 , stride 2 129x129</p><p>Block2 , stride 2 65x65</p><p>Block3 , stride 2 33x33</p><p>Block4 , stride 1, rate 2, 33x33</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv1x1, 33x33</head><p>Upsample by 4 times</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat 129x129</head><p>Conv3x3, 129x129</p><p>Conv3x3, 129x129</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv3x3, 129x129</head><p>Bilinear by 4 times</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction 513x513</head><p>Argmax Input image 513x513</p><p>Encoder Decoder Convolutional Decoder Upsampling <ref type="figure">Figure 1</ref>: An example of the encoder-decoder architecture used by DeepLabv3+. Its decoder fuses low-level features of downsample ratio = 4 and upsamples high-level features before merging them. Finally, bilinear upsampling is applied to restore the full-resolution prediction. "rate" denotes the atrous rate in atrous convolution. extracted by CNNs. Importantly, the sharing convolutional computation mechanism makes training and inference computationally very efficient. In the original FCNs, several stages of strided convolutions and/or spatial pooling reduce the final image prediction typically by a factor of 32, thus losing fine image structure information and leading to inaccurate predictions, especially at the object boundaries. DeepLab <ref type="bibr" target="#b2">[3]</ref> applies atrous (a.k.a dilation) convolutions, achieving large receptive fields while maintaining a higher-resolution feature map. Alternatively the encoder-decoder architecture is often used to address this problem. The encoder-decoder architecture views the backbone CNN as an encoder, responsible for encoding a raw input image into lower-resolution feature maps (e.g., <ref type="bibr" target="#b0">1</ref> r of the input image size with r = 8, 16, or 32). Afterwards, a decoder is used to recover the pixelwise prediction from the lower-resolution feature maps. In previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> convolutional decoder yields high-resolution feature maps and bilinear upsampling is finally applied to the resulting feature maps to obtain the desired pixel-wise prediction. The decoder commonly fuses low-level features to capture the fine-grained information lost by convolution and pooling operations in CNNs. A standard DeepLabv3+ encoderdecoder architecture is illustrated in <ref type="figure">Fig. 1</ref>. A drawback of the oversimple bilinear upsampling is its limited capability in recovering the pixel-wise prediction accurately. Bilinear upsampling does not take into account the correlation among the prediction of each pixel since it is data independent. As a consequence, the convolutional decoder is required to produce relatively higher-resolution feature maps in order to obtain good final prediction (e.g., <ref type="bibr">1 4</ref> or 1 8 of the input size). This requirement causes two issues for semantic segmentation.</p><p>1) The overall strides of the encode must be reduced very aggressively by using multiple atrous convolutions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref>. The price is much heavier computation complexity and memory footprint, hampering the training on large data and deployment for real-time applications.</p><p>For example, in order to achieve state-of-the-art performance, the recent DeepLabv3+ <ref type="bibr" target="#b4">[5]</ref> reduces the overall strides of its encoder by four times (from 32 to 8). Thus inference of DeepLabv3+ is very slow.</p><p>2) The decoder is often needed to fuse features at very low levels. For example, DeepLabv3+ fuses features of downsample ratio = 4 1 in block1 as shown in <ref type="figure">Fig. 1</ref>. It is because that the fineness of the final prediction is actually dominated by the resolution of the fused low-level fea-1 downsample ratio denotes the ratio of the resolution of the feature maps to that of the input image. tures due to the inability of bilinear. As a result, in order to produce high-resolution prediction, the decoder has to fuse the high-resolution features at a low level. This constraint narrows down the design space of the feature aggregation and therefore is likely to cause a suboptimal combination of features to be aggregated in the decoder. In experiments, we show that a better feature aggregation strategy can be found if the feature aggregation can be designed without the constraint imposed by the resolution of feature maps.</p><p>In order to tackle the aforementioned issues caused by bilinear, here we propose a new data-dependent upsampling method, termed DUpsamling, to recover the pixel-wise prediction from the final outputs of the CNNs, replacing bilinear upsampling used extensively in previous works. Our proposed DUpsampling takes advantages of the redundancy in the segmentation label space and proves to be capable of accurately recovering the pixel-wise prediction from relatively coarse CNNs outputs, alleviating the need for precise responses from the convolutional decoder.</p><p>As a result, the encoder is not required to overly reduce its overall strides, dramatically reducing the computation time and memory footprint of the whole segmentation framework. Meanwhile, due to the effectiveness of DUpsampling, it allows the decoder to downsample the fused features to the lowest resolution of feature maps before merging them. This downsampling not only reduces the amount of computation of the decoder, but much more importantly it decouples the resolution of fused features and that of the final prediction. This decoupling allows the decoder to make use of arbitrary feature aggregation and thus a better feature aggregation can be leveraged so as to boost the segmentation performance as much as possible.</p><p>Finally, DUpsampling can be seamlessly incorporated into the network with a standard 1×1 convolution and thus needs no ad-hoc coding. Our overall framework is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>We summarize our main contributions as follows.</p><p>• We propose a simple yet effective Data-dependent Upsampling (DUpsampling) method to recover the pixelwise segmentation prediction from the coarse outputs of the convolutional decoder, replacing the incapable bilinear used extensively in previous methods. • Taking advantages of our proposed DUpsampling, we can avoid overly reducing the overall strides of the encoder, significantly reducing the computation time and memory footprint of the semantic segmentation method by a factor of 3 or so. • DUpsampling also allows the decoder to downsample the fused features to the lowest resolution of feature maps before merging them. The downsampling not only reduces the amount of computation of the decoder dramatically but also enlarges the design space of feature aggregation, allowing a better feature aggregation to be exploited in the decoder. • Together with the above contributions, we propose a new decoder scheme, which compares favourably with state-of-the-art decoders while using ∼20% amount of computation. With the proposed decoder, the framework illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref> achieves new state-of-the-art performance: mIOU of 88.1% 2 on PASCAL VOC <ref type="bibr" target="#b7">[8]</ref> with only 30% computation of the previous best framework of DeepLabv3+ <ref type="bibr" target="#b4">[5]</ref>. We also set a new mIOU record of 52.5% on the PASCAL Context dataset <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Efforts have been devoted to improve pixel-wise predictions with FCNs. They can be roughly divided into two groups: atrous convolution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref> and encoder-decoder architectures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Atrous convolution. A straightforward approach is to reduce the overall strides of backbone CNNs by dropping some strided convolutions or pooling layers. However, simply reducing these strides would diminish the receptive field of convolution networks substantially, which proves to be crucial to semantic segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b18">19]</ref>. Atrous convolutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">33]</ref> can be used to keep the receptive field unchanged, meanwhile not downsampling the feature map resolution too much. The major drawback of atrous convolutions is much heavier computation complexity and larger memory requirement as the size of those atrous convolutional kernels, as well as the resulted feature maps, become much larger <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Encoder-decoder architectures. Encoder-decoder architectures are proposed to overcome the drawback of atrous convolutions and are widely used for semantic segmentation. DeconvNet <ref type="bibr" target="#b23">[24]</ref> uses stacked deconvolutional layers to recover the full-resolution prediction gradually. The method has the potential to produce high-resolution prediction but is difficult to train due to many parameters introduced by the decoder. SegNet <ref type="bibr" target="#b1">[2]</ref> shares a similar idea with DeconvNet but uses indices in pooling layers to guide the recovery process, resulting in better performance. Re-fineNet <ref type="bibr" target="#b15">[16]</ref> further fuse low-level features to improve the performance. Recently, DeepLabv3+ <ref type="bibr" target="#b4">[5]</ref> takes advantages of both encoder-decoder architectures and atrous convolution, achieving best reported performance on a few datasets to date. Although efforts have been spent on designing a better decoder, so far almost none of them can bypass the restriction on the resolutions of the fused features and exploit better feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this section, we firstly reformulate semantic segmentation with our proposed DUpsampling and then present the adaptive-temperature softmax function which makes the training with DUsampling much easier. Finally, we show how the framework can be largely improved with the fusion of downsampled low-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Beyond Bilinear: Data-dependent Upsampling</head><p>In this section, we firstly consider the simplest decoder, which is only composed of upsampling. Let F ∈ RH ×W ×C denote the final outputs of the encoder CNNs and Y ∈ {0, 1, 2, ..., C} H×W be the ground truth label map, where C andC denotes the number of classes of segmentation and the number of channels of the final outputs, respectively. Y is commonly encoded with one-hot encoding, i.e., Y ∈ {0, 1} H×W ×C . Note that F is typically of a factor of 16 or 32 in spatial size of the ground-truth Y. In other words,H H =W W = 1 16 or 1 32 . Since semantic segmentation requires per-pixel prediction, F needs to be upsampled to the spatial size of Y before computing the training loss.</p><p>Typically in semantic segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12]</ref>, the training loss function is formulated as:</p><formula xml:id="formula_0">L(F, Y) = Loss(softmax(bilinear(F)), Y)).<label>(1)</label></formula><p>Here Loss is often the cross-entropy loss, and bilinear is used to upsample F to the spatial size of Y. We argue that bilinear unsampling may not be the optimal choice here. As we show in the experiments (Sec. 4.1.1), bilinear is oversimple and has an inferior upper bound in terms of reconstructing (best possible reconstruction quality). In order to compensate the loss caused by bilinear, the employed deep network is consequently required to output higher-resolution</p><formula xml:id="formula_1">2H x 2W x N/4 H x W x C F R W Rearrange 1 x C C x N 1 x N 2 x 2 x N/4</formula><p>DUpsampling <ref type="figure">Figure 3</ref>: The proposed DUpsampling. In the figure, DUpsampling is used to upsample the CNNs outputs F by twice. R denotes the resulting maps. W, computed with the method described in Sec. 3.1, is the inverse projection matrix of DUpsampling. In practice, the upsampling ratio is typically 16 or 32.</p><p>feature maps, which are input to the bilinear operator. As mentioned above, the solution is to apply atrous convolutions, with the price of high computation complexity. For example, reducing the overall strides from 16 to 8 incurs more than 3 times computation.</p><p>An important observation is that the semantic segmentation label Y of an image is not i.i.d. and there contains structure information so that Y can be compressed considerably, with almost no loss. Therefore, unlike previous methods, which upsample F to the spatial size of Y, we instead attempt to compress Y intoỸ ∈ RH ×W ×C and then compute the training loss between F andỸ. Note that F andỸ are of the same size.</p><p>In order to compress Y intoỸ, we seek a transform under some metric to minimize the reconstruction error between Y andỸ. Specifically, let r indicate the ratio of H toH, which is usually 16 or 32. Next, Y is divided into an H r × W r grid of sub-windows of size r × r (if H or W is not dividable by r, a padding is applied). For each sub-window S ∈ {0, 1} r×r×C , we reshape S into a vector v ∈ {0, 1} N , with N = r × r × C. Finally, we compress the vector v to a lower-dimensional vector x ∈ RC and then vertically and horizontally stack all x's to formỸ.</p><p>Although a variety of ways can be used to achieve the compression, we find that simply using linear projecting, i.e., multiplying v v v by a matrix P ∈ RC ×N works well in this case. Formally, we have,</p><formula xml:id="formula_2">x = Pv;ṽ = Wx,<label>(2)</label></formula><p>where P ∈ RC ×N is used to compress v into x. W ∈ R N ×C is the inverse projection matrix (a.k.a. reconstruction matrix) and used to reconstruct x back to v.ṽ is the reconstructed v. We have omitted the offset term here. In practice prior to the compression, v is centered by subtracting its mean over the training set.</p><p>The matrices P and W can be found by minimizing the reconstruction error between v andṽ over the training set.</p><p>Formally,</p><formula xml:id="formula_3">P * , W * = arg min P,W v ||v −ṽ|| 2 = arg min P,W v ||v − WPv|| 2 .<label>(3)</label></formula><p>This objective can be iteratively optimized with standard stochastic gradient descent (SGD). With an orthogonality constraint, we can simply use principal component analysis (PCA) <ref type="bibr" target="#b28">[29]</ref> to achieve a closed-form solution for the objective. UsingỸ as the target, we may pre-train the network with a regression loss by observing that the compressed labelsỸ is real-valued</p><formula xml:id="formula_4">L(F, Y) = ||F −Ỹ|| 2 .<label>(4)</label></formula><p>Thus any regression loss, 2 being a typical example as in Eq. (4), can be employed here. Alternatively, a more direct approach is to compute the loss in the space of Y. Therefore, instead of compressing Y intoỸ, we up-sample F with the learned reconstruction matrix W and then compute the pixel classification loss between the decompressed F and Y:</p><formula xml:id="formula_5">L(F, Y) = Loss(softmax(DUpsample(F)), Y).<label>(5)</label></formula><p>With linear reconstruction, DUpsample(F) applies linear upsampling of Wf f f to each feature f f f ∈ RC in the tensor F. Comparing with Eq. (1), we have replaced the bilinear upsampling with a data-dependent upsampling, learned from the ground-truth labels. This upsampling procedure is essentially the same as applying a 1×1 convolution along the spatial dimensions, with convolutional kernels stored in W. The decompression is illustrated in <ref type="figure">Fig. 3</ref>. Note that, besides the linear upsampling presented above, we have also conducted experiments using a nonlinear auto-encoder for upsampling. Training of the autoencoder is also to minimize the reconstruction loss, and is more general than the linear case. Empirically, we observe that the final semantic prediction accuracy is almost the same as using the much simpler linear reconstruction. Therefore we focus on using the linear reconstruction in the sequel.</p><p>Discussion with Depth-to-Space and Sub-pixel. The simplest linear form of DUpsample can be viewed as an improved version of Depth-to-Space in <ref type="bibr" target="#b27">[28]</ref> or Sub-pixel in <ref type="bibr" target="#b25">[26]</ref> with pre-computed upsampling filters. Depth-to-Space and Sub-pixel are typically used to upsample the inputs by a modest upsample ratio (e.g., ≤ 4), in order to avoid incurring too many trainable parameters resulting in difficulties in optimization. In contrast, as the upsampling filters in our method are pre-computed, the upsample ratio of DUpsamling can be very large (e.g., 16 or 32) if needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Incorporating DUpsampling with Adaptivetemperature Softmax</head><p>So far, we have shown that DUpsampling can be used to replace the incapable bilinear upsampling in semantic segmentation. The next step is to incorporate the DUpsampling into the encoder-decoder framework, resulting in an end-toend trainable system. While DUpsampling can be realized with a 1×1 convolution operation, incorporating directly it into the framework encounters difficulties in optimization.</p><p>Probably due to the W is computed with one-hot encoded Y, we find that the combination of vanilla softmax and DUpsampling has difficulty in producing sharp enough activation. As a result, the cross-entroy loss is stuck during the training process (as shown in experiment 4.1.4), which makes the training process slow to converge.</p><p>In order to tackle the issue, we instead employ the softmax function with temperature <ref type="bibr" target="#b12">[13]</ref>, which adds a temperature T into vanilla softmax function to sharpen/soften the activation of softmax.</p><formula xml:id="formula_6">softmax(z i ) = exp(z i /T ) j exp(z j /T ) .<label>(6)</label></formula><p>We find that T can be learned automatically using the standard back-propagation algorithm, eliminating the need for tuning. We show in experiments that this adaptivetemperature softmax makes training converge much faster without introducing extra hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Flexible Aggregation of Convolutional Features</head><p>The extremely deep CNNs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref> lead to the success in computer vision. However, the depth also causes the loss of fine-grained information essential to semantic segmentation. It has been shown by a number of works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5</ref>] that combining the low-level convolutional features can improve the segmentation performance significantly.</p><p>Let F be the eventual CNNs feature maps used to produce the final pixel-wise prediction by bilinear or aforementioned DUpsampling. F i and F last represent the feature maps at level i of the backbone and last convolutional feature maps of the backbone, respectively. For simplicity we focus on fusing one level of low-level features, but it is straightforward to extend it to multi-level fusion, which perhaps boosts the performance further. The feature aggregation in previous decoders shown in <ref type="figure">Fig. 1</ref> can be formulated as,</p><formula xml:id="formula_7">F = f (concat(F i , upsample(F last ))),<label>(7)</label></formula><p>where f denotes a CNN and upsample is usually bilinear. concat is a concatenation operator along the channel. As described above, this arrangement comes with two problems. 1) f is applied after upsampling. Since f is a CNN, whose amount of computation depends on the spatial size of inputs, this arrangement would render the decoder inefficient computationally. Moreover, the computational overhead prevents the decoder from exploiting features at a very low level.</p><p>2) The resolution of fused low-level features F i is equivalent to that of F, which is typically around 1 4 resolution of the final prediction due to the incapable bilinear used to produce the final pixel-wise prediction. In order to obtain high-resolution prediction, the decoder can only choose the feature aggregation with high-resolution low-level features.</p><p>In contrast, in our proposed framework, the responsibility to restore the full-resolution prediction has been largely shifted to DUpsampling. Therefore, we can safely downsample any level of used low-level features to the resolution of last feature maps F last (the lowest resolution of feature maps) and then fuse these features to produce final prediction, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. Formally, Eq. <ref type="formula" target="#formula_7">(7)</ref> is changed to,</p><formula xml:id="formula_8">F = f (concat(downsample(F i ), F last )),<label>(8)</label></formula><p>where downsample is bilinear in our case. This rearrangement not only keeps the features always to be computed efficiently at the lowest resolution, but also decouples the resolution of low-level features F i and that of the final segmentation prediction, allowing any level of features to be fused. In experiments, we show the flexible feature fusion enables us to exploit a better feature fusion to boost the segmentation performance as much as possible.</p><p>Only when cooperating with the aforementioned DUpsampling, the scheme of downsampling low-level features can work. Otherwise, the performance is bounded by the upper bound of the incapable upsampling method of the decoder. This is the reason why previous methods are required to upsample the low-resolution high-level feature maps back to the spatial size of fused low-level feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The proposed models are evaluated on the PASCAL VOC 2012 semantic segmentation benchmark <ref type="bibr" target="#b7">[8]</ref> and PAS-CAL Context benchmark <ref type="bibr" target="#b22">[23]</ref>. For both benchmarks, we measure the performance in terms of pixel intersectionover-union averaged across the present classes (i.e., mIOU).</p><p>PASCAL VOC is the dataset widely used for semantic segmentation. It consists of 21 classes including background. The splits of PASCAL VOC are 1, 464, 1, 449 and 1, 456 for training, validation and test, respectively. The ablation study of our work is conducted over its val set. Also, we report our performance over test set to compare with other state-of-the-art methods.</p><p>PASCAL Context is much larger than PASCAL VOC, including 4, 998 images for training and 5, 105 images for validation. Following previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>, we choose the most frequent 59 classes plus one background class (i.e., 60 classes in total) in our experiments. There is not a test server available and therefore we follow previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref> to report our result on val set.</p><p>Cityscapes is a large-scale benchmark for semantic urban scene parsing. It contains 2, 975 images for training, 500 images for validation and 1, 525 images for testing. Additionally, it also provides about 20, 000 weakly annotated images.</p><p>Implementation details.</p><p>For all ablation experiments on PASCAL VOC, we opt for ResNet-50 <ref type="bibr" target="#b10">[11]</ref> and Xception-65 <ref type="bibr" target="#b5">[6]</ref> as our backbone networks, both of which are modified as in <ref type="bibr" target="#b4">[5]</ref>. Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, we use "poly" as our learning rate policy for all experiments. The initial learning rate is set as 0.007 and total iteration is 30k for ablation experiments on PASCAL VOC. For all ResNet-based experiments, weight decay is set to 0.0001. The batch size is set to 48, but the batch normalization <ref type="bibr" target="#b14">[15]</ref> statistics are computed with a batch of 12 images. For all Xception-based experiments, weight decay is 0.00004. We use a batch size of 32 but compute the batch normalization statistics within a batch of 16 images. We follow the practice <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35]</ref> to use the weights pre-trained on ImageNet <ref type="bibr" target="#b6">[7]</ref> to initialize backbone networks. All weights of newly added layers are initialized with Gaussian distribution of variance 0.01 and mean 0. T in adaptive-temperature softmax is initialized to 1.C is set as 64 for ResNet-50 based experiments and 128 for Xception-65 based experiments. Finally, following previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>, we augment the training data by randomly scaling the images from 0.5 to 2.0 and left-right flipping them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>Our work focuses on the decoder part of the segmentation architecture. Therefore, for all ablation experiments, we use the same encoder, as shown in <ref type="figure">Fig. 1</ref>. The encoder yields the final feature maps with the 1 16 or 1 32 size of the original image. The decoder aims to decode the lowresolution feature maps into the prediction with the same resolution as the original image. In this section, we will investigate different decoder schemes, and demonstrate our proposed decoder's advantages. We make use of official train set instead of SBD <ref type="bibr" target="#b9">[10]</ref> since it provides more consis- tent annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">DUpsampling vs. Bilinear</head><p>First of all, we design experiments to show that the upper bound of bilinear is much lower than that of DUpsampling, which results in limited performance of bilinear. Specifically, we design a light-weight CNN including five convolutional layers with kernel size being 3 and stride of 2, which is fed with ground truth labels instead of raw images. Next, DUpsampling or bilinear is added on top of that to recover the pixel-wise prediction. This is similar to the decoder part in the encoder-decoder architecture. By training the two networks, with DUpsampling or bilinear as decoder respectively, the ability to restore the pixel-wise prediction can be quantitatively measured via their performance over the val set, which can be viewed as the upper bound of both methods. We use the training protocol described in implementation details to train the two networks, except that the total iterations and initial learning rate are set as 100k and 0.07, respectively. "output stride" indicates the ratio of input image spatial resolution to the final CNN feature maps resolution. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the upper bound performance of DUpsampling is well above that of bilinear both when output stride being 32 and 16.</p><p>Given the superior upper bound performance of DUpsampling, we further carry out experiments with raw input images. In the experiments, we employ ResNet-50 as the backbone network. Unsurprisingly, by merely replacing the bilinear with DUpsampling, the mIOU on PASCAL VOC val set is improved by 1.3 points and 1 point, when the output stride is 32 and 16 respectively, as shown in <ref type="table" target="#tab_0">Table  1</ref>. The improvement is significant because mIOU is strict. Interestingly, the DUpsampling of output stride being 32 achieves similar performance to the bilinear case of output stride being 16. This shows that the proposed DUpsampling may eliminate the need for expensive computationally highresolution feature maps from the CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Flexible aggregation of convolutional features</head><p>Due to the flexibility of our proposed decoder, we can employ any combination of features to improve segmentation performance, regardless of the resolution of fused features.  For ResNet-50, we experiment with many different combinations of features, as shown in <ref type="table" target="#tab_2">Table 2</ref>. The best one is the combination of conv1 3 + b3u6u3, achieving mIOU 74.20% over val set. Additionally, as shown in <ref type="table" target="#tab_2">Table 2</ref>, the amount of computation changes little when features at different levels are fused, which allows us to choose the best feature fusion without considering the price of computation incurred by the resolution of fused features.</p><p>In order to understand how the fusion works, we visualize the segmentation results with and without low-level features in <ref type="figure" target="#fig_2">Fig. 4</ref>. Intuitively, the one fusing low-level features yields more consistent segmentation, which suggests the downsampled low-level features are still able to refine the segmentation prediction substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparison with the vanilla bilinear decoder</head><p>We further compare our proposed decoder scheme with the vanilla bilinear decoder shown in <ref type="figure">Fig. 1</ref>, which fuses low-level features b1u2c3 (downsample ratio = 4). As shown in <ref type="table">Table 3</ref>, it achieves mIOU 73.26% on val set with ResNet-50 as the backbone. By replacing vanilla decoder with our proposed decoder in <ref type="figure" target="#fig_0">Fig. 2</ref>, the performance is improved to 74.03%. Because of the same low-level features used, the improvement should be due to the capable DUpsampling instead of bilinear used to restore the fullresolution prediction. Furthermore, we explore a better feature fusion conv1 3 + b3u6c3 for proposed deocder and improve the overall performance slightly to 74.20%. When the    <ref type="table">Table 3</ref>: mIOU over the PASCAL VOC val set when using different fusion strategies of features. bxuycz denotes low-level features named block x/unit y/conv z in ResNet or Xception. "ef" and "mf" respectively indicate "entry flow" and "middle flow" in Xception. "-" means out-of-memory. "ratio" denotes the ratio of the resolution of feature maps to the resolution of the input image (i.e., downsample ratio). "FLOPS" denotes the amount of computation of the decoders.  vanilla decoder uses the fusion of features, it incurs much heavier computation computation complexity and runs out of our GPUs memory due to the high resolution of conv1 3, which prevents the vanilla decoder from exploiting the lowlevel features. We also experiment our proposed decoder with Xception-65 as the backbone. Similarly, with the same lowlevel features efb2u1c3 (downsample ratio = 4), our proposed decoder improves the performance from 78.70% to 79.09%, as shown in <ref type="table">Table 3</ref>. When using a better low-level features mfb1u16c3 (downsample ratio = 16), the vanilla decoder just improves the performance negligibly by 0.04% because its performance is constrained by the incapable bilinear upsampling used to restore the full-resolution prediction. In contrast, our proposed decoder can still benefit a lot from the better feature fusion due to the use of much powerful DUpsampling. As shown in <ref type="table">Table 3</ref>, with the better feature fusion, the performance of our proposed decoder is improved to 79.67%. Moreover, since we downsample low-level features before fusing, our proposed decoder requires much fewer FLOPS than the vanilla decoder of the best performance, as shown in <ref type="table">Table 3</ref>.</p><p>Finally, we compare our proposed decoder with the vanilla bilinear decoder on the Cityscapes val set. Follow-Method mIOU (%) PSPNet <ref type="bibr" target="#b35">[36]</ref> 85.4 DeepLabv3 <ref type="bibr" target="#b3">[4]</ref> 85.7 EncNet <ref type="bibr" target="#b33">[34]</ref> 85.9 DFN <ref type="bibr" target="#b31">[32]</ref> 86.2 IDW-CNN <ref type="bibr" target="#b26">[27]</ref> 86.3 CASIA IVA SDN <ref type="bibr" target="#b8">[9]</ref> 86.6 DIS <ref type="bibr" target="#b21">[22]</ref> 86.8 DeepLabv3+ <ref type="bibr" target="#b4">[5]</ref> (Xception-65) 87.8 Our proposed (Xception-65) 88.1 ing <ref type="bibr" target="#b4">[5]</ref>, Xception-71 is used as our backbone and the number of iterations is increased to 90k with a initial learning rate being 0.01. As shown in <ref type="table" target="#tab_5">Table 4</ref>, under the same training and testing settings, our proposed decoder achieves a comparable performance with the vanilla one while using much less computation.  As mentioned before, the adaptive-temperature softmax eases the training of the proposed DUpsampling method. When training the framework with vanilla softmax with T being 1, it achieves 69.81% over val set, which is significantly lower than 73.15% of the counterpart with adaptivetemperature softmax. We further plot training losses for vanilla softmax and adaptive-temperature softmax in <ref type="figure" target="#fig_4">Fig.  5</ref>, which shows the advantage of this adaptive-temperature softmax.</p><p>Method mIOU (%) FCN-8s <ref type="bibr" target="#b20">[21]</ref> 37.8 CRF-RNN <ref type="bibr" target="#b36">[37]</ref> 39.3 HO CRF <ref type="bibr" target="#b0">[1]</ref> 41.3 Piecewise <ref type="bibr" target="#b16">[17]</ref> 43.3 VeryDeep <ref type="bibr" target="#b29">[30]</ref> 44.5 DeepLabv2 <ref type="bibr" target="#b2">[3]</ref> 45.7 RefineNet <ref type="bibr" target="#b15">[16]</ref> 47.3 EncNet <ref type="bibr" target="#b33">[34]</ref> 51.7 Our proposed (Xception-65) 51.4 Our proposed (Xception-71) 52.5 <ref type="table">Table 6</ref>: State-of-the-art methods on PASCAL Context val set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with state-of-the-art Methods</head><p>Finally, we compare the framework of our proposed decoder with state-of-the-art methods. To compete with these state-of-the-art methods, we choose Xception-65 as the backbone network and the best feature aggregation in the ablation study for our decoder.</p><p>Following previous methods, SBD <ref type="bibr" target="#b9">[10]</ref> and COCO <ref type="bibr" target="#b17">[18]</ref> are used to train the model as well. Specifically, the model is successively trained over COCO, SBD and PASCAL VOC trainval set, with the training protocol described in implementation details. Each round is initialized with the last round model and the base learning rate is reduced accordingly (i.e. 0.007 for COCO, 0.001 for SBD and 0.0001 for trainval). We use 500k iterations when training over COCO and 30k iterations for the last two rounds. Additionally, following previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, we make use of multi-scale testing and left-right flipping when inferring over test set.</p><p>As shown in <ref type="table" target="#tab_6">Table 5</ref>, our framework sets the new record on PASCAL VOC and improve the previous method DeepLabv3+ with the same backbone by 0.3%, which is significant due to the benchmark has been very competitive. Meanwhile, since our proposed decoder can eliminate the need for high-resolution feature maps, we employ output stride being 16 instead of 8 in DeepLabv3+ when inferring over test set. As a result, our whole framework only takes 30% computation of DeepLabv3+ (897.94B vs. 3055.35B in Multiply-Adds) to achieve the state-of-the-art performance. The performance of our proposed framework on PASCAL Context val set is shown in <ref type="table">Table 6</ref>. With Xception-71 as backbone, our framework sets the new stateof-the-art on this benchmark dataset without pre-training on COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a flexible and light-weight decoder scheme for semantic image segmentation. This novel decoder employs our proposed DUpsampling to produce the pixel-wise prediction, which eliminates the need for compu-tationally inefficient high-resolution feature maps from the underlying CNNs and decouples the resolution of the fused low-level features and that of the final prediction. This decoupling expands the design space of feature aggregation of the decoder, allowing almost arbitrary features aggregation to be exploited to boost the segmentation performance as much as possible. Meanwhile, our proposed decoder avoids upsampling low-resolution high-level feature maps back to the spatial size of high-resolution low-level feature maps, reducing the computation of decoder remarkably. Experiments demonstrate that our proposed decoder has advantages of both effectiveness and efficiency over the vanilla decoder extensively used in previous semantic segmentation methods. Finally, the framework with the proposed decoder attains the state-of-the-art performance while requiring much less computation than previous state-of-theart methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material:</head><p>In this supplementary material, we 1) provide our result on PASCAL VOC <ref type="bibr" target="#b7">[8]</ref> test set without COCO <ref type="bibr" target="#b17">[18]</ref> pretraining and 2) showcase more visualization results of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">PASCAL VOC without COCO Pre-training</head><p>In this experiment, following previous works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b33">34]</ref> without COCO pre-training, we train our model on SBD <ref type="bibr" target="#b9">[10]</ref> and then fine-tune it on official trainval set. We use the same training protocol as described in the main paper. The multi-scale testing and left-right flipping are employed when our model is evaluated on test set. No any post-processing is used. The final performance is obtained by uploading our test results to the official test server. As shown in <ref type="table">Table 7</ref>, our proposed framework surpasses previous published methods by a large margin.</p><p>Method mIOU (%) DPN <ref type="bibr" target="#b19">[20]</ref> 74.1 Piecewise <ref type="bibr" target="#b16">[17]</ref> 75.3 ResNet-38 <ref type="bibr" target="#b30">[31]</ref> 82.5 PSPNet <ref type="bibr" target="#b35">[36]</ref> 82.6 DFN <ref type="bibr" target="#b31">[32]</ref> 82.7 EncNet <ref type="bibr" target="#b33">[34]</ref> 82.9 Our proposed (Xception-65) 85.3 <ref type="table">Table 7</ref>: State-of-the-art methods on PASCAL VOC test set without COCO pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Visualization</head><p>The visualization results of our method are shown in <ref type="figure" target="#fig_5">Fig.  6</ref> and <ref type="figure" target="#fig_6">Fig. 7</ref>. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, without any postprocessing, the proposed method works very well in a lot of challenging cases. Small, distant and incomplete objects can be segmented well. Meanwhile, as shown in <ref type="figure" target="#fig_6">Fig. 7</ref>, although we employ "output stride" being 16 when evaluating, which results in low-resolution CNNs output feature maps, our model can still yield fine-grained segmentation due to the use of proposed DUpsamling.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>, a decoder consists of a few convolutional layers and a bilinear upsampling. The light-weight arXiv:1903.02120v3 [cs.CV] 5 Apr 2019 The framework with our proposed decoder. The major differences from the previous framework shown inFig. 1 are 1)all fused features are downsampled to the lowest features resolution before merging.2) The incapble bilinear is replaced with our proposed DUpsampling to recover the full-resolution prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Image w/o low-level features w/ low-level featuresGround truth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The prediction results with low-level features and without low-level features. ResNet-50 is used as the backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 1 . 4</head><label>14</label><figDesc>Impact of adaptive-temperature softmax</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Training losses for vanilla softmax and adaptivetemperature softmax.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization results from val set. The proposed method works reliably in a lot of challenging cases including small, distant and incomplete objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Visualization results from val set. The proposed method can yield fine-grained segmentation, with low-resolution CNNs output feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>mIOU over the PASCAL VOC val set of DUpsampling vs. bilinear upsampling. "output stride" indicates the ratio of input image spatial resolution to final output resolution. mIOU* denotes the upper bound.</figDesc><table><row><cell>Method</cell><cell cols="3">output stride mIOU (%) mIOU* (%)</cell></row><row><cell>bilinear</cell><cell>32</cell><cell>70.77</cell><cell>94.80</cell></row><row><cell>DUpsampling</cell><cell>32</cell><cell>72.09</cell><cell>99.90</cell></row><row><cell>bilinear</cell><cell>16</cell><cell>72.15</cell><cell>98.40</cell></row><row><cell>DUpsampling</cell><cell>16</cell><cell>73.15</cell><cell>99.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>mIOU over PASCAL VOC val set when using different fusion of features. bxuycz denotes low-level features named block x/unit y/conv z in ResNet. "FLOPS" denotes the amount of computation of the decoder including feature aggregation, convolutional decoder and the final upsampling.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>mIOU on the Cityscapes val set. Our proposed decoder with much less computation complexity achieves a similar performance as the vanilla decoder.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>State-of-the-art methods on PASCAL VOC test set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The results on PASCAL VOC test set can be found at http:// host.robots.ox.ac.uk:8080/anonymous/UYT221.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments The authors would like to thank Huawei Technologies for the donation of GPU cloud computing resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher order conditional random fields in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="524" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1610" to="02357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Stacked deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04943</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Knowledge adaptation for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04688</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Laurens Van Der Maaten, and Kilian Weinberger. Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
	<note>Anton Van Den Hengel, and Ian Reid</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep dual learning for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large kernel mattersimprove semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1743" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning object interactions and descriptions for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5859" to="5867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05847</idno>
		<title level="m">The devil is in the decoder</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Principal component analysis. Chemometrics and Intelligent Laboratory Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svante</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Geladi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bridging category-level and instance-level semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep gated attention networks for largescale street-level scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="702" to="714" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
