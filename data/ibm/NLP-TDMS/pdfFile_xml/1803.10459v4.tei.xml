<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graphite: Iterative Generative Modeling of Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Zweig</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
						</author>
						<title level="a" type="main">Graphite: Iterative Generative Modeling of Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graphs are a fundamental abstraction for modeling relational data. However, graphs are discrete and combinatorial in nature, and learning representations suitable for machine learning tasks poses statistical and computational challenges. In this work, we propose Graphite, an algorithmic framework for unsupervised learning of representations over nodes in large graphs using deep latent variable generative models. Our model parameterizes variational autoencoders (VAE) with graph neural networks, and uses a novel iterative graph refinement strategy inspired by low-rank approximations for decoding. On a wide variety of synthetic and benchmark datasets, Graphite outperforms competing approaches for the tasks of density estimation, link prediction, and node classification. Finally, we derive a theoretical connection between message passing in graph neural networks and mean-field variational inference.</p><p>Kearnes, S., McCloskey, K., Berndl, M., Pande, V., and Riley, P. Molecular graph convolutions: moving beyond -supervised learning with deep generative models. In Advances in Neural Information Processing Systems, 2014.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Latent variable generative modeling is an effective approach for unsupervised representation learning of highdimensional data <ref type="bibr" target="#b27">(Loehlin, 1998)</ref>. In recent years, representations learned by latent variable models parameterized by deep neural networks have shown impressive performance on many tasks such as semi-supervised learning and structured prediction <ref type="bibr">(Kingma et al., 2014;</ref><ref type="bibr" target="#b47">Sohn et al., 2015)</ref>. However, these successes have been largely restricted to specific data modalities such as images and speech. In particular, it is challenging to apply current deep generative models for large scale graph-structured data which arise in a wide variety of domains in physical sciences, information sciences, and social sciences.</p><p>To effectively model the relational structure of large graphs for deep learning, prior works have proposed to use graph Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s). neural networks <ref type="bibr" target="#b12">(Gori et al., 2005;</ref><ref type="bibr" target="#b40">Scarselli et al., 2009;</ref><ref type="bibr" target="#b4">Bruna et al., 2013)</ref>. A graph neural network learns nodelevel representations by parameterizing an iterative message passing procedure between nodes and their neighbors. The tasks which have benefited from graph neural networks, including semi-supervised learning <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref> and few shot learning <ref type="bibr" target="#b10">(Garcia &amp; Bruna, 2018)</ref>, involve encoding an input graph to a final output representation (such as the labels associated with the nodes). The inverse problem of learning to decode a hidden representation into a graph, as in the case of a latent variable generative model, is a pressing challenge that we address in this work.</p><p>We propose Graphite, a latent variable generative model for graphs based on variational autoencoding <ref type="bibr">(Kingma &amp; Welling, 2014)</ref>. Specifically, we learn a directed model expressing a joint distribution over the entries of adjacency matrix of graphs and latent feature vectors for every node. Our framework uses graph neural networks for inference (encoding) and generation <ref type="bibr">(decoding)</ref>. While the encoding is straightforward and can use any existing graph neural network, the decoding of these latent features to reconstruct the original graph is done using a multi-layer iterative procedure. The procedure starts with an initial reconstruction based on the inferred latent features, and iteratively refines the reconstructed graph via a message passing operation. The iterative refinement can be efficiently implemented using graph neural networks. In addition to the Graphite model, we also contribute to the theoretical understanding of graph neural networks by deriving equivalences between message passing in graph neural networks with mean-field inference in latent variable models via kernel embeddings <ref type="bibr" target="#b6">Dai et al., 2016)</ref>, formalizing what has thus far has been largely speculated empirically to the best of our knowledge <ref type="bibr" target="#b60">(Yoon et al., 2018)</ref>.</p><p>In contrast to recent works focussing on generation of small graphs e.g., molecules <ref type="bibr" target="#b61">(You et al., 2018;</ref><ref type="bibr" target="#b26">Li et al., 2018)</ref>, the Graphite framework is particularly suited for representation learning on large graphs. Such representations are useful for several downstream tasks. In particular, we demonstrate that representations learned via Graphite outperform competing approaches for graph representation learning empirically for the tasks of density estimation (over entire graphs), link prediction, and semi-supervised node classification on synthetic and benchmark datasets. arXiv:1803.10459v4 [stat.ML] 15 May 2019</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Throughout this work, we assume that all probability distributions admit absolutely continuous densities on a suitable reference measure. Consider a weighted undirected graph G = (V, E) where V and E denote index sets of nodes and edges respectively. Additionally, we denote the (optional) feature matrix associated with the graph as X ∈ R n×m for an m-dimensional signal associated with each node, for e.g., these could refer to the user attributes in a social network. We represent the graph structure using a symmetric adjacency matrix A ∈ R n×n where n = |V | and the entries A ij denote the weight of the edge between node i and j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Weisfeiler-Lehman algorithm</head><p>The Weisfeiler-Lehman (WL) algorithm <ref type="bibr" target="#b57">(Weisfeiler &amp; Lehman, 1968;</ref><ref type="bibr" target="#b7">Douglas, 2011)</ref> is a heuristic test of graph isomorphism between any two graphs G and G . The algorithm proceeds in iterations. Before the first iteration, we label every node in G and G with a scalar isomorphism invariant initialization (e.g., node degrees). That is, if G and G are assumed to be isomorphic, then an isomorphism invariant initialization is one where the matching nodes establishing the isomorphism in G and G have the same labels (a.k.a. messages). Let</p><formula xml:id="formula_0">H (0) = [h (l) 1 , h (l) 2 , · · · , h (l)</formula><p>n ] T denote the vector of initializations for the nodes in the graph at iteration l ∈ N ∪ 0. At every iteration l &gt; 0, we perform a relabelling of nodes in G and G based on a message passing update rule:</p><formula xml:id="formula_1">H (l) ← hash AH (l−1)<label>(1)</label></formula><p>where A denotes the adjacency matrix of the corresponding graph and hash : R n → R n is any suitable hash function e.g., a non-linear activation. Hence, the message for every node is computed as a hashed sum of the messages from the neighboring nodes (since A ij = 0 only if i and j are neighbors). We repeat the process for a specified number of iterations, or until convergence. If the label sets for the nodes in G and G are equal (which can be checked using sorting in O(n log n) time), then the algorithm declares the two graphs G and G to be isomorphic.</p><p>The "k-dim" WL algorithm extends the 1-dim algorithm above by simultaneously passing messages of length k (each initialized with some isomorphism invariant scheme). A positive test for isomorphism requires equality in all k dimensions for nodes in G and G after the termination of message passing. This algorithmic test is a heuristic which guarantees no false negatives but can give false positives wherein two non-isomorphic graphs can be falsely declared isomorphic. Empirically, the test has been shown to fail on some regular graphs but gives excellent performance on real-world graphs <ref type="bibr" target="#b44">(Shervashidze et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph neural networks</head><p>Intuitively, the WL algorithm encodes the structure of the graph in the form of messages at every node. Graph neural networks (GNN) build on this observation and parameterize an unfolding of the iterative message passing procedure which we describe next.</p><p>A GNN consists of many layers, indexed by l ∈ N with each layer associated with an activation η l and a dimensionality d l . In addition to the input graph A, every layer l ∈ N of the GNN takes as input the activations from the previous layer H (l−1) ∈ R n×d l−1 , a family of linear transformations F l : R n×n → R n×n , and a matrix of learnable weight parameters W l ∈ R d l−1 ×d l and optional bias parameters B l ∈ R n×d l . Recursively, the layer wise propagation rule in a GNN is given by:</p><formula xml:id="formula_2">H (l) ← η l   B l + f ∈F l f (A)H (l−1) W l  <label>(2)</label></formula><p>with the base cases H (0) = X and d 0 = m. Here, m is the feature dimensionality. If there are no explicit node features, we set H (0) = I n (identity) and d 0 = n. Several variants of graph neural networks have been proposed in prior work. For instance, graph convolutional networks (GCN) <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref> instantiate graph neural networks with a permutation equivariant propagation rule:</p><formula xml:id="formula_3">H (l) ← η l B l +ÃH (l−1) W l<label>(3)</label></formula><p>whereÃ = D −1/2 AD −1/2 is the symmetric diagonalization of A given the diagonal degree matrix D (i.e., D ii = (i,j)∈E A ij ), and same base cases as before. Comparing the above with the WL update rule in Eq.</p><p>(1), we can see that the activations for every layer in a GCN are computed via parameterized, scaled activations (messages) of the previous layer being propagated over the graph, with the hash function implicitly specified using an activation function η l .</p><p>Our framework is agnostic to instantiations of message passing rule of a graph neural network in Eq.</p><p>(2), and we use graph convolutional networks for experimental validation due to the permutation equivariance property. For brevity, we denote the output H for the final layer of a multi-layer graph neural network with input adjacency matrix A, node feature matrix X, and parameters W, B as H = GNN W,B (A, X), with appropriate activation functions and linear transformations applied at each hidden layer of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generative Modeling via Graphite</head><p>For generative modeling of graphs, we are interested in learning a parameterized distribution over adjacency matri- ces A. In this work, we restrict ourselves to modeling graph structure only, and any additional information in the form of node features X is incorporated as conditioning evidence.</p><p>In Graphite, we adopt a latent variable approach for modeling the generative process. That is, we introduce latent variable vectors Z i ∈ R k and evidence feature vectors X i ∈ R m for each node i ∈ {1, 2, · · · , n} along with an observed variable for each pair of nodes A ij ∈ R. Unless necessary, we use a succinct representation Z ∈ R n×k , X ∈ R n×m , and A ∈ R n×n for the variables henceforth. The conditional independencies between the variables can be summarized in the directed graphical model (using plate notation) in <ref type="figure" target="#fig_0">Figure 1</ref>. We can learn the model parameters θ by maximizing the marginal likelihood of the observed adjacency matrix conditioned on X:</p><formula xml:id="formula_4">max θ log p θ (A|X) = log Z p θ (A, Z|X)dZ<label>(4)</label></formula><p>Here, p(Z|X) is a fixed prior distribution over the latent features of every node e.g., isotropic Gaussian. If we have multiple graphs in our dataset, we maximize the expected log-likelihoods over all the corresponding adjacency matrices. We can obtain a tractable, stochastic evidence lower bound (ELBO) to the above objective by introducing a variational posterior q φ (Z|A, X) with parameters φ:</p><formula xml:id="formula_5">log p θ (A|X) ≥ E q φ (Z|A,X) log p θ (A, Z|X) q φ (Z|A, X)<label>(5)</label></formula><p>The lower bound is tight when the variational posterior q φ (Z|A, X) matches the true posterior p θ (Z|A, X) and hence maximizing the above objective optimizes for the parameters that define the best approximation to the true posterior within the variational family <ref type="bibr" target="#b2">(Blei et al., 2017)</ref>. We now discuss parameterizations for specifying q φ (Z|A, X) (i.e., encoder) and p θ (A|Z, X) (i.e., decoder).</p><p>Encoding using forward message passing. Typically we use the mean field approximation for defining the variational family and hence:</p><formula xml:id="formula_6">q φ (Z|A, X) ≈ n i=1 q φi (Z i |A, X)<label>(6)</label></formula><p>Additionally, we would like to make distributional assumptions on each variational marginal density q φi (Z i |A, X) such that it is reparameterizable and easy-to-sample, such that the gradients w.r.t. φ i have low variance <ref type="bibr">(Kingma &amp; Welling, 2014)</ref>. In Graphite, we assume isotropic Gaussian variational marginals with diagonal covariance. The parameters for the variational marginals q φi (Z|A, X) are specified using a graph neural network:</p><formula xml:id="formula_7">µ, σ = GNN φ (A, X)<label>(7)</label></formula><p>where µ and σ denote the vector of means and standard deviations for the variational marginals</p><formula xml:id="formula_8">{q φi (Z i |A, X)} n i=1 and φ = {φ i } n i=1</formula><p>are the full set of variational parameters.</p><p>Decoding using reverse message passing. For specifying the observation model p θ (A|Z, X), we cannot directly use a graph neural network since we do not have an input graph for message passing. To sidestep this issue, we propose an iterative two-step approach that alternates between defining an intermediate graph and then gradually refining this graph through message passing. Formally, given a latent matrix Z and an input feature matrix X, we iterate over the following sequence of operations:</p><formula xml:id="formula_9">A = ZZ T Z 2 + 11 T ,<label>(8)</label></formula><formula xml:id="formula_10">Z * = GNN θ ( A, [Z|X])<label>(9)</label></formula><p>where the second argument to the GNN is a concatenation of Z and X. The first step constructs an intermediate weighted graph A ∈ R n×n by applying an inner product of Z with itself and adding an additional constant of 1 to ensure entries are non-negative. And the second step performs a pass through a parameterized graph neural network. We can repeat the above sequence to gradually refine the feature matrix Z * . The final distribution over graph parameters is obtained using an inner product step on Z * ∈ R n×k * akin to Eq. (8), where k * ∈ N is determined via the network architecture. For efficient sampling, we assume the observation model factorizes:</p><formula xml:id="formula_11">p θ (A|Z, X) = n i=1 n j=1 p (i,j) θ (A ij |Z * ).<label>(10)</label></formula><p>The distribution over the individual edges can be expressed as a Bernoulli or Gaussian distribution for unweighted and real-valued edges respectively. E.g., the edge probabilities for an unweighted graph are given as sigmoid(Z * Z * T ). For representation learning of large graphs, we require the encoding and decoding steps in Graphite to be computationally efficient. On the surface, the decoding step involves inner products of potentially dense matrices Z, which is an O(n 2 k) operation. Here, k is the dimension of the per-node latent vectors Z i used to define A.</p><p>For any intermediate decoding step as in Eq. <ref type="formula" target="#formula_9">(8)</ref>, we propose to offset this expensive computation by using the associativity property of matrix multiplications for the message passing step in Eq. (9). For notational brevity, consider the simplified graph propagation rule for a GNN:</p><formula xml:id="formula_12">H (l) ← η l AH (l−1)</formula><p>where A is defined in Eq. (8).</p><p>Instead of directly taking an inner product of Z with itself, we note that the subsequent operation involves another matrix multiplication and hence, we can perform right multiplication instead. If d l and d l−1 denote the size of the layers H (l) and H (l−1) respectively, then the time complexity of propagation based on right multiplication is given by O(nkd l−1 + nd l−1 d l ).</p><p>The above trick sidesteps the quadratic n 2 complexity for decoding in the intermediate layers without any loss in statistical accuracy. The final layer however still involves an inner product with respect to Z * between potentially dense matrices. However, since the edges are generated independently, we can approximate the loss objective by performing a Monte Carlo evaluation of the reconstructed adjacency matrix parameters in Eq. (10). By adaptively choosing the number of entries for Monte Carlo approximation, we can trade-off statistical accuracy for computational budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>We evaluate Graphite on tasks involving entire graphs, nodes, and edges. We consider two variants of our proposed framework: the Graphite-VAE, which corresponds to a directed latent variable model as described in Section 3 and Graphite-AE, which corresponds to an autoencoder trained to minimize the error in reconstructing an input adjacency matrix. For unweighted graphs (i.e., A ∈ {0, 1} n×n ), the reconstruction terms in the objectives for both Graphite-VAE and Graphite-AE minimize the negative cross entropy between the input and reconstructed adjacency matrices. For weighted graphs, we use the mean squared error. Additional hyperparameter details are described in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Reconstruction &amp; density estimation</head><p>In the first set of tasks, we evaluate learning in Graphite based on held-out reconstruction losses and log-likelihoods estimated by the learned Graphite-VAE and Graphite-AE models respectively on a collection of graphs with varying sizes. In direct contrast to modalities such as images, graphs cannot be straightforwardly reduced to a fixed number of vertices for input to a graph convolutional network. One simplifying modification taken by <ref type="bibr" target="#b3">Bojchevski et al. (2018)</ref> is to consider only the largest connected component for evaluating and optimizing the objective, which we appeal to as well. Thus by setting the dimensions of Z * to a maximum number of vertices, Graphite can be used for inference tasks over entire graphs with potentially smaller sizes by considering only the largest connected component.</p><p>We create datasets from six graph families with fixed, known generative processes: the Erdos-Renyi, ego-nets, random regular graphs, random geometric graphs, random Power Law Tree and Barabasi-Albert. For each family, 300 graph instances were sampled with each instance having 10 − 20 nodes and evenly split into train/validation/test instances. As a benchmark comparison, we compare against the Graph Autoencoder/Variational Graph Autoencoder (GAE/VGAE) <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2016)</ref>. The GAE/VGAE models consist of an encoding procedure similar to Graphite. However, the decoder has no learnable parameters and reconstruction is done solely through an inner product operation (such as the one in Eq. (8)).</p><p>The mean reconstruction errors and the negative loglikelihood results on a test set of instances are shown in <ref type="table" target="#tab_0">Table 1</ref>. Both Graphite-AE and Graphite-VAE outperform AE and VGAE significantly on these tasks, indicating the usefulness of learned decoders in Graphite. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Link prediction</head><p>The task of link prediction is to predict whether an edge exists between a pair of nodes <ref type="bibr" target="#b27">(Loehlin, 1998)</ref>. Even though Graphite learns a distribution over graphs, it can be used for predictive tasks within a single graph. In order to do so, we learn a model for a random, connected training subgraph of the true graph. For validation and testing, we add a balanced set of positive and negative (false) edges to the original graph and evaluate the model performance based on the reconstruction probabilities assigned to the validation and test edges (similar to denoising of the input graph). In our experiments, we held out a set of 5% edges for validation, 10% edges for testing, and train all models on the remaining subgraph. Additionally, the validation and testing sets also each contain an equal number of non-edges.</p><p>Datasets. We compared across standard benchmark citation network datasets: Cora, Citeseer, and Pubmed with papers as nodes and citations as edges <ref type="bibr" target="#b42">(Sen et al., 2008)</ref>. The node-level features correspond to the text attributes in the papers. The dataset statistics are summarized in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Baselines and evaluation metrics. We evaluate performance based on the Area Under the ROC Curve (AUC) and Average Precision (AP) metrics. We evaluated Graphite-VAE and Graphite-AE against the following baselines: Spectral Clustering (SC) <ref type="bibr" target="#b49">(Tang &amp; Liu, 2011)</ref>, DeepWalk (Perozzi et al., 2014), node2vec <ref type="bibr" target="#b14">(Grover &amp; Leskovec, 2016)</ref>, and GAE/VGAE <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2016)</ref>. SC, DeepWalk, and node2vec do not provide the ability to incorporate node features while learning embeddings, and hence we evaluate them only on the featureless datasets.</p><p>Results. The AUC and AP results (along with standard errors) are shown in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table">Table 4</ref> respectively averaged over 50 random train/validation/test splits. On both metrics, Graphite-VAE gives the best performance overall. Graphite-AE also gives good results, generally outperforming its closest competitor GAE.</p><p>Qualitative evaluation. We visualize the embeddings learned by Graphite and given by a 2D t-SNE projection <ref type="bibr" target="#b29">(Maaten &amp; Hinton, 2008)</ref> of the latent feature vectors (given as rows for Z with λ = 0.5) on the Cora dataset in <ref type="figure" target="#fig_1">Figure 2</ref>. Even without any access to label information for the nodes during training, the name models are able to cluster the nodes (papers) as per their labels (paper categories).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semi-supervised node classification</head><p>Given labels for a subset of nodes in an underlying graph, the goal of this task is to predict the labels for the remaining nodes. We consider a transductive setting, where we have access to the test nodes (without their labels) during training.</p><p>Closest approach to Graphite for this task is a supervised graph convolutional network (GCN) trained end-to-end. We consider an extension of this baseline, wherein we augment the GCN objective with the Graphite objective and a hyperparameter to control the relative importance of the two terms in the combined objective. The parameters φ for the encoder are shared across these two objectives, with an additional GCN layer for mapping the encoder output to softmax probabilities over the requisite number of classes. All parameters are learned jointly. Results. The classification accuracy of the semisupervised models is given in <ref type="table">Table 5</ref>. We find that Graphitehybrid outperforms the competing models on all datasets and in particular the GCN approach which is the closest baseline. Recent work in Graph Attention Networks shows that extending GCN by incoporating attention can boost performance on this task <ref type="bibr" target="#b50">(Veličković et al., 2018)</ref>. Using GATs in place of GCNs for parameterizing Graphite could yield similar performance boost in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Theoretical Analysis</head><p>In this section, we derive a theoretical connection between message passing in graph neural networks and approximate inference in related undirected graphical models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Kernel embeddings</head><p>We first provide a brief background on kernel embeddings. A kernel defines a notion of similarity between pairs of objects <ref type="bibr" target="#b41">(Schölkopf &amp; Smola, 2002;</ref><ref type="bibr" target="#b43">Shawe-Taylor &amp; Cristianini, 2004</ref>). Let K : Z × Z → R be the kernel function defined over a space of objects, say Z. With every kernel function K, we have an associated feature map ψ : Z → H where H is a potentially infinite dimensional feature space.</p><p>Kernel methods can be used to specify embeddings of distributions of arbitrary objects <ref type="bibr" target="#b13">Gretton et al., 2007)</ref>. Formally, we denote these functional mappings as T ψ : P → H where P specifies the space of all distributions on Z. These mappings, referred to as kernel embeddings of distributions, are defined as:</p><formula xml:id="formula_13">T ψ (p) := E Z∼p [ψ(Z)]<label>(11)</label></formula><p>for any p ∈ P. We are particularly interested in kernels with feature maps ψ that define injective embeddings, i.e., for any pair of distributions p 1 and p 2 , we have T ψ (p 1 ) = T ψ (p 2 ) if p 1 = p 2 . For injective embeddings, we can compute functionals of any distribution by directly applying a corresponding function on its embedding. Formally, for every function O : P → R d , d ∈ N and injective embedding T ψ , there exists a functionÕ ψ : H → R d such that:</p><formula xml:id="formula_14">O(p) =Õ ψ (T ψ (p)) ∀p ∈ P.<label>(12)</label></formula><p>Informally, we can see that the operatorÕ ψ can be defined as the composition of O with the inverse of T ψ .</p><p>2 3 1 (a) Input graph with edge set E = {(1, 2), (1, 3)}.</p><formula xml:id="formula_15">X 2 Z 2 X 3 Z 3 X 1 Z 1 A 12 A 23</formula><p>A 13 (b) Latent variable model G satisfying Property 1 with A12 = 0, A23 = 0, A13 = 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Connections with mean-field inference</head><p>Locality preference for representational learning is a key inductive bias for graphs. We formulate this using an (undirected) graphical model G over X, A, and {Z 1 , · · · , Z n }. As in a GNN, we assume that X and A are observed and specify conditional independence structure in a conditional distribution over the latent variables, denoted as r(Z 1 , · · · , Z n |A, X). We are particularly interested in models that satisfy the following property. Property 1. The edge set E defined by the adjacency matrix A is an undirected I-map for the distribution r(Z 1 , · · · , Z n |A, X).</p><p>In words, the above property implies that according to the conditional distribution over Z, any individual Z i is independent of all other Z j when conditioned on A, X, and the neighboring latent variables of node i as determined by the edge set E. See <ref type="figure" target="#fig_2">Figure 3</ref> for an illustration.</p><p>A mean-field (MF) approximation for G approximates the conditional distribution r(Z 1 , · · · , Z n |A, X) as:</p><formula xml:id="formula_16">r(Z 1 , · · · , Z n |A, X) ≈ n i=1 q φi (Z i |A, X)<label>(13)</label></formula><p>where φ i denotes the set of parameters for the i-th variational marginal. These parameters are optimized by minimizing the KL-divergence between the variational and the true conditional distributions:</p><formula xml:id="formula_17">min φ1,··· ,φn KL n i=1 q φi (Z i |A, X) r(Z 1 , · · · , Z n |A, X)<label>(14)</label></formula><p>Using standard variational arguments <ref type="bibr" target="#b53">(Wainwright et al., 2008)</ref>, we know that the optimal variational marginals assume the following functional form:</p><formula xml:id="formula_18">q φi (Z i |A, X) = O M F G Z i , {q φj } j∈N (i)<label>(15)</label></formula><p>where N (i) denotes the neighbors of Z i in G and O is a function determined by the fixed point equations that depends on the potentials associated with G. Importantly, the above functional form suggests that the optimal marginals in mean field inference are locally consistent that they are only a function of the neighboring marginals. An iterative algorithm for mean-field inference is to perform message passing over the underlying graph until convergence. With an appropriate initialization at l = 0, the updated marginals at iteration l ∈ N are given as:</p><formula xml:id="formula_19">q (l) φi (Z i |A, X) = O M F G Z i , {q (l−1) φj } j∈N (i) .<label>(16)</label></formula><p>We will sidestep deriving O, and instead use the kernel embeddings of the variational marginals to directly reason in the embedding space. That is, we assume we have an injective embedding for each marginal q φi given by</p><formula xml:id="formula_20">µ i = E Zi∼q φ i [ψ(Z i )]</formula><p>for some feature map ψ : R k → R k and directly use the equivalence established in Eq. (12) iteratively. For mean-field inference via message passing as in Eq. <ref type="formula" target="#formula_1">(16)</ref>, this gives us the following recursive expression for iteratively updating the embeddings at iteration l ∈ N:</p><formula xml:id="formula_21">µ (l) i =Õ M F ψ,G {µ (l−1) j } j∈N (i)<label>(17)</label></formula><p>with an appropriate base case for µ (0)</p><p>i . We then have the following result:</p><p>Theorem 2. Let G be any undirected latent variable model such that the conditional distribution r(Z 1 , · · · , Z n |A, X) expressed by the model satisfies Property 1.</p><p>Then there exists a choice of η l , F l , W l , and B l such that for all {µ</p><formula xml:id="formula_22">(l−1) i } n i=1 , the GNN propagation rule in Eq. (2) is computationally equivalent to updating {µ (l−1) i } n i=1</formula><p>via a first order approximation of Eq. (17).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. See Appendix A.</head><p>While η l and F l are typically fixed beforehand, the parameters W l , and B l are directly learned from data in practice. Hence we have shown that a GNN is a good model for computation with respect to latent variable models that attempt to capture inductive biases relevant to graphs, i.e., ones where the latent feature vector for every node is conditionally independent from everything else given the feature vectors of its neighbors (and A, X). Note that such a graphical model would satisfy Property 1 but is in general different from the posterior specified by the one in <ref type="figure" target="#fig_0">Figure 1</ref>. However if the true (but unknown) posterior on the latent variables for the model proposed in <ref type="figure" target="#fig_0">Figure 1</ref> could be expressed as an equivalent model satisfying the desired property, then Theorem 2 indeed suggests the use of GNNs for parameterizing variational posteriors, as we do so in the case of Graphite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion &amp; Related Work</head><p>Our framework effectively marries probabilistic modeling and representation learning on graphs. We review some of the dominant prior works in these fields below.</p><p>Probabilistic modeling of graphs. The earliest probabilistic models of graphs proposed to generate graphs by creating an edge between any pair of nodes with a constant probability <ref type="bibr" target="#b9">(Erdös &amp; Rényi, 1959)</ref>. Several alternatives have been proposed since; e.g., the small-world model generates graphs that exhibit local clustering <ref type="bibr" target="#b56">(Watts &amp; Strogatz, 1998)</ref>, the Barabasi-Albert models preferential attachment wherein high-degree nodes are likely to form edges with newly added nodes <ref type="bibr" target="#b0">(Barabasi &amp; Albert, 1999)</ref>, the stochastic block model is based on inter and intra community linkages <ref type="bibr" target="#b18">(Holland et al., 1983)</ref> etc. We direct the interested reader to prominent surveys on this topic <ref type="bibr" target="#b33">(Newman, 2003;</ref><ref type="bibr" target="#b32">Mitzenmacher, 2004;</ref><ref type="bibr" target="#b5">Chakrabarti &amp; Faloutsos, 2006)</ref>.</p><p>Representation learning on graphs. For representation learning on graphs, there are broadly three kinds of approaches: matrix factorization, random walk based approaches, and graph neural networks. We include a brief discussion on the first two kinds in Appendix C and refer the reader to <ref type="bibr" target="#b17">Hamilton et al. (2017b)</ref> for a recent survey.</p><p>Graph neural networks, a collective term for networks that operate over graphs using message passing, have shown success on several downstream applications, e.g., <ref type="bibr" target="#b8">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b25">Li et al., 2016;</ref><ref type="bibr">Kearnes et al., 2016;</ref><ref type="bibr" target="#b24">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b16">Hamilton et al., 2017a)</ref> and the references therein. <ref type="bibr" target="#b11">Gilmer et al. (2017)</ref> provides a comprehensive characterization of these networks in the message passing setup. We used Graph Convolution Networks, partly to provide a direct comparison with GAE/VGAE and leave the exploration of other GNN variants for future work.</p><p>Latent variable models for graphs. Hierarchical Bayesian models parameterized by deep neural networks have been recently proposed for graphs <ref type="bibr" target="#b19">(Hu et al., 2017;</ref><ref type="bibr" target="#b54">Wang et al., 2017)</ref>. Besides being restricted to single graphs, these models are limited since inference requires running expensive Markov chains <ref type="bibr" target="#b19">(Hu et al., 2017)</ref> or are task-specific <ref type="bibr" target="#b54">(Wang et al., 2017)</ref>. <ref type="bibr" target="#b21">Johnson (2017)</ref> and <ref type="bibr" target="#b22">Kipf et al. (2018)</ref> generate graphs as latent representations learned directly from data. In contrast, we are interested in modeling observed (and not latent) relational structure. Finally, there has been a fair share of recent work for generation of special kinds of graphs, such as parsed trees of source code <ref type="bibr" target="#b30">(Maddison &amp; Tarlow, 2014)</ref> and SMILES representations for molecules <ref type="bibr" target="#b34">(Olivecrona et al., 2017)</ref>.</p><p>Several deep generative models for graphs have recently been proposed. Amongst adversarial generation approaches, <ref type="bibr" target="#b55">Wang et al. (2018)</ref> and <ref type="bibr" target="#b3">Bojchevski et al. (2018)</ref> model local graph neighborhoods and random walks on graphs respectively. <ref type="bibr" target="#b26">Li et al. (2018)</ref> and <ref type="bibr" target="#b61">You et al. (2018)</ref> model graphs as sequences and generate graphs via autoregressive procedures. Adversarial and autoregressive approaches are successful at generating graphs, but do not directly allow for inferring latent variables via encoders. Latent variable generative models have also been proposed for generating small molecular graphs <ref type="bibr" target="#b20">(Jin et al., 2018;</ref><ref type="bibr" target="#b38">Samanta et al., 2018;</ref><ref type="bibr" target="#b45">Simonovsky &amp; Komodakis, 2018)</ref>. These methods involve an expensive decoding procedure that limits scaling to large graphs. Finally, closest to our framework is the GAE/VGAE approach <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2016)</ref> discussed in Section 4. <ref type="bibr" target="#b35">Pan et al. (2018)</ref> extends this approach with an adversarial regularization framework but retain the inner product decoder. Our work proposes a novel multi-step decoding mechanism based on graph refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion &amp; Future Work</head><p>We proposed Graphite, a scalable deep generative model for graphs based on variational autoencoding. The encoders and decoders in Graphite are parameterized by graph neural networks that propagate information locally on a graph. Our proposed decoder performs a multi-layer iterative decoding comprising of alternate inner product operations and message passing on the intermediate graph.</p><p>Current generative models for graphs are not permutationinvariant and are learned by feeding graphs with a fixed or heuristic ordering of nodes. This is an exciting challenge for future work, which could potentially be resolved by incorporate graph representations robust to permutation invariances <ref type="bibr" target="#b52">(Verma &amp; Zhang, 2017)</ref> or modeling distributions over permutations of node orderings via recent approaches such as NeuralSort <ref type="bibr" target="#b15">(Grover et al., 2019)</ref>. Extending Graphite for modeling richer graphical structure such as heterogeneous and time-varying graphs, as well as integrating domain knowledge within Graphite decoders for applications in generative design and synthesis e.g., molecules, programs, and parse trees is another interesting future direction.</p><p>Finally, our theoretical results in Section 5 suggest that a principled design of layerwise propagation rules in graph neural networks inspired by additional message passing inference schemes <ref type="bibr" target="#b6">(Dai et al., 2016;</ref><ref type="bibr" target="#b11">Gilmer et al., 2017)</ref> is another avenue for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Theorem 2</head><p>Proof. For simplicity, we state the proof for a single variational marginal embedding µ <ref type="bibr">(l)</ref> i and consider that µ <ref type="bibr">(l)</ref> i for all l ∈ N ∪ 0 are unidimensional.</p><p>Let us denote N (l) i ∈ R n to be the vector of neighboring kernel embeddings at iteration l such that the j-th entry of N (l) i corresponds to µ (l) j if j ∈ N (i) and zero otherwise. Hence, we can rewrite Eq. (17) as:</p><formula xml:id="formula_23">µ (l) i =Õ ψ,G N (l−1) i<label>(18)</label></formula><p>where we have overloadedÕ ψ,,G to now denote a function that takes as argument an n-dimensional vector of marginal embeddings.</p><p>Assuming that the functionÕ ψ,G is differentiable, a firstorder Taylor expansion of Eq. (18) around the origin 0 is given by:</p><formula xml:id="formula_24">µ (l) i ≈Õ ψ,G (0) + N (l−1) i · ∇Õ ψ,G (0) .<label>(19)</label></formula><p>Since every marginal density is unidimensional, we now consider a GNN with a single activation per node in every layer, i.e., H (l) ∈ R n for all l ∈ N∪0. This also implies that the bias can be expressed as an n-dimensional vector, i.e., B l ∈ R n and we have a single weight parameter W l ∈ R.</p><p>For a single entry of H (l) , we can specify Eq.</p><p>(2) componentwise as:</p><formula xml:id="formula_25">H (l) i = η l   B l,i + f ∈F l f (A i )H (l−1) W l  <label>(20)</label></formula><p>where A i denotes the i-th row of A and is non-zero only for entries corresponding to the neighbors of node i. Now, consider the following instantiation of Eq. (20):</p><p>• η l ← I (identity function)</p><formula xml:id="formula_26">• B l,i ←Õ ψ,G (0) • A family of n transformations F l = {f l,j } n j=1 where f l,j (A i ) = ∂Õ ψ,G ∂µ (l−1) j (0) A ij • H (l−1) i ← µ (l−1) i • W l ← 1.</formula><p>With the above substitutions, we can equate the first order approximation in Eq. (18) to the GNN message passing rule in Eq. (20), thus completing the proof. With vectorized notation and use of matrix calculus in Eqs. <ref type="bibr">(18)</ref><ref type="bibr">(19)</ref><ref type="bibr">(20)</ref>, the derivation above also applies to entire vectors of variational marginal embeddings with arbitrary dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Specifications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Link prediction</head><p>We used the SC implementation from <ref type="bibr" target="#b36">(Pedregosa et al., 2011)</ref> and public implementations for others made available by the authors. For SC, we used a dimension size of 128. For DeepWalk and node2vec which uses a skipgram like objective on random walks from the graph, we used the same dimension size and default settings used in <ref type="bibr" target="#b37">(Perozzi et al., 2014)</ref> and <ref type="bibr" target="#b14">(Grover &amp; Leskovec, 2016)</ref> respectively of 10 random walks of length 80 per node and a context size of 10. For node2vec, we searched over the random walk bias parameters using a grid search in {0.25, 0.5, 1, 2, 4} as prescribed in the original work. For GAE and VGAE, we used the same architecture as VGAE and Adam optimizer with learning rate of 0.01.</p><p>For Graphite-AE and Graphite-VAE, we used an architecture of 32-32 units for the encoder and 16-32-16 units for the decoder (two rounds of iterative decoding before a final inner product). The model is trained using the Adam optimizer (Kingma &amp; Welling, 2014) with a learning rate of 0.01. All activations were RELUs.The dropout rate (for edges) and λ were tuned as hyperparameters on the validation set to optimize the AUC, whereas traditional dropout was set to 0 for all datasets. Additionally, we trained every model for 500 iterations and used the model checkpoint with the best validation loss for testing. Scores are reported as an average of 50 runs with different train/validation/test splits (with the requirement that the training graph necessarily be connected).</p><p>For Graphite, we observed that using a form of skip connections to define a linear combination of the initial embedding Z and the final embedding Z * is particularly useful. The skip connection consists of a tunable hyperparameter λ controlling the relative weights of the embeddings. The final embedding of Graphite is a function of the initial embedding Z and the last induced embedding Z * . We consider two functions to aggregate them into a final embedding. That is, (1 − λ)Z + λZ * and Z + λZ * / Z * , which correspond to a convex combination of two embeddings, and an incremental update to the initial embedding in a given direction, respectively. Note that in either case, GAE and VGAE reduce to a special case of Graphite, using only a single inner-product decoder (i.e., λ = 0). On Cora and Pubmed final embeddings were derived through convex combination, on Citeseer through incremental update.</p><p>Scalability. We experimented with learning VGAE and Graphite models by subsampling |E| random entries for Monte Carlo evaluation of the objective at each iteration. The corresponding AUC scores are shown in <ref type="table" target="#tab_3">Table 6</ref>. The results suggest that Graphite can effectively scale to large graphs without significant loss in accuracy. The AUC results  trained with edge subsampling as we vary the subsampling coefficient c are shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Semi-supervised node classification</head><p>We report the baseline results for SemiEmb <ref type="bibr" target="#b58">(Weston et al., 2008)</ref>, DeepWalk <ref type="bibr" target="#b37">(Perozzi et al., 2014)</ref>, ICA <ref type="bibr" target="#b28">(Lu &amp; Getoor, 2003)</ref> and Planetoid <ref type="bibr" target="#b59">(Yang et al., 2016)</ref> as specified in <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref>. GCN uses a 32-16 architecture with ReLu activations and early stopping after 10 epochs without increasing validation accuracy. The Graphite model uses the same architecture as in link prediction (with no edge dropout). The parameters of the posterior distributions are concatenated with node features to predict the final output. The parameters are learned using the Adam optimizer (Kingma &amp; Welling, 2014) with a learning rate of 0.01. All accuracies are taken as an average of 100 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Density estimation</head><p>To accommodate for input graphs of different sizes, we learn a model architecture specified for the maximum possible nodes (i.e., 20 in this case). While feeding in smaller graphs, we simply add dummy nodes disconnected from the rest of the graph. The dummy nodes have no influence on the gradient updates for the parameters affecting the latent or observed variables involving nodes in the true graph. For the experiments on density estimation, we pick a graph family, then train and validate on graphs sampled exclusively from that family. We consider graphs with nodes ranging between 10 and 20 nodes belonging to the following graph families :</p><p>• Erdos-Renyi <ref type="bibr" target="#b9">(Erdös &amp; Rényi, 1959)</ref>: each edge independently sampled with probability p = 0.5</p><p>• Ego Network: a random Erdos-Renyi graph with all nodes neighbors of one randomly chosen node</p><p>• Random Regular: uniformly random regular graph with degree d = 4</p><p>• Random Geometric: graph induced by uniformly random points in unit square with edges between points at euclidean distance less than r = 0.5</p><p>• Random Power Tree: Tree generated by randomly swapping elements from a degree distribution to satisfy a power law distribution for γ = 3</p><p>• Barabasi-Albert <ref type="bibr" target="#b0">(Barabasi &amp; Albert, 1999)</ref>: Preferential attachment graph generation with attachment edge count m = 4</p><p>We use convex combinations over three successively induced embeddings. Scores are reported over an average of 50 runs. Additionally, a two-layer neural net is applied to the initially sampled embedding Z before being fed to the inner product decoder for GAE and VGAE, or being fed to the iterations of Eqs. <ref type="formula" target="#formula_9">(8)</ref> and <ref type="formula" target="#formula_10">(9)</ref> for both Graphite-AE and Graphite-VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Related Work</head><p>Factorization based approaches, such as Laplacian Eigenmaps <ref type="bibr" target="#b1">(Belkin &amp; Niyogi, 2002)</ref> and IsoMaps <ref type="bibr" target="#b39">(Saxena et al., 2004)</ref>, operate on a matrix representation of the graph, such as the adjacency matrix or the graph Laplacian. These approaches are closely related to dimensionality reduction and can be computationally expensive for large graphs.</p><p>Random-walk methods are based on variations of the skipgram objective <ref type="bibr" target="#b31">(Mikolov et al., 2013)</ref> and learn representations by linearizing the graph through random walks. These methods, in particular DeepWalk <ref type="bibr" target="#b37">(Perozzi et al., 2014)</ref>, LINE <ref type="bibr" target="#b48">(Tang et al., 2015)</ref>, and node2vec <ref type="bibr" target="#b14">(Grover &amp; Leskovec, 2016)</ref>, learn general-purpose unsupervised representations that have been shown to give excellent performance for semisupervised node classification and link prediction. Planetoid <ref type="bibr" target="#b59">(Yang et al., 2016)</ref> learn representations based on a similar objective specifically for semi-supervised node classification by explicitly accounting for the available label information during learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Latent variable model for Graphite. Observed evidence variables in gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>. t-SNE embeddings of the latent feature vectors for the Cora dataset. Colors denote labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Interpreting message passing in Graph Neural Networks via Kernel Embeddings and Mean-field inference</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>AUC score of VGAE and Graphite with subsampled edges on the Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Mean reconstruction errors and negative log-likelihood estimates (in nats) for autoencoders and variational autoencoders respectively on test instances from six different generative families. Lower is better. ± 7.58 197.3 ± 1.99 198.5 ± 4.78 514.26 ± 41.58 519.44 ± 36.30 236.29 ± 15.13 Graphite-AE 195.56 ± 1.49 182.79 ± 1.45 191.41 ± 1.99 181.14 ± 4.48 201.22 ± 2.42 192.38 ± 1.61 VGAE 273.82 ± 0.07 273.76 ± 0.06 275.29 ± 0.08 274.09 ± 0.06 278.86 ± 0.12 274.4 ± 0.08 Graphite-VAE 270.22 ± 0.15 270.70 ± 0.32 266.54 ± 0.12 269.71 ± 0.08 263.92 ± 0.14 268.73 ± 0.09 3.1. Scalable learning &amp; inference in Graphite</figDesc><table><row><cell></cell><cell>Erdos-Renyi</cell><cell>Ego</cell><cell>Regular</cell><cell>Geometric</cell><cell>Power Law</cell><cell>Barabasi-Albert</cell></row><row><cell>GAE</cell><cell>221.79</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Citation network statisticsNodes Edges Node Features Labels</figDesc><table><row><cell>Cora</cell><cell>2708</cell><cell>5429</cell><cell>1433</cell><cell>7</cell></row><row><cell cols="2">Citeseer 3327</cell><cell>4732</cell><cell>3703</cell><cell>6</cell></row><row><cell cols="3">Pubmed 19717 44338</cell><cell>500</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Area Under the ROC Curve (AUC) for link prediction (* denotes dataset with features). Higher is better.</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora*</cell><cell>Citeseer*</cell><cell>Pubmed*</cell></row><row><cell></cell><cell>SC</cell><cell cols="3">89.9 ± 0.20 91.5 ± 0.17 94.9 ± 0.04</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DeepWalk</cell><cell cols="3">85.0 ± 0.17 88.6 ± 0.15 91.5 ± 0.04</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">node2vec</cell><cell cols="3">85.6 ± 0.15 89.4 ± 0.14 91.9 ± 0.04</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>GAE</cell><cell cols="5">90.2 ± 0.16 92.0 ± 0.14 92.5 ± 0.06 93.9 ± 0.11 94.9 ± 0.13 96.8 ± 0.04</cell></row><row><cell></cell><cell>VGAE</cell><cell cols="5">90.1 ± 0.15 92.0 ± 0.17 92.3 ± 0.06 94.1 ± 0.11 96.7 ± 0.08 95.5 ± 0.13</cell></row><row><cell cols="7">Graphite-AE 91.0 ± 0.15 92.6 ± 0.16 94.5 ± 0.05 94.2 ± 0.13 96.2 ± 0.10 97.8 ± 0.03</cell></row><row><cell cols="7">Graphite-VAE 91.5 ± 0.15 93.5 ± 0.13 94.6 ± 0.04 94.7 ± 0.11 97.3 ± 0.06 97.4 ± 0.04</cell></row><row><cell cols="7">Table 4. Average Precision (AP) scores for link prediction (* denotes dataset with features). Higher is better.</cell></row><row><cell></cell><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora*</cell><cell>Citeseer*</cell><cell>Pubmed*</cell></row><row><cell></cell><cell>SC</cell><cell cols="3">92.8 ± 0.12 94.4 ± 0.11 96.0 ± 0.03</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DeepWalk</cell><cell cols="3">86.6 ± 0.17 90.3 ± 0.12 91.9 ± 0.05</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">node2vec</cell><cell cols="3">87.5 ± 0.14 91.3 ± 0.13 92.3 ± 0.05</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>GAE</cell><cell cols="5">92.4 ± 0.12 94.0 ± 0.12 94.3 ± 0.5 94.3 ± 0.12 94.8 ± 0.15 96.8 ± 0.04</cell></row><row><cell></cell><cell>VGAE</cell><cell cols="5">92.3 ± 0.12 94.2 ± 0.12 94.2 ± 0.04 94.6 ± 0.11 97.0 ± 0.08 95.5 ± 0.12</cell></row><row><cell></cell><cell>Cora*</cell><cell>Citeseer*</cell><cell>Pubmed*</cell><cell></cell><cell></cell></row><row><cell>SemiEmb</cell><cell>59.0</cell><cell>59.6</cell><cell>71.1</cell><cell></cell><cell></cell></row><row><cell>DeepWalk</cell><cell>67.2</cell><cell>43.2</cell><cell>65.3</cell><cell></cell><cell></cell></row><row><cell>ICA</cell><cell>75.1</cell><cell>69.1</cell><cell>73.9</cell><cell></cell><cell></cell></row><row><cell>Planetoid</cell><cell>75.7</cell><cell>64.7</cell><cell>77.2</cell><cell></cell><cell></cell></row><row><cell>GCN</cell><cell>81.5</cell><cell>70.3</cell><cell>79.0</cell><cell></cell><cell></cell></row><row><cell>Graphite</cell><cell cols="3">82.1 ± 0.06 71.0 ± 0.07 79.3 ± 0.03</cell><cell></cell><cell></cell></row></table><note>Graphite-AE 92.8 ± 0.13 94.1 ± 0.14 95.7 ± 0.06 94.5 ± 0.14 96.1 ± 0.12 97.7 ± 0.03 Graphite-VAE 93.2 ± 0.13 95.0 ± 0.10 96.0 ± 0.03 94.9 ± 0.13 97.4 ± 0.06 97.4 ± 0.04 Table 5. Classification accuracies (* denotes dataset with features). Baseline numbers from Kipf &amp; Welling (2017).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>AUC scores for link prediction with Monte Carlo subsampling during training. Higher is better.</figDesc><table><row><cell></cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell>VGAE</cell><cell>89.6</cell><cell>92.2</cell><cell>92.3</cell></row><row><cell cols="2">Graphite 90.5</cell><cell>92.5</cell><cell>93.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Department of Computer Science, Stanford University, USA. Correspondence to: Aditya Grover &lt;adityag@cs.stanford.edu&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research has been supported by Siemens, a Future of Life Institute grant, NSF grants (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), AFOSR (FA9550-19-1-0024), and an Amazon AWS Machine Learning Grant. AG is supported by a Microsoft Research Ph.D. fellowship and a Stanford Data Science Scholarship. We would like to thank Daniel Levy for helpful comments on early drafts.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-L</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating graphs via random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Netgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graph mining: Laws, generators, and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ACM Computing Surveys</publisher>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Weisfeiler-Lehman method and graph isomorphism testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Douglas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.5211</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erdös</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publicationes Mathematicae (Debrecen)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic optimization of sorting networks via continuous relaxations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels: First steps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="137" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep generative models for relational data with side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internatiional Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning graphical state transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Latent variable models: An introduction to factor, path, and structural analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Loehlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Lawrence Erlbaum Associates Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structured generative models of natural source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A brief history of generative models for power law and lognormal distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet mathematics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="226" to="251" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The structure and function of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="256" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Molecular de-novo design through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepwalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Designing random graph models using variational autoencoders with applications to chemical design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05283</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-linear dimensionality reduction by locally linear isomaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning with kernels: Support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Kernel methods for pattern analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphvae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03480</idno>
		<title level="m">Towards generation of small graphs using variational autoencoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A hilbert space embedding for distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithmic Learning Theory</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Leveraging social media networks for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="478" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Graphite: Iterative Generative Modeling of Graphs</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Relational deep learning: A deep latent variable model for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">GraphGAN: Graph representation learning with generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Collective dynamics of small-world networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">6684</biblScope>
			<biblScope unit="page">440</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pitkow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07710</idno>
		<title level="m">probabilistic graphical models by graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">GraphRNN: A deep generative model for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
