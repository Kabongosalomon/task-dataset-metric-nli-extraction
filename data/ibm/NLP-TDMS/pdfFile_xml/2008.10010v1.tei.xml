<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020. 2020. October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Seattle</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename><surname>Acm Reference</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrabha</forename><surname>Format: K R Prajwal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mukhopadhyay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C V Jawa-Har</forename><surname>Namboodiri</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IIIT</orgName>
								<address>
									<region>Hyderabad</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">IIIT, Hyderabad</orgName>
								<address>
									<country>India Vinay P. Namboodiri</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Bath</orgName>
								<address>
									<country>England C V Jawahar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">IIIT</orgName>
								<address>
									<region>Hyderabad</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
						<meeting>the 28th ACM International Conference on Multimedia (MM &apos;20) <address><addrLine>Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 12-16, 2020. 2020. October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413532</idno>
					<note>ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00. ACM, New York, NY, USA, 10 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Computer vision</term>
					<term>Learning from critiques</term>
					<term>Phonology / morphology KEYWORDS lip sync</term>
					<term>video generation</term>
					<term>talking face generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Our novel Wav2Lip model produces significantly more accurate lip-synchronization in dynamic, unconstrained talking face videos. Quantitative metrics indicate that the lip-sync in our generated videos are almost as good as real-synced videos. Thus, we believe that our model can enable a wide range of real-world applications where previous speaker-independent lipsyncing approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> struggle to produce satisfactory results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training * Both of the authors have contributed equally to this research. phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model and evaluation benchmarks on our website: cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expertis-all-you-need-for-speech-to-lip-generation-in-the-wild. The code and models are released here: github.com/Rudrabha/Wav2Lip. You arXiv:2008.10010v1 [cs.CV] 23 Aug 2020 can also try out the interactive demo at this link: bhaasha.iiit.ac.in/ lipsync.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the exponential rise in the consumption of audio-visual content <ref type="bibr" target="#b20">[21]</ref>, rapid video content creation has become a quintessential need. At the same time, making these videos accessible in different languages is also a key challenge. For instance, a deep learning lecture series, a famous movie, or a public address to the nation, if translated to desired target languages, can become accessible to millions of new viewers. A crucial aspect of translating such talking face videos or creating new ones is correcting the lip sync to match the desired target speech. Consequently, lip-syncing talking face videos to match a given input audio stream has received considerable attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref> in the research community.</p><p>Initial works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> using deep learning in this space learned a mapping from speech representations to lip landmarks using several hours of a single speaker. More recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> in this line directly generate images from speech representations and show exceptional generation quality for specific speakers which they have been trained upon. Numerous practical applications, however, require models that can readily work for generic identities and speech inputs. This has led to the creation of speaker-independent speech to lip generation models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> that are trained on thousands of identities and voices. They can generate accurate lip motion on a single, static image of any identity in any voice, including that of a synthetic speech generated by a text-to-speech system <ref type="bibr" target="#b17">[18]</ref>. However, to be used for applications like translating a lecture/TV series, for example, these models need to be able to morph the broad diversity of lip shapes present in these dynamic, unconstrained videos as well, and not just on static images.</p><p>Our work builds upon this latter class of speaker-independent works that aspire to lip-sync talking face videos of any identity and voice. We find that these models that work well for static images are unable to accurately morph the large variety of lip shapes in unconstrained video content, leading to significant portions of the generated video being out-of-sync with the new target audio. A viewer can recognize an out-of-sync video segment as small as just ≈ 0.05 − 0.1 seconds <ref type="bibr" target="#b8">[9]</ref> in duration. Thus, convincingly lip-syncing a real-world video to an entirely new speech is quite challenging, given the tiny degree of allowed error. Further, the fact that we are aiming for a speaker-independent approach without any additional speaker-specific data overhead makes our task even more difficult. Real-world videos contain rapid pose, scale, and illumination changes and the generated face result must also seamlessly blend into the original target video.</p><p>We start by inspecting the existing speaker-independent approaches for speech to lip generation. We find that these models do not adequately penalize wrong lip shapes, either as a result of using only reconstruction losses or weak lip-sync discriminators. We adapt a powerful lip-sync discriminator that can enforce the generator to consistently produce accurate, realistic lip motion. Next, we re-examine the current evaluation protocols and devise new, rigorous evaluation benchmarks derived from three standard test sets. We also propose reliable evaluation metrics using Sync-Net <ref type="bibr" target="#b8">[9]</ref> to precisely evaluate lip sync in unconstrained videos. We also collect and release ReSyncED, a challenging set of real-world videos that can benchmark how the models will perform in practice. We conduct extensive quantitative and subjective human evaluations and outperform previous methods by a large margin across all benchmarks. Our key contributions/claims are as follows:</p><p>• We propose a novel lip-synchronization network, Wav2Lip, that is significantly more accurate than previous works for lip-syncing arbitrary talking face videos in the wild with arbitrary speech. • We propose a new evaluation framework, consisting of new benchmarks and metrics, to enable a fair judgment of lip synchronization in unconstrained videos. • We collect and release ReSyncED, a Real-world lip-Sync Evaluation Dataset to benchmark the performance of the lip-sync models on completely unseen videos in the wild. • Wav2Lip is the first speaker-independent model to generate videos with lip-sync accuracy that matches the real synced videos. Human evaluations indicate that the generated videos of Wav2Lip are preferred over existing methods and unsynced versions more than 90% of the time.</p><p>A demo video can be found on our website 1 with several qualitative examples that clearly illustrate the impact of our model. We will also release an interactive demo on the website allowing users to try out the model using audio and video samples of their choice. The rest of the paper is organized as follows: Section 2 surveys the recent developments in the area of speech to lip generation, Section 3 discusses the issues with the existing works and describes our proposed approach to mitigate them, Section 4 proposes a new, reliable evaluation framework. We describe the various potential applications and address some of the ethical concerns in Section 5 and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Constrained Talking Face Generation from Speech</head><p>We first review works on talking face generation that are either constrained by the range of identities they can generate or the range of vocabulary they are limited to. Realistic generation of talking face videos was achieved by a few recent works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> on videos of Barack Obama. They learn a mapping between the input audio and the corresponding lip landmarks. As they are trained on only a specific speaker, they cannot synthesize for new identities or voices. They also require a large amount of data of a particular speaker, typically a few hours. A recent work along this line <ref type="bibr" target="#b12">[13]</ref> proposes to seamlessly edit videos of individual speakers by adding or removing phrases from the speech. They still require an hour of data per speaker to achieve this task. Very recently, another work <ref type="bibr" target="#b22">[23]</ref> tries to minimize this data overhead by using a two-stage approach, where they first learn speaker-independent features and then learn a rendering mapping with ≈ 5 minutes of data of the desired speaker. However, they train their speaker-independent network on a significantly smaller corpus and also have an additional overhead of requiring clean training data of each target speaker to generate for that speaker. Another limitation of existing works is in terms of the vocabulary. Several works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref> train on datasets with a limited set of words such as GRID <ref type="bibr" target="#b9">[10]</ref> (56 words), TIMIT <ref type="bibr" target="#b13">[14]</ref> and LRW <ref type="bibr" target="#b7">[8]</ref> (1000 words) which significantly hampers a model from learning the vast diversity of phoneme-viseme mappings in real videos <ref type="bibr" target="#b17">[18]</ref>. Our work focuses on lip-syncing unconstrained talking face videos to match any target speech, not limited by identities, voices, or vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unconstrained Talking Face Generation from Speech</head><p>Despite the rise in the number of works on speech-driven face generation, surprisingly, very few works have been designed to lip-sync videos of arbitrary identities, voices, and languages. They are not trained on a small set of identities or a small vocabulary. This allows them to, at test time, lip-sync random identities for any speech. To the best of our knowledge, only two such prominent works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> exist in the current literature. Note that <ref type="bibr" target="#b16">[17]</ref> is an extended version of <ref type="bibr" target="#b6">[7]</ref>. Both these works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> formulate the task of learning to lip-sync in the wild as follows: Given a short speech segment S and a random reference face image R, the task of the network is to generate a lip-synced version L д of the input face that matches the audio. Additionally, the LipGAN model also inputs the target face with bottom-half masked to act as a pose prior. This was crucial as it allowed the generated face crops to be seamlessly pasted back into the original video without further post-processing. It also trains a discriminator in conjunction with the generator to discriminate in-sync or out-of-sync audio-video pairs. Both these works, however, suffer from a significant limitation: they work very well on static images of arbitrary identities but produce inaccurate lip generation when trying to lip-sync unconstrained videos in the wild. In contrast to the GAN setup used in LipGAN <ref type="bibr" target="#b17">[18]</ref>, we use a pre-trained, accurate lip-sync discriminator that is not trained further with the generator. We observe that this is an important design choice to achieve much better lip-sync results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ACCURATE SPEECH-DRIVEN LIP-SYNCING FOR VIDEOS IN THE WILD</head><p>Our core architecture can be summed up as "Generating accurate lip-sync by learning from a well-trained lip-sync expert". To understand this design choice, we first identify two key reasons why existing architectures (section 2.2) produce inaccurate lip-sync for videos in the wild. We argue that the loss functions, namely the L1 reconstruction loss used in both the existing works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and the discriminator loss in LipGAN <ref type="bibr" target="#b17">[18]</ref> are inadequate to penalize inaccurate lip-sync generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pixel-level Reconstruction loss is a Weak Judge of Lip-sync</head><p>The face reconstruction loss is computed for the whole image, to ensure correct pose generation, preservation of identity, and even background around the face. The lip region corresponds to less than 4% of the total reconstruction loss (based on the spatial extent), so a lot of surrounding image reconstruction is first optimized before the network starts to perform fine-grained lip shape correction. This is further supported by the fact that the network begins morphing lips only at around half-way (≈ 11 th epoch) through its training process (≈ 20 epochs <ref type="bibr" target="#b17">[18]</ref>). Thus, it is crucial to have an additional discriminator to judge lip-sync, as also done in LipGAN <ref type="bibr" target="#b17">[18]</ref>. But, how powerful is the discriminator employed in LipGAN?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Weak Lip-sync Discriminator</head><p>We find that the LipGAN's lip-sync discriminator is only about 56% accurate while detecting off-sync audio-lip pairs on the LRS2 test set. For comparison, the expert discriminator that we will use in this work is 91% accurate on the same test set. We hypothesize two major reasons for this difference. Firstly, LipGAN's discriminator uses a single frame to check for lip-sync. In <ref type="table">Table 3</ref>, we show that a small temporal context is very helpful while detecting lip-sync. Secondly, the generated images during training contain a lot of artifacts due to the large scale and pose variations. We argue that training the discriminator in a GAN setup on these noisy generated images, as done in LipGAN, results in the discriminator focusing on the visual artifacts instead of the audio-lip correspondence. This leads to a large drop in off-sync detection accuracy <ref type="table">(Table 3)</ref>. We argue and show that the "real", accurate concept of lip-sync captured from the actual video frames can be used to accurately discriminate and enforce lip-sync in the generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A Lip-sync Expert Is All You Need</head><p>Based on the above two findings, we propose to use a pre-trained expert lip-sync discriminator that is accurate in detecting sync in real videos. Also, it should not be fine-tuned further on the generated frames like it is done in LipGAN. One such network that has been used to correct lip-sync errors for creating large lip-sync datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> is the SyncNet <ref type="bibr" target="#b8">[9]</ref> model. We propose to adapt and train a modified version of SyncNet <ref type="bibr" target="#b8">[9]</ref> for our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.1</head><p>Overview of SyncNet. SyncNet <ref type="bibr" target="#b8">[9]</ref> inputs a window V of T v consecutive face frames (lower half only) and a speech segment S of size T a × D, where T v and T a are the video and audio timesteps respectively. It is trained to discriminate sync between audio and video by randomly sampling an audio window T a × D that is either aligned with the video (in-sync) or from a different time-step (out-of-sync). It contains a face encoder and an audio encoder, both comprising of a stack of 2D-convolutions. L2 distance is computed between the embeddings generated from these encoders, and the model is trained with a max-margin loss to minimize (or maximize) the distance between synced (or unsynced) pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Our approach generates accurate lip-sync by learning from an "already well-trained lip-sync expert". Unlike previous works that employ only a reconstruction loss <ref type="bibr" target="#b16">[17]</ref> or train a discriminator in a GAN setup <ref type="bibr" target="#b17">[18]</ref>, we use a pre-trained discriminator that is already quite accurate at detecting lip-sync errors. We show that fine-tuning it further on the noisy generated faces hampers the discriminator's ability to measure lip-sync, thus also affecting the generated lip shapes. Additionally, we also employ a visual quality discriminator to improve the visual quality along with the sync accuracy.</p><p>3.3.2 Our expert lip-sync discriminator. We make the following changes to SyncNet <ref type="bibr" target="#b8">[9]</ref> to train an expert lip-sync discriminator that suits our lip generation task. Firstly, instead of feeding grayscale images concatenated channel-wise as in the original model, we feed color images. Secondly, our model is significantly deeper, with residual skip connections <ref type="bibr" target="#b14">[15]</ref>. Thirdly, inspired by this public implementation 2 , we use a different loss function: cosine-similarity with binary cross-entropy loss. That is, we compute a dot product between the ReLU-activated video and speech embeddings v, s to yield a single value between [0, 1] for each sample that indicates the probability that the input audio-video pair is in sync:</p><formula xml:id="formula_0">P sync = v · s max(∥v ∥ 2 · ∥s ∥ 2 , ϵ)<label>(1)</label></formula><p>We train our expert lip-sync discriminator on the LRS2 train split (≈ 29 hours) with a batch size of 64, with T v = 5 frames using the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with an initial learning rate of 1e −3 . Our expert lip-sync discriminator is about 91% accurate on the LRS2 test set, while the discriminator used in LipGAN is only 56% accurate on the same test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Generating Accurate Lip-sync by learning from a Lip-sync Expert</head><p>Now that we have an accurate lip-sync discriminator, we can now use it to penalize the generator ( <ref type="figure">Figure 2</ref>) for inaccurate generation during the training time. We start by describing the generator architecture. <ref type="bibr" target="#b1">2</ref> github.com/joonson/syncnet_trainer 3.4.1 Generator Architecture Details. We use a similar generator architecture as used by LipGAN <ref type="bibr" target="#b17">[18]</ref>. Our key contribution lies in training this with the expert discriminator. The generator G contains three blocks: (i) Identity Encoder, (ii) Speech Encoder, and a (iii) Face Decoder. The Identity Encoder is a stack of residual convolutional layers that encode a random reference frame R, concatenated with a pose-prior P (target-face with lower-half masked) along the channel axis. The Speech Encoder is also a stack of 2Dconvolutions to encode the input speech segment S which is then concatenated with the face representation. The decoder is also a stack of convolutional layers, along with transpose convolutions for upsampling. The generator is trained to minimize L1 reconstruction loss between the generated frames L д and ground-truth frames L G :</p><formula xml:id="formula_1">L recon = 1 N N i=1 ||L д − L G || 1<label>(2)</label></formula><p>Thus, the generator is similar to the previous works, a 2D-CNN encoder-decoder network that generates each frame independently. How do we then employ our pre-trained expert lip-sync discriminator that needs a temporal window of T v = 5 frames as input?</p><p>3.4.2 Penalizing Inaccurate Lip Generation. During training, as the expert discriminator trained in section 3.3 processes T v = 5 contiguous frames at a time, we would also need the generator G to generate all the T v = 5 frames. We sample a random contiguous window for the reference frames, to ensure as much temporal consistency of pose, etc. across the T v window. As our generator processes each frame independently, we stack the time-steps along the batch dimension while feeding the reference frames to get an input shape of (N · T v , H,W , 3), where N, H, W are the batch-size, height, and width respectively. While feeding the generated frames to the expert discriminator, the time-steps are concatenated along the channel-dimension as was also done during the training of the discriminator. The resulting input shape to the expert discriminator is (N , H /2,W , 3·T v ), where only the lower half of the generated face is used for discrimination. The generator is also trained to minimize the "expert sync-loss" E sync from the expert discriminator:</p><formula xml:id="formula_2">E sync = 1 N N i=1 − log(P i sync )<label>(3)</label></formula><p>where P i sync is calculated according to <ref type="bibr">Equation 1</ref>. Note that the expert discriminator's weights remain frozen during the training of the generator. This strong discrimination based purely on the lip-sync concept learned from real videos forces the generator to also achieve realistic lip-sync to minimize the lip-sync loss E sync .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Generating Photo-realistic Faces</head><p>In our experiments, we observed that using a strong lip-sync discriminator forces the generator to produce accurate lip shapes. However, it sometimes results in the morphed regions to be slightly blurry or contain slight artifacts. To mitigate this minor loss in quality, we train a simple visual quality discriminator in a GAN setup along with the generator. Thus, we have two discriminators, one for sync accuracy and another for better visual quality. The lip-sync discriminator is not trained in a GAN setup for reasons explained in 3.2. On the other hand, since the visual quality discriminator does not perform any checks on lip-sync and only penalizes unrealistic face generations, it is trained on the generated faces.</p><p>The discriminator D consists of a stack of convolutional blocks. Each block consists of a convolutional layer followed by a Leaky ReLU activation <ref type="bibr" target="#b19">[20]</ref>. The discriminator is trained to maximize the objective function L disc (Equation 5):</p><formula xml:id="formula_3">L дen = E x ∼L д [loд(1 − D(x)] (4) L disc = E x ∼L G [loд(D(x))] + L дen<label>(5)</label></formula><p>where L д corresponds to the images from the generator G, and L G corresponds to the real images.</p><p>The generator minimizes <ref type="bibr">Equation 6</ref>, which is the weighted sum of the reconstruction loss (Equation 2), the synchronization loss (Equation 3) and the adversarial loss L дen (Equation 4):</p><formula xml:id="formula_4">L total = (1 − s w − s д ) · L recon + s w · E sync + s д · L дen<label>(6)</label></formula><p>where s w is the synchronization penalty weight, s д is the adversarial loss which are empirically set to 0.03 and 0.07 in all our experiments. Thus, our complete network is optimized for both superior sync-accuracy and quality using two disjoint discriminators.</p><p>We train our model only on the LRS2 train set <ref type="bibr" target="#b0">[1]</ref>, with a batch size of 80. We use the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with an initial learning rate of 1e −4 and betas β 1 = 0.5, β 2 = 0.999 for both the generator and visual quality discriminator D. Note that the lip-sync discriminator is not fine-tuned further, so its weights are frozen. We conclude the description of our proposed architecture by explaining how it works during the inference on real videos. Similar to Lip-GAN <ref type="bibr" target="#b17">[18]</ref>, the model generates a talking face video frame-by-frame. The visual input at each time-step is the current face crop (from the source frame), concatenated with the same current face crop with lower-half masked to be used as a pose prior. Thus, during inference, the model does not need to change the pose, significantly reducing artifacts. The corresponding audio segment is also given as input to the speech sub-network, and the network generates the input face crop, but with the mouth region morphed.</p><p>All our code and models will be released publicly. We will now quantitatively evaluate our novel approach against previous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">QUANTITATIVE EVALUATION</head><p>Despite training only on the LRS2 train set, we evaluate our model across 3 different datasets. But before doing so, we re-investigate the current evaluation framework followed in prior works and why it is far from being an ideal way to evaluate works in this space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Re-thinking the Evaluation Framework for Speech-driven Lip-Syncing in the Wild</head><p>The current evaluation framework for speaker-independent lipsyncing judges the models differently from how it is used while lip-syncing a real video. Specifically, instead of feeding the current frame as a reference (as described in the previous section), a random frame in the video is chosen as the reference to not leak the correct lip information during evaluation. We strongly argue that the evaluation framework in the previous paragraph is not ideal for evaluating the lip-sync quality and accuracy. Upon a closer examination of the above-mentioned evaluation system, we observed a few key limitations, which we discuss below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.1</head><p>Does not reflect the real-world usage. As discussed before, during generation at test time, the model must not change the pose, as the generated face needs to be seamlessly pasted into the frame. However, the current evaluation framework feeds random reference frames in the input, thus demanding the network to change the pose. Thus, the above system does not evaluate how the model would be used in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Inconsistent evaluation.</head><p>As the reference frames are chosen at random, this means the test data is not consistent across different works. This would lead to an unfair comparison and hinder the reproducibility of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.3</head><p>Does not support checking for temporal consistency. As the reference frames are randomly chosen at each time-step, temporal consistency is already lost as the frames are generated at random poses and scales. The current framework cannot support a new metric or a future method that aims to study the temporal consistency aspect of this problem.</p><p>4.1.4 Current metrics are not specific to lip-sync. The existing metrics, such as SSIM <ref type="bibr" target="#b26">[27]</ref> and PSNR, were developed to evaluate overall image quality and not fine-grained lip-sync errors. Although LMD <ref type="bibr" target="#b3">[4]</ref> focuses on the lip region, we found that lip landmarks can be quite inaccurate on generated faces. Thus, there is a need for a metric that is designed specifically for measuring lip-sync errors.</p><p>LRW <ref type="bibr" target="#b7">[8]</ref> LRS2  <ref type="table">Table 1</ref>: We propose two new metrics "Lip-Sync Error-Distance" (lower is better) and "Lip-Sync Error-Confidence" (higher is better), that can reliably measure the lip-sync accuracy in unconstrained videos. We see that the lip-sync accuracy of the videos generated using Wav2Lip is almost as good as real synced videos. Note that we only train on the train set on LRS2 <ref type="bibr" target="#b0">[1]</ref>, but we comfortably generalize across all datasets without any further fine-tuning. We also report the FID score (lower is better), which clearly shows that using a visual quality discriminator improves the quality by a significant margin.</p><formula xml:id="formula_5">[1] LRS3 [3] Method LSE-D ↓ LSE-C ↑ FID ↓ LSE-D ↓ LSE-C ↑ FID ↓ LSE-D ↓ LSE-C ↑ FID ↓</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Novel Benchmark and Metric for Evaluating Lip-Sync in the Wild</head><p>The reason for sampling random frames for evaluation is because, the current frame is already in sync with the speech, leading to leakage of lip-shape in the input itself. And previous works have not tried sampling different speech segments instead of sampling a different frame, as the ground-truth lip shape for the sampled speech is unavailable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">A Metric to</head><p>Measure the Lip-Sync Error. We propose to use the pre-trained SyncNet <ref type="bibr" target="#b8">[9]</ref> available publicly 3 to measure the lip-sync error between the generated frames and the randomly chosen speech segment. The accuracy of SyncNet averaged over a video clip is over 99% <ref type="bibr" target="#b8">[9]</ref>. Thus, we believe this can be a good automatic evaluation method that explicitly tests for accurate lipsync in unconstrained videos in the wild. Note that this is not the expert lip-sync discriminator that we have trained above, but the one released by Chung and Zisserman <ref type="bibr" target="#b8">[9]</ref>, which was trained on a different, non-public dataset. Using a SyncNet resolves major issues of the existing evaluation framework. We no longer need to sample random, temporally incoherent frames and SyncNet also takes into account short-range temporal consistency while evaluating lip-sync. Thus, we propose two new metrics automatically determined using the SyncNet model. The first is the average error measure calculated in terms of the distance between the lip and audio representations, which we code-name as "LSE-D" ("Lip Sync Error -Distance"). A lower LSE-D denotes a higher audio-visual match, i.e., the speech and lip movements are in sync. The second metric is the average confidence score, which we code-name as "LSE-C" (Lip Sync Error -Confidence). Higher the confidence, the better the audio-video correlation. A lower confidence score denotes that there are several portions of the video with completely out-of-sync lip movements. Further details can be found in the SyncNet paper <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">A Consistent</head><p>Benchmark to Evaluate Lip-sync in the wild. Now that we have an automatic, reliable metric that can be computed for any video and audio pairs, we can sample random speech samples instead of a random frame at each time-step. Thus, we can create a list of pairs of video and a pseudo-randomly chosen audio as a consistent test set. We create three consistent benchmarks test sets, one each using the test set videos of LRS2 <ref type="bibr" target="#b0">[1]</ref>, LRW <ref type="bibr" target="#b7">[8]</ref>, and LRS3 <ref type="bibr" target="#b2">[3]</ref> respectively. For each video V s , we take the audio from another randomly-sampled video V t with the condition that the length of 3 github.com/joonson/syncnet_python the speech V t be less than V s . We create 14K audio-video pairs using LRS2. Using the LRW test set, we create 28K pairs, and this set measures the performance on frontal/near-frontal videos <ref type="bibr" target="#b1">[2]</ref>. We also create 14K pairs using the LRS3 test set, which will be a benchmark for lip-syncing in profile views as well. The complete evaluation toolkit will be publicly released for consistent and reliable benchmarking of lip-syncing videos in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparing the Models on the New Benchmark</head><p>We compare the previous two approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> on our newly created test set using the LSE-D and LSE-C metrics. During inference, we now feed the same reference and pose-prior at each time-step, similar to how it has been described before in the architecture section. The mean LSE-D and LSE-C scores are shown in <ref type="table">Table 1</ref> for the audio-video pairs in all three test splits. Additionally, to measure the quality of the generated faces, we also report the FrÃľchet Inception Distance (FID). Our method outperforms previous approaches by a large margin indicating the significant effect of strong lip-sync discrimination. We can also see the significant improvement in quality after using a visual quality discriminator along with a lip-sync expert discriminator. However, we observe a minor drop in sync accuracy after using the visual quality discriminator. Thus, we will release both of these models, as they have a slight trade-off between visual quality and sync accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Real-World Evaluation</head><p>Apart from evaluating on just the standard datasets, our new evaluation framework and metrics allow us to evaluate on real-world videos on which these models are most likely to be used. Further, given the sensitivity of humans to audio-lip synchronization <ref type="bibr" target="#b8">[9]</ref>, it is necessary to also evaluate our results with the help of human evaluators. Thus, contrary to the previous works on speakerindependent lip-syncing, we conduct both quantitative and human evaluation experiments on unconstrained real videos from the web for the first time. Thus, we collect and publicly release "ReSyncED" a "Real-world Evaluation Dataset" to subjectively and objectively benchmark the performance of lip-sync works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Curating</head><p>ReSyncED. All our videos are downloaded from YouTube. We specifically choose three types of video examples. The first type "Dubbed", contains videos where the audio is naturally out-of-sync, such as dubbed movie clips or public addresses that are live translated to a different language (so the addresser's lips  <ref type="table">Table 2</ref>: Real world evaluation using our newly collected ReSyncED benchmark. We evaluate using both quantitative metrics and human evaluation scores across three classes of real videos. We can see that in all cases, the Wav2Lip model produces high-quality, accurate lip-syncing videos. Specifically, the metrics indicate that our lip-synced videos are as good as the real synced videos. We also note that human evaluations indicate that there is a scope for improvement when trying to lip-sync TTS generated speech. Finally, it is worth noting that our lip-synced videos are preferred over existing methods or the actual unsynced videos over 90% of the time.</p><p>are out-of-sync with the translated speech). The second type is "Random", where we have a collection of videos and we create random audio-visual pairs similar to 4.2.2. The third and final type of videos, "TTS", has been specifically chosen for benchmarking the lip-syncing performance on synthetic speech obtained from a text-to-speech system. This is essential for future works that aspire to automatically translate videos (Face-to-Face Translation <ref type="bibr" target="#b17">[18]</ref>) or rapidly create new video content. We manually transcribe the text, use Google Translate (about 5 languages totally) and publicly available text-to-speech models to generate synthetic translated speech for the videos in this category. The task is to correct lip movements in the original videos to match this synthetic speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.2</head><p>Real-world Evaluation on ReSyncED. We first evaluate the generated real video results using our new automatic metrics, "LSE-D" and "LSE-C" obtained from SyncNet <ref type="bibr" target="#b8">[9]</ref>. For the human evaluation, we ask 14 evaluators to judge the different synced versions of the videos based on the following parameters: (a) Sync Accuracy (b) Visual Quality (to evaluate the extent of visual artifacts), (c) Overall Experience (to evaluate the overall experience of the audio-visual content), and (d) Preference, where the viewer chooses the version of the video that is most appealing to watch. The first three parameters are scored between 1 − 5, and (d) is a single-choice voting, and we report the percentage of votes obtained by a model. We evaluate each of the three classes of videos separately and report our results in <ref type="table">Table 2</ref>. An outcome worth noting is that the previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> which produce several out-of-sync segments are less preferred over the unsynced version as the latter still preserves good Visual quality. Thus, ours is the first work that provides a significant improvement over unsynced talking face videos in-thewild. We also show some qualitative comparisons in <ref type="figure">Figure 3</ref> which contains a few generated samples from the ReSyncED test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Is our expert discriminator best among the alternatives?</head><p>Model  <ref type="table">Table 3</ref>: A larger temporal window allows for better lip-sync discrimination. On the other hand, training the lip-sync discriminator on the generated faces deteriorates its ability to detect off-sync audio-lip pairs. Consequently, training a lipsync generator using such a discriminator leads to poorly lip-synced videos.</p><p>Our expert discriminator uses T v = 5 video frames to measure the lip-sync error. It is also not fine-tuned on the generated faces in a GAN setup. We justify these two design choices in this ablation study. We can test the discriminator's performance by randomly sampling in-sync and off-sync pairs from the LRS2 test set. We vary the size of T v = 1, 3, 5 to understand its effect on detecting sync. We also fine-tune/freeze each of the three variants of T v while training the Wav2Lip model. Thus, we get a total of 6 variations in <ref type="table">Table 3</ref> from which we can clearly make two observations. Increasing the temporal window size T v consistently provides a better lip-sync discrimination performance. More importantly, we see that if we fine-tune the discriminator on the generated faces that contain artifacts, then the discriminator loses its ability to detect out-of-sync audio-visual pairs. We argue that this happens because the finetuned discriminator focuses on the visual artifacts in the generated <ref type="figure">Figure 3</ref>: Examples of faces generated from our proposed models (green and yellow outlines). We compare with the current best approach <ref type="bibr" target="#b17">[18]</ref> (red outline). The text is shown for illustration to denote the utterance being spoken in the frame shown. We can see that our model produces accurate, natural lip shapes. The addition of a visual quality discriminator also significantly improves the visual quality. We strongly encourage the reader to check out the demo video on our website. faces for discrimination, rather than the fine-grained audio-lip correspondence. Thus, it classifies the real unsynced pairs as "in-sync", since these real face images do not contain any artifacts. Down the line, using such a weak discriminator leads to poor lip-sync penalization for our generator, resulting in poorly lip-synced talking face videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATIONS &amp; FAIR USE</head><p>At a time when our content consumption and social communication is becoming increasingly audio-visual, there is a dire need for large-scale video translation and creation. Wav2Lip can play a vital role in fulfilling these needs, as it is accurate for videos in the wild. For instance, online lecture videos that are typically in English can now be lip-synced to (automatically) dubbed speech in other local languages ( <ref type="table">Table 2</ref>, last block). We can also lip-sync dubbed movies making them pleasant to watch (Table 2, first block). Every day throughout the globe, press conferences and public addresses are live translated but the addresser's lips are out of sync with the translated speech. Our model can seamlessly correct this. Automatically animating the lips of CGI characters to the voice actors' speech can save several hours of manual effort while creating animated movies and rich, conversational game content. We demonstrate our model on all these applications and more in the demo video on our website.</p><p>We believe that it is also essential to discuss and promote fair use of the increasingly capable lip-sync works. The vast applicability of our models with near-realistic lip-syncing capabilities for any identity and voice raises concerns about the potential for misuse. Thus, we strongly suggest that any result created using our code and models must unambiguously present itself as synthetic. In addition to the strong positive impact mentioned above, our intention to completely open-source our work is that it can simultaneously also encourage efforts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> in detecting manipulated video content and their misuse. We believe that Wav2Lip can enable several positive applications and also encourage productive discussions and research efforts regarding fair use of synthetic content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we proposed a novel approach to generate accurate lipsynced videos in the wild. We have highlighted two major reasons why current approaches are inaccurate while lip-syncing unconstrained talking face videos. Based on this, we argued that a pretrained, accurate lip-sync "expert" can enforce accurate, natural lip motion generation. Before evaluating our model, we re-examined the current quantitative evaluation framework and highlight several major issues. To resolve them, we proposed several new evaluation benchmarks and metrics, and also a real-world evaluation set. We believe future works can be reliably judged in this new framework. Our Wav2Lip model outperforms the current approaches by a large margin in both quantitative metrics and human evaluations. We also investigated the reasons behind our design choices in the discriminator in an ablation study. We encourage the readers to view the demo video on our website. We believe our efforts and ideas in this problem can lead to new directions such as synthesizing expressions and head-poses along with the accurate lip movements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MethodVideo Type LSE-D ↓ LSE-C ↑ FID ↓ Sync Acc. Visual Qual. Overall Experience Preference</figDesc><table><row><cell>Unsynced Orig. Videos</cell><cell></cell><cell>12.63</cell><cell>0.896</cell><cell>-</cell><cell>0.21</cell><cell>4.81</cell><cell>3.07</cell><cell>3.15%</cell></row><row><cell>Speech2Vid [17]</cell><cell></cell><cell>14.76</cell><cell>1.121</cell><cell>19.31</cell><cell>1.14</cell><cell>0.93</cell><cell>0.84</cell><cell>0.00%</cell></row><row><cell>LipGAN [18]</cell><cell>Dubbed</cell><cell>10.61</cell><cell>2.857</cell><cell>12.87</cell><cell>2.98</cell><cell>3.91</cell><cell>3.45</cell><cell>2.35%</cell></row><row><cell>Wav2Lip (ours)</cell><cell></cell><cell>6.843</cell><cell>7.265</cell><cell>15.65</cell><cell>4.13</cell><cell>3.87</cell><cell>4.04</cell><cell>34.3%</cell></row><row><cell>Wav2Lip + GAN (ours)</cell><cell></cell><cell>7.318</cell><cell>6.851</cell><cell>11.84</cell><cell>4.08</cell><cell>4.12</cell><cell>4.13</cell><cell>60.2%</cell></row><row><cell>Without Lip-syncing</cell><cell></cell><cell>17.12</cell><cell>2.014</cell><cell>-</cell><cell>0.15</cell><cell>4.56</cell><cell>2.98</cell><cell>3.24%</cell></row><row><cell>Speech2Vid [17]</cell><cell></cell><cell>15.22</cell><cell>1.086</cell><cell>19.98</cell><cell>0.87</cell><cell>0.79</cell><cell>0.73</cell><cell>0.00%</cell></row><row><cell>LipGAN [18]</cell><cell>Random</cell><cell>11.01</cell><cell>3.341</cell><cell>14.60</cell><cell>3.42</cell><cell>3.77</cell><cell>3.57</cell><cell>3.16%</cell></row><row><cell>Wav2Lip (ours)</cell><cell></cell><cell>6.691</cell><cell>8.220</cell><cell>14.47</cell><cell>4.24</cell><cell>3.68</cell><cell>4.01</cell><cell>29.1%</cell></row><row><cell>Wav2Lip + GAN (ours)</cell><cell></cell><cell>7.066</cell><cell>8.011</cell><cell>13.12</cell><cell>4.18</cell><cell>4.05</cell><cell>4.15</cell><cell>64.5%</cell></row><row><cell>Without Lip-syncing</cell><cell></cell><cell>16.89</cell><cell>2.557</cell><cell>-</cell><cell>0.11</cell><cell>4.67</cell><cell>3.32</cell><cell>8.32%</cell></row><row><cell>Speech2Vid [17]</cell><cell></cell><cell>14.39</cell><cell>1.471</cell><cell>17.96</cell><cell>0.76</cell><cell>0.71</cell><cell>0.69</cell><cell>0.00%</cell></row><row><cell>LipGAN [18] Wav2Lip (ours)</cell><cell>TTS</cell><cell>10.90 6.659</cell><cell>3.279 8.126</cell><cell>11.91 12.77</cell><cell>2.87 3.98</cell><cell>3.69 3.87</cell><cell>3.14 3.92</cell><cell>1.64% 41.2%</cell></row><row><cell>Wav2Lip + GAN (ours)</cell><cell></cell><cell>7.225</cell><cell>7.651</cell><cell>11.15</cell><cell>3.85</cell><cell>4.13</cell><cell>4.05</cell><cell>51.2%</cell></row><row><cell>Untranslated Videos</cell><cell></cell><cell>7.767</cell><cell>7.047</cell><cell>-</cell><cell>4.83</cell><cell>4.91</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-forspeech-to-lip-generation-in-the-wild</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02108</idno>
		<title level="m">Deep Audio-Visual Speech Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Conversation: Deep Audio-Visual Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00496</idno>
		<title level="m">LRS3-TED: a large-scale dataset for visual speech recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="520" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sound to Visual: Hierarchical Cross-Modal Talking Face Video Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02966</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">You said that? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-view Lip-reading</title>
		<imprint>
			<publisher>ACCV</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An audiovisual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pflaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jikuo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07397</idno>
		<title level="m">The DeepFake Detection Challenge Dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text-based editing of talking-head video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TCD-TIMIT: An audio-visual corpus of continuous speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Harte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eoin</forename><surname>Gillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="603" to="615" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Fake Image Detection based on Pairwise Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Xiu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Yen</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">370</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">You said that?: Synthesising talking faces from audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="1767" to="1779" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards Automatic Face-to-Face Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Prajwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrabha</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerin</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1428" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Obamanet: Photo-realistic lip-sync from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01442</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Alexandre de Brébisson, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Percent of Millennial Smartphone Owners Use their Device for Video Calling, According to The NPD Group</title>
		<idno>NPD. 2016. 52</idno>
		<ptr target="https://www.npd.com/wps/portal/npd/us/news/press-releases/2016/52-percent-of-millennial-smartphone-owners-use-their-device-for-video-calling-according-to-the-npd-group/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ayush Tewari, Christian Theobalt, and Matthias Nießner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05566</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Voice Puppetry: Audio-driven Facial Reenactment</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Tolosana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Vera-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aythami</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Ortega-Garcia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00179</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards Untrusted Social Video Verification to Combat Deepfakes via Face Geometry Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleanor</forename><surname>Tursman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seny</forename><surname>Kamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tompkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Realistic speech-driven facial animation with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07860</idno>
		<title level="m">Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
