<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance Selection for GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Guelph Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance Selection for GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in Generative Adversarial Networks (GANs) have led to their widespread adoption for the purposes of generating high quality synthetic imagery. While capable of generating photo-realistic images, these models often produce unrealistic samples which fall outside of the data manifold. Several recently proposed techniques attempt to avoid spurious samples, either by rejecting them after generation, or by truncating the model's latent space. While effective, these methods are inefficient, as a large fraction of training time and model capacity are dedicated towards samples that will ultimately go unused. In this work we propose a novel approach to improve sample quality: altering the training dataset via instance selection before model training has taken place. By refining the empirical data distribution before training, we redirect model capacity towards high-density regions, which ultimately improves sample fidelity, lowers model capacity requirements, and significantly reduces training time. Code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in Generative Adversarial Networks (GANs) have enabled these models to be considered a tool of choice for vision synthesis tasks that demand high fidelity outputs, such as image and video generation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>, image editing <ref type="bibr" target="#b40">[41]</ref>, inpainting <ref type="bibr" target="#b34">[35]</ref>, and superresolution <ref type="bibr" target="#b31">[32]</ref>. However, when sampling from a trained GAN model, outputs may be unrealistic just as often as they appear photo-realistic. GANs fit a model to a data distribution with the help of a discriminator network. Low quality samples produced by these models are often attributed to poor modeling of the low-density regions of the data manifold <ref type="bibr" target="#b10">[11]</ref>. The majority of current techniques attempt to eliminate low quality samples after the model is trained, either by changing the model distribution by truncating the latent space <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref> or by performing some form of rejection sampling using a trained discriminator to inform the rejection process <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31]</ref>. Nevertheless, these methods are inefficient with respect to model capacity and training time, since much of the capacity and optimization efforts dedicated to representing the sparse regions of the data manifold are wasted.</p><p>In this paper, we analyze the use of instance selection <ref type="bibr" target="#b20">[21]</ref> in the generative setting. We address the problem of uneven model sample quality before GAN model training has begun, rather than after it has finished. We note that dataset collection is a noisy process, and that many of the currently used datasets for generative model training and evaluation were not purposely created for this task. Thus, through a dataset curation step, we remove low density regions from the data manifold prior to model optimization and show that this direct dataset intervention <ref type="bibr" target="#b0">(1)</ref> improves overall image sample quality in exchange for some reduction in diversity, (2) lowers model capacity requirements, and (3) reduces training time. To remove the sparsest parts of the image manifold, images are first projected into an embedding space of perceptually meaningful representations. A scoring function is then fit to asses the manifold density in the neighbourhood of each embedded data point in the dataset. Finally, data points with the lowest manifold density scores are removed from the dataset. In our experiments, we evaluate a variety of image embeddings and scoring functions, observing that Inceptionv3 and Gaussian likelihood are well suited for the respective roles. Overall, we make the following contributions:</p><p>• We propose dataset curation via instance selection to improve the output quality of GANs. • We show that the manifold density in the perceptual embedding space of a given dataset is predictive of GAN performance, and therefore a good scoring function for instance selection. • We demonstrate the model capacity savings of instance selection by achieving state-ofthe-art performance (in terms of FID) on 64 × 64 resolution ImageNet generation using a Self-Attention GAN with 1 /2 the amount of trainable parameters of the current best model. • We demonstrate training time savings by training a 128 × 128 resolution BigGAN on ImageNet in 1 /4 the time of the baseline, while also achieving superior performance across all image fidelity metrics. • We exhibit the overall computational savings of instance selection by training a 256 ×256 resolution BigGAN on ImageNet with only 4 V100 GPUs in 11 days. Our model achieves better image fidelity than the baseline model while using 1 /2 as many trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generative modelling of images is a very challenging problem due to the high dimensional nature of images and the complexity of the distributions they form. Several different approaches towards image generation have been proposed, with GANs currently the state-of-the-art in terms of image generation quality. In this work we will focus primarily on GANs, but other types of generative models might also benefit from instance selection prior to model fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sample Filtering in GANs</head><p>One way to improve the sample quality from GANs without making any changes to the architecture or optimization algorithm is by applying techniques which automatically filter out poor quality samples from a trained model. Discriminator Rejection Sampling (DRS) <ref type="bibr" target="#b0">[1]</ref> accomplishes this by performing rejection sampling on the generator. This process is informed by the discriminator, which is reused to estimate density ratios between the real and generated image manifolds. Metropolis-Hastings GAN (MH-GAN) <ref type="bibr" target="#b30">[31]</ref> builds on DRS by i) calibrating the discriminator to achieve more accurate density ratio estimates, and by ii) applying Markov chain Monte Carlo (MCMC) instead of rejection sampling for better performance on high dimensional data. Ding et al. <ref type="bibr" target="#b7">[8]</ref> further improve density ratio estimates by fine-tuning a pretrained ImageNet classifier for the task. For more efficient sampling, Discriminator Driven Latent Sampling (DDLS) <ref type="bibr" target="#b4">[5]</ref> iteratively updates samples in the latent space to push them closer to realistic outputs.</p><p>Instead of filtering samples after the GAN has been trained, some methods do so during the training procedure. Latent Optimisation for Generative Adversarial Networks (LOGAN) <ref type="bibr" target="#b32">[33]</ref> optimizes latent samples each iteration at the cost of an additional forward and backward pass. Sinha et al. <ref type="bibr" target="#b26">[27]</ref> demonstrate that gradients from low quality generated samples drive the model away from the nearest mode rather than towards it. As such, gradients from the worst samples each iteration during training may be ignored to improve generation quality.</p><p>Perhaps the most well known approach for increasing sample fidelity in GANs is the "truncation trick" <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>. The truncation trick is used in the popular models BigGAN <ref type="bibr" target="#b1">[2]</ref> and StyleGAN <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> to improve image quality by manipulating the latent distribution. The original truncation trick as used by BigGAN consists of replacing the latent distribution with a truncated distribution during inference, such that any latent sample that falls outside of some acceptable range is resampled. StyleGAN uses a similar strategy by interpolating samples towards the mean of the latent space instead of resampling them. By moving samples closer to the interior regions of the latent space, sample diversity can effectively be traded for visual fidelity. Our instance selection technique has an effect similar to the truncation trick, but with the added benefit of also reducing model capacity and training time requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instance Selection</head><p>Instance selection is a data preprocessing technique commonly used in the classification setting to select a subset of data from a larger collection <ref type="bibr" target="#b20">[21]</ref>. In general, instance selection methods either attempt to reduce the size of the dataset to a more manageable size while retaining informative data points, or try to clean the dataset by eliminating noisy data points. Though commonly used in the setting of big data, instance selection has received little attention from the generative modelling community. Nuha et al. <ref type="bibr" target="#b19">[20]</ref> explore the impact of reducing the size of the training set when training GANs. However, they select data points randomly, and no significant improvement in performance is observed from the removal of data. Core-set selection has been shown to be useful for improving GAN performance when training with small mini-batches, but it ultimately does not improve image fidelity over large mini-batch training <ref type="bibr" target="#b25">[26]</ref>. Whereas core-set selection attempts to select mini-batches that mimic the distribution of the original dataset, our proposed technique purposefully redefines the target distribution so as to maximize the density of the data manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Instance Selection for GANs</head><p>In the context of generative modeling, our motivation is to automatically remove the sparsest regions of the data manifold, specifically those parts that GANs struggle to capture. To do so, we define an image embedding function F and a scoring function H.</p><p>Embedding function F projects images into an embedding space. More precisely, given a dataset of images X , the dataset of embedded images Z is obtained by applying the embedding function z = F (x) to each data point x ∈ X . For the task of image generation we suggest using perceptually aligned embedding functions <ref type="bibr" target="#b36">[37]</ref>, such as the feature space of a pretrained image classifier.</p><p>Scoring function H is used to to assess the manifold density in a neighbourhood around each embedded data point z. In our experiments, we compare three choices of scoring function: log likelihood under a standard Gaussian model, log likelihood under a Probabilistic Principal Component Analysis (PPCA) <ref type="bibr" target="#b28">[29]</ref> model, and distance to the K th nearest neighbour (KNN Distance). We select Gaussian and PPCA as simple, well known density models. KNN Distance has previously been used as a measure of local manifold density in classical instance selection <ref type="bibr" target="#b2">[3]</ref>, and has been shown to be useful for defining non-linear image manifolds <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>The Gaussian model is fit to the embedded dataset by computing the empirical mean µ and the sample covariance Σ of Z. The score of each embedded image z is computed as follows:</p><formula xml:id="formula_0">H Gaussian (z) = − 1 2 [ln(|Σ|) + (z − µ) T Σ −1 (z − µ) + d ln(2π)],<label>(1)</label></formula><p>where d is the dimension of z.</p><p>PPCA is fit to the embedded dataset using any standard PPCA solver <ref type="bibr" target="#b21">[22]</ref>. We set the number of principal components such that 95% of the variance in the data is preserved. Embedded images are scored as follows:</p><formula xml:id="formula_1">H PPCA (z) = − 1 2 [ln(|C|) + Tr((z − µ) T C −1 (z − µ)) + d ln(2π)], C = WW T + σ 2 I,<label>(2)</label></formula><p>where W is the fit model weight matrix, µ is the empirical mean of Z, σ is the residual variance, I is the identity matrix, and d is the dimension of z.</p><p>KNN Distance is used to score data points by calculating the Euclidean distance between z and Z \ {z}, then returning the distance to the K th nearest element. To convert to a score, we make the resulting distance negative, such that smaller distances return larger values. Formally, we can evaluate:</p><formula xml:id="formula_2">H KNN (z, K, Z) = − min K ||z − z i || 2 : z i ∈ Z \ {z} ,<label>(3)</label></formula><p>where min K is defined as the K th smallest value in a set. In our experiments we set K = 5.</p><p>To perform instance selection, we compute scores H(F (x)) for each data point and keep all data points with scores above some threshold ψ. For convenience, we often set ψ to be equal to some percentile of the scores, such that we preserve the top N % of the best scoring data points. Thus, given an initial training set consisting of data points x ∈ X we construct our reduced training set X by computing:</p><formula xml:id="formula_3">X = {x ∈ X s.t. H(F (x)) &gt; ψ}.<label>(4)</label></formula><p>To illustrate why removing data points from the training set might be a good idea, we look at the most and least likely images from the red fox class of ImageNet ( <ref type="figure">Figure 1</ref>). Likelihood is determined by a Gaussian model fit on feature embeddings from a pretrained Inceptionv3 classifier. We notice a stark contrast between the content of the images. The most likely images (a) are similarly cropped around the fox's face, while the least likely images (b) have many odd viewpoints and often suffer from occlusion. It is logical to imagine how a generative model trained on these unusual instances may try to generate samples that mimic such conditions, resulting in undesirable outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we review evaluation metrics, motivate selecting instances based on manifold density, and then analyze the impact of applying instance selection to GAN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Metrics</head><p>We use a variety of evaluation metrics to diagnose the effect that training with instance selection has on the learned distribution, including: (1) Inception Score (IS) <ref type="bibr" target="#b23">[24]</ref>, (2) Fréchet Inception Distance (FID) <ref type="bibr" target="#b9">[10]</ref>, (3) Precision and Recall (P&amp;R) <ref type="bibr" target="#b13">[14]</ref>, and (4) Density and Coverage (D&amp;C) <ref type="bibr" target="#b18">[19]</ref>. In all cases where a reference distribution is required we use the original training distribution. Using the distribution produced after instance selection would unfairly favour the evaluation of instance selection, since the reference distribution could be changed to one that is trivially easy to generate. A detailed description of each evaluation metric is provided in the supplementary material ( §A).</p><p>When calculating FID we follow Brock et al. <ref type="bibr" target="#b1">[2]</ref> in using all images in the training set to estimate the reference distribution, and sampling 50 k images to make up the generated distribution. For P&amp;R and D&amp;C we use an Inceptionv3 embedding. 1 N and M are set to 10 k samples for both the reference and generated distributions, and K is set equal to 5 as recommended by Naeem et al. <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relationship Between Dataset Manifold Density and GAN Performance</head><p>An image manifold is more accurately defined in regions where many data points are in close proximity to each other <ref type="bibr" target="#b13">[14]</ref>. Since GANs attempt to reproduce an image manifold based on data points from a given dataset, we suspect that they should perform better on datasets with well-defined manifolds (i.e. no sparse manifold regions). To verify this hypothesis, we use the ImageNet 2 dataset <ref type="bibr" target="#b6">[7]</ref> and treat each of the 1000 classes as a separate dataset. Ideally, we would fit a separate GAN on each class to obtain a ground truth measure of performance, but this is very computationally expensive. Instead, we use a single class-conditional BigGAN from <ref type="bibr" target="#b1">[2]</ref> that has been pretrained on ImageNet at 128 × 128 resolution. For each class, we sample 700 real images from the dataset, and generate 700 class-conditioned samples with the BigGAN. To measure the density for each class manifold we compare three different methods: Gaussian likelihood, Probabilistic Principal Component Analysis (PPCA) likelihood, and distance to the K th neighbour (KNN Distance) ( §3). Images are projected into the feature space of an Inceptionv3 model, and a manifold density score is computed on the features using one of our scoring functions. As an indicator of the true GAN output quality we compute FID between the real and generated distributions for each class.</p><p>We observe a strong correlation between each of the manifold density measures and GAN output quality ( <ref type="figure" target="#fig_1">Figure 2</ref>). This correlation confirms our hypothesis, suggesting that dataset manifold density is an important factor for achieving high quality generated samples with GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Embedding and Scoring Function</head><p>Having established that dataset manifold density is correlated with GAN performance, we explore artificially increasing the overall density of the training set by removing data points that lie in low density regions of the data manifold. To this end, we train several Self-Attention GANs (SAGAN) <ref type="bibr" target="#b35">[36]</ref> on ImageNet at 64 × 64 resolution. Each model is trained on a different 50% subset of ImageNet, as chosen by instance selection using different embedding and scoring functions as described in §3. Instance selection is applied per-class. We use the default settings for SAGAN, except that we use a batch size of 128 instead of 256, apply the self-attention module at 32 × 32 resolution instead of 64 × 64, and reduce the number of channels in each layer by half in order to reduce the computational cost of our initial exploratory experiments. All models are trained for 200k iterations. The results of these experiments are shown in <ref type="table" target="#tab_0">Table 1</ref>. For reference, we include scores achieved by real (i.e. not generated) data in <ref type="table" target="#tab_4">Table 5</ref> in the supplementary material. All runs utilizing instance selection significantly outperform the baseline model trained on the full dataset, despite only having access to half as much training data <ref type="table" target="#tab_0">(Table 1)</ref>. We observe a large increase in image fidelity, as indicated by the improvements in Inception Score, Precision, and Density, and a slight drop in overall diversity, as measured by Recall. Coverage, which measures realism-constrained diversity, benefits greatly from the more realistic samples and thus sees an increase, despite the reduction in overall diversity. Since the increase in image quality is much greater than the decrease in diversity, FID also improves. To verify that the gains are not simply caused by the reduction in dataset size we train a model on a 50% subset that was uniform-randomly sampled from the full dataset. Here, we observe little change in performance compared to the baseline, indicating that performance improvements are indeed due to careful selection of training data, rather than the reduction of dataset size.</p><p>We find that all three candidate scoring functions: Gaussian likelihood, PPCA likelihood, and KNN distance, significantly outperform the full dataset baseline. Gaussian likelihood slightly outperforms the alternatives, so we use it as the scoring function in the remainder of our experiments.</p><p>To understand the importance of the embedding function, we compare several different model embeddings that have been trained on different datasets: Inceptionv3 <ref type="bibr" target="#b27">[28]</ref> trained on ImageNet, ResNet50 <ref type="bibr" target="#b8">[9]</ref> trained on Places365 <ref type="bibr" target="#b39">[40]</ref>, ImageNet, and with SwAV unsupervised pretraining <ref type="bibr" target="#b3">[4]</ref>, and ResNeXt-101 32x8d <ref type="bibr" target="#b33">[34]</ref> trained with weak supervision on Instagram 1B <ref type="bibr" target="#b14">[15]</ref>. We also compare a randomly initialized Inceptionv3 with no pretraining as a random embedding. For all architectures, features are extracted after the global average pooling layer. We find that all feature embeddings improve performance over the full dataset baseline except for the randomly initialized network. These results suggest that an embedding function that is well aligned with the target domain is required in order for instance selection to be effective. The ImageNet pretrained Inceptionv3 embedding performs best overall, and was chosen as the embedding function for the rest of our experiments. We note that using an Inceptionv3 embedding both in instance selection and in the evaluation metrics may yield some non-negligible advantage in evaluation, since selected instances are those that the network prefers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Retention Ratio</head><p>An important consideration when performing instance selection is determining what proportion of the original dataset to keep, a hyperparameter which we call retention ratio. To investigate the impact of the retention ratio on training, we train ten SAGANs on ImageNet, each retaining different amounts of the original dataset in 10% intervals. GAN hyperparameters are the same as in §4.3, except that we extend training until 500k iterations in order to observe model behaviours over a longer training window. Results are shown in <ref type="figure" target="#fig_2">Figure 3</ref> and <ref type="table" target="#tab_5">Table 6</ref> in the supplementary material.  As larger portions of the original dataset are removed we see consistent improvements in image fidelity (increasing Inception Score, Precision, and Density) and reductions in sample diversity (decreasing Recall). Interestingly, metrics which take into account both realism and diversity (FID and Coverage) continue to see gains until roughly 70% of the dataset has been removed, at which point they begin to decrease. This behaviour suggests that, given the ability of current state-of-the-art models to learn from limited data, sample fidelity is valued much more than diversity. When too much of the dataset is removed some models collapse prematurely, likely due to the discriminator quickly overfitting the small training set. It is expected that applying data augmentation could resolve this issue <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref>. To further improve image fidelity, instance selection could be combined with the truncation trick ( §E).</p><p>Our best performing SAGAN model in terms of FID was trained on only 40% of the ImageNet dataset, yet outperforms FQ-BigGAN <ref type="bibr" target="#b38">[39]</ref>, the current state-of-the-art model for the task of 64 × 64 ImageNet generation. Despite using 2× less parameters and a 4× smaller batch size, our SAGAN achieves a better FID (9.07 vs. 9.76). As indicated by these scores and the errors made by a pretrained classifier, samples from our instance selection model are significantly more recognizable than those from the baseline model trained on the full dataset ( <ref type="figure" target="#fig_3">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">128 × 128 ImageNet</head><p>To examine the impact of instance selection on the training time of large-scale models, we train two BigGAN models on 128 × 128 ImageNet 3 . Our baseline model uses the default hyperparameters from BigGAN <ref type="bibr" target="#b1">[2]</ref>, with the exception that we reduce the channel multiplier from 96 to 64 (i.e. half of the capacity) and only use a single discriminator update instead of two for faster training. Our instance selection model uses the same settings as the baseline, but is trained on 50% of the dataset. Although large batch sizes are critical for achieving good performance with the baseline BigGAN <ref type="bibr" target="#b1">[2]</ref>, we found them to degrade performance when combined with instance selection. Therefore, we reduce the batch size from BigGAN's default of 2048 to 256 for the instance selection model. Both models are trained on 8 NVIDIA V100 GPUs with 16GB of RAM, using gradient accumulation to achieve the necessary batch sizes.</p><p>Despite using a much smaller batch size, our model trained with instance selection outperforms the baseline in all metrics except for Recall <ref type="table" target="#tab_1">(Table 2)</ref>, as expected due to the diversity/fidelity trade-off. The instance selection model trains significantly faster than the baseline, requiring less than four days while the baseline requires more than two weeks.  <ref type="table" target="#tab_2">Table 3</ref>, and samples are shown in <ref type="figure">Figure 5</ref> and §G in the supplementary material.</p><p>Our instance selection model trains in less than 11 days, and uses approximately one order of magnitude less multiply-accumulate operations (MACS) than the baseline throughout the duration of training. Despite having half as much capacity, our model outperforms the baseline in all image fidelity focused metrics (Inception Score, Precision, and Density), and achieves comparable performance on metrics that jointly consider image quality and diversity (FID and Coverage). As expected, the better image quality comes at the cost of overall sample diversity (indicated by Recall). To our knowledge, this is the first time photorealistic generation of 256 × 256 ImageNet images has been achieved without the use of specialized hardware (i.e. hundreds of TPUs). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Instance Selection in Practice</head><p>As the experiments have shown, instance selection stands as a useful tool for trading away sample diversity in exchange for improvements in image fidelity, faster training, and lower model capacity requirements. We believe that this trade-off is a worthwhile hyperparameter to tune in consideration of the available compute budget, just as it is common practice to adjust model capacity or batch size to fit within the memory constraints of the available hardware.</p><p>The control over the diversity/fidelity trade-off afforded by instance selection also yields a tool that can be used to better understand the behaviour and limitations of existing evaluation metrics. For instance, in some cases when applying instance selection, we observed that certain diversity-sensitive metrics (such as FID and Coverage) improved, even though the diversity of the training set had been significantly reduced. We leave it for future work to determine whether this is a limitation of these metrics, or a behaviour that should be expected.</p><p>Finally, instance selection can be used to automatically curate new datasets for the task of image generation. Existing datasets that are designed for image synthesis often use manual filtering and hand-crafted cropping and alignment tools to increase the dataset manifold density <ref type="bibr" target="#b10">[11]</ref>. As an alternative to these time-intensive procedures, instance selection provides a generic solution that can quickly be applied to any uncurated set of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Folk wisdom suggests more data is better, however, it is known that areas of the data manifold that are sparsely represented pose a challenge to current GANs <ref type="bibr" target="#b10">[11]</ref>. To directly address this challenge we introduce a new tool: dataset curation via instance selection. Our motivation is to remove sparse regions of the data manifold before training, acknowledging that they will ultimately be poorly represented by the GAN, and therefore, that attempting to capture them is an inefficient use of model capacity. Moreover, popular post-processing methods such as rejection sampling or latent space truncation will likely ignore these regions as represented by the model. There are multiple benefits of taking the instance selection approach: (1) We improve sample fidelity across a variety of metrics compared to training on uncurated data; <ref type="bibr" target="#b1">(2)</ref> We demonstrate that reallocating model capacity to denser regions of the data manifold leads to efficiency gains, meaning that we can achieve SOTA quality with smaller-capacity models trained in far less time. To our knowledge, instance selection has not yet been formally analyzed in the generative setting. However, we argue that it is more important here than in supervised learning because of the absence of an annotation phase where humans often perform some kind of formal or informal curation.</p><p>We have only considered the setting where curation is performed up-front, prior to training. However, our results suggest that dynamic curation, including curriculum learning informed by the kinds of perceptually aligned embeddings we consider here, is an interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The application of instance selection to the task of image generative modelling brings with it several benefits. Gains in image generation quality are an obvious improvement, but perhaps more impactful to the broader community are the reductions in model capacity and training time that are afforded. Reducing the computational barrier to entry for training large-scale generative models provides many individuals, including students, AI artists, and ML enthusiasts, with access to models that are otherwise restricted to only the most well resourced labs. In addition to greater accessibility, lowering the computational requirements for training large-scale generative models also reduces associated energy costs and CO 2 emissions associated with the training process.</p><p>One side effect of our instance selection method is that, by nature of design, generated results are more likely to reflect the content that makes up the majority of the training set. As such, dataset bias is amplified as instances that are poorly represented in the dataset may be completely ignored. However, this limitation can be addressed by properly balancing the training set before instance selection is applied or alternatively, ensuring a more diverse &amp; inclusive data collection effort to begin with.</p><p>As with any form of generative model, there is some potential for misuse. A common example is "deepfakes", where a generative model is used to manipulate images or videos well enough that humans cannot distinguish real from fake. While often used to create humorous videos in which actors' faces are swapped, deepfakes also have the potential for more nefarious uses, such as for blackmail or spreading misinformation. Fortunately, much recent effort has been dedicated towards automatic detection of these false images <ref type="bibr" target="#b29">[30]</ref>. These techniques attempt to find manipulated media by detecting inconsistencies, such as in the synchronization of lip movement and speech audio, or generation artifacts, such as missing reflections or other minute details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Description of Evaluation Metrics</head><p>We use a variety of evaluation metrics to diagnose the effect that training with instance selection has on the learned distribution. In all cases where a reference distribution is required we use the original training distribution, and not the distribution produced after instance selection. Doing so would unfairly favour the evaluation of instance selection, since the reference distribution could be changed to one that is trivially easy to generate.</p><p>Inception Score (IS) <ref type="bibr" target="#b23">[24]</ref> evaluates samples by extracting class probabilities from an ImageNet pretrained Inceptionv3 classifier and measuring the distribution of outputs over all samples. The Inception Score is maximized when a model produces highly recognizable outputs for each of the ImageNet classes. One of the major limitations of the Inception Score is its insensitivity to mode collapse within each class. A model that produces a single high quality image for each category can still achieve a good score.</p><p>Fréchet Inception Distance (FID) <ref type="bibr" target="#b9">[10]</ref> measures the distance between a generated distribution and a reference distribution, as approximated by a Gaussian fit to samples projected into the feature space of a pretrained Inceptionv3 model. FID has been shown to correlate well with image quality, and is capable of detecting mode collapse and mode adding. However, FID does not differentiate between fidelity and diversity. As such, it is difficult to assess whether a model has achieved a good FID score based on good mode coverage, or because it produces high quality samples.</p><p>Precision and Recall (P&amp;R) <ref type="bibr" target="#b13">[14]</ref> were designed to address the limitations of FID by providing separate metrics to evaluate fidelity and diversity. To calculate P&amp;R, image manifolds are created by first embedding each image in a given distribution into the feature space of a pretrained classifier. A radius is then extended from each data point to its K th nearest neighbour to form a hypersphere, and the union of all hyperspheres represents the image manifold. Precision is described as the percentage of generated samples that fall within the manifold of real images. Recall is described as the percentage of real images which fall within the manifold of generated samples. A limitation of P&amp;R is that they are susceptible to outliers, both in the reference and generated distributions <ref type="bibr" target="#b18">[19]</ref>. Outliers artificially inflate the size of the image manifolds, increasing the rate at which samples fall into those manifolds. Thus, a dataset or model that produces many outliers may achieve scores that are better than the quality of the samples would indicate.</p><p>Density and Coverage (D&amp;C) <ref type="bibr" target="#b18">[19]</ref> have recently been proposed as robust alternatives to Precision and Recall. Density can be seen as an extension of Precision which measures how many real image manifolds a generated sample falls within on average. Coverage is described as the percentage of real images that have a generated sample fall within their manifold.</p><p>Classification Accuracy Score (CAS) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> was introduced for evaluating the usefulness of conditional generative models for augmenting downstream tasks such as image classification. To compute CAS, generated samples are used to train a classifier, which is then used to classify real data from a test set. Generally, it is observed that models with greater sample diversity achieve higher CAS, with image fidelity being of less importance. We do not evaluate CAS for the majority of our experiments as it is very computationally expensive to compute, but we do report it in § B, <ref type="table" target="#tab_3">Table 4</ref> for our 128 × 128 ImageNet BigGAN experiments as a reference for how instance selection affects CAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Evaluation Metrics -Classification Accuracy Score (CAS)</head><p>We compute CAS by training a ResNet50 on samples from each of our 128 × 128 BigGAN models using the standard ImageNet pipeline from PyTorch 5 . We find that the model trained without instance selection achieves the best CAS, which is expected given that this model also produces more diverse samples (as measured by Recall). Interestingly, CAS for the BigGAN trained with instance selection drops by less than 1%, despite it only having seen 50% of the ImageNet training set. This result might suggest that neither of the models evaluated does a good job at generating recognizable outliers from the ImageNet training set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Scores of Evaluation Metrics on Real Data</head><p>For each evaluation metric we compute scores on real data <ref type="table" target="#tab_4">(Table 5</ref>) as a reference for comparison with the values produced by generative models. These values can be thought of as the scores which would be achieved by a generative model that perfectly captures the target distribution. Metrics are evaluated on the ImageNet validation set, using all 50k data points for IS and FID and 10k randomly selected data points for P&amp;R and D&amp;C. Note that it is possible for generative models to surpass the scores of real data for metrics that focus on image fidelity, such as IS, P, and D, but these models often have proportionally lower diversity scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Retention Ratio Experiment Numerical Results</head><p>In <ref type="table" target="#tab_5">Table 6</ref> we include numerical results for the retention ratio experiments conducted in §4.4. These values accompany the plots in <ref type="figure" target="#fig_2">Figure 3</ref>. We also report the performance of BigGAN and FQ-BigGAN from <ref type="bibr" target="#b38">[39]</ref> for comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Complementarity of Instance Selection and Truncation</head><p>The truncation trick is a simple and popular technique which is used to increase the visual fidelity of samples from a GAN at the expense of reduced diversity <ref type="bibr" target="#b1">[2]</ref>. This trade-off is achieved by biasing latent samples towards the interior regions of the latent distribution, either by truncating the distribution, or by interpolating latent samples towards the mean <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>To examine the compatability between the truncation trick and instance selection, we truncate latent vectors of the models trained in §4.4, varying the truncation threshold from 1.0 to 0.1 <ref type="figure">(Figure 6</ref>). We observe that combining both techniques results in a greater improvement in visual fidelity than either method applied in isolation. We anticipate that other post-hoc filtering methods could also see complimentary benefits when combined with instance selection, such as DRS, MH-GAN, and DDLS.  <ref type="figure">Figure 6</ref>: Truncation trick applied to models trained with instance selection for truncation thresholds 1 to 0.1. The base models (threshold = 1) are marked with a •. Up and to the right is best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Insights for Applying Instance Selection to GANs</head><p>We found that, while instance selection could be used to achieve significant gains in model performance, some changes to other hyperparameters were necessary in order to ensure training stability.</p><p>Here we detail some techniques that we found to work well in our experiments.</p><p>• Reduce batch size -Contrary to evidence from BigGAN <ref type="bibr" target="#b1">[2]</ref> suggesting that larger batch sizes improve GAN performance, we found batch sizes larger than 256 to degrade performance when training with instance selection. We speculate that because we have simplified the training distribution by removing the difficult examples, the discriminator overfits the training set much faster. We posit that the smaller batch size could be acting as a form of regularization by reducing the accuracy of the gradients, thereby allowing the generator to train for longer before the discriminator overfits the training set and the model collapses. • Reduce model capacity -Since the complexity of the training set is reduced when applying instance selection, we found it necessary in some cases to also reduce model capacity.</p><p>Training models with too much capacity lead to early collapse, also likely caused by the discriminator quickly overfitting the training set. We note that with proper regularization, models trained with instance selection could still benefit from more capacity. • Apply additional regularization -We have not experimented much with applying GAN regularization methods to our models, but think that it could be important for combating the aforementioned discriminator overfitting problem. Applying techniques such as R1 regularization <ref type="bibr" target="#b17">[18]</ref> or recently proposed GAN data augmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref> could allow for instance selection to be combined with the benefits of larger batch sizes and model capacity.</p><p>We leave this investigation for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Sample Sheets</head><p>We generate several different sample visualizations in order to better understand the impact that instance selection has on GAN behaviour.</p><p>In <ref type="figure">Figure 7</ref> we showcase some photorealistic samples generated by a 256 × 256 BigGAN model trained with instance selection.</p><p>In <ref type="figure">Figure 8</ref> we compare randomly selected samples from the official pretrained 256 × 256 BigGAN <ref type="figure">(Figure 8a</ref>) with random samples from our 256 × 256 BigGAN trained with 50% instance selection <ref type="figure">(Figure 8b)</ref>. Samples from the instance selection model appear more realistic on average.</p><p>To better understand how instance selection affects sample diversity, we visualize image manifolds of different datasets and models by organizing images in 2D using UMAP <ref type="bibr" target="#b16">[17]</ref>  <ref type="figure">(Figure 9</ref>). We only plot a single class so that we can see variations across the image manifold in greater detail than if multiple classes were plotted simultaneously. All image samples share the same 2D embedding, such that manifolds are comparable between datasets and models. We observe that even though instance selection has removed 50% of the images from the original dataset <ref type="figure">(Figure 9a</ref>), it still retains coverage over most of the original image manifold <ref type="figure">(Figure 9b</ref>). Only images containing extreme viewpoints are omitted. The GANs trained on the original and reduced datasets both cover less of the image manifold than their respective source datasets. While the baseline GAN ( <ref type="figure">Figure 9c</ref>) covers more of the image manifold than the GAN trained with instance selection <ref type="figure">(Figure 9d</ref>), samples from these extra regions often appear less realistic. resolution. Manifolds are created by embedding all images into an Inceptionv3 feature space, then projecting them into 2D with UMAP <ref type="bibr" target="#b16">[17]</ref>. All images share the same 2D embedding such that subplots are comparable. Instance selection removes images from the dataset that have unusual viewpoints or pose. Both GANs appear to cover less of the image manifold than their respective source datasets. The GAN trained on the full dataset covers some regions of the image manifold that are not covered by the model trained with instance selection, however, these regions are more likely to appear unrealistic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )Figure 1 :</head><label>a1</label><figDesc>Images with highest likelihood (b) Images with least likelihood Examples of the (a) most and (b) least likely resized images of red foxes from the ImageNet dataset, as determined by a Gaussian model fit on images in an Inceptionv3 embedding space. High likelihood images share a similar visual structure, while low likelihood samples are more varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Correlation between manifold density estimates and FID for each class in the ImageNet dataset. Lower values on the x-axis indicate a more dense dataset manifold. Lower values on the y-axis indicate better quality generated samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>SAGAN trained on 64 × 64 ImageNet, with instance selection used to reduced the dataset by varying amounts. Retention ratio = 100 indicates a model trained on the full dataset (i.e. no instance selection). The application of instance selection boosts overall performance significantly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Samples of bird classes from SAGAN trained on 64×64 ImageNet. Each row is conditioned on a different class. Red borders indicate misclassification by a row-specific pretrained Inceptionv3 classifier. Instance selection (b) significantly improves sample fidelity and class consistency compared to the baseline (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 9 :</head><label>79</label><figDesc>Photorealistic samples from BigGAN trained on 256 × 256 ImageNet with 50% instance selection. Samples are manually selected to showcase the best quality outputs from this model.(a) Full dataset (b) Dataset after 50% instance selection (c) Samples from GAN trained on full dataset (d) Samples from GAN trained on 50% of dataset Visualization of the image manifolds for the red pandas class from (a) the full ImageNet dataset, (b) the dataset after 50% instance selection, (c) samples from a GAN trained on the full dataset, and (d) samples from a GAN trained on 50% of the dataset. All images are at 128 × 128</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of embedding and scoring functions on 64 × 64 ImageNet image generation task. All tests train a SAGAN model for 200k iterations. Models trained with instance selection significantly outperform models trained without instance selection, despite training on a fraction of the available data. RR is the retention ratio (percentage of dataset trained on). Best results in bold.</figDesc><table><row><cell>Instance Selection</cell><cell>RR (%)</cell><cell>Embedding</cell><cell>Pretraining</cell><cell cols="3">IS ↑ FID ↓ P ↑</cell><cell>R ↑</cell><cell>D ↑</cell><cell>C ↑</cell></row><row><cell>None</cell><cell>100</cell><cell>-</cell><cell>-</cell><cell>15.4</cell><cell>21.4</cell><cell cols="3">0.66 0.62 0.64 0.64</cell></row><row><cell>Uniform</cell><cell>50</cell><cell>-</cell><cell>-</cell><cell>15.5</cell><cell>22.8</cell><cell cols="3">0.65 0.62 0.65 0.65</cell></row><row><cell>Gaussian</cell><cell>50</cell><cell>Inceptionv3</cell><cell>ImageNet</cell><cell>25.7</cell><cell>12.6</cell><cell cols="3">0.77 0.59 0.97 0.83</cell></row><row><cell>PPCA</cell><cell>50</cell><cell>Inceptionv3</cell><cell>ImageNet</cell><cell>25.5</cell><cell>13.2</cell><cell cols="3">0.76 0.58 0.97 0.82</cell></row><row><cell cols="2">KNN Dist 50</cell><cell>Inceptionv3</cell><cell>ImageNet</cell><cell>25.4</cell><cell>13.1</cell><cell cols="3">0.76 0.58 0.97 0.82</cell></row><row><cell>Gaussian</cell><cell>50</cell><cell>Inceptionv3</cell><cell cols="2">Random init 15.5</cell><cell>21.9</cell><cell cols="3">0.66 0.61 0.68 0.65</cell></row><row><cell>Gaussian</cell><cell>50</cell><cell>ResNet-50</cell><cell>Places365</cell><cell>20.6</cell><cell>16.5</cell><cell cols="3">0.74 0.59 0.88 0.76</cell></row><row><cell>Gaussian</cell><cell>50</cell><cell>ResNet-50</cell><cell>SwAV</cell><cell>20.3</cell><cell>16.7</cell><cell cols="3">0.74 0.57 0.89 0.76</cell></row><row><cell>Gaussian</cell><cell>50</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>22.0</cell><cell>14.6</cell><cell cols="3">0.76 0.59 0.92 0.79</cell></row><row><cell>Gaussian</cell><cell cols="4">50 ResNeXt-101 Instagram 1B 24.1</cell><cell>14.1</cell><cell cols="3">0.73 0.61 0.86 0.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of models on the 128 × 128 ImageNet image generation task. Both models use a channel multiplier of 64 and a single discriminator update per generator update. The baseline model uses a batch size of 2048, while the instance selection model uses a batch size of 256.</figDesc><table><row><cell>Model</cell><cell>IS ↑</cell><cell cols="2">FID ↓ P ↑</cell><cell>R ↑</cell><cell>D ↑</cell><cell>C ↑</cell><cell>Time ↓</cell><cell>Hardware</cell></row><row><cell>BigGAN</cell><cell>68.8</cell><cell>11.5</cell><cell cols="5">0.76 0.66 0.9 0.84 14.8 days</cell><cell>8 V100</cell></row><row><cell cols="2">BigGAN + Inst. Sel. 114.3</cell><cell>9.6</cell><cell cols="5">0.88 0.50 1.34 0.90 3.7 days</cell><cell>8 V100</cell></row></table><note>Figure 5: Samples from BigGAN trained on 256 × 256 ImageNet, with the truncation trick. Samples are selected to demonstrate the highest quality outputs for each model. The baseline model (a) struggles to produce convincing facial details, which the instance selection model (b) successfully achieves. Zoom in for best viewing.4.6 256 ×256 ImageNet To further demonstrate instance selection we train a BigGAN on ImageNet at 256 × 256 resolution using 4 V100s with 32GB of RAM each. Since training a baseline model without instance selection with the same hardware setup would take an excessively long time (1-2 months), we instead compare to the 256 × 256 BigGAN from Brock et al. [2] using the official pretrained weights 4 . Compared to this baseline, our model uses half the capacity (channel multiplier reduced from 96 to 64), 8× smaller batch size (from 2048 to 256), and applies the self-attention block in the generator at a resolution of 64 × 64 instead of 128 × 128. The retention ratio for instance selection is set to 50%. Similar to the baseline, we use two discriminator update steps per generator update for this experiment. Quantitative results are presented in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of models for 256 × 256 ImageNet image generation. The instance selection model uses half as many parameters as the baseline model. All metrics are computed using PyTorch Inceptionv3 embeddings, and may therefore differ from numbers computed with TensorFlow.</figDesc><table><row><cell>Model</cell><cell>IS ↑</cell><cell cols="2">FID ↓ P ↑</cell><cell>R ↑</cell><cell>D ↑</cell><cell>C ↑</cell><cell>Time</cell><cell>Hardware</cell></row><row><cell>BigGAN</cell><cell>135.4</cell><cell>9.8</cell><cell cols="6">0.86 0.70 1.18 0.92 1-2 days 256 TPUv3</cell></row><row><cell cols="2">BigGAN + Inst. Sel. 165.3</cell><cell>10.6</cell><cell cols="5">0.91 0.52 1.48 0.93 10.7 days</cell><cell>4 V100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>CAS for BigGAN trained with and without instance selection. Following<ref type="bibr" target="#b22">[23]</ref>, both models use a truncation ratio of 1.5 when generating samples for increased diversity.</figDesc><table><row><cell>Training Set</cell><cell cols="3">Resolution Top-5 Accuracy Top-1 Accuracy</cell></row><row><cell>BigGAN</cell><cell>128 × 128</cell><cell>18.73</cell><cell>9.21</cell></row><row><cell cols="2">BigGAN + 50% inst. sel. 128 × 128</cell><cell>17.94</cell><cell>8.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Scores of real data from the ImageNet validation set for all evaluation metrics.</figDesc><table><row><cell>Resolution</cell><cell>IS ↑</cell><cell cols="2">FID ↓ P ↑</cell><cell>R ↑</cell><cell>D ↑</cell><cell>C ↑</cell></row><row><cell>64 × 64</cell><cell>59.1</cell><cell>1.0</cell><cell cols="4">0.79 0.79 0.99 0.96</cell></row><row><cell>128 × 128</cell><cell>148.2</cell><cell>1.2</cell><cell cols="4">0.84 0.82 1.01 0.96</cell></row><row><cell>256 × 256</cell><cell>225.9</cell><cell>1.4</cell><cell cols="4">0.85 0.83 1.01 0.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance of models trained on 64 × 64 resolution ImageNet. A retention ratio of less than 100 indicates that instance selection is used. Best results in bold.</figDesc><table><row><cell>Model</cell><cell>Params (M)</cell><cell>Batch Size</cell><cell>Retention Ratio (%)</cell><cell>IS ↑</cell><cell cols="2">FID ↓ P ↑</cell><cell>R ↑</cell><cell>D ↑</cell><cell>C ↑</cell></row><row><cell>BigGAN</cell><cell>52.54</cell><cell>512</cell><cell>100</cell><cell cols="2">25.43 10.55</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FQ-BigGAN</cell><cell>52.54</cell><cell>512</cell><cell>100</cell><cell>25.96</cell><cell>9.67</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>100</cell><cell cols="6">17.77 17.23 0.68 0.66 0.72 0.71</cell></row><row><cell></cell><cell></cell><cell></cell><cell>90</cell><cell cols="6">18.98 15.85 0.70 0.66 0.75 0.74</cell></row><row><cell></cell><cell></cell><cell></cell><cell>80</cell><cell cols="6">21.62 13.17 0.74 0.65 0.87 0.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell>70</cell><cell cols="6">23.95 11.98 0.75 0.64 0.92 0.82</cell></row><row><cell>SAGAN</cell><cell>23.64</cell><cell>128</cell><cell>60 50</cell><cell cols="6">27.95 10.35 0.78 0.63 0.99 0.87 31.04 9.63 0.79 0.62 1.07 0.88</cell></row><row><cell></cell><cell></cell><cell></cell><cell>40</cell><cell>37.10</cell><cell>9.07</cell><cell cols="4">0.81 0.60 1.12 0.90</cell></row><row><cell></cell><cell></cell><cell></cell><cell>30</cell><cell>41.85</cell><cell>9.75</cell><cell cols="4">0.83 0.55 1.19 0.90</cell></row><row><cell></cell><cell></cell><cell></cell><cell>20</cell><cell cols="6">43.30 12.36 0.82 0.49 1.17 0.88</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell cols="6">37.16 19.24 0.79 0.33 1.07 0.78</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the PyTorch pretrained Inceptionv3 embedding for all metrics.<ref type="bibr" target="#b1">2</ref> Use of ImageNet is only for noncommercial, research purposes, and not for training networks deployed in production or for other commercial uses.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Baseline (trained on 100% of dataset) (b) Instance Selection (trained on 40% of dataset)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use the official BigGAN implementation from https://github.com/ajbrock/BigGAN-PyTorch. 7 (a) Baseline (trained on 100% of dataset) (b) Instance Selection (trained on 50% of dataset)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Pretrained BigGAN weights from https://colab.research.google.com/github/tensorflow/ hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/pytorch/examples/tree/master/imagenet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>The authors would like to thank Colin Brennan, with whom discussions about dataset learnability kicked off this project, and Brendan Duke, for being a constant sounding board. Resources used in preparing this research were provided to GWT and TD, in part, by NSERC, the Canada Foundation for Innovation, the Province of Ontario, the Government of Canada through CIFAR, Compute Canada, and companies sponsoring the Vector Institute: http://www.vectorinstitute.ai/#partners.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discriminator rejection sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno>abs/1810.06758</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A density-based approach for instance selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Luis Carbonera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mara</forename><surname>Abel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Your gan is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/2003.06060</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient video generation on complex datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06571</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Subsampling generative adversarial networks: Density ratio estimation in feature space with softplus loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Jane</forename><surname>Xin Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04958</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Megapixel size image creation using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marchesi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00082</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04406</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Muhammad Ferjad Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09797</idno>
		<title level="m">Reliable fidelity and diversity metrics for generative models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training dataset reduction on generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fajar Ulin Nuha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia computer science</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="133" to="139" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A review of instance selection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturo</forename><surname>Olvera-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Carrasco-Ochoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Martínez-Trinidad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="143" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classification accuracy score for conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12268" to="12279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How good is my gan?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Small-gan: Speeding up gan training using core-sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13540</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Top-k training of gans: Improving generators by making critics less critical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno>abs/2002.06224</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher M</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deepfakes and beyond: A survey of face manipulation and fake detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Tolosana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Vera-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aythami</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Ortega-Garcia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00179</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Metropolis-hastings generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saatci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logan</surname></persName>
		</author>
		<idno>abs/1912.00953</idno>
	</analytic>
	<monogr>
		<title level="m">Latent optimisation for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for dataefficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10738</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02088</idno>
		<title level="m">Feature quantization improves gan training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">In-domain gan inversion for real image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00049</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Baseline (100% of dataset)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">w\ Instance selection (50% of dataset)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Uncurated samples from BigGAN models trained on 256 ×256 resolution ImageNet. Each row is conditioned on a different class (from top): Red-breasted Merganser</title>
		<meeting><address><addrLine>Lynx, Collie, Mink, Gibbon, Barn, Castle, Drilling Platform, Promontory</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
