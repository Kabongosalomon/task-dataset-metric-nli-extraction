<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Multi-target regression trees with stacked leaf models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saulo</forename><forename type="middle">Martiello</forename><surname>Mastelini</surname></persName>
							<email>mastelini@usp.br</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Mathematics and Computer Sciences</orgName>
								<orgName type="institution">University of São Paulo São Carlos</orgName>
								<address>
									<postCode>13566-590</postCode>
									<region>BR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvio</forename><surname>Barbon</surname><genName>Jr</genName></persName>
							<email>barbon@uel.br</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department State</orgName>
								<orgName type="institution">University of Londrina Londrina</orgName>
								<address>
									<postCode>86057-970</postCode>
									<region>BR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Carlos</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Mathematics and Computer Sciences</orgName>
								<orgName type="institution">University of São Paulo São Carlos</orgName>
								<address>
									<postCode>13566-590</postCode>
									<region>BR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ponce</forename><surname>De</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Mathematics and Computer Sciences</orgName>
								<orgName type="institution">University of São Paulo São Carlos</orgName>
								<address>
									<postCode>13566-590</postCode>
									<region>BR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Ferreira De Carvalho</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Mathematics and Computer Sciences</orgName>
								<orgName type="institution">University of São Paulo São Carlos</orgName>
								<address>
									<postCode>13566-590</postCode>
									<region>BR</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Multi-target regression trees with stacked leaf models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the current challenges in machine learning is how to deal with data coming at increasing rates in data streams. New predictive learning strategies are needed to cope with the high throughput data and concept drift. One of the data stream mining tasks where new learning strategies are needed is multi-target regression, due to its applicability in a high number of real world problems. While reliable and effective learning strategies have been proposed for batch multi-target regression, few have been proposed for multi-target online learning in data streams. Besides, most of the existing solutions do not consider the occurrence of inter-target correlations when making predictions. In this work, we propose a novel online learning strategy for multi-target regression in data streams. The proposed strategy extends existing online decision tree learning algorithm to explore inter-target dependencies while making predictions. For such, the proposed strategy, called Stacked Single-target Hoeffding Tree (SST-HT), uses the inter-target dependencies as an additional information source to enhance predictive accuracy. Throughout an extensive experimental setup, we evaluate our proposal against state-of-the-art decision tree-based algorithms for online multi-target regression. According to the experimental results, SST-HT presents superior predictive accuracy, with a small increase in the processing time and memory requirements.</p><p>Recent advances of computing technologies have increased the amount of data being produced, resulting in data streams of potentially unbounded size. These advances also boosted the speed in which computers process and exchange data. While previous generations of machine learning (ML) algorithms were concerned with processing relatively small amounts of data (in batches), without time restrictions, the new challenges brought by big data changed the needs and shifted the research efforts to other directions.</p><p>As data can arrive fast and in large volumes, the data stream algorithms must be able to process each incoming example just once (since all data cannot be indefinitely stored) <ref type="bibr" target="#b34">(Read et al., 2012)</ref>. Moreover, the induced models must be ready to predict new cases at any point and expect an infinite</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction data stream (despite using finite and limited resources regarding time and memory) <ref type="bibr" target="#b34">(Read et al., 2012;</ref><ref type="bibr" target="#b20">Kocev et al., 2013;</ref><ref type="bibr" target="#b37">Sousa and Gama, 2018)</ref>.</p><p>The continuous data flow may present novel characteristics and bring new challenges for which traditional ML algorithms were not designed to deal with. They include concept drift (CD), novelty detection, among other aspects <ref type="bibr" target="#b11">(Gama, 2010;</ref><ref type="bibr" target="#b22">Krempl et al., 2014)</ref>. Nonetheless, in this work we focus on stationary streams. Besides, data streams are varied and can come from many sources, ranging from sensor networks and manufacturing processes to video streams and user operations in a web browser <ref type="bibr" target="#b1">Bifet et al. (2018)</ref>. Data streams impose new requirements to ML algorithms, such as fast and incremental learning, robustness to noise, and low memory need.</p><p>In the data stream mining literature, most of the efforts have been devoted to dealing with singletarget (ST) tasks, mostly for classification <ref type="bibr" target="#b11">(Gama, 2010;</ref><ref type="bibr" target="#b28">Nguyen et al., 2015;</ref><ref type="bibr" target="#b14">Gomes et al., 2017;</ref><ref type="bibr" target="#b21">Krawczyk et al., 2017)</ref>. A small number of studies addresses other tasks, for instance single-target regression (STR) <ref type="bibr" target="#b18">(Ikonomovska et al., 2011b;</ref><ref type="bibr" target="#b9">Duarte et al., 2016;</ref><ref type="bibr" target="#b15">Gouk et al., 2019)</ref>. Moreover, little attention has been given to structured output tasks <ref type="bibr" target="#b20">(Kocev et al., 2013;</ref><ref type="bibr" target="#b2">Borchani et al., 2015;</ref><ref type="bibr" target="#b40">Waegeman et al., 2018)</ref>, i.e., when multiple target variables are related to the same set of input features. Notwithstanding, this type of prediction tasks reflects many aspects of real-world problems, including several problems associated with data streams, such as predicting river flow properties, multiple product sales and airline ticket prices <ref type="bibr" target="#b34">(Read et al., 2012;</ref><ref type="bibr" target="#b38">Spyromitros-Xioufis et al., 2016;</ref><ref type="bibr" target="#b37">Sousa and Gama, 2018)</ref>. In this work, we focus on multi-target regression (MTR) tasks, which are concerned, as the name implies, with the simultaneous prediction of multiple continuous target values. These targets can be correlated, since they are explained by the same set of predictive feature values or represent correlated quantities in real-world problems. As a consequence, this information can be used by ML algorithms to improve the overall predictive performance.</p><p>ML research in MTR is a relatively new research area, even for batch data mining problems <ref type="bibr" target="#b2">(Borchani et al., 2015;</ref><ref type="bibr" target="#b38">Spyromitros-Xioufis et al., 2016;</ref><ref type="bibr" target="#b24">Mastelini et al., 2017;</ref><ref type="bibr" target="#b26">Melki et al., 2017;</ref><ref type="bibr" target="#b23">Mastelini et al., 2018;</ref><ref type="bibr" target="#b36">Santana et al., 2018)</ref>. There is still room for improvement in the few existing online solutions <ref type="bibr" target="#b17">(Ikonomovska et al., 2011a;</ref><ref type="bibr" target="#b8">Duarte and Gama, 2015;</ref><ref type="bibr" target="#b32">Osojnik et al., 2018)</ref>, e.g., by improving the way in which inter-target dependencies are explored. In fact, some of the ideas implemented for batch data can be adapted to online scenarios, without largely impacting the necessary computational resources. However, as online MTR algorithms must cope with the requirements of learning from data streams, the search for a balance between performance and feasible MTR solutions is essential.</p><p>We propose a new MTR algorithm, called Stacked Single-Target Hoeffding Tree (SST-HT), which extends existing incremental decision tree induction algorithms. SST-HT combines the simultaneous prediction of multiple targets to explore their inter-dependencies. SST-HT follows the common trend in batch MTR literature of using stacked single-target (SST) regression models to improve predictive performance <ref type="bibr" target="#b38">(Spyromitros-Xioufis et al., 2016;</ref><ref type="bibr" target="#b24">Mastelini et al., 2017</ref><ref type="bibr" target="#b23">Mastelini et al., , 2018</ref><ref type="bibr" target="#b26">Melki et al., 2017;</ref><ref type="bibr" target="#b36">Santana et al., 2018)</ref>. Nevertheless, SST-HT does not change the way the decision trees are built, i.e., how the decision splits are performed, nor it highly impacts the required computational resources. Therefore, we expect the same tree structure and improved predictive performance in comparison with traditional tree-based algorithms for MTR in data streams. Two variations of SST-HT are evaluated: the first only uses the stacked regressors as predictors, and the second dynamically selects between stacked regressors, the target mean, or linear predictors.</p><p>Experimental results show the superior predictive performance of SST-HT, when compared with variations of the iSOUP-Tree algorithm in sixteen MTR datasets. To the best of our knowledge, this is the highest number of datasets used so far as benchmarks for MTR in data streams. Six of the evaluated datasets were first employed as MTR resources in this research. Among these new datasets, SCFP was specially tailored for this work by adapting an existing dataset and adding textual (provided by a word embedding model <ref type="bibr" target="#b33">(Pennington et al., 2014)</ref>) and geolocated information (see Appendix A for more details). The remaining newly introduced datasets are derived from wellknown and publicly available real-world data, commonly used in other data analysis tasks. In the experiments carried out for this study, SST-HT obtained the best predictive performance, while adding small extra memory consumption, and a processing time linearly comparable to the other algorithms.</p><p>The remainder of this work is organized as follows. Section 2 presents a brief background on MTR solutions for data streams, as well as a literature review on the subject. Section 3 provides the theoretical foundation of traditional Hoeffding Tree (HT) algorithms, the basis of SST-HT and related algorithms. SST-HT is described in detail in Section 4. Section 5 presents our experimental setup, including the datasets, evaluation strategy, and metrics, as well as the configurations used for the decision tree algorithms. The obtained results are discussed in Section 6, and our final considerations presented in Section 7. Finally, detailed information concerning the datasets used and the obtained results are presented in Appendices A and B.</p><p>2 Background and Related Work MTR deals with the prediction of multiple continuous target variables, using the same set of predictive variables. This task can be seen as an extension of STR tasks <ref type="bibr" target="#b2">(Borchani et al., 2015;</ref><ref type="bibr" target="#b32">Osojnik et al., 2018)</ref>. Nevertheless, MTR aims not only at modelling the input to output relations, but also possible inter-output dependencies. This can improve the representation of the problem to be solved, and, as a result, the predictive performance. On the other hand, this additional effort demands solutions specially tailored for MTR tasks, which are often more complex than using a separate STR model for each target variable. Formally, a MTR task can be described as the search for a function f , able to model the relation between a set X of m input variables (real, ordinal or nominal values) and a set Y ⊂ R d of d output variables. Therefore, a MTR task can be represented by the expression</p><formula xml:id="formula_0">f : X → Y.</formula><p>Function f can be used to predictŷ for an instance x ∈ X. When inducing f , it is expected that the predicted values,ŷ, are as close as possible to the true values, y.</p><p>According to <ref type="bibr" target="#b20">Kocev et al. (2013)</ref>, MTR algorithms follow two main approaches: global and local. Global algorithms use a single model to predict all target variables at once. These algorithms implicitly model the inter-target dependencies, and offer more compact and less computationally costly solutions, which are more suited to online scenarios. Local algorithms combine traditional STR solutions and often manipulate or modify the input space to insert inter-target dependency information within the modelling process. Thus, local algorithms use multiple ST regressors to solve an MTR task, often more than one regressor for each target variable <ref type="bibr" target="#b38">(Spyromitros-Xioufis et al., 2016;</ref><ref type="bibr" target="#b24">Mastelini et al., 2017;</ref><ref type="bibr" target="#b36">Santana et al., 2018;</ref><ref type="bibr" target="#b23">Mastelini et al., 2018)</ref>. As a result, they have a higher cost than global algorithms. The simplest local solution for MTR tasks, as previously mentioned, is the induction of an STR for each target, overlooking inter-target dependencies. In this work we apply local-based techniques within a global tree-based MTR algorithm.</p><p>MTR have been widely used in batch learning applications <ref type="bibr" target="#b2">(Borchani et al., 2015;</ref><ref type="bibr" target="#b38">Spyromitros-Xioufis et al., 2016;</ref><ref type="bibr" target="#b24">Mastelini et al., 2017;</ref><ref type="bibr" target="#b26">Melki et al., 2017;</ref><ref type="bibr" target="#b23">Mastelini et al., 2018;</ref><ref type="bibr" target="#b36">Santana et al., 2018)</ref>, since they are related to several real-life problems, such as prediction of river flow properties, online sales, airline ticket prices and poultry meat properties <ref type="bibr" target="#b38">(Spyromitros-Xioufis et al., 2016;</ref><ref type="bibr" target="#b36">Santana et al., 2018)</ref>. However, there are few papers investigating the use of MTR (and even STR) problems in online learning tasks. Applications of MTR in data streams not only have the same computational constraints as in online ST classification applications, but they also bring the additional challenge of simultaneously producing multiple predictions <ref type="bibr" target="#b2">(Borchani et al., 2015;</ref><ref type="bibr" target="#b40">Waegeman et al., 2018)</ref>.</p><p>One of the key works where regression algorithms were used for data stream mining was <ref type="bibr" target="#b18">Ikonomovska et al. (2011b)</ref>. In this paper, the authors proposed an online and incremental algorithm to induce regression trees in the presence of CD. The proposed algorithm, called FIMT-DD (Fast Incremental Model Tree with Drift Detection), adopts the Hoeffding's bound theorem to decide whether a split decision must be made <ref type="bibr" target="#b7">(Domingos and Hulten, 2000;</ref><ref type="bibr" target="#b11">Gama, 2010)</ref>. The main aspects of their algorithm are very similar to VFDT (Very Fast Decision Tree) <ref type="bibr" target="#b7">(Domingos and Hulten, 2000)</ref>. FIMT-DD uses perceptrons with linear activation function at the tree's leaves to provide the responses. As it is one of the first works dealing with regression problems on data streams, the authors mostly evaluated their approach against traditional batch regression algorithms. Notwithstanding, their research pioneered the research on STR and MTR for data streams. However, FIMT-DD was designed to deal only with numerical attributes. This limits its application to numerical data only, unless some data transformation technique, e.g., one hot encoding, is used.</p><p>The same authors also proposed the FIMT-MT (Fast Incremental Model Tree -Multitarget) <ref type="bibr" target="#b17">(Ikonomovska et al., 2011a)</ref>, an extension of the FIMT-DD algorithm for MTR settings. This proposal, a global approach, uses aspects of predictive clustering trees <ref type="bibr" target="#b20">(Kocev et al., 2013)</ref> to make decision splits on multiple targets. FIMT-MT considers each split as the induction of a cluster. Thus, the root node corresponds to the cluster that contains all the data. Each new split tries to reduce the intra-cluster variance of the new created partitions, while maximizing the inter-cluster variance. Similarly to the FIMT-DD algorithm, FIMT-MT has perceptrons in its leaves, one model per target. On the other hand, no mechanism for dealing with CD is inherited from the original STR algorithm. FIMT-MT also supports only numerical attributes.</p><p>Deviating from tree-based strategies, <ref type="bibr" target="#b0">Almeida et al. (2013)</ref> proposed the Adaptive Model Rules (AMRules) algorithm for online STR tasks. This algorithm was expanded by <ref type="bibr" target="#b9">Duarte et al. (2016)</ref>. AMRules also uses linear perceptrons as the consequent of the rules. Moreover, it employs a built-in mechanism for dealing with CD based on the Page-Hinkley (PH) test <ref type="bibr" target="#b18">(Ikonomovska et al., 2011b)</ref>. When detecting a CD, AMRules simply drops outdated decision rules. In addition, the decision rule algorithm also has a routine to detect anomalous examples, e.g., noisy data. These examples are not used to update the decision models. <ref type="bibr" target="#b8">Duarte and Gama (2015)</ref> also expanded the AMRules framework to allow their use in MTR tasks. This expanded version is also based on the principle that the created partitions must reduce the variance in the output space. However, different from the previous solution, AMRules-MTR does not lie in the global/local categorization, since it can specialize in subsets of targets. When executing a rule expansion test, if the variance in the target space is reduced only for some targets, a new decision rule encompassing the targets benefited by the split is created. In a complementary manner, a rule without the expansion is also created for the remaining targets. Therefore, AMRules-MTR creates decision rules which can encompass all the targets, some of them, or even a single target. Hence this algorithm should be characterized as a hybrid of a local and global approach. More recently, AMRules was also adapted to deal with multi-label classification tasks <ref type="bibr" target="#b37">(Sousa and Gama, 2018)</ref>. Tree-based solutions do not have similar mechanism to explored inter-target dependencies.</p><p>Following the trend of applying tree-based algorithms to data streams, <ref type="bibr" target="#b29">(Osojnik et al., 2015a</ref><ref type="bibr" target="#b32">(Osojnik et al., , 2018</ref> proposed an extension for the FIMT-MT algorithm, called iSOUP-Tree (incremental Structured Output Prediction Tree). This algorithm builds upon the research of <ref type="bibr" target="#b17">Ikonomovska et al. (2011a)</ref> by adding support to categorical features and using an adaptive prediction model in the leaves. Instead of using only perceptrons, iSOUP-Tree also maintains a mean predictor for each target. Besides, it selects the best current model by monitoring a faded error metric for each model. The authors adapted the iSOUP-Tree algorithm for multiple settings, including ensembles (Bagging and Random Forest) and Option Trees <ref type="bibr" target="#b32">(Osojnik et al., 2018)</ref>. Moreover, they investigated the application of all the variations of the MTR algorithm to multi-label classification tasks <ref type="bibr" target="#b30">(Osojnik et al., 2015b</ref><ref type="bibr" target="#b31">(Osojnik et al., , 2017</ref>.</p><p>Neither of the previous tree-based solutions effectively take advantage of inter-target dependencies when making predictions. In all of them, individual models are created for each target. Thus, they ignore how the targets relate to each other. Inspired by <ref type="bibr" target="#b38">Spyromitros-Xioufis et al. (2016)</ref>, we propose the Single-target Hoeffding Tree (SST-HT) algorithm, which is based on iSOUP-Tree and use Stacked Single-target (SST) predictors in the tree's leaves. SST-HT can deal with the mutable characteristics of streaming tasks by automatically selecting the best current predictor for each target, i.e., whether to use SST, the standard perceptron, or the most straightforward mean predictor. Since SST-HT is based on iSOUP-Tree algorithm, and, consequently FIMT-MT, we will first present the base algorithm for building incremental MTR decision trees and later describe how SST-HT works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Online Multi-target regression Trees</head><p>This section presents the traditional strategies for inducing decision tree algorithms for data streams. First, the general Hoeffding Tree algorithm is presented, followed by its application for MTR tasks. This variant will be from here onward referred to as the Multi-target regression Hoeffding Tree (MTR-HT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hoeffding Tree algorithm</head><p>The previous tree-based solutions for online STR and MTR use the Hoeffding bound (HB) <ref type="bibr" target="#b16">(Hoeffding, 1963)</ref> for performing decision splits. This idea was first proposed by <ref type="bibr" target="#b7">Domingos and Hulten (2000)</ref> in their well-known work proposing the VFDT algorithm. HB provides statistical evidence that, given enough observations, the current most promising split decision is the best one. Therefore, splits are only performed when enough statistical evidence is gathered by the decision tree induction algorithm. Thus, the split decisions have statistical guarantees to deviate from the expected value by at most a value ξ.</p><p>Suppose a heuristic measure h that provides a score for each attempted split decision for a predictive feature. At time step or instance n, the current heuristic value is denoted by h n . The higher the h, the better the candidate input space partitioning is. Let x b be the input feature with the current best split candidate, with a score of h b . Besides, let x sb be the feature with the second best split heuristic score h sb . By monitoring the ratio h sb h b over the time, a new random variable r ∈ [0, 1] can be derived by using</p><formula xml:id="formula_1">r ∈ h 1 sb h 1 b , . . . , h n sb h n b , h n+1 sb h n+1 b , . . . .</formula><p>Considering that a stream can be potentially unbounded, calculating the expected value of r is not trivial. On the other hand, the variable mean value in time step n,r n , can be easily calculated as</p><formula xml:id="formula_2">r n = 1 n h 1 sb h 1 b + h 2 sb h 2 b + . . . + h n sb h n b = 1 n n i=1 h i sb h i b .</formula><p>Using Hoeffding's inequality <ref type="bibr" target="#b16">(Hoeffding, 1963)</ref>, we can state that the expected value of E(r) will not deviate from its sample mean at time step n by more than a factor ξ, with a confidence level 1 − δ. For brevity, herein the time/instance indicator n will be omitted from the mathematical expressions. Equation 1 gives the simplified form of the Hoeffding's inequality (considering the range of r) subjected to δ.</p><formula xml:id="formula_3">P (|r − E(r)| &gt; ξ) ≤ 2e −2nξ 2<label>(1)</label></formula><p>From Equation 1, we can isolate ξ in terms of δ in the following form</p><formula xml:id="formula_4">ξ = 1 2n ln 2 δ .<label>(2)</label></formula><p>The value of ξ obtained from Equation 2 enables us to bound a deviation interval in the form E(r) ∈ [r − ξ,r + ξ], with confidence level 1 − δ. Thus, ifr + ξ &lt; 1 then E(r) &lt; 1. Hence, we can assume that the split decision in the predictive feature that generated h b is indeed the best choice for making a new partition. Nonetheless, in some cases, two decision splits may achieve almost equal heuristic scores. This implies that they are equally good choices. An extreme example of that situation is the presence of repeated or redundant input features. In these cases, if the value of ξ is substantially shrunk, no split decision will be made. To avoid this situation, an additional threshold or tie-break parameter τ is added. Hence, a split is performed ifr + ξ &lt; 1 or ξ &lt; τ .</p><p>It worth mentioning that, though some issues with the statistical guarantees of the HB usage in HTs have been identified, still, these algorithms present consistent performance in practice <ref type="bibr" target="#b19">(Ikonomovska et al., 2015)</ref>. Corrections to these problems have been proposed in the literature, e.g., the works of <ref type="bibr" target="#b35">Rutkowski et al. (2013)</ref> and <ref type="bibr" target="#b25">Matuszyk et al. (2013)</ref>. Nonetheless, the choice for HT is still a trend for practitioners. Independently of the raised concerns, our proposal can be easily adapted to work with any heuristic strategy to control tree growth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-target regression Hoeffding Trees</head><p>Both the FIMT-MT and iSOUP-Tree employ the intra-cluster variance reduction (ICVarR) as the heuristic score, following the steps of the predictive clustering framework <ref type="bibr" target="#b20">(Kocev et al., 2013)</ref>.</p><p>In this proposal, the variance measures the dispersion of the objects in the partition (i.e., a cluster) from their center of mass (the centroid) <ref type="bibr" target="#b17">(Ikonomovska et al., 2011a)</ref>. The ICVarR calculation for a set of partitions P over Y is given by</p><formula xml:id="formula_5">ICVarR(P ) = ICVar(P ) − p∈P |p| |P | ICVar(p),</formula><p>where the intra-cluster variance (ICVar) is calculated for an example y ∈ Y as follows:</p><formula xml:id="formula_6">ICVar(y) = 1 d d t=1</formula><p>Var(y t ).</p><p>Sufficient statistics must be stored to incrementally estimate the variance for each target. These variances are used to evaluate the split candidates. As shown in recent MTR literature in data streams <ref type="bibr" target="#b17">(Ikonomovska et al., 2011a;</ref><ref type="bibr" target="#b32">Osojnik et al., 2018)</ref>, maintaining a counter of the number of elements seen (n), the sum of each target y t ( y t ), t ∈ {1, ..., d}, and the sum of their square values ( y 2 t ) for each leaf is enough to calculate the required measures. Besides, we can also standardize the features X and targets by maintaining the same set of statistics for the inputs. This action is especially relevant to avoid possible different scales for the features and targets having an negative impact on the linear prediction models and in the obtained ICVar. The input features and targets are standardized using the z-score approach, i.e., they are centered by their mean values and scaled by their standard deviation <ref type="bibr" target="#b32">(Osojnik et al., 2018)</ref>.</p><p>Numerical attributes are monitored using the Extended Binary Search Tree (E-BST), as proposed by <ref type="bibr">Ikonomovska et al. (2011b,a)</ref> and later expanded by <ref type="bibr" target="#b32">Osojnik et al. (2018)</ref>. Often, this structure is also referred to in the literature as attribute observer. The FIMT-MT algorithm does not support categorical features, as previously mentioned, but this functionality was added by the iSOUP-Tree. This algorithm creates a tree branch for each possible value in the nominal feature after splitting.</p><p>The original proposal of FIMT-MT only uses perceptron models with linear activation functions at the leaves; one predictor for each target. These models are incrementally trained using the Delta rule <ref type="bibr" target="#b18">(Ikonomovska et al., 2011b)</ref> for linear weight updating. The iSOUP-Tree algorithm introduces the use of adaptive models for each target, i.e., it decides between using the perceptrons or a more straightforward mean predictor for each new incoming instance. To this end, a fading metric of error is monitored to assess which is the current best performer for each target.</p><p>As previously mentioned, neither of the employed predicting strategies for the leaf nodes effectively take into consideration the existence of inter-target correlations. We reason that this possibility should be considered when making predictions. We believe it can increase the accuracy of the whole tree model, as well as leverage intrinsic characteristics of the dataset for making predictions. Next, we describe how SST-HT is capable of considering this aspect during the tree construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Online Multi-target regression Tree with stacked leaf models</head><p>The algorithm proposed in this paper, the Stacked Single-target Hoeffding Tree (SST-HT), was tailored to encompass the best aspects of the existing MTR tree-based solutions, while increasing the prediction performance. Our reasoning is that if there was a partition in the target space, the targets in this space must be inter-correlated. By using stacked models <ref type="bibr" target="#b12">(Gama and Brazdil, 2000;</ref><ref type="bibr" target="#b38">Spyromitros-Xioufis et al., 2016)</ref> for the leaves, these inter-correlations can be explored to decrease the prediction error. Figure1 illustrates our proposal. SST-HT builds upon the original MTR-HT tree algorithm by using an additional layer of prediction models at the leaf nodes. The other structures, such as attribute observers and statistics, are directly inherited from MTR-HT, as indicated in the figure.</p><p>The traditional use of linear models in the leaf nodes is the creation of as many perceptrons as the number of targets d. Therefore, the predictionsŷ are computed separately, only considering the original problem's features and a bias term. As previously mentioned, the input features for each instance are standardized using the z-score strategy, resulting in a normalized instancex. The normalized predictionỹ t for the t-th target is calculated as follows: </p><formula xml:id="formula_7">y t = β 0,t + m j=1 β j,txj ,</formula><p>where β j , j ∈ {0, 1, ..., m} are the weights of the linear model. Given the standardized value of the expected responsey, the linear predictor's weights are updated with the Delta rule</p><formula xml:id="formula_8">β j,t ← β j,t + η(ỹ t −y t )x j ,</formula><p>where η represents the learning rate. In the standard MTR-HT models, the final predictions are computed by transforming theỹ values back to their original scales and ranges. Our proposal, in turn, adds another layer of linear models to combine and enhance the predictions from MTR-HT models. We call these newly added predictors meta models, whereas the regressors at the first layer are referred to as base models.</p><p>It is worth mentioning that since both base and meta predictors are linear transformations, they could be merged into a single transformation matrix. This could be done by multiplying the neurons' weight matrices to obtain a more compact representation. Nonetheless, this operation is costly to perform for each new sample, and thus was avoided in our implementation. Stacking multiple linear predictors is redundant in batch scenarios, as pointed out by <ref type="bibr" target="#b2">Borchani et al. (2015)</ref>. A single linear transformation can achieve the same results. However, as our online predictors process each incoming sample just once, i.e., they solve the linear system incrementally, we experimentally observed that the stacking of linear regressors improved the predictive performance. In the future, we intend to investigate non-linear alternatives for incremental regression. They need to be fast, robust, and flexible to deal with arbitrary regression regions. Moreover, regularization strategies can also be investigated.</p><p>After applying the meta models, the new normalized responses are computed as follows:</p><formula xml:id="formula_9">y t = γ 0,t + d k=1 γ k,tỹk,t ,</formula><p>and their corresponding weights γ k,t , k ∈ {0, 1, ..., d} are updated using the delta rule as well:</p><formula xml:id="formula_10">γ k,t ← γ k,t + η(ỹ t −y t )ỹ t .</formula><p>It must be observed that in these weight update expressions there are no input values for the bias terms. This value is typically set to the unit value, as we did. Besides, both expressions use the same learning factor (η). In the experiments carried out in this paper with SST-HT, the same η value was used for both the base and meta models. The use of different learning rates for the base and meta layers will be investigated in future research. Another possibility is to use decaying factors for the learning rates. However, fine-tuning the prediction models is out-of-the-scope of the current work.</p><p>Similar to the iSOUP-Tree, SST-HT uses an adaptive model to continuously select one among a set of predictors. While the iSOUP-Tree chooses between the perceptron and mean predictors, SST-HT adds a third model: a stacked perceptron predictor. In the same way as the preceding algorithm, SST-HT uses a fading error metric for online predictor selection. As in the iSOUP-Tree, we monitor prediction performance using the faded Mean Absolute Error (fMAE). This metric, presented in Equation 3 1 , uses an exponential decay to assign more importance to the most recent cases. In this equation, learner ∈ {mean, perceptron, stacked perceptron}.</p><formula xml:id="formula_11">fMAE learner (t) = n j=1 0.95 n−j |y t −ŷ t | n j=1 0.95 n−j<label>(3)</label></formula><p>SST-HT allows the selection of either a specific prediction model to use or a dynamic selection. Note that the automatic selection of predictors does not impact the tree structure, since the splits only consider the increase in partitions' homogeneity, regardless of the predictive errors. Therefore, we expect the same tree structures for SST-HT and traditional MTR HTs. On the other hand, due to the additional set of predictors required at each leaf node, we expect a small increase in the memory use and increased training times. It is worth mentioning that, as the number of input features is typically much smaller than the number of targets, i.e., d m, the meta models have less adjustable parameters (weights) than their base counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>This section describes how the experiments were carried out, including datasets, settings for the tree predictors, performance metrics and evaluation strategy. All the experiments were executed using the scikit-multiflow 2 framework <ref type="bibr" target="#b27">(Montiel et al., 2018)</ref>, which is an open and free platform for data stream mining. Besides, our scripts were executed in a CentOS 7.2 Linux server using a single processing blade which contained 128 GB of DDR3 1866MHz memory, and two ten-core processors Intel Xeon E5-2680v2 at 2.8 GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>A total of 16 datasets, synthetic and real, and from different application domains were used in the experiments. The main characteristics of these datasets can be seen in <ref type="table" target="#tab_0">Table 1</ref>. Some of these have been used in previous MTR studies <ref type="bibr" target="#b8">(Duarte and Gama, 2015;</ref><ref type="bibr" target="#b32">Osojnik et al., 2018)</ref>, whereas the datasets CPU, NPSDecay, SCFP, Sulfur, and Wine are used for the first time, in the context of online MTR, in this work.</p><p>All these new datasets, except for NPSDecay, are derived from real-world data. They are originally used for batch or online ST classification and regression. NPSDecay is built upon synthetic data collected from a vessel simulator <ref type="bibr" target="#b4">(Cipollini et al., 2018)</ref>. Moreover, the version of SCFP used was specially tailored for this work. A description of the datasets can be seen in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Settings used in the tree predictors</head><p>During the experiments, we fixed some hyper-parameters for the tree predictors according to values proposed in the literature <ref type="bibr" target="#b7">(Domingos and Hulten, 2000;</ref><ref type="bibr" target="#b8">Duarte and Gama, 2015;</ref><ref type="bibr" target="#b32">Osojnik et al., 2018)</ref>. Split attempts were performed at intervals of 200 examples. We set the significance level for the HB calculation to δ = 10 −7 , and the tie-break parameter to τ = 0.05.</p><p>Additionally, in all cases, 200 examples were used to initiate the tree predictors, providing a 'warm' start for the evaluations. Finally, the perceptron weights were started with uniform random values in the range [−1, 1]. In case of a split, new leaf nodes inherit their ancestors' weights.</p><p>Regarding the decision tree induction algorithms, we compared two variants of our proposal with iSOUP-Tree and two variants of the MTR-HT algorithm. <ref type="table" target="#tab_1">Table 2</ref> summarizes the variants used in the comparisons, including their main characteristics and acronyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation strategy</head><p>To compare the predictive performance of the algorithms, we used the prequential strategy <ref type="bibr" target="#b11">(Gama, 2010)</ref>. In this strategy, after an example is evaluated by a predictive model, it is used to update the model. For all the metrics used, we computed their mean value and also considered windowed measurements. For such, we employed a non-overlapping sliding window of size 200. All MTR algorithms were applied thirty times to each dataset with varying seeds for the pseudo-random generators. In order to reduce effects of randomness and operational system external influences, we used the average of the thirty results. These effects relate mostly to the running time measurements, but the perceptron models in some of the tree variants ought to be also affected.</p><p>In particular, we used the Average Root Mean Square Error (RMSE) as the error metric. Both errors per sliding window and an overall measurement using all the seen data were considered in our analysis. Equation 4 shows how the RMSE is calculated.</p><formula xml:id="formula_12">RMSE = 1 d d t=1 N i=1 (y t i −ŷ t i ) 2 N<label>(4)</label></formula><p>The average amount of time spent by each algorithm (in seconds) and the total of memory resources consumed by the predictors (in MB) are also reported. We performed statistical tests to verify whether the differences in the predictive performance of the models are statistically significant, regarding the evaluation metrics. The Friedman test and post-hoc Nemenyi test were used (with α = 0.05), as described in <ref type="bibr" target="#b6">Demšar (2006)</ref>. We considered the windowed evaluations to perform the statistical tests. These measurements describe how the algorithms evolved over the time. We summed the error metric values accounted for each window to perform the tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>This section presents and discuss the main experimental results observed for the compared MTR algorithms. The results are discussed regarding predictive performance, running time and model size. We also highlight some cases in details, while presenting detailed information about all the evaluated datasets in the supplementary material (see Appendix B). <ref type="table" target="#tab_2">Table 3</ref> summarizes the predictive performance of the investigated algorithms considering the mean measured errors, i.e., considering the average of the errors after processing the whole stream. The smallest RMSE value observed for each dataset is highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Predictive Performance</head><p>As shown in the table, SST-HT Adaptive presented the best predictive performance in the majority of datasets (10 out of 16 datasets). The simplest, most straightforward SST-HT version presented the smallest RMSE for only one dataset (Eunite03). The same observation holds for MTR-HT Mean (Bicycles) and MTR-HT Perceptron (Wine). The second best performer in this analysis was the iSOUP-Tree, which obtained the best predictive performance in 3 out of 16 datasets. In general, as expected, the adaptive variants of the tree predictors obtained the best predictive performance most of the time.</p><p>The evolution of the observed error over time was also considered in our analysis. We observed different patterns, depending on the dataset being considered. To illustrate what occurs, we present line plots for two of the evaluated datasets, Bicycles and SCM1d, in <ref type="figure" target="#fig_1">Figures 2a and 2b</ref>, respectively. In the first case, almost all algorithms presented the same behavior regarding the RMSE values. Until around 8000 examples, the SST-HT Adaptive presented the smallest RMSE values. However, from this point until the end of the stream, the simplest MTR-HT Mean presented the smallest RMSE. These results show that the underlying concepts of the data became stable. Hence, the mean value of the targets provided the best responses for the new cases. Interestingly, the model selection procedure did not appear to be effective in detecting this phenomenon, nor did the linear models behave well with the new samples. An alternative to overcome this problem is to use decaying factors different from 0.95 to the faded error (refer to Section 4). This alternative would give less importance to the past cases, giving more attention to the current state. Ideally, the decaying factor could be set dynamically based on the characteristics of the incoming data.</p><p>In the second case, SST-HT Adaptive maintained the most accurate predictions for the whole stream. On the other hand, MTR-HT Perceptron presented the worst predictive performance. SST-HT presented the second best predictive performance until just after 4000 examples, when there was a sudden increase in its RMSE values, presenting the second worst performance at the end of the stream. SST-HT and MTR-HT Perceptron presented similar error curves. The adaptive methods, differently, seemed to identify that the mean predictor became the best option, presenting error curves whose slopes were close to those presented by MTR-HT Mean . Different from what occurred with the Bicycles dataset, the model selection mechanism worked well for the SCM1d dataset. This, again, reinforces the hypothesis that the tree must dynamically define the level of importance it gives to the current and past data.</p><p>Another possibility for these results is that in these previous cases the input to output and interoutput relations were not linear in the data partitions used. This could result in a better performance of the mean predictor when compared with the linear regressors. As previously presented, the investigation of non-linear regression methods for the leaf models is a promising venue for future research. Similar plots for all the considered datasets are presented in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Running time</head><p>As expected, the most simple algorithm (MTR-HT Mean ) was also the fastest in almost all the cases, as shown in <ref type="table" target="#tab_3">Table 4</ref>. Again, the smallest running time per dataset is in bold. The MTR-HT Perceptron variant presented the second fastest running times in most of the cases. SST-HT, in general, performed as fast as the iSOUP-Tree for the majority of the datasets. As expected, SST-HT Adaptive presented the longest running time. This variant, in addition to performing a dynamic selection between predictors, maintains and updates three different prediction models per leaf node. However, apart from MTR-HT Mean , the difference between the SST-HT Adaptive and the other algorithms is is not very significant, regarding the running times. Moreover, this difference is compensated by the gains in predictive performance of our method. Concerning the cases where MTR-  Considering that all the compared algorithms only differ in the strategy that the leaf nodes use to generate predictions, an approximately linear relationship between the running times of the algorithms was observed. This was expected, since the amount of extra processing performed by the different algorithms is leaf-wise constant. This comparison for the dataset SCFP is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model size</head><p>When considering the size of the generated models, we obtained results very similar to those observed in the running times. This, again, was expected, given the use of an extra layer of predictors by SST-HT. The total sizes of the trained models at the end of the data streams are summarized in <ref type="table" target="#tab_4">Table 5</ref>. The results are presented in Megabytes (MB). Excluding the variations of SST-HT (SST-HT and SST-HT Adaptive ), the sizes of the investigated algorithms only differed in small amounts, regardless of the dataset used (with 8, 7, and 1 wins for the MTR-HT Mean , iSOUP-Tree, and MTR-HT Perceptron , respectively). The differences were very small in the majority of the cases.</p><p>The two versions of SST-HT spent more memory resources than the others. However, the additional amount of memory needed was minimal in almost all the cases. This difference was negligible for most of the real-world application datasets.</p><p>The relation between the amount of memory used by the different algorithms over time was also linear for nearly all cases. Considering that SST-HT does not impact the tree growth characteristics, the extra memory usage is constant in all the cases. This can be verified in detail for each dataset in Appendix B.3. As a matter of illustration, we present the memory usage varying on time for the NPSDecay dataset in <ref type="figure">Figure 4</ref>. This dataset has the highest number of examples among all the sets considered in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Statistical test analysis</head><p>Given that we observed different performance behaviors among the algorithms and along the streams, we applied a statistical significance test to assess the significance of the observed differences. For such, we considered the metrics measured in the sliding windows, as presented in Section 5.3. The Friedman statistical test and the Nemenyi post-hoc test were used to compare the algorithms. The results of these tests are graphically illustrated, as recommended in <ref type="bibr" target="#b6">Demšar (2006)</ref>. The main findings of this analysis can be seen in <ref type="figure">Figure 5</ref>.</p><p>Concerning RMSE, both SST-HT Adaptive and iSOUP-Tree appeared in the first group. We did not observe statistically significant differences among their predictive performance. Nonetheless, we firmly believe that given more observation, i.e., datasets, our algorithm's variant would perform  significantly better than iSOUP-Tree in the statistical analysis. The remaining algorithms appear in a second group, including the non-adaptive version of SST-HT. These algorithms are statistically equivalent regarding their predictive performance. Therefore, the adaptive choice of predictors proved itself to be one of the most relevant aspects to determine the final performance of the HT algorithms for regression.</p><p>When we consider the running time, as the prediction strategy becomes more sophisticated, the rankings become worse. We expected this behavior. Nonetheless, most of the considered algorithms did not present statistically significant differences in our analysis. It is noteworthy that the variants of SST-HT were statistically equivalent to iSOUP-Tree in this test.</p><p>Lastly, regarding memory usage, our proposal has a disadvantage when faced with its competitors. This fact was again expected since SST-HT needs to train and monitor and an additional set of predictors. Hence, it ends up having an approximately constant increase in its size when compared with iSOUP-Tree.</p><p>In summary, SST-HT compared favorably to its competitors concerning its prediction error. This observation is especially true when we take into account the dynamic mechanism for leaf predictor choice. SST-HT adds extra memory and running time burden, but its prediction performance compensates for this fact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Final Considerations</head><p>In this work, we presented an extension for online MTR decision tree algorithms that better explores the characteristics of these kinds of problems. Our proposal, called SST-HT, improves the prediction performance without affecting the structure of the tree models. The main idea behind SST-HT is to use stacked linear models at the leaf nodes to capture and model the possible existing inter-target dependencies. Thus, the split decisions are made in the same way as those from the traditional online MTR tree algorithm. Similarly to existing solutions, SST-HT is also able to dynamically select the most adequate predictor for each instance. SST-HT, however, selects between three predictors: mean, perceptron or stacked perceptron predictors.</p><p>We evaluated two variations of SST-HT, experimentally comparing them with three well-known tree algorithms for dealing with MTR tasks in data streams. A large set of 16 benchmark datasets was used in the experimental evaluation. To the best of our knowledge, this is the most extensive set of online MTR datasets used so far. In the experiments carried out, the proposed algorithm obtained the most accurate predictions in the majority of the cases without demanding large increases in the amounts of computational resources.</p><p>As future work, we intend to verify the possibility of extending our ideas to ensembles of decision rules, like those in AMRules. In this sense, the modeling of inter-target dependencies could be further improved, since AMRules creates rules which encompass subsets of targets with the highest inter-correlation. Moreover, we intend to evaluate other possibilities of stacked regression models for the leaves. Our goal is to find fast, robust, and flexible non-linear alternatives to the linear models. Using SST-HT as the base model for traditional online ensemble algorithms is another possibility for future research. Besides, we also want to evaluate alternatives for monitoring the necessary statistics for splitting numerical attributes, reducing the cost of this procedure. Finally, the application of our proposal to correlated tasks, e.g., online multi-label classification, could also be investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Used Datasets</head><p>This appendix describes the datasets that were used in the experiments. Firstly, the datasets already reported as online MTR tasks in the literature are described. Next, the datasets that were used for the first time in this work are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Existing datasets</head><p>This section briefly describes the datasets used in the experiments that were already reported in the literature <ref type="bibr" target="#b8">(Duarte and Gama, 2015;</ref><ref type="bibr" target="#b38">Spyromitros-Xioufis et al., 2016;</ref><ref type="bibr" target="#b32">Osojnik et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicycles</head><p>The Bicycles dataset has already been used in multiple online MTR research <ref type="bibr" target="#b8">(Duarte and Gama, 2015;</ref><ref type="bibr" target="#b32">Osojnik et al., 2018)</ref>. It describes the hourly count of rental bikes, considering the period between 2011 and 2012 in the Capital bikeshare system <ref type="bibr" target="#b8">(Duarte and Gama, 2015)</ref>. The data contains weather and seasonal information for each rent event. The task consists of predicting the count of casual (non-registered), registered and total users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eunite03</head><p>The Eunite03 dataset was used during the competition of the 3rd European Symposium on Intelligent Technologies, Hybrid Systems and their implementation on Smart Adaptive Systems <ref type="formula" target="#formula_4">(2003)</ref>. The dataset describes a process of continuous production of manufactured glasses <ref type="bibr" target="#b8">(Duarte and Gama, 2015)</ref>. The input features describe the parameters used when producing the glass products, while the outputs refers to the glass quality.</p><p>2Dplanes, FriedD, FriedAsyncD, and MV 2Dplanes, FriedD, and FriedAsyncD are MTR artificial datasets generated by <ref type="bibr" target="#b8">Duarte and Gama (2015)</ref>. They are modifications of well-known artificial ST regression tasks <ref type="bibr" target="#b3">(Breiman, 2017)</ref>. The FriedD and FriedAsyncD datasets contain one CD for each of the output targets. In FriedD the CD occur simultaneously for all the target variables in the middle of the data stream, while in FriedAsyncD the CDs occur asynchronously <ref type="bibr" target="#b8">(Duarte and Gama, 2015)</ref>. Lastly, MV was also constructed by <ref type="bibr" target="#b8">Duarte and Gama (2015)</ref> based on an ST regression artificial problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RF1 and RF2</head><p>The RF1 and RF2 (River Flow) datasets were firstly reported by <ref type="bibr" target="#b38">Spyromitros-Xioufis et al. (2016)</ref> and ever since then they have been used in MTR data streaming tasks <ref type="bibr" target="#b8">(Duarte and Gama, 2015;</ref><ref type="bibr" target="#b32">Osojnik et al., 2018)</ref>. The datasets concern the prediction of river network flows considering a time window of 48 h in the future, at specific locations. Hourly flow observations were registered for 8 sites in the Mississippi River network (US) considering a period of one year (from September 2011 to September 2012). The data was obtained from the US National Weather Service. Each observation includes the most recent data, as well as delayed measurements considering intervals ranging from <ref type="bibr">[6,</ref><ref type="bibr">12,</ref><ref type="bibr">18,</ref><ref type="bibr">24,</ref><ref type="bibr">36,</ref><ref type="bibr">48,</ref><ref type="bibr">60]</ref> hours in the past. The first dataset, RF1, uses only the sensor data, whereas the second one, RF2, adds precipitation forecast information (expected rainfall) for each of the measurement sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCM1d and SCM20d</head><p>The SCM (Supply Chain Management) was extracted from the Trading Agent Competition in Supply Chain Management (TACSCM) tournament in 2010. Again, these datasets were firstly proposed by <ref type="bibr" target="#b38">Spyromitros-Xioufis et al. (2016)</ref>, and were applied in data stream problems of MTR <ref type="bibr" target="#b8">(Duarte and Gama, 2015;</ref><ref type="bibr" target="#b32">Osojnik et al., 2018)</ref>. Each example corresponds to an observation day in the tournament (from a total of 220 days in each game and 18 games during the whole tournament). The input variables correspond to the observed prices considering a specific tournament day. Additionally, four time-delayed observations are added for each observed product and component (delays of 1, 2, 4 and 8 days) aiming at facilitating the anticipation of trends. Each dataset has 16 targets, which correspond to the predictions for the next day mean price (SCM1d) or mean price for 20-days in the future (SCM20d), concerning each product in the simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 New datasets</head><p>This section describes the datasets that were firstly evaluated as MTR tasks in streaming scenarios in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CPU</head><p>The Computer Activity database 3 , collected around 1996 at the University of Toronto, records multiple performance measures, such as the number of bytes read and written from the system memory. All data was collected from a computer Sun Sparctation model 20/712, which had 2 CPUs (Central Processing Unit) and 128 MegaBytes of main memory. The records concern the monitoring of normal computer usage, for example, browsing through the web or using text editors. The records were gathered at intervals of five seconds. Originally, the tasks related to this dataset concerned predicting the percent of the time the CPU ran in user mode. However, taking into consideration that the data also contains the amount of time the CPU ran in system mode, and the period it stayed in idle due to waiting for block IO or any other circumstances, the task was tackled as an incremental MTR problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electricity</head><p>The Electricity dataset is an adapted version of the well-known ELEC2 dataset <ref type="bibr" target="#b13">(Gama et al., 2004)</ref>, which is commonly used in online classification tasks. The original task corresponds to identifying the change of the price (up or down) in the Australian New South Wales Electricity Market. In this market the prices are not fixed, and are affected by aspects of demand and supply, and set every five minutes. The data comprehends an interval between 1996 and 1998, and each example in the dataset refers to a period of 30 minutes. It is a scenario with a potential to multiples changes, given that transfers to/from the neighboring state of Victoria are performed to alleviate fluctuations. In this adapted version of the task, the original label property was discarded, and the prices for the New South Wales and Victoria states were set as the new targets to be predicted. As input features, we selected the remaining data properties: the measured electricity demands for those markets, the measurement time stamp, the day of the week, and the scheduled electricity transfer between the two states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NPSDecay</head><p>The NPSDecay dataset <ref type="bibr" target="#b4">(Cipollini et al., 2018)</ref> concerns the prediction of performance decay in a Naval Propulsion System (NPS) over time. The data comes from a vessel (frigate) simulator which was specially tailored and fine-tuned over the years to represent the components of a possible real vessel. The simulated vessel has a combined diesel, electric, and gas propulsion plant. The targets correspond to decay coefficients for the main components of the NPS system, namely: the gas turbine, the gas turbine compressor, the hull, and the propeller. Hence, in this task the following coefficients must be predicted:</p><p>• Propeller Thrust decay state coefficient (Kkt);</p><p>• Propeller Torque decay state coefficient (Kkq);</p><p>• Hull decay state coefficient (Khull);</p><p>• Gas Turbine Compressor decay state coefficient (KMcompr);</p><p>• Gas Turbine decay state coefficient (KMturb).</p><p>A total of 25 features related to parameters that indirectly represent the system state are available for each measurement of the performance decay coefficients. The dataset is available in OPenML, as well as in a website made available by its authors 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCFP</head><p>The See Click Fix Prediction (SCFP) competition 5 was firstly held by Kaggle as a hackathon. Later on, the dataset adopted in that competition was used in a new competition promoted by the same organization. The dataset concerns registers of issues subjected by the population to the Open311 6 service. The original task consists of predicting the number of views, comments, and votes an issue would receive. The original dataset contains textual information about a summary and description of the issue, as well as geolocated data, the publication source <ref type="bibr">(mobile, desktop, etc.)</ref>, and a tag type for the publication. The original dataset contains missing data in many fields. A random 1% sample of the mentioned dataset was used by <ref type="bibr" target="#b38">Spyromitros-Xioufis et al. (2016)</ref> in batch scenarios. However, their version simply overlooked the textual information contained in the examples, using only the other fields, as well as some hand-engineered features.</p><p>In our processed version of the original dataset, the categorical values were encoded using numeric values. The missing fields were encoded with −1. Following the approach of <ref type="bibr" target="#b38">Spyromitros-Xioufis et al. (2016)</ref>, in addition to the latitude and longitude fields, an additional attribute concerning the distance of the published issue to its city downtown (in meters) was also added. Besides, another field denoting the time interval (in hours) since the last registered issue was included in the dataset. Moreover, our main contribution to the previous and reduced version of SCFP was taking into account the textual information of the dataset. To this end, the summary of the issues was considered. We adopted a pre-trained word embedding <ref type="bibr" target="#b33">(Pennington et al., 2014)</ref> model 7 with an array of 50 positions to encode each of the non-stopwords in the summary field of the issues. The mean vector among all the considered words was then taken as a representation of the issue's summary. Therefore, 50 additional features were added to our version of SCFP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sulfur</head><p>The Sulfur dataset concerns the prediction of pollutants concentration (H 2 S and SO 2 ), given air and gas flows as inputs. The dataset is available at OPenML <ref type="bibr" target="#b39">(Vanschoren et al., 2014)</ref>, and corresponds to the data described in <ref type="bibr" target="#b10">Fortuna et al. (2007)</ref>. In the Sulfur dataset, no pre-processing step was performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wine</head><p>The Wine dataset <ref type="bibr" target="#b5">(Cortez et al., 2009</ref>) describes the chemical properties of red and white wine examples. The input features correspond to objective tests, for instance, acidity and pH tests. Originally, the only target was the sensory data (a human-based score, given by the median of three evaluations made by wine experts). Notwithstanding, for the purposes of evaluating a multi-output scenario, the fixed and volatile acidity, as well as the citric acid amounts were joined along with the quality score as new targets. Thus, the new task consists of predicting acidity levels and a quality score, modeling how those quantities relate to each other. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the SST-HT algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example of the different patterns observed for the compared algorithms in the datasets Bicycles and SCM1d, regarding the RMSE HT Mean was not the fastest algorithm, this occurred probably due to factors external to the algorithms, given the standard deviation of the observations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Running time for the SCFP dataset Similar behaviors were observed for all the datasets. Detailed results for running time are presented in Appendix B.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Memory usage by the compared algorithms on the NPSDecay dataset CD Friedman test and Nemenyi post-hoc test results. Tests performed for RMSE (top), Running time (middle), and Model size (bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>BFigure 6 :Figure 7 :Figure 7 :Figure 8 :Figure 8 :</head><label>67788</label><figDesc>Time-varying observations for error, running time and model size This appendix presents line plots for the observed errors, running time, and model size considering all the evaluated datasets. Time varying results for the measured RMSE values (continuation) Accounted running times for all the evaluated datasets Accounted running times for all the evaluated datasets (continuation) Time varying model size for all the evaluated datasets Time varying model size for all the evaluated datasets (continuation)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets used in the experiments. Datasets marked with in the Source column were first proposed or adapted to MTR tasks in this research (please refer to Appendix A for more details)</figDesc><table><row><cell>Dataset</cell><cell>#Examples</cell><cell>#Numeric Inputs</cell><cell>#Categorical Inputs</cell><cell>#Outputs</cell><cell>Source</cell></row><row><cell>2Dplanes</cell><cell>256,000</cell><cell>20</cell><cell>0</cell><cell cols="2">8 Duarte and Gama (2015)</cell></row><row><cell>Bicycles</cell><cell>17,379</cell><cell>4</cell><cell>9</cell><cell cols="2">3 Duarte and Gama (2015)</cell></row><row><cell>CPU</cell><cell>8,192</cell><cell>22</cell><cell>0</cell><cell>4 -,</cell><cell></cell></row><row><cell>Electricity</cell><cell>45,312</cell><cell>6</cell><cell>0</cell><cell cols="2">2 Gama et al. (2004),</cell></row><row><cell>Eunite03</cell><cell>8,064</cell><cell>29</cell><cell>0</cell><cell cols="2">5 Duarte and Gama (2015)</cell></row><row><cell>FriedD</cell><cell>256,000</cell><cell>10</cell><cell>0</cell><cell cols="2">4 Duarte and Gama (2015)</cell></row><row><cell>FriedAsyncD</cell><cell>256,000</cell><cell>10</cell><cell>0</cell><cell cols="2">4 Duarte and Gama (2015)</cell></row><row><cell>MV</cell><cell>256,000</cell><cell>16</cell><cell>4</cell><cell cols="2">9 Duarte and Gama (2015)</cell></row><row><cell>NPSDecay</cell><cell>455,109</cell><cell>25</cell><cell>0</cell><cell cols="2">5 Cipollini et al. (2018),</cell></row><row><cell>RF1</cell><cell>9,005</cell><cell>64</cell><cell>0</cell><cell cols="2">8 Spyromitros-Xioufis et al. (2016)</cell></row><row><cell>RF2</cell><cell>7,679</cell><cell>576</cell><cell>0</cell><cell cols="2">8 Spyromitros-Xioufis et al. (2016)</cell></row><row><cell>SCFP</cell><cell>223,129</cell><cell>54</cell><cell>3</cell><cell>3 -,</cell><cell></cell></row><row><cell>SCM1d</cell><cell>9,803</cell><cell>280</cell><cell>0</cell><cell cols="2">16 Spyromitros-Xioufis et al. (2016)</cell></row><row><cell>SCM20d</cell><cell>8,966</cell><cell>61</cell><cell>0</cell><cell cols="2">16 Spyromitros-Xioufis et al. (2016)</cell></row><row><cell>Sulfur</cell><cell>10,081</cell><cell>5</cell><cell>0</cell><cell cols="2">2 Fortuna et al. (2007),</cell></row><row><cell>Wine</cell><cell>6,497</cell><cell>8</cell><cell>0</cell><cell cols="2">4 Cortez et al. (2009),</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Description of the algorithms used in the comparisons Acronym Description MTR-HT Mean MTR-HT variant that uses the mean of the targets as responses at the leaf nodes</figDesc><table><row><cell>MTR-HT Perceptron</cell><cell>MTR-HT variant that uses a perceptron model per target at the leaf nodes</cell></row><row><cell>iSOUP-Tree</cell><cell>ISOUP-Tree algorithm that dynamically selects between the two pre-vious prediction variants</cell></row><row><cell>SST-HT</cell><cell>Variant of MTR-HT (proposed algorithm) that always use the stacked regressors for making predictions</cell></row><row><cell>SST-HT Adaptive</cell><cell>Variant of MTR-HT (proposed algorithm) that dynamically selects between the mean, perceptron, and stacked perceptron predictors for</cell></row><row><cell></cell><cell>each target</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mean RMSE values observed (after processing the whole stream) ± 0.00 2.7340 ± 0.00 4.8736 ± 0.00 2.7372 ± 0.00 Bicycles 87.3947 ± 0.00 139.7503 ± 0.01 101.0994 ± 0.00 135.6794 ± 0.02 102.4765 ± 0.00 CPU 5.1473 ± 0.00 4.3383 ± 0.00 3.4078 ± 0.00 4.8185 ± 0.00 3.4265 ± 0.00</figDesc><table><row><cell>Dataset</cell><cell cols="2">MTR-HT Mean MTR-HT Perceptron</cell><cell>iSOUP-Tree</cell><cell>SST-HT</cell><cell>SST-HT Adaptive</cell></row><row><cell cols="3">2Dplanes 4.5075 Electricity 2.7507 ± 0.00 0.0242 ± 0.00 0.0313 ± 0.00</cell><cell>0.0207 ± 0.00</cell><cell>0.0264 ± 0.00</cell><cell>0.0206 ± 0.00</cell></row><row><cell>Eunite03</cell><cell cols="4">24.0007 ± 0.00 26.3034 ± 0.00 26.0206 ± 0.00 22.0181 ± 0.00</cell><cell>22.9048 ± 0.00</cell></row><row><cell>FriedD</cell><cell>10.4575 ± 0.00</cell><cell>8.5829 ± 0.00</cell><cell>7.9927 ± 0.00</cell><cell>8.2212 ± 0.00</cell><cell>7.3887 ± 0.00</cell></row><row><cell>FriedAsyncD</cell><cell>9.0928 ± 0.00</cell><cell>8.4728 ± 0.00</cell><cell>7.4457 ± 0.00</cell><cell>8.1805 ± 0.00</cell><cell>6.8746 ± 0.00</cell></row><row><cell>MV</cell><cell cols="4">23.7614 ± 0.00 32.6842 ± 0.00 23.7486 ± 0.00 33.3511 ± 0.00</cell><cell>23.7659 ± 0.00</cell></row><row><cell>NPSDecay</cell><cell>0.0195 ± 0.00</cell><cell>0.0251 ± 0.00</cell><cell>0.0147 ± 0.00</cell><cell>0.0205 ± 0.00</cell><cell>0.0137 ± 0.00</cell></row><row><cell>RF1</cell><cell cols="4">23.3911 ± 0.00 28.0100 ± 0.00 12.5794 ± 0.00 19.5451 ± 0.00</cell><cell>9.1290 ± 0.00</cell></row><row><cell>RF2</cell><cell cols="5">23.7385 ± 0.00 59.2034 ± 0.00 21.2524 ± 0.00 55.0323 ± 0.00 18.9755 ± 0.00</cell></row><row><cell>SCFP</cell><cell cols="2">10.1314 ± 0.00 10.0563 ± 0.00</cell><cell>9.4049 ± 0.00</cell><cell>9.8438 ± 0.00</cell><cell>9.3349 ± 0.00</cell></row><row><cell>SCM1d</cell><cell cols="5">234.3647 ± 0.00 355.7236 ± 0.00 215.4752 ± 0.00 298.3532 ± 0.00 198.0668 ± 0.00</cell></row><row><cell>SCM20d</cell><cell cols="5">246.2019 ± 0.00 194.4146 ± 0.00 145.7449 ± 0.00 176.4975 ± 0.00 135.0029 ± 0.00</cell></row><row><cell>Sulfur</cell><cell>0.0580 ± 0.00</cell><cell>0.0441 ± 0.00</cell><cell>0.0439 ± 0.00</cell><cell>0.0445 ± 0.00</cell><cell>0.0438 ± 0.00</cell></row><row><cell>Wine</cell><cell cols="2">0.5958 ± 0.00 0.4424 ± 0.00</cell><cell>0.4445 ± 0.00</cell><cell>0.4605 ± 0.00</cell><cell>0.4438 ± 0.00</cell></row><row><cell>Average rank</cell><cell>3.75</cell><cell>4.12</cell><cell>2.00</cell><cell>3.62</cell><cell>1.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Running time for each dataset (in seconds) ± 10.31 179.8277 ± 28.29 203.8099 ± 27.35 213.5590 ± 26.37 274.0997 ± 42.55 FriedD 389.1604 ± 20.73 460.1738 ± 29.18 485.8485 ± 28.41 479.3183 ± 34.06 518.8329 ± 32.35 FriedAsyncD 387.3880 ± 18.42 455.2480 ± 28.32 478.9144 ± 25.80 482.7184 ± 31.05 526.5462 ± 32.65 MV 672.7499 ± 26.85 753.1689 ± 33.84 789.6671 ± 38.35 798.6512 ± 40.73 862.2696 ± 51.03 NPSDecay 1790.9435 ± 72.67 1939.6547 ± 68.31 2002.5621 ± 70.72 1993.3690 ± 70.17 2084.4066 ± 93.22 RF1 154.3603 ± 21.47 174.3120 ± 45.29 161.3790 ± 28.66 187.6669 ± 46.69 175.2095 ± 34.45 RF2 136.3630 ± 19.10 146.9837 ± 29.91 151.8701 ± 36.37 153.9039 ± 30.42 130.7061 ± 9.33 SCFP 506.8263 ± 24.40 587.0308 ± 39.92 616.7821 ± 54.29 595.3749 ± 44.09 610.9264 ± 46.10 SCM1d 713.6154 ± 73.12 726.5279 ± 82.65 726.7681 ± 73.21 738.1430 ± 91.63 683.5466 ± 53.69</figDesc><table><row><cell>Dataset</cell><cell>MTR-HT Mean</cell><cell>MTR-HT Perceptron</cell><cell>iSOUP-Tree</cell><cell>SST-HT</cell><cell>SST-HT Adaptive</cell></row><row><cell cols="2">2Dplanes 81.2263 Bicycles 6.3643 ± 1.56</cell><cell>11.5208 ± 3.00</cell><cell>11.8662 ± 2.29</cell><cell>11.9889 ± 2.60</cell><cell>14.8656 ± 3.36</cell></row><row><cell>CPU</cell><cell>23.7227 ± 3.42</cell><cell>29.0668 ± 8.23</cell><cell>27.1192 ± 4.18</cell><cell>30.8000 ± 8.89</cell><cell>30.2394 ± 8.23</cell></row><row><cell>Electricity</cell><cell>19.2018 ± 3.51</cell><cell>31.1544 ± 5.64</cell><cell>34.2767 ± 5.52</cell><cell>34.1326 ± 6.12</cell><cell>37.2195 ± 2.39</cell></row><row><cell>Eunite03</cell><cell>9.6498 ± 2.00</cell><cell>12.3035 ± 2.99</cell><cell>12.1218 ± 1.40</cell><cell>13.8561 ± 3.49</cell><cell>14.0753 ± 2.57</cell></row><row><cell>SCM20d</cell><cell>79.9444 ± 11.90</cell><cell>83.3493 ± 8.96</cell><cell>86.5577 ± 12.65</cell><cell>92.6893 ± 19.69</cell><cell>90.2755 ± 12.45</cell></row><row><cell>Sulfur</cell><cell>9.5295 ± 2.48</cell><cell>13.1352 ± 3.85</cell><cell>12.8251 ± 3.19</cell><cell>13.1586 ± 3.44</cell><cell>13.8657 ± 3.46</cell></row><row><cell>Wine</cell><cell>2.5600 ± 0.53</cell><cell>4.6482 ± 1.12</cell><cell>5.0032 ± 1.19</cell><cell>5.2934 ± 1.35</cell><cell>5.4540 ± 0.64</cell></row><row><cell>Average rank</cell><cell>1.12</cell><cell>2.38</cell><cell>3.19</cell><cell>4.06</cell><cell>4.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Total model size for each dataset (in MB) FriedD 993.7507 ± 0.00 993.8051 ± 0.00 993.5337 ± 0.00 1005.1122 ± 0.00 1004.6132 ± 0.00 FriedAsyncD 967.7357 ± 0.01 967.7916 ± 0.00 968.1268 ± 0.01 978.8781 ± 0.00 978.0164 ± 0.00 MV 1021.7871 ± 0.01 1021.8641 ± 0.00 1020.5312 ± 0.02 1032.7374 ± 0.07 1031.3735 ± 0.06 NPSDecay 861.0028 ± 0.04 860.9132 ± 0.04 860.0335 ± 0.02 870.7448 ± 0.18 869.4314 ± 0.16</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="2">MTR-HT Mean MTR-HT Perceptron</cell><cell cols="2">iSOUP-Tree</cell><cell>SST-HT</cell><cell>SST-HT Adaptive</cell></row><row><cell cols="2">2Dplanes</cell><cell>19.3818 ± 0.01</cell><cell>19.4535 ± 0.01</cell><cell cols="2">19.2903 ± 0.01</cell><cell>19.6516 ± 0.00</cell><cell>19.5243 ± 0.01</cell></row><row><cell cols="2">Bicycles</cell><cell>3.5271 ± 0.00</cell><cell>3.5320 ± 0.00</cell><cell cols="2">3.5491 ± 0.00</cell><cell>3.5916 ± 0.01</cell><cell>3.5977 ± 0.00</cell></row><row><cell cols="2">CPU</cell><cell>22.0923 ± 0.01</cell><cell>22.0974 ± 0.01</cell><cell cols="2">22.0127 ± 0.01</cell><cell>22.3349 ± 0.00</cell><cell>22.2608 ± 0.01</cell></row><row><cell cols="2">Electricity</cell><cell>12.4277 ± 0.00</cell><cell>12.4434 ± 0.00</cell><cell cols="2">12.4639 ± 0.00</cell><cell>12.6361 ± 0.00</cell><cell>12.6669 ± 0.00</cell></row><row><cell cols="2">Eunite03</cell><cell>8.3868 ± 0.01</cell><cell>8.3782 ± 0.05</cell><cell cols="2">8.1995 ± 0.06</cell><cell>8.7168 ± 0.01</cell><cell>8.7260 ± 0.01</cell></row><row><cell cols="2">RF1</cell><cell>11.3013 ± 0.01</cell><cell>11.3637 ± 0.02</cell><cell cols="2">11.3241 ± 0.04</cell><cell>12.9458 ± 0.04</cell><cell>12.8036 ± 0.20</cell></row><row><cell cols="2">RF2</cell><cell>34.4144 ± 0.02</cell><cell>34.5717 ± 0.02</cell><cell cols="2">34.5742 ± 0.02</cell><cell>36.5764 ± 0.03</cell><cell>36.6312 ± 0.03</cell></row><row><cell cols="2">SCFP</cell><cell cols="5">332.7387 ± 0.02 332.8372 ± 0.02 332.2960 ± 0.09 337.0597 ± 0.10 335.9908 ± 0.08</cell></row><row><cell cols="2">SCM1d</cell><cell cols="2">660.6949 ± 0.08 660.5839 ± 0.08</cell><cell cols="3">660.6439 ± 0.29 675.4183 ± 0.00 675.4258 ± 0.01</cell></row><row><cell cols="2">SCM20d</cell><cell cols="2">275.8278 ± 0.01 275.8724 ± 0.02</cell><cell cols="3">275.8520 ± 0.01 276.6837 ± 0.04 276.6777 ± 0.01</cell></row><row><cell cols="2">Sulfur</cell><cell>9.5019 ± 0.00</cell><cell>9.5055 ± 0.00</cell><cell cols="2">9.5099 ± 0.00</cell><cell>9.6541 ± 0.00</cell><cell>9.6615 ± 0.00</cell></row><row><cell cols="2">Wine</cell><cell>3.4698 ± 0.00</cell><cell>3.4738 ± 0.01</cell><cell cols="2">3.4722 ± 0.01</cell><cell>3.5140 ± 0.01</cell><cell>3.5166 ± 0.01</cell></row><row><cell cols="2">Average rank</cell><cell>1.69</cell><cell>2.44</cell><cell></cell><cell>1.88</cell><cell>4.56</cell><cell>4.44</cell></row><row><cell></cell><cell></cell><cell>MTR-HTMean</cell><cell>MTR-HTPerceptron</cell><cell>iSOUP-Tree</cell><cell>SST-HT</cell><cell>SST-HTAdaptive</cell></row><row><cell></cell><cell>800</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Memory (MB)</cell><cell>400 600</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>100000</cell><cell cols="2">200000 Instances</cell><cell>300000</cell><cell>400000</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the decaying factor is fixed in the expression. We intend to evaluate strategies for dynamically choosing the decaying factors of this metric in the future.2 Available in: https://github.com/scikit-multiflow/scikit-multiflow</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.cs.toronto.edu/~delve/data/comp-activ/desc.html 4 https://sites.google.com/view/cbm/home</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.kaggle.com/c/see-click-predict-fix 6 http://www.open311.org/ 7 https://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank FAPESP (São Paulo Research Foundation) for its financial support (grants #2018/07319-6, #2016/18615-0 and #2013/07375-0) and Intel Inc. for providing equipment for some of the experiments. The authors would also like to give our special thanks to Ricardo Sousa and Professor João Gama for kindly providing some of the datasets used in our experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive model rules from data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="480" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Machine Learning for Data Streams with Practical Examples in MOA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gavaldà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<ptr target="https://moa.cms.waikato.ac.nz/book/" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey on multi-output regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Borchani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bielza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Larrañaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="216" to="233" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Condition-based maintenance of naval propulsion systems: Data analysis with minimal feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cipollini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coraddu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliability Engineering &amp; System Safety</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling wine preferences by data mining from physicochemical properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cerdeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="553" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining high-speed data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the sixth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-target regression from high-speed data streams with adaptive model rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Science and Advanced Analytics (DSAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive model rules from high-speed data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Soft sensors for monitoring and control of industrial processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Graziani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Xibilia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Knowledge discovery from data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cascade generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="343" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning with drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Medas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brazilian symposium on artificial intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on ensemble learning for data stream classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Barddal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Enembreck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07777</idno>
		<title level="m">Stochastic Gradient Trees. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incremental multi-target model trees for data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ikonomovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM symposium on applied computing</title>
		<meeting>the 2011 ACM symposium on applied computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="988" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning model trees from evolving data streams. Data mining and knowledge discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ikonomovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="128" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online tree-based ensembles and option trees for regression on evolving data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ikonomovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="458" to="470" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tree ensembles for predicting structured outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="817" to="833" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensemble learning for data stream analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Minku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stefanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Woźniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="132" to="156" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Open challenges for data stream mining research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krempl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Žliobaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brzeziński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Last</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Noack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sievi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spiliopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-output tree chaining: An interpretative modelling and lightweight multitarget approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mastelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G T</forename><surname>Da Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">K</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Signal Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dstars: A multi-target deep structure for tracking asynchronous regressor stack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mastelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Brazilian Conference on Intelligent Systems (BRACIS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Correcting the usage of the hoeffding inequality in stream mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matuszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krempl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spiliopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Data Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="298" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-target support vector regression via correlation regressor chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Melki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kecman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scikit-multiflow: a multi-output streaming framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdessalem</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2915" to="2914" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey on data stream clustering and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Woon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and information systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="569" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparison of tree-based methods for multi-target regression on data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osojnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on New Frontiers in Mining Complex Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="17" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-label classification via multi-target regression on data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osojnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Discovery Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="170" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-label classification via multi-target regression on data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osojnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="745" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tree-based methods for online multi-target regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osojnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="339" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scalable and efficient multi-label classification for evolving data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="243" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Decision trees for mining data streams based on the gaussian approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rutkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pietruczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="119" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Predicting poultry meat characteristics using an enhanced multi-target regression method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Geronimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mastelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Barbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">I</forename><surname>Ida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biosystems Engineering</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="193" to="204" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-label classification from high-speed data streams with adaptive model rules and random rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-target regression via input space expansion: treating targets as inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Groves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="55" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Openml: Networked science in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Waegeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dembczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Huellermeier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02352</idno>
		<title level="m">Multi-Target Prediction: A Unifying View on Problems and Methods. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
