<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRESNET: GRAPH RESIDUAL NETWORK FOR REVIV- ING DEEP GNNS FROM SUSPENDED ANIMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
							<email>jiawei@ifmlab.org</email>
							<affiliation key="aff0">
								<orgName type="institution">IFM Lab Florida State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IFM Lab Florida State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRESNET: GRAPH RESIDUAL NETWORK FOR REVIV- ING DEEP GNNS FROM SUSPENDED ANIMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Under review as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes' raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node's representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNN), e.g., graph convolutional network (GCN) <ref type="bibr" target="#b18">Kipf &amp; Welling (2016)</ref> and graph attention network (GAT) <ref type="bibr" target="#b36">Veličković et al. (2018)</ref>, based on the approximated spectral graph convolutional operator <ref type="bibr" target="#b12">Hammond et al. (2011)</ref>, can learn the representations of the graph data effectively. Meanwhile, such GNNs have also received lots of criticism, since as these GNNs' architectures go deep, the models' performance will get degraded, which is similar to observations on other deep models (e.g., convolutional neural network) as reported in <ref type="bibr" target="#b15">He et al. (2015)</ref>. Meanwhile, different from the existing deep models, when the GNN model depth reaches a certain limit (e.g., depth ≥ 5 for GCN with the bias term disabled or depth ≥ 8 for GCN with the bias term enabled on the Cora dataset), the model will not respond to the training data any more and become not learnable. Formally, we name such an observation as the GNNs' suspended animation problem, whereas the corresponding model depth is named as the suspended animation limit of GNNs. Here, we need to add a remark: to simplify the presentations in this paper, we will first take vanilla GCN as the base model example to illustrate our discoveries and proposed solutions in the method sections. Meanwhile, empirical tests on several other existing GNNs, e.g., <ref type="bibr">GAT Veličković et al. (2018)</ref> and <ref type="bibr">LOOPYNET Zhang et al. (2018)</ref>, will also be studied in the experiment section of this paper.</p><p>As illustrated in <ref type="figure">Figure 1</ref>, we provide the learning performance of the GCN model on the Cora dataset, where the learning settings (including train/validation/test sets partition, algorithm implementation and fine-tuned hyper-parameters) are identical to those introduced in <ref type="bibr" target="#b18">Kipf &amp; Welling (2016)</ref>. The GCN model with the bias term disable of seven different depths, i.e., GCN(1-layer)-GCN(7-layer), are compared. Here, the layer number denotes the sum of hidden and output layers, which is also equal to the number of spectral graph convolutional layers involved in the model. For instance, besides the input layer, GCN(7-layer) has 6 hidden layer and 1 output layer, both of which involve the spectral graph convolutional operations. According to the plots, GCN(2-layer) and GCN(3-layer) have comparable performance, which both outperform  Under review as a conference paper at ICLR 2020 (a) Training Accuracy (b) Testing Accuracy <ref type="figure">Figure 1</ref>: The learning performance of GCN (bias disabled) with 1-layer, 2-layer, . . . , 7-layer on the Cora dataset. The x axis denotes the iterations over the whole training set. The y axis of the left plot denotes the training accuracy, and that of the right plot denotes the testing accuracy.</p><p>while, as the model depth increases from 3 to 7, its learning performance on both the training set and the testing set degrades greatly. It is easy to identify that such degradation is not caused by overfitting the training data. What's more, much more surprisingly, as the model depth goes deeper to 5 or more, it will suffer from the suspended animation problem and does not respond to the training data anymore. (Similar phenomena can be observed for GCN (bias enabled) and GAT as illustrated by Figures 4 and 5 in the appendix of this paper, whose suspended animation limits are 8 and 5, respectively. Meanwhile, on LOOPYNET, we didn't observe such a problem as shown in <ref type="figure">Figure 6</ref> in the appendix, and we will state the reasons in Section 6 in detail.)</p><p>In this paper, we will investigate the causes of the GNNs' suspended animation problem, and analyze if such a problem also exists in all other GNN models or not. GNNs are very different from the traditional deep learning models, since the extensive connections among the nodes render their learning process no longer independent but strongly correlated. Therefore, the existing solutions proposed to resolve such problems, e.g., residual learning methods used in ResNet for <ref type="bibr">CNN He et al. (2015)</ref>, cannot work well for GNNs actually. In this paper, several different novel graph residual terms will be studied for GNNs specially. Equipped with the new graph residual terms, we will further introduce a new graph neural network architecture, namely graph residual neural network (GRESNET), to resolve the observed problem. Instead of merely stacking the spectral graph convolution layers on each other, the extensively connected high-ways created in GRESNET allow the raw features or intermediate representations of the nodes to be fed into each layer of the model. We will study the effectiveness of the GRESNET architecture and those different graph residuals for several existing vanilla GNNs. In addition, theoretic analyses on GRESNET will be provided in this paper as well to demonstrate its effectiveness from the norm-preservation perspective.</p><p>The remaining parts of this paper are organized as follows. In Section 2, we will introduce the related work of this paper. The suspended animation problem with the spectral graph convolutional operator will be discussed in Section 3, and the suspended animation limit will be analyzed in Section 4. Graph residual learning will be introduced in Section 5, whose effectiveness will be tested in Section 6. Finally, we will conclude this paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Graph Neural Network: Graph neural networks <ref type="bibr" target="#b26">Monti et al. (2017)</ref>; <ref type="bibr" target="#b1">Atwood &amp; Towsley (2016)</ref>; <ref type="bibr" target="#b24">Masci et al. (2015)</ref>; <ref type="bibr" target="#b18">Kipf &amp; Welling (2016)</ref>; <ref type="bibr" target="#b5">Battaglia et al. (2018)</ref>; <ref type="bibr" target="#b3">Bai et al. (2018)</ref>; <ref type="bibr" target="#b31">Scarselli et al. (2009)</ref>; ; <ref type="bibr" target="#b27">Niepert et al. (2016)</ref> have become a popular research topic in recent years. Traditional deep models cannot be directly applied to graph data due to the graph interconnected structures. Many efforts have been devoted to extend deep neural networks on graphs for representation learning. GCN proposed in <ref type="bibr" target="#b18">Kipf &amp; Welling (2016)</ref> feeds the generalized spectral features into the convolutional layer for representation learning. Similar to GCN, deep loopy graph neural network <ref type="bibr" target="#b42">Zhang (2018)</ref> proposes to update the node states in a synchronous manner, and it introduces a spanning tree based learning algorithm for training the model. LOOPYNET accepts nodes' raw features into each layer of the model, and it can effectively fight against the suspended animation problem according to the studied in this paper. <ref type="bibr">GAT Veličković et al. (2018)</ref> leverages masked self-attentional layers to address the shortcomings of GCN. In this year, we have also witnessed some preliminary works on heterogeneous graph neural networks <ref type="bibr" target="#b37">Wang et al. (2019)</ref>; . Similar to GCN, <ref type="bibr">GEM Liu et al. (2018)</ref> utilizes one single layer of attention to capture the impacts of both neighbors and network heterogeneity, which cannot work well on real-world complex networks. Based on GAT, <ref type="bibr">HAN Wang et al. (2019)</ref> learns the attention coefficients between the neighbors based on a set of manually crafted meta paths <ref type="bibr" target="#b34">Sun et al. (2011)</ref>, which may require heavy human involvements. <ref type="bibr">DIFNN Zhang et al. (2018)</ref> introduce a diffusive neural network for the graph structured data specifically, which doesn't suffer from the oversmoothing problem due to the involvement of the neural gates and residual inputs for all the layers. Due to the limited space, we can only name a few number of the representative graph neural network here. The readers are also suggested to refer to page 1 , which provides a summary of the latest graph neural network research papers with code on the node classification problem.</p><p>Residual Network: Residual learning <ref type="bibr" target="#b33">Srivastava et al. (2015)</ref>; <ref type="bibr" target="#b15">He et al. (2015)</ref>; <ref type="bibr" target="#b2">Bae et al. (2016)</ref>; <ref type="bibr" target="#b13">Han et al. (2016)</ref>; <ref type="bibr" target="#b11">Gomez et al. (2017)</ref>; <ref type="bibr" target="#b35">Tai et al. (2017)</ref>; <ref type="bibr" target="#b40">Yu et al. (2017)</ref>; <ref type="bibr" target="#b0">Ahn et al. (2018)</ref>; <ref type="bibr" target="#b20">Li et al. (2018a)</ref>; <ref type="bibr" target="#b6">Behrmann et al. (2019)</ref> has been utilized to improve the learning performance, especially for the neural network models with very deep architectures. To ease gradient-based training of very deep networks, <ref type="bibr" target="#b33">Srivastava et al. (2015)</ref> introduces the highway network to allow unimpeded information flow across several layers. Innovated by the high-way structure, <ref type="bibr" target="#b15">He et al. (2015)</ref> introduces the residual network to simplify highway network by removing the fusion gates. After that, residual learning has been widely adopted for deep model training and optimization. <ref type="bibr" target="#b2">Bae et al. (2016)</ref> introduces residual learning for image restoration via persistent homology-guided manifold simplification; <ref type="bibr" target="#b35">Tai et al. (2017)</ref>; <ref type="bibr" target="#b20">Li et al. (2018a)</ref> introduces a recursive residual network for image resolution adjustment. Some improvement of residual network has also been proposed in recent years. A reversible residual network is introduced in <ref type="bibr" target="#b11">Gomez et al. (2017)</ref>, where each layer's activations can be reconstructed exactly from the next layer's; <ref type="bibr" target="#b13">Han et al. (2016)</ref> improves the conventional model shape with a pyramidal residual network instead; <ref type="bibr" target="#b40">Yu et al. (2017)</ref> introduce the dilated residual network to increases the resolution of output feature maps without reducing the receptive field of individual neurons; and <ref type="bibr" target="#b0">Ahn et al. (2018)</ref> studies the cascading residual network as an accurate and lightweight deep network for image super-resolution. Readers can also refer to <ref type="bibr" target="#b14">He (2016)</ref> for a detailed tutorial on residual learning and applications in neural network studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SUSPENDED ANIMATION PROBLEM WITH GCN MODEL</head><p>In this part, we will provide an analysis about the suspended animation problem of the spectral graph convolutional operator used in GCN to interpret the causes of the observations illustrated in <ref type="figure">Figure 1</ref>. In addition, given an input network data, we will provide the theoretic bound of the suspended animation limit for the GCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VANILLA GRAPH CONVOLUTIONAL NETWORK REVISIT</head><p>To make this paper self-contained, we will provide a brief revisit of the vanilla GCN model in this part. Formally, given an input network G = (V, E), its network structure information can be denoted as an adjacency matrix A = {0, 1} n×n (where |V| = n). GCN defines the normalized adjacency matrix of the input network asÂ =D − 1 2ÃD − 1 2 , whereÃ = A + I n×n andD is the diagonal matrix ofÃ with entryD(i, i) = jÃ (i, j). Given all the nodes in V together with their raw feature inputs X ∈ R n×dx (d x denotes the node raw feature length), GCN defines the spectral graph convolutional operator to learn the nodes' representations as follows:</p><formula xml:id="formula_0">H = SGC (X; G, W) = ReLU Â XW ,<label>(1)</label></formula><p>where W ∈ R dx×d h is the variable involved in the operator.</p><p>Furthermore, GCN can involve a deep architecture by stacking multiple spectral graph convolutional layers on each other, which will be able to learn very complex high-level representations of the nodes. Here, let's assume the model depth to be K (i.e., the number of hidden layers and output layer), and the corresponding node representation updating equations can be denoted as:</p><formula xml:id="formula_1">       H (0) = X H (k) = ReLU Â H (k−1) W (k−1) , ∀k ∈ {1, 2, · · · , K − 1}, Y = softmax Â H (K−1) W (K) .</formula><p>(2) 1 https://paperswithcode.com/task/node-classification 3.2 SUSPENDED ANIMATION PROBLEM WITH GCN By investigating the spectral graph convolutional operator defined above, we observe that it actually involves two sequential steps:</p><formula xml:id="formula_2">H (k) = ReLU Â H (k−1) W (k−1) ⇔ MC Layer: T (k) =ÂH (k−1) FC Layer: H (k) = ReLU T (k) W (k−1) ,<label>(3)</label></formula><p>where the first term on the right-hand-side defines a 1-step Markov chain (MC or a random walk) based on the graph and the second term is a fully-connected (FC) layer parameterized by variable W (k−1) . Similar observations have been reported in <ref type="bibr" target="#b21">Li et al. (2018b)</ref> as well, but it interprets the spectral graph convolutional operator in a different way as the Laplacian smoothing operator used in mesh smoothing in graphics instead.</p><p>Therefore, stacking multiple spectral graph convolutional layers on top of each other is equivalent to the stacking of multiple 1-step Markov chain layers and fully-connected layers in a crosswise way.</p><p>Considering that the variables W (k−1) for the vector dimension adjustment are shared among all the nodes, given two nodes with identical representations, the fully-connected layers (parameterized by W (k−1) will still generate identical representations as well. In other words, the fully-connected layers with shared variables for all the nodes will not have significant impacts on the convergence of Markov chain layers actually. Therefore, in the following analysis, we will simplify the model structure by assuming the mapping defined by fully-connected layers to the identity mapping. We will investigate the Markov chain layers closely by picking them out of the model to compose the Markov chain of multiple steps:</p><formula xml:id="formula_3">T (0) = X, T (k) =ÂT (k−1) , ∀k ∈ {1, 2, · · · , K}.<label>(4)</label></formula><p>Meanwhile, the Markov chain layers may converge with k layers iff T (k) = T (k−1) , i.e., the representations before and after the updating are identical (or very close), which is highly dependent on the input network structure, i.e., matrixÂ, actually. DEFINITION 1. (Irreducible and Aperiodic Network): Given an input network G = (V, E), G is irreducible iff for any two nodes</p><formula xml:id="formula_4">v i , v j ∈ V, node v i is accessible to v j . Meanwhile, G is aperiodic iff G is not bipartite. LEMMA 1.</formula><p>Given an unweighted input graph G, which is irreducible, finite and aperiodic, if its corresponding matrix is asymmetric, starting from any initial distribution vector x ∈ R n×1 (x ≥ 0 and x 1 = 1), the Markov chain operating on the graph has one unique stationary distribution vector π * such that lim t→∞Â t x = π * , where π * (i) = d(vi) 2|E| . Meanwhile, if matrixÂ is symmetric, the stationary distribution vector π * will be a uniform distribution over the nodes, i.e., π * (i) = 1 n .</p><p>Based on the above Lemma 1, we can derive similar results for the multiple Markov chain layers in the GCN model based on the nodes' feature inputs, which will reduce the learned nodes' representations to the stationary representation matrix. THEOREM 1. Given a input network G = (V, E), which is unweighted, irreducible, finite and aperiodic, if there exist enough nested Markov chain layers in the GCN model, it will reduce the nodes'</p><p>representations from the column-normalized feature matrix X ∈ R n×dx to the stationary representation Π * = [π * , π * , · · · , π * ] ∈ R n×dx . Furthermore, if G is undirected, then the stationary representation will become Π * = 1 n · 1 n×dx .</p><p>Theorem 1 illustrates the causes of the GNNs' suspended animation problem. Proofs of Lemma 1 and Theorem 1 are provided in the appendix attached to this paper at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SUSPENDED ANIMATION LIMIT ANALYSIS</head><p>Here, we will study the suspended animation limit of the GCN model based on its spectral convolutional operator analysis, especially the Markov chain layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SUSPENDED ANIMATION LIMIT BASED INPUT NETWORK STRUCTURE</head><p>Formally, we define the suspended animation limit of GCN as follows:</p><p>DEFINITION 2. (Suspended Animation Limit): The suspended animation limit of GCN on network G is defined as the smallest model depth τ such that for any nodes' column-normalized featured matrix input X in the network G the following inequality holds:</p><formula xml:id="formula_5">GCN(X; τ ) − Π * 1 ≤ .<label>(5)</label></formula><p>For representation convenience, we can also denote the suspended animation limit of GCN defined on network G as ζ(G) (or ζ for simplicity if there is no ambiguity problems).</p><p>Based on the above definition, for GCN with identity FC mappings, there exists a tight bound of the suspended animation limit for the input network.</p><p>THEOREM 2. Let 1 ≥ λ 1 ≥ λ 2 ≥ · · · ≥ λ n be the eigen-values of matrixÂ defined based on network G, then the corresponding suspended animation limit of the GCN model on G is bounded</p><formula xml:id="formula_6">ζ ≤ O log min i 1 π * (i) 1 − max{λ 2 , |λ n |} .<label>(6)</label></formula><p>In the case that the network G is a d-regular, then the suspended animation limit of the GCN model on G can be simplified as</p><formula xml:id="formula_7">ζ ≤ O log n 1 − max{λ 2 , |λ n |} .<label>(7)</label></formula><p>The suspended animation limit bound derived in the above theorem generally indicates that the network structure G determines the maximum allows depth of GCN. Among all the eigen-values ofÂ defined on network G, λ 2 measures how far G is from being disconnected; and λ n measures how far G is from being bipartite. In the case that G is reducible (i.e., λ 2 = 1) or bipartite (i.e., λ n = −1), we have ζ → ∞ and the model will not suffer from the suspended animation problem.</p><p>In the appendix of <ref type="bibr" target="#b18">Kipf &amp; Welling (2016)</ref>, the authors introduce a naive-residual based variant of GCN, and the sepctral graph convolutional operator is changed as follows (the activation function is also changed to sigmoid function instead):</p><formula xml:id="formula_8">H (k) = σ Â H (k−1) W (k−1) + H (k−1) ,<label>(8)</label></formula><p>For the identity fully-connected layer mapping, the above equation can be reduced to the following lazy Markov chain based layers</p><formula xml:id="formula_9">H (k) = 2 · 1 2Â H (k−1) + 1 2 H (k−1) .<label>(9)</label></formula><p>Such a residual term will not help resolve the problem, and it will still suffer from the suspended animation problem with the following suspended animation limit bound:</p><p>COROLLARY 1. Let 1 ≥ λ 1 ≥ λ 2 ≥ · · · ≥ λ n be the eigen-values of matrixÂ defined based on network G, then the corresponding suspended animation limit of the GCN model (with lazy Markov chain based layers) on G is bounded Node residual terms are assumed to be independent and determined by the current state only. graph-naive residual R H (k−1) , X; G =ÂH (k−1)</p><formula xml:id="formula_10">ζ ≤ O log min i 1 π * (i)</formula><p>Node residual terms are correlated based on network structure, and can be determined by the current state. raw residual R H (k−1) , X; G = X Node residual terms are assumed to be independent and determined by the raw input features only. graph-raw residual R H (k−1) , X; G =ÂX Node residual terms are correlated based on network structure, and are determined by the raw input features.</p><p>[π * , π * , · · · , π * ], where π * (i) is determined by the degree of node v i in the network. For any two nodes v i , v j ∈ V, the differences of their learned representation can be denoted as</p><formula xml:id="formula_11">d(v i , v j ) = Π(i, :) − Π(j, :) 1 = dx k=1 |Π(i, k) − Π(i, k)| = d x |d(v i ) − d(v j )| 2|E| .<label>(11)</label></formula><p>According to <ref type="bibr" target="#b9">Faloutsos et al. (1999)</ref>, the node degree distributions in most of the networks follow the power-law, i.e., majority of the nodes are of very small degrees. Therefore, for massive nodes in the input network with the same (or close) degrees, the differences between their learned representations will become not distinguishable. • Raw Feature Coding: Besides the input network, the raw feature coding can also affect the learning performance greatly. Here, we can take the GCN with one single Markov chain layer and one identity mapping layer. For any two nodes v i , v j ∈ V with raw feature vectors X(i, :) and X(j, :), we can denote the differences between their learned representations as follows:</p><formula xml:id="formula_12">d(v i , v j ) = T(i, :) − T(j, :) 1 = Â (i, :) −Â(j, :) X 1 .<label>(12)</label></formula><p>For the one-hot feature coding used in the source code of <ref type="bibr">GCN Kipf &amp; Welling (2016)</ref> and other GNNs, matrix X can be also very sparse as well. Meanwhile, vectorÂ(i, :) −Â(j, :) is also a sparse vector, which renders the right-hand-side term to be a very small value. • Training Set Size: Actually, the nodes have identical representation and the same labels will not degrade the learning performance of the model. However, if such node instances actually belong to different classes, it will become a great challenge for both the training and test stages of the GCN model. • Gradient Vanishing/Exploding: Similar to the existing deep models, deep GNNs will also suffer from the gradient vanishing/exploding problems <ref type="bibr" target="#b29">Pascanu et al. (2012)</ref>, which will also greatly affect the learning performance of the models.</p><p>Although these factors mentioned above are not involved in the suspended animation limit bound representation, but they do have great impacts on the GCN model in practical applications. In the following section, we will introduce graph residual network GRESNET, which can be useful for resolving such a problem for GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GRAPH RESIDUAL NETWORK</head><p>Different from the residual learning in other areas, e.g., computer vision <ref type="bibr" target="#b15">He et al. (2015)</ref>, where the objective data instances are independent with each other, in the inter-connected network learning setting, the residual learning of the nodes in the network are extensively connected instead. It renders the existing residual learning strategy less effective for improving the performance of GCN.  <ref type="bibr" target="#b15">He et al. (2015)</ref>). This reformulation is motivated by the counterintuitive phenomena about the degradation problem observed on the deep CNN. Different from the learning settings of CNN, where the data instances are assumed to be independent, the nodes inside the input network studied in GCN are closely correlated. Viewed in such a perspective, new residual learning mechanism should be introduced for GCN specifically. Here, we need to add a remark: For the presentation simplicity in this paper, given the objective H(x) = F (x) + R(x), we will misuse the terminologies here: we will name F (x) as the approximated mapping of H(x), and call R(x) as the graph residual term. Formally, by incorporating the residual learning mechanism into the GCN model, the node representation updating equations (i.e., Equation 2) can be rewritten as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">GRAPH RESIDUAL LEARNING</head><formula xml:id="formula_13">F (x) + R(x) (where R(x) = x is used in</formula><formula xml:id="formula_14">       H (0) = X H (k) = ReLU Â H (k−1) W (k−1) + R H (k−1) , X; G , ∀k ∈ {1, 2, · · · , K − 1}, Y = softmax Â H (K−1) W (K) + R H (K−1) , X; G .<label>(13)</label></formula><p>The graph residual term R H (k−1) , X; G , ∀k ∈ {1, 2, · · · , K} can be defined in different ways. We have also examined to put R H (k−1) , X; G outside of the ReLU(·) function for the hidden layers (i.e., H (k) = ReLU Â H (k−1) W (k−1) + R H (k−1) , X; G ), whose performance is not as good as what we show above. In the appendix of <ref type="bibr" target="#b18">Kipf &amp; Welling (2016)</ref>, by following the ResNet (CNN) <ref type="bibr" target="#b15">He et al. (2015)</ref>, the graph residual term R H (k−1) , X; G is simply defined as H (k−1) , which is named as the naive residual term in this paper (Here, term "naive" has no disparaging meanings). However, according to the studies, such a simple and independent residual term for the nodes fail to capture information in the inter-connected graph learning settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">GRESNET ARCHITECTURE</head><p>In this paper, we introduce several other different representation of the graph residual terms, which are summarized in <ref type="table" target="#tab_1">Table 1</ref>. If feature dimension adjustment is needed, e.g., for raw residual term, an extra variable matrix W adj can be added to redefine the terms (which are not shown in this paper). For the graph residual term representations in <ref type="table" target="#tab_1">Table 1</ref>, naive residual and raw residual are based on the assumption that node residuals are independent and determined by either the current state or the raw features. Meanwhile, the graph naive residual and graph raw residual assume the residual terms of different nodes are correlated instead, which can be computed with the current state or raw features. We also illustrate the architectures of vanilla 2-layer GCN and the 7-layer GRESNETs (taking GCN as the base model) with different graph residual terms in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Vanilla GCN: For the vanilla GCN network used in <ref type="bibr" target="#b18">Kipf &amp; Welling (2016)</ref>, i.e., the left plot of <ref type="figure" target="#fig_0">Figure 2</ref>, given the inputs G and X, it employs two SGC layers to project the input to the objective GRESNET Network: For the GRESNET network, i.e., the right four models in <ref type="figure" target="#fig_0">Figure 2</ref>, they accept the identical inputs as vanilla GCN but will create graph residual terms to be added to the intermediate representations. Depending on the specific residual term representations adopted, the corresponding high-way connections can be different. For the hidden layers involved in the models, their length is also 16, and ReLU is used as the activation function for the intermediate layers.</p><p>By comparing the outputŶ learned by the models against the ground-truth Y of the training instances, all the variables involved in the model, i.e., Θ, can be effectively learned with the backpropagation algorithm to minimize the loss functions (Y,Ŷ; Θ) (or (Θ) for simplicity). In the following part, we will demonstrate that for the GRESNET model added with graph residual terms. It can effectively avoid dramatic changes to the nodes' representations between sequential layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">GRAPH RESIDUAL LEARNING EFFECTIVENESS ANALYSIS</head><p>In this part, we will illustrate why the inclusion of the graph residual learning can be effective for learning deep graph neural networks. Here, we assume the ultimate model that we want to learn as H : X → Y, where X and Y denote the feature and label spaces, respectively. For analysis simplicity, we have some assumptions about the function H <ref type="bibr" target="#b41">Zaeemzadeh et al. (2018)</ref>. ASSUMPTION 1. Function H is differentiable, invertible and satisfies the following conditions:</p><formula xml:id="formula_15">• ∀x, y, z ∈ X with bounded norm, ∃α &gt; 0, (H (x) − H (y))z ≤ α · x − y · z , • ∀x, y ∈ X with bounded norm, ∃β &gt; 0, H −1 (x) − H −1 (y) ≤ β · x − y ,</formula><p>• ∃x ∈ X with bounded norm such that Det(H (x)) &gt; 0. In the above conditions, terms α and β are constants.</p><p>To model the function H, the GRESNET actually defines a sequence of K sequential mappings with these K layers:</p><p>x</p><formula xml:id="formula_16">(k) = F (k−1) (x (k−1) ) + R (k−1) (x (k−1) ),<label>(14)</label></formula><p>where x (k−1) and x (k) denote the intermediate node representations serving as input and output of the k th layer. F (k−1) (·) and R (k−1) (·) denote the function approximation and residual term learned by the k − 1 th layer of the model. When training these K sequential mappings, we have the following theorem hold for the representation gradients in the learning process. THEOREM 3. Let H denote the objective function that we want to model, which satisfies Assumption 1, in learning the K-layer GRESNET model, we have the following inequality hold: where δ ≤ c · log(2K) K and c = c 1 · max{α · β · (1 + β), β · (2 + α) + α} for some c 1 &gt; 0.</p><formula xml:id="formula_17">(1 − δ) ∂ (Θ) ∂x (k) 2 ≤ ∂ (Θ) ∂x (k−1) 2 ≤ (1 + δ) ∂ (Θ) ∂x (k) 2 ,<label>(15)</label></formula><p>Proof of Theorem 3 will be provided in the appendix. The above theorem indicates that in the learning process, the norm of loss function against the intermediate representations doesn't change significantly between sequential layers. In other words, GRESNET can maintain effective representations for the inputs and overcome the suspended animation problem. In addition, we observe that the bound of the gap term δ, i.e., c · log(2K) K , decreases as K increases (when K ≥ 2). Therefore, for deeper GRESNET, the model will lead to much tighter gradient norm changes, which is a desired property. In the following section, we will provide the experimental results of GRESNET compared against their vanilla models on several graph benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>To demonstrate the effectiveness of GRESNET in improving the learning performance for graph neural networks with deep architectures, extensive experiments will be done on several graph benchmark datasets. Similar to the previous works on node classification <ref type="bibr" target="#b18">Kipf &amp; Welling (2016)</ref>; <ref type="bibr" target="#b36">Veličković et al. (2018)</ref>, the graph benchmark datasets used in the experiments include Cora, Citeseer and Pubmed from <ref type="bibr" target="#b32">Sen et al. (2008)</ref>. For fair comparison, we follow exactly the same experimental settings as <ref type="bibr" target="#b18">Kipf &amp; Welling (2016)</ref> on these datasets.</p><p>In this paper, we aim at studying the suspended animation problems with the existing graph neural networks, e.g., <ref type="bibr">GCN Kipf &amp; Welling (2016)</ref>, <ref type="bibr">GAT Veličković et al. (2018)</ref> and <ref type="bibr">LOOPYNET Zhang (2018)</ref>, where LOOPYNET is not based on the spectral graph convolutional operator. We also aim to investigate the effectiveness of these proposed graph residual terms in improving their learning performance, especially for the models with deep architectures. In addition, to make the experiments self-contained, we also provide the latest performance of the other baseline methods on the same datasets in this paper, which include state-of-the-art graph neural networks, e.g., <ref type="bibr">APPNP Klicpera et al. (2019)</ref>, <ref type="bibr">GOCN Jiang et al. (2019)</ref> and GraphNAS <ref type="bibr" target="#b10">Gao et al. (2019)</ref>, existing graph embedding models, like DeepWalk <ref type="bibr" target="#b30">Perozzi et al. (2014)</ref>, Planetoid <ref type="bibr" target="#b39">Yang et al. (2016)</ref> and MoNet <ref type="bibr" target="#b25">Monti et al. (2016)</ref> and representation learning approaches, like ManiReg <ref type="bibr" target="#b7">Belkin et al. (2006)</ref>, SemiEmb <ref type="bibr" target="#b38">Weston et al. (2008)</ref>, <ref type="bibr">LP Zhu et al. (2003)</ref> and <ref type="bibr">ICA Lu &amp; Getoor (2003)</ref>. <ref type="table">Table 3</ref>: Learning result accuracy of node classification methods. In the table, '-' denotes the results of the methods on these datasets are not reported in the existing works. Performance of GCN, GAT and LOOPYNET shown in <ref type="table" target="#tab_3">Table 2</ref> are not provided here to avoid reporting duplicated results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Datasets (Accuracy) Cora Citeseer Pubmed LP <ref type="bibr" target="#b45">(Zhu et al. (2003)</ref>) 0.680 0.453 0.630 ICA <ref type="bibr" target="#b23">(Lu &amp; Getoor (2003)</ref>) 0.751 0.691 0.739 ManiReg <ref type="bibr" target="#b7">(Belkin et al. (2006)</ref>) 0.595 0.601 0.707 SemiEmb <ref type="bibr" target="#b38">(Weston et al. (2008))</ref> 0.590 0.596 0.711 DeepWalk <ref type="bibr" target="#b30">(Perozzi et al. (2014)</ref> raw features in all the layer (it is quite similar to the raw residual term introduced in this paper).</p><p>As the model depth increase, performance of LOOPYNET remains very close but converge much more slowly. By taking GCN as the base model, we also show the performance of GRESNET with different residual terms in <ref type="figure" target="#fig_1">Figure 3</ref>. By comparing these plots with <ref type="figure">Figure 1</ref>, both naive and graphnaive residual terms help stabilize the performance of deep GRESNET(GCN)s. Meanwhile, for the raw and graph-raw residual terms, their contributions are exceptional. With these two residual terms, deep GRESNET(GCN)s can achieve even better performance than the shallow vanilla GCNs.</p><p>Besides the results on the Cora dataset, in <ref type="table" target="#tab_3">Table 2</ref>, we illustrate the best observed performance by GRESNET with different residual terms based on GCN, GAT and LOOPYNET base models respectively on all the datasets. Both the best accuracy score and the achieved model depth are provided. According to the results, for the vanilla models, GCN, GAT and LOOPYNET can all obtain the best performance with shallow architectures. For instance on Cora, GCN(2-layer) obtains 0.815; GAT(2-layer) gets 0.830; and LOOPYNET(2-layer) achieves 0.826, respectively. Added with the residual terms, the performance of all these models will get improved. In addition, deep GRESNET(GCN), GRESNET(GAT) and GRESNET(LOOPYNET) will be able to achieve much better results than the shallow vanilla models, especially the ones with the graph-raw residual terms. The best scores and the model depth for each base model on these datasets are also highlighted. The time costs of learning the GRESNET model is almost identical to the required time costs of learning the vanilla models with the same depth, which are not reported in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">A COMPLETE COMPARISON WITH EXISTING NODE CLASSIFICATION METHODS</head><p>Besides the comparison with GCN, GAT and LOOPYNET shown in <ref type="table" target="#tab_3">Table 2</ref>, to make the experimental studies more complete, we also compare GRESNET(GCN), GRESNET(GAT) and GRES-NET(LOOPYNET) with both the classic and the state-of-the-art models, whose results are provided in <ref type="table">Table 3</ref>. In the table, we didn't indicate the depth of the GRESNET models and results of GCN, GAT and LOOPYNET (shown in <ref type="table" target="#tab_3">Table 2</ref> already) are not included. According to the results, compared against these baseline methods, GRESNETs can also outperform them with great advantages. Without the complex model architecture extension or optimization techniques used by the latest methods APPNP <ref type="bibr" target="#b19">Klicpera et al. (2019)</ref>, <ref type="bibr">GOCN Jiang et al. (2019)</ref> and GraphNAS <ref type="bibr" target="#b10">Gao et al. (2019)</ref>, adding the simple graph residual terms into the base models along can already improve the learning performance greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we focus on studying the suspended animation problem with the existing graph neural network models, especially the spectral graph convolutional operator. We provide a theoretic analysis about the causes of the suspended animation problem and derive the bound for the maximum allowed graph neural network depth, i.e., the suspended animation limit. To resolve such a problem, we introduce a novel framework GRESNET, which works well for learning deep representations from the graph data. Assisted with these new graph residual terms, we demonstrate that GRESNET can effectively resolve the suspended animation problem with both theoretic analysis and empirical experiments on several benchmark node classification datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">APPENDIX</head><p>where σ max (·) and σ min (·) denote the maximum and minimum singular values of the input matrix, respectively.</p><p>Proof. Due to the triangle inequality, the upper bound is easy to prove: σ max (I + M) = I + M 2 ≤ I 2 + M 2 = 1 + σ max (M).</p><p>In the case that σ max (M) ≥ 1, the lower bound is trivial to prove since I + M is non-singular, we have σ min (I + M) &gt; 0.</p><p>Meanwhile, in the case that σ max (M) &lt; 1, it is easy to know that |λ max (M)| &lt; 1, where λ max (·) denotes the latest eigenvalue of the input matrix.</p><formula xml:id="formula_21">σ min (I + M) = (I + M) −1 −1 2 = ∞ k=1 (−1) k M k −1 2 ≥ ∞ k=1 (−1) k M k 2 −1 ≥ ∞ k=1 M k 2 −1 = 1 1 − M 2 −1 = 1 − σ max (M),<label>(43)</label></formula><p>which concludes the proof for the lower bound. THEOREM 3. Let H denote the objective function that we want to model, which satisfies Assumption 1, in learning the K-layer GRESNET model, we have the following inequality hold:</p><formula xml:id="formula_22">(1 − δ) ∂ (Θ) ∂x (k) 2 ≤ ∂ (Θ) ∂x (k−1) 2 ≤ (1 + δ) ∂ (Θ) ∂x (k) 2 ,<label>(44)</label></formula><p>where δ ≤ c log(2K) K and c = c 1 max{αβ(1 + β), β(2 + α) + α} for some c 1 &gt; 0.</p><p>Proof. We can represent the Jacobian matrix J of x (k) with x (k−1) . Therefore, we have</p><formula xml:id="formula_23">∂ (Θ) ∂x (k−1) = ∂ (Θ) ∂x (k) ∂x (k) ∂x (k−1) = J ∂ (Θ) ∂x (k) .<label>(45)</label></formula><p>Matrix J can be rewritten as J = I + ∇F (k−1) (x (k−1) ), where ∇F (k−1) (x (k−1) ) = lim t→0 + F (k−1) (x (k−1) + tv) − F (k−1) (x (k−1) ) t 2 .</p><p>Meanwhile, it is easy to know that</p><formula xml:id="formula_25">σ min (J) ∂ (Θ) ∂x (k) 2 ≤ J ∂ (Θ) ∂x (k) 2 ≤ σ max (J) ∂ (Θ) ∂x (k) 2 .<label>(47)</label></formula><p>Based on the above lemma, we have</p><formula xml:id="formula_26">(1 − σ) ∂ (Θ) ∂x (k) 2 ≤ ∂ (Θ) ∂x (k−1) 2 ≤ (1 + σ) ∂ (Θ) ∂x (k) 2 ,<label>(48)</label></formula><p>where σ = σ max (∇F (k−1) (x (k−1) )).</p><p>Furthermore, we know that</p><formula xml:id="formula_27">σ max (∇F (k−1) (x (k−1) ) = sup v ∇F (k−1) (x (k−1) )v 2 v 2 = lim t→0 + sup v F (k−1) (x (k−1) + tv) − F (k−1) (x (k−1) ) 2 t v 2 ≤ F (k−1) L ,<label>(49)</label></formula><p>where · L denotes the Lipschitz seminorm of the input function and it is defined as</p><formula xml:id="formula_28">F (k−1) L = sup x =y F (k−1) (x) − F (k−1) (y) 2 x − y 2 .<label>(50)</label></formula><p>Meanwhile, according to the Theorem 1 in <ref type="bibr" target="#b4">Bartlett et al. (2018)</ref> (whose proof will not be introduced here), we know that</p><formula xml:id="formula_29">F (k−1) L ≤ c log 2K K ,<label>(51)</label></formula><p>which concludes the proof. In the above equation, K denotes the layer depth of the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt;Ŷ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt;Ŷ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt;Ŷ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; SGC G &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / V t + 0 R d S S 5 A / s b V i R u a b h 3 H o i B U = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s a L Y k W W m I U J I E L 2 V v m Y M P e 3 m V 3 z 4 R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S A T X x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 z h V D F s s F r H q B F S j 4 B J b h h u B n U Q h j Q K B j 8 H 4 e u Y / P q H S P J Y P Z p K g H 9 G h 5 C F n 1 F j p v n p T 7 Z c r b s 2 d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w k s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u 1 z y 3 5 t 3 V K 4 2 r P I 4 i n M A p n I M H F 9 C A W 2 h C C x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B S 3 Y 0 l &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c N d i W G T h Q s 7 u 8 o H n k B R i f D 5 r u w = " &gt; A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 6 t L N Y C u 4 K k k 3 u i y 6 c V n B P q A p Z T K d t E M n k z B z I 5 T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J E i k M u u 6 3 s 7 G 5 t b 2 z W 9 o r 7 x 8 c H h 1 X T k 4 7 J k 4 1 4 2 0 W y 1 j 3 A m q 4 F I q 3 U a D k v U R z G g W S d 4 P p X e 5 3 n 7 g 2 I l a P O E v 4 I K J j J U L B K F r J r / k R x U k Q Z r 1 5 b V i p u n V 3 A b J O v I J U o U B r W P n y R z F L I 6 6 Q S W p M 3 3 M T H G R U o 2 C S z 8 t + a n h C 2 Z S O e d 9 S R S N u B t k i 8 5 x c W m V E w l j b p 5 A s 1 N 8 b G Y 2 M m U W B n c w j m l U v F / / z + i m G N 4 N M q C R F r t j y U J h K g j H J C y A j o T l D O b O E M i 1 s V s I m V F O G t q a y L c F b / f I 6 6 T T q n l v 3 H h r V 5 m 1 R R w n O 4 Q K u w I N r a M I 9 t K A N D B J 4 h l d 4 c 1 L n x X l 3 P p a j G 0 6 x c w Z / 4 H z + A I j A k V Y = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o z 9 b n o K A m X r k l l 8 d m 1 p i U 9 c H r k 8 = " &gt; A A A B + 3 i c b V C 7 T s M w F H X K q 5 R X K C O L R Y v E V C V d Y K x g Y S w S f a C m q h z X a a 0 6 T m T f I K o o v 8 L C A E K s / A g b f 4 P T Z o C W I 1 k 6 O u d e 3 e P j x 4 J r c J x v q 7 S x u b W 9 U 9 6 t 7 O 0 f H B 7 Z x 9 W u j h J F W Y d G I l J 9 n 2 g m u G Q d 4 C B Y P 1 a M h L 5 g P X 9 2 k / u 9 R 6 Y 0 j + Q 9 z G M 2 D M l E 8 o B T A k Y a 2 d W 6 N y W Q e i G B q R + k D 1 l W H 9 k 1 p + E s g N e J W 5 A a K t A e 2 V / e O K J J y C R Q Q b Q e u E 4 M w 5 Q o 4 F S w r O I l m s W E z s i E D Q y V J G R 6 m C 6 y Z / j c K G M c R M o 8 C X i h / t 5 I S a j 1 P P T N Z J 5 R r 3 q 5 + J 8 3 S C C 4 G q Z c x g k w S Z e H g k R g i H B e B B 5 z x S i I u S G E K m 6 y Y j o l i l A w d V V M C e 7 q l 9 d J t 9 l w n Y Z 7 1 6 y 1 r o s 6 y u g U n a E L 5 K J L 1 E K 3 q I 0 6 i K I n 9 I x e 0 Z u V W S / W u / W x H C 1 Z x c 4 J + g P r 8 w f m o 5 R V &lt; / l a t e x i t &gt; A comparison of vanilla GCN and GRESNET (GCN) with different graph residual terms. The vanilla GCN has two layers, and the GRESNET models have a deep architecture which involves seven layers of SGC operators. The intermediate ReLU activation functions used between sequential layers in GRESNET are omitted for simplicity. NotationÂ denotes the normalized adjacency matrix of input network G, which indicates the correlated graph residual terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The learning performance of GRESNET with GCN as the base model and different graph residual terms on the Cora dataset: (a)-(b) GRESNET(GCN, naive); (c)-(d) GRESNET(GCN, graph-naive); (e)-(f) GRESNET(GCN, raw); (g)-(h) GRESNET(GCN, graph-raw). For plot (b), the curves corresponding to the 5-layer, 6-layer and 7-layer models are hidden by the legend. For plot (c) and (d), the curve of 7-layer model is hidden by the legend. labels. For the intermediate representations, the hidden layer length is 16, and ReLU is used as the activation function, whereas softmax is used for output label normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>The learning performance of GCN (bias enabled) on the Cora dataset. (a) Training Accuracy (GAT) (b) Testing Accuracy (GAT) The learning performance of GAT on the Cora dataset (a) Training Accuracy (LOOPYNET) (b) Testing Accuracy (LOOPYNET) The learning performance of LOOPYNET on the Cora dataset 8.3 PROOF OF THEOREM 3 Prior to introducing the proof of Theorem 3, we first introduce the following lemma. LEMMA 3. For any non-singular matrix I + M, we have 1 − σ max (M) ≤ σ min (I + M) ≤ σ max (I + M) ≤ 1 + σ max (M),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Mean-1 arXiv:1909.05729v2 [cs.LG] 24 Sep 2019</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>A Summary of Graph Residual Terms and Physical Meanings.</figDesc><table><row><cell>Name</cell><cell>Residual Term</cell><cell>Description</cell></row><row><cell>naive resid-ual</cell><cell>R H (k−1) , X; G = H (k−1)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Best performance (accuracy) and model depth summarization of GRESNET with different residual terms on the benchmark datasets (we take GCN, GAT and LOOPYNET as the base models).</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="3">Datasets (Accuracy &amp; Model Depth)</cell></row><row><cell cols="2">Base Models Residuals</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell cols="3">vanilla GCN (Kipf &amp; Welling (2016)) 0.815 2-layer</cell><cell>0.703 2-layer</cell><cell>0.790 2-layer</cell></row><row><cell></cell><cell>naive</cell><cell>0.814 3-layer</cell><cell>0.710 3-layer</cell><cell>0.814 3-layer</cell></row><row><cell>GCN</cell><cell>graph-naive raw</cell><cell>0.833 2-layer 0.826 4-layer</cell><cell>0.715 3-layer 0.727 4-layer</cell><cell>0.811 2-layer 0.810 3-layer</cell></row><row><cell></cell><cell>graph-raw</cell><cell>0.843 5-layer</cell><cell>0.722 4-layer</cell><cell>0.817 7-layer</cell></row><row><cell cols="3">vanilla GAT (Veličković et al. (2018)) 0.830 2-layer</cell><cell>0.725 2-layer</cell><cell>0.790 2-layer</cell></row><row><cell></cell><cell>naive</cell><cell>0.844 5-layer</cell><cell>0.735 5-layer</cell><cell>0.809 3-layer</cell></row><row><cell>GAT</cell><cell>graph-naive raw</cell><cell>0.855 3-layer 0.842 3-layer</cell><cell>0.732 4-layer 0.733 3-layer</cell><cell>0.815 5-layer 0.814 4-layer</cell></row><row><cell></cell><cell>graph-raw</cell><cell>0.847 3-layer</cell><cell>0.729 5-layer</cell><cell>0.822 4-layer</cell></row><row><cell cols="3">vanilla LOOPYNET (Zhang (2018)) 0.826 2-layer</cell><cell>0.716 2-layer</cell><cell>0.812 2-layer</cell></row><row><cell></cell><cell>naive</cell><cell>0.833 2-layer</cell><cell>0.728 3-layer</cell><cell>0.830 4-layer</cell></row><row><cell>LOOPYNET</cell><cell>graph-naive raw</cell><cell>0.832 2-layer 0.836 2-layer</cell><cell>0.728 3-layer 0.730 5-layer</cell><cell>0.819 2-layer 0.828 4-layer</cell></row><row><cell></cell><cell>graph-raw</cell><cell>0.839 4-layer</cell><cell>0.737 5-layer</cell><cell>0.814 4-layer</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/anonymous-sourcecode/GResNet 3 GPU Server: ASUS X99-E WS motherboard, Intel Core i7 CPU 6850K@3.6GHz (6 cores), 3 Nvidia GeForce GTX 1080 Ti GPU (11 GB buffer each), 128 GB DDR4 memory and 128 GB SSD swap. For the deep models which cannot fit in the GPU memory, we run them with CPU instead.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reproducibility: Both the datasets and source code used in this paper can be accessed via link 2 . Detailed information about the server used to run the model can be found at the footnote 3 . 6.1 EFFECTIVENESS OF THE GRAPH RESIDUAL TERMS In addition to <ref type="figure">Figure 1</ref> for GCN (bias disabled) on the Cora dataset, as shown in <ref type="figure">Figures 4-6</ref> in the appendix, for the GCN (bias enabled) and GAT with deep architectures, we have observed similar suspended animation problems. Meanwhile, the performance of LOOPYNET is different. Since LOOPYNET is not based on the spectral graph convolution operator, which accepts nodes' Under review as a conference paper at ICLR 2020 8.2 PROOFS OF THEOREM AND LEMMA 8.2.1 PROOF OF LEMMA 1 LEMMA 1. Given an irreducible, finite and aperiodic graph G, starting from any initial distribution vector x ∈ R n×1 (x ≥ 0 and x 1 = 1), the Markov chain operating on the graph has one unique stationary distribution vector π * such that lim t→∞Â t x = π * , where π * (i) = d(vi) 2|E| . If matrixÂ is symmetric (i.e., G is undirected), π * will be a uniform distribution over the nodes, i.e., π * (i) = 1 n .</p><p>Proof. The stationary distribution vector existence and uniqueness has been proved in <ref type="bibr" target="#b28">Norris (1998)</ref>.</p><p>Here, we need to identify on vector π at convergence such thatÂπ = π, i.e.,</p><p>According to the definition ofÂ, it is easy to have</p><p>where w i,j denotes the initial connection weight between v i and v j . For the unweighted network, w i,j will be in {0, 1} indicating if v i and v j are connected or not. Notation d w (i) denotes the rough degree of node v i in the network subject to the weight w : E ∈ R, which sums the weight of edges connected to the nodes in the network. So, it is enough to have π(j) ∝ d w (j). More precisely, we can set</p><p>In this case,</p><p>Meanwhile, for the symmetric and normalized adjacency matrixÂ, we can prove the stationary distribution π * (i) = 1 n in a similar way, which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">PROOF OF THEOREM 1</head><p>THEOREM 1. Given a input network G = (V, E), which is unweighted, irreducible, finite and aperiodic, if there exist enough nested Markov chain layers in the GCN model, it will reduce the nodes'</p><p>representations from the column-normalized feature matrix X ∈ R n×dx to the stationary representation Π * = [π * , π * , · · · , π * ] ∈ R n×dx . Furthermore, if G is undirected, then the stationary representation will become Π * = 1 n · 1 n×dx .</p><p>Proof. This theorem can be proved based on Lemma 1. For any initial state distribution vector</p><p>x ∈ R |V|×1 , for the Markov chain at convergence, we have</p><p>We misuse the notationÂ * = lim t→∞Â t . In this case,</p><p>A * X = [Â * X(:, 1),Â * X(:, 2), · · · ,Â * X(:, d x )] = [π * , π * , · · · , π * ],</p><p>which together with Lemma 1 conclude the proof.</p><p>Prior to introducing the proof of Theorem 2, we will introduce the following lemma first.</p><p>LEMMA 2. For any vector x ∈ R n , the following inequality holds:</p><p>Proof. According to Hölder's inequality Hölder <ref type="formula">(1889)</ref>, for ∀a, b ∈ R n×1 and r &gt; 1,</p><p>Let |a(i)| = |x i | p , |b(i)| = 1 and r = q p ,</p><p>Therefore,</p><p>(25) THEOREM 2. Let 1 ≥ λ 1 ≥ λ 2 ≥ · · · ≥ λ n be the eigen-values of matrixÂ defined based on network G, then the corresponding suspended animation limit of the GCN model on G is tightly bounded</p><p>In the case that the network G is a d-regular, then the suspended animation limit of the GCN model on G can be simplified as</p><p>Proof. Instead of proving the above inequality directly, we propose to prove that ζ is suspended animation limit by the following inequality instead</p><p>which can derive the following inequality according to Lemma 2:</p><p>Under review as a conference paper at ICLR 2020</p><p>Let v 1 , v 2 , · · · , v n be the eigenvectors ofÂ and ∀x</p><p>where</p><p>Considering that λ ζ</p><p>√ n ] , then</p><p>and</p><p>where Therefore, we have</p><p>where λ max = max{|λ 2 |, |λ 3 |, · · · , |λ n |} = max{λ 2 , |λ n |}.</p><p>Therefore, to ensure</p><p>8.2.4 PROOF OF COROLLARY 1 COROLLARY 1. Let 1 ≥ λ 1 ≥ λ 2 ≥ · · · ≥ λ n be the eigen-values of matrixÂ defined based on network G, then the corresponding suspended animation limit of the GCN model (with lazy Markov chain based layers) on G is tightly bounded</p><p>Proof. For the lazy Markov chain layer, we have its updating equation as follows T = 1 2Â H (k−1) + 1 2 H (k−1) = 1 2 (Â + diag({d w (i)} vi∈V ))H (k−1) =ÃH (k−1) .</p><p>It is easy to show thatÃ is positive definite and we have its eigen-values λ 1 ≥ λ 2 ≥ · · · ≥ λ n ≥ 0. Therefore, λ max = max{|λ 2 |, |λ 3 |, · · · , |λ n |} = max{λ 2 , |λ n |} = λ 2 ,</p><p>which together with Theorem 2 conclude the proof.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fast, accurate, and, lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<idno>abs/1803.08664</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Beyond deep residual learning for image restoration: Persistent homologyguided manifold simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<idno>abs/1611.06345</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10866</idno>
		<title level="m">Convolutional set matching for graph similarity</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Representing smooth functions as compositions of near-identity functions with implications for deep network optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Long</surname></persName>
		</author>
		<idno>abs/1804.05012</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On power-law relationships of the internet topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Graphnas: Graph neural architecture search with reinforcement learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno>abs/1610.02915</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual networks: Deep learning gets way deeper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf" />
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
	<note>Online; accessed 15</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ueber einen mittelwertsatz. Nachrichten von der Königl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hölder</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1889</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graph optimized convolutional networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining neural networks with personalized pagerank for classification on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-scale residual network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1801.07606</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural networks for malicious account detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno>abs/1611.08402</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Norris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Understanding the exploding gradient problem. CoRR, abs/1211</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5063</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Neur. Netw</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Co-author relationship prediction in heterogeneous bibliographic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASONAM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1603.08861</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<title level="m">Dilated residual networks. CoRR, abs/1705.09914</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Norm-preservation: Why residual networks can become extremely deep? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaeemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rahnavard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1805.07477</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep loopy neural network model for graph structured data representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1805.07504</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fake news detection with deep diffusive network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1805.08751</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1812.08434</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
