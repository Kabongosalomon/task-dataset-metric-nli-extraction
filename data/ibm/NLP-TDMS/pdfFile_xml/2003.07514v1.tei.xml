<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predictively Encoded Graph Convolutional Network for Noise-Robust Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gwangju Institution of Science and Technology</orgName>
								<orgName type="institution">Curtin University</orgName>
								<address>
									<settlement>Gwangju</settlement>
									<region>WA</region>
									<country>Australia, South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsang</forename><surname>Yoon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gwangju Institution of Science and Technology</orgName>
								<orgName type="institution">Curtin University</orgName>
								<address>
									<settlement>Gwangju</settlement>
									<region>WA</region>
									<country>Australia, South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moongu</forename><surname>Jeon</surname></persName>
							<email>mgjeon@gist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Gwangju Institution of Science and Technology</orgName>
								<orgName type="institution">Curtin University</orgName>
								<address>
									<settlement>Gwangju</settlement>
									<region>WA</region>
									<country>Australia, South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predictively Encoded Graph Convolutional Network for Noise-Robust Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Predictive encoding</term>
					<term>graph convolutional network</term>
					<term>noise- robust</term>
					<term>skeleton-based action recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In skeleton-based action recognition, graph convolutional networks (GCNs), which model human body skeletons using graphical components such as nodes and connections, have achieved remarkable performance recently. However, current state-of-the-art methods for skeleton-based action recognition usually work on the assumption that the completely observed skeletons will be provided. This may be problematic to apply this assumption in real scenarios since there is always a possibility that captured skeletons are incomplete or noisy. In this work, we propose a skeleton-based action recognition method which is robust to noise information of given skeleton features. The key insight of our approach is to train a model by maximizing the mutual information between normal and noisy skeletons using a predictive coding manner. We have conducted comprehensive experiments about skeletonbased action recognition with defected skeletons using NTU-RGB+D and Kinetics-Skeleton datasets. The experimental results demonstrate that our approach achieves outstanding performance when skeleton samples are noised compared with existing state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Action recognition is one of the important areas in computer vision studies, for understanding human behaviours using a computational system. It can be applied to various applications for industrial system <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b30">31]</ref>, medical software <ref type="bibr" target="#b2">[3]</ref>, and multimedia <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b48">49]</ref>. Because of the industrial and practical importance of this literature, the interest of this literature is increasing rapidly in recent years, and numerous studies have been proposed. In general, various modalities, such as appearance <ref type="bibr" target="#b11">[12]</ref>, depth <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b25">26]</ref>, motion flow <ref type="bibr" target="#b47">[48]</ref>, and skeleton-features <ref type="bibr" target="#b37">[38]</ref> are utilized to recognize human actions. With the great advancements of deep learning which is a method to learn useful representation automatically, various approaches employ convolutional neural networks (CNNs) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref> and recurrent neural networks (RNNs) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b54">55]</ref> to train the spatio-temporal information and arXiv:2003.07514v1 [cs.CV] 17 Mar 2020 to recognize human actions. These CNNs and RNNs based approaches using RGB images and motion flows (e.g. , optical flow) achieved outstanding performances than the previous methods based on hand-crafted features <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50]</ref>. The drawback of these approaches is that the learnt representations are may not focused on actions since entire areas of video frames are exploited to learn the representations <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b12">13]</ref>. Skeleton features provide quantized information about peoples' joints and bones. Compared to RGBs and motion flows, the skeleton features can provide more compact and useful information in the dynamic circumstance and complicated background <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Early deep-learning based approaches using skeleton-features manually create skeleton data as a sequence of joint-coordinate vectores <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b54">55]</ref> or as a pesudo-image <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>, and apply the data to RNNs or CNNs to inference corresponding action classes. However, these approaches are unable to indicate the dependency between correlated joints <ref type="bibr" target="#b36">[37]</ref>. Intuitively, skeleton-features can be represented as a graph structure since their components are homeomorphic. For instance, joints and bones of skeleton-features can be defined as the vertices and connections of the graph. Recently, Graph Convolutional Networks (GCNs), which are the graphical framework using convolution neural network, have been achieved a great number of successes in skeleton-based action recognition <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b24">25]</ref>. In the graph convolution, nodes are filtered in two ways, namely spatial and spectral. Spectral approach filters the nodes based on the Laplacian matrix and eigenvectors while spatial approach filters the nodes with local neighborhood nodes. ST-GCN <ref type="bibr" target="#b50">[51]</ref> is the first work to use spatial approach GCNs to handle skeleton model and has shown impressive improvements. However, the spatial graph in ST-GCN is a predefined graph which only relies on the physical structure of human body. This makes hard to capture the relationship between closely related joints such as both two hands in hand-related action. To tackle this limitation, many methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref> were proposed to build adaptive graph to pay the dynamic attention to each joint based on the performing action.</p><p>Unfortunately, all these approaches assume that the complete skeleton features would be provided. It is almost impossible to guarantee that extracting of perfect skeleton samples from a real-world system. Song et al. <ref type="bibr" target="#b40">[41]</ref> have proposed a GCN based method which can deal with 'incomplete skeletons' defined as spatially occluded or temporally missed skeleton features. Even though recent studies for pose estimation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46]</ref> and constructing skeleton-features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>, have shown precise and scene-condition invariant performances, still, there is a possibility that extraed skeleton may contain a piece of inaccurate information. Song et al.</p><p>proposed RA-GCN <ref type="bibr" target="#b40">[41]</ref> model to learn distinctive features of currently unactivated joints (missed joints) in multiple streams by utilizing class activation maps (CAM), but it is still problematic. To improve the performance of action recognition using skeleton-features, it should have addressed how a model processes noisy skeleton samples.</p><p>We present Predictively encoded Graph Convolutional Networks (PeGCNs), which can learn noise-robust representation for skeleton-based action recognition using GCN. The key insight of our model is to learn such representations by predicting the perfect sample from noisy sample in latent space via autoregression model. We use a probabilistic contrastive loss to capture the most useful information for predicting a perfect sample. To demonstrate the efficiency of PeGCNs on skeleton-based action recogntion with noised samples, we have conducted various experiments using NTU-RGB+D <ref type="bibr" target="#b33">[34]</ref> and Kinetics-skeleton <ref type="bibr" target="#b50">[51]</ref> datasets. The experimental results show that PeGCNs can provide noise-robust action recognition performance using skeleton features, and it surpasses existing methods.</p><p>The key contributions of our works can abridge as follows. First, we propose a novel method for noise-robust skeleton-based action recognition, called Predictively Encoded Graph Convolutional Network (PeGCN), which performs better than the existing state-of-the-art methods on either general skeleton-based action recognition and that with noisy skeleton samples. Second, predictive encoding loss on latent space captures useful representations to predict complete skeleton features from noisy skeleton and improves action recognition performance with noisy samples. In addition to these contributions, we also provide comprehensive experiments on skeleton-based action recognition with noised samples. Our experiments include various ablation studies and comparisons with existing GCN based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Skeleton-based action recognition using deep learning</head><p>The recent success of deep-learning techniques had a significant impact on the studies for human action recognition. To model spatio-temporal features of actions, many works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b33">34]</ref> attempt to extract appearance information with Convolutional Neural Networks (CNNs) and temporal information with Recurrent Neural Networks (RNNs). TS-LSTM <ref type="bibr" target="#b21">[22]</ref> uses multiple temporal-windows to handle both short/mid/long-range actions dynamically. Zhang et al. <ref type="bibr" target="#b54">[55]</ref> proposed view-adaptive action model (VA-LSTM) which is robust to view point change. However, CNN/LSTM based methods usually represent the skeleton data as a sequence of vectors which cannot express the dependencies enough between related joints. The skeleton model can be seen as a graph structure where joints and bones correspond to the vertices and edges, respectively.</p><p>Recently, ST-GCN [51] successfully adopted graph convolution networks (GCNs) to handle graphs in arbitrary forms and it was the first method which applied GCNs to the skeleton-based action recognition. After Yan et al. <ref type="bibr" target="#b50">[51]</ref> proposed ST-GCN, lots of works using graph convolution networks (GCNs) were proposed. The GCNs have two main approaches to apply: spectral approach <ref type="bibr" target="#b23">[24]</ref> and spatial approach <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref>. The spectral approach first performs Eigen decomposition on graph Laplacian matrix to get Eigenvalues and Eigenvectors. With these Eigen features, graph convolution is performed on sub-graph with graph Fourier transform. In this way, no locally-connected node partitioning is required. On the other hand, in spatial perspective method performs graph convolution directly on graph nodes with it's neighbourhood nodes. This ap-proach is widely accepted in action recognition (e.g. ST-GCN) since it takes less computational cost than spectral approach.</p><p>The main drawbacks of ST-GCN is the spatial graph which is predefined only relying on the physical structure of human body and is fixed to all the GCN layers. These drawbacks make hard to capture not only the relationships between closely related joints such as both two hands in hand-related action, but also the dynamics of each action. To tackle these limitations, many methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref> were proposed to build adaptive graph to pay attention dynamically to each joint based on the performing action. The adaptive graph is trainable mask which learns relationships between any joints which can increase both flexibility and generality in constructing the graph. Shi et al. <ref type="bibr" target="#b36">[37]</ref> proposed 2s-AGCN model which has two adaptive graphs: 1) global-graph and 2) local-graph. Both of them are trained and updated jointly with CNNs in end-to-end manner. The global-graph learns common patterns for all the samples while local graph learns unique patterns of each individual sample. Lie et al. <ref type="bibr" target="#b24">[25]</ref> proposed actional links (A-links) to learn action-specific dependencies, and structural-links (S-links) for higher-order relationships between joints.</p><p>While most of works were using undirected graph, Shi et al. <ref type="bibr" target="#b35">[36]</ref> proposed directed graph based model (DGNN) where direction of the graph plays important role in graph convolution for updating features of edges and vertices. Si et al. <ref type="bibr" target="#b37">[38]</ref> combines LSTM with GCN (AGC-LSTM ) to learn a spatio-temporal representations from sequential skeletons, while most of GCN based action recognition models acquire temporal information with 1d-convolution on the temporal-axis. Spatial based GCNs usually distribute graphs into multiple subgraphs with distance partitioning or spatial configuration partitioning proposed in <ref type="bibr" target="#b50">[51]</ref>. In contrast to these common partitioning strategies, Thakkar et al. <ref type="bibr" target="#b42">[43]</ref> proposed part-based GCN (PB-GCN) that learns relationships between five body parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Predictively Encoded Graph Convolutional Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation and Intuition</head><p>For developing the precise action recognition method, it is important to learn a global representation which can represent every detail of given video clip for the entire time period. To learn a suitable global representation, a model needs outstanding generalization ability which can be robust to the diverse types of noise. Variation of skeleton features depending on geometric conditions, such as a viewpoint of cameras or acting objects, can be regarded as a sort of noise skeleton features. Missing of skeleton features (a.k.a., incomplete skeleton features <ref type="bibr" target="#b40">[41]</ref> (see <ref type="figure" target="#fig_0">Fig. 1</ref>) by spatial or temporal occlusions, are also a kind of noisy skeleton features. These noise patterns are inherently unpredictable. It is, therefore, intractable to model noise information explicitly in a data-driven approaches.</p><p>Deep learning is well known as an effective way to improve generalization performance of a model for various visual recognition studies <ref type="bibr">[</ref>   <ref type="bibr" target="#b40">[41]</ref>, which are spatially and temporally occluded. (c) is the noisy skeleton sample generated by our noising approach using a noise level 5.</p><p>unified framework of a graph structure and deep learning, so it also has advantage improved generalization performance. Based on this advantage, The dominant approach to training the skeleton-based action recognition methods based on GCNs is initially extracting information from skeleton samples using GCNs and then computing the unimodal loss e.g. cross-entropy <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref>. It can be regarded as direct end-to-end learning for a model p(ō|x) between skeleton samples x and a corresponding acting classesō. However this approach, which directly derives a mapping model for p(ō|x) and p(ō|x ) from a complete sample x or an incomplete sample x to class labelō, is computationally intensive and a waste of representation capacity of the model. For example, the mapping between x andō directly can be thought as using every detail of input samples all the time whether it is necessary or not. A slight noise, which can be alleviated during generalization via a non-linear network structure, does not need to be considered seriously. As a result, it may not suitable to derive a mapping model p(ō|x) directly for deriving the optimal global representation.</p><p>The key insight of PeGCN for noise-robust skeleton-based action recognition is to learn the representations that encode to the underlying shared information between complete sample and noisy sample via predicting missing information in the latent space. This idea is inspired by the predictive coding <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">32]</ref> which is one of the oldest techniques in signal processing for data compression, and recently it is applied to unsupervised learning for learning word representations <ref type="bibr" target="#b28">[29]</ref> by predicting neighbouring words. The approach to latent space has the following advantages: First, since action recognition processes relatively long time samples than the others including event detection <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b51">52]</ref> or change detection <ref type="bibr" target="#b15">[16]</ref>, action recognition models need to infer more global structure. When inferring the global structure, high-level information i.e., latent space, is more suitable than the low-level information. Second, the global noise on the latent representation is likely to be a serious noise which can affect the recognition performance seriously than local noise which can be reduced via non-linear weighted kernel structures of deep learning. When predicting proper information from noise skeleton features, we initially map the normal skeleton features x and noise skeleton feature x into compact distributed vector representations (a.k.a., latent features) α and α respectively, via non-linear mapping function, and train the model in a way that maximally preserves the mutual information between α and α . The mutual information is defined by,</p><formula xml:id="formula_0">I(α; α ) = α,α p(α, α )log p(α|α ) p(α) .<label>(1)</label></formula><p>By maximizing the mutual information between two encoded representations (which are bounded by the MI between the input signals), we extract the underlying latent variable robust to the global noise. In the training step, the normal skeleton samples x and the corresponding noisy skeleton samples x are provided. First, the GCN module f gcn produces latent representations α and α from x and x , respectively. Next, the autoregressive module f ar extracts the context latent representationᾱ from the latent representation α only. As argued in the previous section, we do not train a model by directly deriving p(o|x ) or p(o|α ). Instead, PeGCN is trained in the way to maximize the mutual information (Eq. 1) between the two latent representations, α and α of the normal and noisy skeleton samples, by modeling a density ratio which preserves the mutual information (Eq. 1) between α and α as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structural details</head><formula xml:id="formula_1">I(α; f ar (α )) = α,far(α ) p(α, f ar (α ))log p(α|f ar (α )) p(α) .<label>(2)</label></formula><p>By using I(α;ᾱ) with autoregressive module f ar , we relieve the model from modelling the high dimensional distribution x or x . Although we cannot evaluate p(x) or p(ō|x) directly, we can use samples from these distributions, allowing us to use the technique as Noise-Contrastive Estimation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17]</ref> and Important Sampling <ref type="bibr" target="#b3">[4]</ref>. The output of autoregressive moduleᾱ can be used if extra context from the representation is useful. One such example is speech recognition, that the receptive field of α may not contain enough information to capture phonetic content. In other cases, where no additional context is required, α might be better instead.</p><p>The noise skeleton features x are generated by adding some noise to randomly picked joints in the original skeleton samples x. The noise is generated based on the bounding box computed using the minimum and maximum values of the x, y, and z coordinates of skeleton samples ( <ref type="figure" target="#fig_3">Fig. 3(a)</ref>). When generating the noise samples in the training and test steps, we set the noise level which is the parameter to decide how many joints would be noised. The generated noisy samples depending on the noise level are represented in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>. The noise skeleton samples that we are regarding in this paper, are different from the spatially or temporally occluded skeleton samples that considered in Song et al. <ref type="bibr" target="#b40">[41]</ref> (see <ref type="figure" target="#fig_0">Fig. 1</ref>). In the real scenarios, the missed joints in the occluded skeleton samples can be defined by a set of joints that have low likelihoods or confidences than a pre-defined threshold. However, a noise is inherently unpredictable so that assumption may not practical.</p><p>The backbone network for our GCN module f gcn is the GCN part of of Js-AGCN <ref type="bibr" target="#b36">[37]</ref>, which composed of adaptive graph convolutional layers, which can make the topology of the graph optimized together with the other parameters of the network in end-to-end learning manner. The adaptive convolutional layer is defined by,</p><formula xml:id="formula_2">f out = Kv k W k f in (A k + B k + C k ),<label>(3)</label></formula><p>where A k is the original normalized adjacency matrix for GCN, B k is the trainable matrix for global attention and C k is a data-dependent graph for learning a unique graph for each sample. We employ the GCN of 2s-AGCN without the fully connected networks located on the after the GCN. We use RNNs using GRUs <ref type="bibr" target="#b7">[8]</ref> for the autoregressive module f ar . This can be easily replaced by other linear transformation or non-linear networks. The detilas of dimensionalities of the GCN module and the autoregressive module of PeGCNs are represented in Appendix A.1. Note that any type of GCN model and autoregressive model can be applied in the proposed method. Probably, more recent advancements in GCNs and autoregressive modelling could help improve results further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and inference</head><p>Both the GCN and autoregressive modules are jointly trained to optimize the loss in order to maximize the mutual information between two latent representations of normal and noise skeleton features, which we call predictive encoding loss. With given set for normal skeleton samples x ∈ {x i } i=1:n of n samples and the corresponding noise skeleton samples x ∈ {x i } i=1:n , the predictive encoding loss is defined by,</p><formula xml:id="formula_3">L pe = − E X,X log p(f gcn (x), f ar (f gcn (x ))) x∈X p(f gcn (x))</formula><p>.</p><p>Optimizing this loss will result in I(α, α ) estimating the density ratio in Eq. 1.</p><p>It is theoretically and experimentally demonstrated by Ooord et al. <ref type="bibr" target="#b31">[32]</ref>. Action recognition should identify an action class of given skeleton sample. Using L pe only can not achieve this goal since it is only focused on the maximizing mutual information between two latent representation. Therefore, as similar to other studies <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b35">36]</ref>, the cross-entropy loss is exploited as follows,</p><formula xml:id="formula_5">L ce = − C iō i log(o i ),<label>(5)</label></formula><p>where C is the numbers of action classes.ō is a given annotation for an action sample, and o is the output of the fully connected network for classification task on the inference step. Consequently, to train the noise-robust skeleton-based action recognition model, the total loss functions is straightforwardly defined by the sum of the crossentropy loss L ce ,and the proposed predictive loss function with the balancing weight λ. It is represented as follows:</p><formula xml:id="formula_6">L total = L ce + λL pe .<label>(6)</label></formula><p>In all our experiments, λ is set by 0.1 for the best performance. The action recognition using PeGCN is straightforward. In the test step, the GCN module f gcn encodes an input skeleton sample into the latent space, and the autoregressive model f ar summarizes the latent feature and generate the context latent representationᾱ. Theᾱ is used as an input of a fully connected networks for action recognition <ref type="figure" target="#fig_1">(Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setting</head><p>To evaluate the action recognition performances of PeGCN and other methods on noise skeleton samples, we use NTU-RGB+D dataset <ref type="bibr" target="#b33">[34]</ref>, which is one of the largest datasets in skeleton-based action recognition, and Kinetics-skeleton (a.k.a., Kinetics) dataset generated from the Kinetics dataset <ref type="bibr" target="#b17">[18]</ref> containing 34,000 video clips. Two experimental protocols: 1) Cross-view (CV) and 2) Crosssubject (CS) are applied for the experiments using NTU-RGB+D dataset. The detail explanations of the two datasets are described in Appendix A.2.</p><p>The settings of common hyperparameters to train PeGCNs are as follows. The numbers of epochs are 50 and 65 for NTU-RGB+D dataset and Kineticsskeleton dataset, respectively. Since our computational resources are limited, the batch size reduced to 32 and it is the half of original batch size of our backbone network <ref type="bibr" target="#b36">[37]</ref> which can affect the action recognition performance of PeGCNs negatively. Stochastic gradient descent and the weight decay are utilized as optimization algorithms. The source code of PeGCN is publicly available on https://github.com/andreYoo/PeGCNs.git. The source code includes the feed function to generate the noise skeleton samples. The experiments are categorized into two parts. One is for the ablation study, and another is for the comparison with existing state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study</head><p>Experimental protocol We have conducted the performance analysis depending on the hyperparameter settings of PeGCN. The hyperparameters that significantly affect the action recognition performance of PeGCNs are the noise level and the composition of loss functions. The performance analysis depending on the setting of noise level and the composition of loss functions in the training step is as follows. First, we construct two PeGCN models trained by L ce (PeGCN cd ) and L total (PeGCN total ), and each model is trained with 1, 3, and 5 noise-levels. Other parameters are set as exactly same to the parameter setting mentioned in the above section. Next, we evaluate these models with noise level between 0 to 5. We have observed the trends of the cross-entropy losses and the predictive coding losses of these models and compared the action recognition accuracies. For efficient experiments, the ablation study is only conducted with the CV protocol of NTU-RGB+D dataset.</p><p>Experimental results. <ref type="table" target="#tab_1">Table 1</ref> shows that action recognition accuracy depending on the noise levels and the setting of the loss functions in the training step. The best accuracy is achieved by the PeGCN total trained with noise level 5. Its achieves 93.21 of accuracy in noise level 1 and 89.39 of accuracy for the noise level 10 in the test step, respectively. The PeGCN ce trained with the noise level 5, achieves 92.87 of accuracy in noise level 1. The PeGCN total trained with noise-level 5, achieves 92.24 of accuracy in the evaluations with the noise level 5. It also produces 89.39 of accuracy in the test with the noise level 10. On the other hand, the PeGCN ce trained with noise-level 5, obtains 87.43 of accuracy on the test with noise level 5. The quantitative results demonstrate that if models are trained at the same noise level, the model trained with the total loss function L total usually performs better, and it also suggests that the performance degradation of the PeGCNs trained by the cross-entropy loss only, is much faster than the others. Not only quantitative results, but also the trend of each losses show the efficiency of the predictive encoding loss when learning the noise-robust representation. The trends of cross-entropy losses in the ablation studies (see <ref type="figure" target="#fig_4">Fig  4(a)</ref>) show that the curves of the PeGCNs trained by the cross-entropy losses only, are converged faster than the PeGCNs trained by the total loss L total usually. It can be thought that the PeGCN trained with the cross-entropy loss only is easier to converged into the poor locally optimized solution than the others.</p><p>Interestingly, The trend of predictive encoding loss (see <ref type="figure" target="#fig_4">Fig 4(b)</ref>) shows that the curve of the PeGCN total trained by noise level 5 is relatively lower than that of the PeGCN total trained by noise level 1 or 3. In the graphical comparison, PeGCNs trained by L total and L ce are compared to each other. These trends can be interpreted as a difficulty of learning with highly noised samples. In the training step, a higher noise level can provide more diversity in the training samples than the lower noise level. Consequently, the ablation study demonstrates that the higher noise-level in the training step can improve the action recognition performance in the test step, but it is not linearly proportional. For the efficient experiments, further studies for comparing PeGCN with existing state-of-the-art methods are only conducted with PeGCN total trained with noise level 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with existing state-of-the-art methods</head><p>Experimental protocol. Our experiments include either the experiments with normal skeleton samples or that with noisy samples. Basically, we follow the general experimental protocol described in NTU-RGB+D dataset <ref type="bibr" target="#b33">[34]</ref> and Kineticsskeleton dataset <ref type="bibr" target="#b17">[18]</ref>. For both datasets, top-1 and top-5 accuracies are computed for the performance comparison. In the experiments using NTU-RBGD dataset, both CV and CS protocols are applied. To evaluate action recognition performance on the noisy setting, we artificially generate noisy samples as follows: First, the number of joints (a.k.a. noise level) for assigning noise values is determined manually. Second, according to the noise level, the joints which would be assigned by noise value, are randomly picked. The selected joints are constant for all frames in the video clip.  After which joints will be noised is decide, random values generated from the bounding-box are assigned to each selected joint in every frame (see <ref type="figure" target="#fig_3">Fig.  3(a)</ref>). To reduce the volatility of performance due to the randomness of noised joints, all experiments are iteratively conducted for 10 times, and the average and standard deviation for the results are used for the comparison. The examples of the artificially generated noisy skeleton samples are illustrated in Appendix A.3.</p><p>Predominantly, we have tried to compare PeGCN with recently proposed state-of-the-art methods. For efficient experiment and fair comparison, methods which were proposed before 2018 or performances are lower than ours by 5% in normal skeleton evaluation (e.g. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b54">55]</ref>) are excluded for the comparison (see <ref type="table" target="#tab_3">Table 2</ref>). Particularly, in the experiments using noisy skeleton samples, methods, which source code did not be released by paper authors, are excluded in the experiments <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b32">33]</ref>. Even if source code exists, some methods are excluded by the following criteria: First, the source codes have released from non-authors <ref type="bibr" target="#b34">[35]</ref>. Second, the paper is not officially published yet on a journal or a conference <ref type="bibr" target="#b32">[33]</ref>. Third, the source codes are argued by other people that they can not obtain the performance reported on a paper <ref type="bibr" target="#b34">[35]</ref>. The detail information of the source code and the pre-trained weight for each model are described in Appendix A.4.</p><p>Experiment with normal skeletons. Initially, we compare PeGCN with other existing state-of-the-art methods on normal skeleton samples. For the consistency of the experiments, several methods are tested using publicly available source codes by ourselves <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b36">37]</ref>. <ref type="table" target="#tab_3">Table 2</ref>   Compared with the state-of-the art performance, PeGCN total produces better or comparable performance than the several methods. Js-AGCN <ref type="bibr" target="#b36">[37]</ref>, which is used as the backbone network for PeGCN total achieves 85.4 and 93.1 accuracies for the CS and CV protocol on NTU-RGB+D dataset. These figure are slightly lower than ours. PeGCN total achives 85.6 and 93.4 accuracies on the two protocal.</p><p>Nevertheless, the performances of PeGCN total is relatively lower than few methods such as MS-AAGCN <ref type="bibr" target="#b35">[36]</ref>, DGNN <ref type="bibr" target="#b34">[35]</ref>, GCN-NAS <ref type="bibr" target="#b32">[33]</ref>, and AS-GCN <ref type="bibr" target="#b24">[25]</ref>. The gap of performances between state-of-the-art methods and PeGCN can be interpreted as follows: MS-AAGCN <ref type="bibr" target="#b35">[36]</ref> has additional attention modules (e.g. Spatial , temporal, channel-wise attention) and exploiting four different modalities including joint and bone information and motion information of them. In training, batch size is twice than ours and adaptive graphs are fixed in the first 5 epochs for better learning explained in DGNN <ref type="bibr" target="#b34">[35]</ref>. MS-AAGCN achieved more top-1 accuracy than us by 5.5%, 2.8% and 4.0 % on CS, CV and Kinetics respectively. Although DGNN <ref type="bibr" target="#b34">[35]</ref> has same batch size 32, it has longer training epoch as 120 while our training epoch for NTU-RGB+D is 50 and kinetics-skeleton is 65. Besides, DGNN utilizes both joint and bone information with directed acyle-graph. This leads <ref type="table" target="#tab_1">None  1  3  5  10   Top1 Top5  Top1  Top5  Top1  Top5  Top1  Top5  Top1  Top5</ref> ST-GCN   improvement of top-1 accuracy 5.4%, 2.7% and 3.1% on CS, CV and Kinetics, respectively. In other methods (such as GCN-NAS <ref type="bibr" target="#b32">[33]</ref> and AS-GCN <ref type="bibr" target="#b24">[25]</ref>) has longer training epochs than ours and learning rate decay more frequently. Experiment with noisy skeletons. The experimental results on the skeletonbased action recognition with noisy samples clearly demonstrate the efficiency of PeGCNs in recognizing actions on noisy skeleton samples. In contrast to the other approaches that performances are rapidly degraded when the noise-levels are deepened, PeGCN shows the noise-robust action recognition performance. As shown in <ref type="table" target="#tab_5">Table 3</ref>, PeGCN total achieves 84.21 and 82.20 of accuracies in the experiments with the noise level 1 and the noise level 5, respectively. The performance gap between these two figures is less than 3%, and it is significantly lower than the other methods. Shi et al. <ref type="bibr" target="#b36">[37]</ref>, which achieves the state-of-the-art performance on the experiments with normal skeleton samples (see <ref type="table" target="#tab_3">Table 2</ref>), produces 84.31 and 51.27 of accuracies on the noise-1 experiments, and the gap between these two accruacies is larger than 30. Js-AGCN <ref type="bibr" target="#b36">[37]</ref> achieved high accuracy that 35.1 and 57.1 for top-1 and top-5 accuracy, respectively. However, performance is dropped when noised samples are given. It recorded 23.06 on the noise-1 and 3.81 on the noise-5 experiments, and the gap between them is larger than 19. In the experiments with the noise level 10 on CS protocol in NTU-RGB+D, while the performances of other methods are all lower than 25%, PeGCN total obtains 77.92% of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise-level</head><p>The experimental results on the CV protocol using NTU-RGB+D dataset likewise suggest that PeGCN can provide more noise-robust performance for skeleton-based action recognition and surpass existing state-of-the-art methods. <ref type="table" target="#tab_1">None  1  3  5  10   Top1 Top5  Top1  Top5  Top1  Top5  Top1  Top5  Top1  Top5</ref> ST-GCN  PeGCN total achieves state-of-the-art performance. As shown in <ref type="table" target="#tab_6">Table 4</ref>, while PeGCN total achieves 99.21 and 89.39 of accuracies on the experiments with noise level 1 and 10, respectively, there is no other method that can provide over the 90% of accuracies even in the noise level 1. Js-AAGCN * <ref type="bibr" target="#b35">[36]</ref> produces 87.87 of accuracy for the noise level 1. However, the recognition performance of Js-AAGCN * is steeply degraded when the noise level is increased. In the experiment with noise level 10, the performance of Js-AAGCN * is 18.99, and it is lower than 23.63 of ST-GCN * <ref type="bibr" target="#b50">[51]</ref> which obtains 83.14 of accuracy in the experiments on the noise level 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise-level</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis and discussion</head><p>The overall results indicate that PeGCN can provide outstanding skeleton-based action recognition robust to noisy samples compared to existing state-of-the-art methods. The accuracies of PeGCN for all noise-level on NTU-RGB+D dataset and Kinetics dataset are higher than the comparison targets. The performance gap between PeGCN and other methods is proportional to the noise level. In the experiment on noise level 10, the performances of almost methods except PeGCN are degraded over 90% compared with the results on normal samples. In addition to the accuracies, the standard deviations also suggest that the advantage of PeGCN for noise-robust skeleton-based action recognition. In experiments for the CV protocol on NTU-RGB+D dataset, between noise levels 1 to 5, while the other methods produce the standard deviations over 0.2 usually, the range of standard deviation of the proposed method is from 0.02 to 0.11. Interestingly, among the experimental results, RA-GCN <ref type="bibr" target="#b40">[41]</ref>, which have proposed for recognizing actions using incomplete skeletons, achieves relatively poor accuracies <ref type="table" target="#tab_5">(Table 3 and Table 4</ref>) than the other methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b34">35]</ref> that do not consider the skeletons with noise information. It may be caused by the difference in the definition of 'noise' on skeleton features. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, Song et al. <ref type="bibr" target="#b40">[41]</ref> assigned 0 to the noised joints that defined by the 'missed joints' by spatially or temporally occlusions. However, in our experiments, the arbitrary value for the joint noising is defined randomly within the bounding box (see <ref type="figure" target="#fig_3">Fig.  3</ref>). Consequently, the entire experimental results demonstrate the efficiency of PeGCN on skeleton-based action recognition with noise skeleton samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we have presented the noise-robust skeleton-based action recognition method based on graph convolutional networks with predictive encoding for latent space, called Predictively encoded Graph Convolutional Networks (PeGCNs). In the training step, PeGCNs learns to improve the representation ability for noise-robust skeleton-based action recognition by predicting complete samples from noisy samples on latent space. PeGCN increases the flexibility of GCNs and is more suitable for action recognition tasks using skeleton features. PeGCN is evaluated on two large-scale action recognition datasets, NTU-RGB+D and Kinetics, and it achieved the state-of-the-art performance on both of them. A.2 NTU-RGB+D dataset and Kinetics dataset NTU-RGB+D dataset <ref type="bibr" target="#b33">[34]</ref> is one of the largest dataset in skeleton based action recognition which contains around 56,000 samples in four different types including depth map, RGB video, IR image and skeleton sequence. The samples are captured by Microsoft Kinect v2 in three different angels (-45, 0, 45) with 40 volunteers. In skeleton sequence, 3d spatial coordinates (X,Y,Z) of 25 joints are provided for each human action. The human actions are captured by one or two performers and consists of 60 indoor activities such as hand-clapping or drinking-water. <ref type="bibr" target="#b33">[34]</ref> also provides two benchmark protocols: 1) Cross-view and 2) Cross-subject. In cross-view protocol, samples are split into training and test set according to camera angle. Each subset contains 37,920 and 18,960 samples respectively. In cross-subject protocol, samples are split into training and test set according to subjects. Some subjects are assigned as training samples and remaining subjects are assigned as test samples. Each training and test sebsets contains 40,320 samples and 16,560 samples respectively. We follow these protocols and report the top-1 accuracy on both benchmarks. Kinetics-skeleton dataset is one of the large-scale skeleton action dataset generated from Kinetics <ref type="bibr" target="#b17">[18]</ref> which contains 34,000 video clips collected from Youtube to have wide variety (such as illumination change, background color) and each video clips are labeled with 400 action classes. Before estimating skeleton model from video, resolution and frame rate of video clips are converted. Skeleton model is estimated with publicly available OpenPose toolbox <ref type="bibr" target="#b5">[6]</ref> which gives 2d locations and 1d confidence of 18 joints. The top two person, whom has the highest average of joint confidences, in video clips are selected if multiple people are in the scene. The length of each skeleton sequence is fixed to 300 by repeating or sampling the sequence. <ref type="bibr" target="#b50">[51]</ref> released this dataset (Kinetics-skeleton) which contains 240,000k samples for training set and 20,000k samples for validation set. We follow same evaluation protocol mentioned in <ref type="bibr" target="#b50">[51]</ref> that Top-1 and Top-5 recognition accuracies are evaluated.</p><p>A.5 Extended comparison on skeleton-based action recognition performance using normal skeletons on NTU-RGB+D dataest A.6 Additional comparison on skeleton-based action recognition performance using noisy skeletons on Kinetics-skeleton dataset </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustrations of various types of noisy skeletons. T is the frame order associated with each skeleton. (a) is original skeleton samples. (b) and (c) are the skeleton samples considered by Song et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The pipeline of PeGCN for the training and inference steps. The backbone network is the GCN of Js-AGCN<ref type="bibr" target="#b36">[37]</ref>. The figures under each layer are the dimensionalities of input and output channels, respectively and GAP is global average pooling operation. The black solid and dotted lines denote the pipelines for the training step. The red solid lines denote the extra pipelines for the inference step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>illustrates the pipeline of PeGCN on the training and inference. PeGCN consists of a GCN module f gcn and an autoregressive module f ar . The GCN module f gcn encodes skeleton samples into a latent space α * = f gcn (x * ), where * indicate the input types: normal one x and α or noise one x and α . The autoregressive module f ar summarizes the latent representation and produces a context latent representationᾱ = f ar (α * ), where α * can be defined by α and α depending on the corresponding input skeletons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Illustrations of how to set the candidate scope for generating noise joint and the examples of noise skeleton samples depending on the noise level. (a) illustrates that how to define the scope for generating noise joint using a given skeleton sample. (b) shows the noise skeleton samples created from original sample depending on the noise level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Trends of the cross-entropy and predictive encoding losses according to the noise level in training PeGCNs on the CV protocol of NTU-RGB+D dataset. (a) indicates the curves of the cross-entropy functions Lce. (b) represents the curves of the predictive encoding losses Lpe. The curves of Lpe are constructed from the PeGCNs trained by L total .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>±0.05) 98.31(±0.03) 86.42(±0.08) 95.52(±0.05) 84.52(±0.10) 91.61(±0.01) 79.41(±0.07) 84.52(±0.01) 61.25(±0.13) 74.36(±0.07) 3 92.63(±0.05) 98.98(±0.03) 91.92(±0.09) 97.51(±0.03) 89.52(±0.09) 94.12(±0.04) 81.25(±0.09) 91.52(±0.05) 78.12(±0.12) 90.21(±0.07) 5 93.21(±0.04) 98.97(±0.02) 92.78(±0.09) 98.91(±0.04) 92.24(±0.08) 98.81(±0.03) 91.08(±0.06) 98.52(±0.03) 89.39(±0.11) 98.29(±0.06) Action recognition accuracies of PeGCNs depending on the setting of the loss functions and the noise level in the training step on the CV protocol of NTU-RGB+D dataset. The figures in parentheses are standard deviations. The boldface figures denote the highest performance for each experiment.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test N-level</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Train N-level</cell><cell>1</cell><cell></cell><cell>3</cell><cell></cell><cell>5</cell><cell></cell><cell>7</cell><cell></cell><cell>10</cell></row><row><cell></cell><cell>Top 1</cell><cell>Top 5</cell><cell>Top 1</cell><cell>Top 5</cell><cell>Top 1</cell><cell>Top 5</cell><cell>Top 1</cell><cell>Top 5</cell><cell>Top 1</cell><cell>Top 5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Training without the predictive encoding loss Lpe (PeGCNce)</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="10">90.31(±0.14) 93.42(±0.07) 79.51(±0.09) 91.65(±0.07) 67.24(±0.13) 72.42(±0.03) 61.08(±0.06) 79.37(±0.03) 55.39(±0.11) 70.42(±0.06)</cell></row><row><cell>3</cell><cell cols="10">91.96(±0.09) 95.97(±0.08) 83.52(±0.21) 93.91(±0.10) 76.31(±0.15) 89.81(±0.07) 71.50(±0.09) 80.31(±0.05) 64.12(±0.17) 73.42(±0.04)</cell></row><row><cell>5</cell><cell cols="10">92.87(±0.08) 97.25(±0.06) 91.62(±0.14) 95.42(±0.08) 87.43(±0.11) 90.31(±0.06) 83.26(±0.09) 88.42(±0.6) 74.37(±0.16) 81.26(±0.09)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Training with the predictive encoding loss Lpe (PeGCNtotal)</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>91.72(</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>contains the top 1 accuracies on the CS and CV protocols of NTU-RGB+D dataset and the top 1 and top 5 accuracies on Kinetics dataset. In the experiments, PeGCN total achieves 85.6 and 93.4 accuracies on the CS and CV protocols of NTU-RGB+D dataset, respectively. PeGCN total produces 34.8 and 57.2 for Top 1 and top5 accuracies in Kineitcs-skeleton dataset. The state-of-the-art performance is achieved by MS-AAGCN [36] with 90.0 for CS protocol and 96.2 for CV protocol. The second highest performance is achieved by DGNN [35] which recorded 89.9 and 96.1 for CS and CV protocol, respectively. In Kinetics-skeleton dataset, MS-AAGCN [36] scores 37.8 for top-1 and 61.0 for top-5. MS-AAGCN scores the second-highest performance again with 37.8 and 61.0 for top-1 and top-5, respectively.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">CS CV Kinetics</cell></row><row><cell>Methods</cell><cell>Nets</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">T1 T1 T1 T5</cell></row><row><cell>VA-LSTM [55]</cell><cell>LSTM</cell><cell>80.7 88.8 -</cell><cell>-</cell></row><row><cell>Clips+CNN+MTLN [19]</cell><cell>CNN</cell><cell>79.6 84.8 -</cell><cell>-</cell></row><row><cell>Synthesized CNN [28]</cell><cell>CNN</cell><cell>80.0 87.2 -</cell><cell>-</cell></row><row><cell>3scale ResNet152 [23]</cell><cell>CNN</cell><cell>85.0 92.3 -</cell><cell>-</cell></row><row><cell>DPRL+GCNN [42]</cell><cell>GCN</cell><cell>83.6 89.8 -</cell><cell>-</cell></row><row><cell>AGC-LSTM [38]</cell><cell cols="2">GCN+LSTM 89.2 95.0 -</cell><cell>-</cell></row><row><cell>AS-GCN [25]</cell><cell>GCN</cell><cell cols="2">86.8 94.2 34.8 56.5</cell></row><row><cell>ST-GCN  *  [51]</cell><cell>GCN</cell><cell cols="2">81.6 88.8 31.6 53.7</cell></row><row><cell>2s RA-GCN  *  [41]</cell><cell>GCN</cell><cell>85.8 93.0 -</cell><cell>-</cell></row><row><cell>3s RA-GCN  *  [41]</cell><cell>GCN</cell><cell>85.9 93.5 -</cell><cell>-</cell></row><row><cell>PB-GCN  *  [43]</cell><cell>GCN</cell><cell>87.0 93.4 -</cell><cell>-</cell></row><row><cell>Js-AGCN  *  (Backbone) [37]</cell><cell>GCN</cell><cell cols="2">85.4 93.1 34.4 57.0</cell></row><row><cell>Bs-AGCN  *  [37]</cell><cell>GCN</cell><cell>87.0 94.1 -</cell><cell>-</cell></row><row><cell>2s-AGCN  *  [37]</cell><cell>GCN</cell><cell>88.8 95.3 -</cell><cell>-</cell></row><row><cell>GCN-NAS(Joint&amp;Bone) [33]</cell><cell>GCN</cell><cell cols="2">89.4 95.7 37.1 60.1</cell></row><row><cell>DGNN [35]</cell><cell>GCN</cell><cell cols="2">89.9 96.1 36.9 59.6</cell></row><row><cell>JB-AAGCN [36]</cell><cell>GCN</cell><cell cols="2">89.4 96.0 37.4 60.4</cell></row><row><cell>MS-AAGCN [36]</cell><cell>GCN</cell><cell cols="2">90.0 96.2 37.8 61.0</cell></row><row><cell>PeGCNtotal</cell><cell>GCN</cell><cell cols="2">85.6 93.4 34.8 57.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Recognition</figDesc><table><row><cell>accuracies on NTU-</cell></row><row><cell>RGB+D dataset and Kinetics-skeleton</cell></row><row><cell>dataset. Note that, '-' indicates that the re-</cell></row><row><cell>sult were not reported and  *  indicates that</cell></row><row><cell>a method is evaluated ourselves. The bold-</cell></row><row><cell>face figures denote the highest performance</cell></row><row><cell>for each experiment. The more comprehen-</cell></row><row><cell>sive comparison between PeGCN and other</cell></row><row><cell>state-of-the-art methods are described in</cell></row><row><cell>Appendix A.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>[51] 81.57 96.85 73.78(±0.24) 93.74(±0.13) 57.76(±0.22) 84.08(±0.24) 42.73(±0.23) 71.52(±0.25) 17.26(±0.22) 42.54(±0.32) Js-AGCN * [37] 86.43 97.28 76.05(±0.33) 92.09(±0.16) 54.92(±0.33) 77.56(±0.23) 35.90(±0.44) 60.67(±0.27) 8.03(±0.26) 24.91(±0.21) Bs-AGCN * [37] 87.04 97.48 79.08(±0.23) 94.03(±0.10) 60.79(±0.27) 83.75(±0.26) 44.30(±0.27) 71.80(±0.20) 18.24(±0.25) 44.07(±0.16) 2s-AGCN * [37] 88.83 98.05 84.31(±0.15) 96.73(±0.07) 69.40(±0.25) 89.97(±0.22) 51.27(±0.28) 78.09(±0.21) 16.28(±0.15) 40.86(±0.38) Js-AAGCN * [36] 87.49 97.45 80.31(±0.18) 93.62(±0.12) 65.87(±0.24) 84.58(±0.23) 51.79(±0.29) 74.06(±0.24) 21.26(±0.23) 43.89(±0.41) 3s RA-GCN * [41] 85.87 98.10 72.02(±0.26) 89.89(±0.20) 45.12(±0.29) 68.79(±0.33) 25.59(±0.25) 48.71(±0.42) 6.11(±0.24) 20.55(±0.31) 2s RA-GCN * [41] 85.83 98.19 71.97(±0.18) 91.00(±0.20) 44.41(±0.23) 70.81(±0.34) 25.35(±0.33) 50.54(±0.23) 6.41(±0.23) 21.10(±0.19) PB-GCN * [43] 86.98 98.25 77.39(±0.32) 94.67(±0.15) 56.35(±0.28) 83.03(±0.12) 37.31(±0.37) 67.87(±0.36) 11.01(±0.15) 34.13(±0.24) PeGCNtotal 84.49 96.79 84.21(±0.11) 96.72(±0.02) 83.28(±0.13) 96.59(±0.10) 82.20(±0.15) 96.28(±0.05) 77.92(±0.14) 94.92(±0.09)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Recognition accuracies depending on the noise level using NTU-RGB+D dataset and the CS protocol. * indicates that the method were trained and tested by ourselves. The boldface figures denote the highest performance for each experiment.[37] 95.25 99.36 84.12(±0.22) 96.16(±0.12) 53.05(±0.36) 78.48(±0.26) 29.39(±0.30) 56.47(±0.33) 6.32(±0.90) 21.71(±2.04) Js-AAGCN * [36] 94.61 99.17 87.87(±0.14) 96.17(±0.08) 71.81(±0.21) 86.81(±0.13) 54.37(±0.27) 74.34(±0.12) 18.99(±0.32) 38.84(±0.28) 3s RA-GCN * [41] 93.51 99.30 79.77(±0.18) 92.74(±0.18) 53.59(±0.32) 76.41(±0.29) 32.71(±0.19) 59.08(±0.37) 8.88(±0.24) 29.53(±0.24) 2s RA-GCN * [41] 92.97 99.28 79.58(±0.16) 92.72(±0.11) 53.34(±0.36) 75.09(±0.24) 32.46(±0.24) 55.84(±0.32) 8.59(±0.11) 24.98(±0.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Noise-level</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>None</cell><cell>1</cell><cell></cell><cell>3</cell><cell></cell><cell>5</cell><cell></cell><cell>10</cell></row><row><cell></cell><cell>Top1 Top5</cell><cell>Top1</cell><cell>Top5</cell><cell>Top1</cell><cell>Top5</cell><cell>Top1</cell><cell>Top5</cell><cell>Top1</cell><cell>Top5</cell></row><row><cell>ST-GCN  *  [51]</cell><cell cols="9">88.76 98.83 83.14(±0.20) 97.44(±0.08) 69.18(±0.22) 91.95(±0.15) 54.07(±0.26) 83.30(±0.28) 23.63(±0.18) 54.58(±0.34)</cell></row><row><cell cols="9">Bs-AGCN  *  [37] 94.12 99.23 56.38(±0.15) 77.82(±0.31) 7.84(±0.21) 22.91(±0.36) 2.44(±0.09) 10.92(±0.22) 2.14(±0.39)</cell><cell>9.65(±1.57)</cell></row><row><cell>Js-AGCN  *  [37]</cell><cell cols="9">94.05 99.08 85.98(±0.20) 96.34(±0.13) 68.49(±0.18) 88.03(±0.20) 51.36(±0.24) 76.61(±0.24) 17.89(±0.20) 42.20(±0.29)</cell></row><row><cell cols="10">2s-AGCN  20)</cell></row><row><cell>PB-GCN  *  [43]</cell><cell cols="5">93.37 99.37 80.11(±0.16) 95.16(±0.12) 54.21(±0.24) 81.5(±0.19)</cell><cell cols="4">33.73(±0.2) 64.55(±0.21) 9.43(±0.13) 31.77(±0.25)</cell></row><row><cell>PeGCNtotal</cell><cell cols="9">93.41 99.02 93.21(±0.04) 98.97(±0.02) 92.78(±0.09) 98.91(±0.04) 92.24(±0.08) 98.81(±0.03) 89.39(±0.11) 98.29(±0.06)</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Recognition accuracies depending on the noise level using the CV protocol of NTU-RGB+D dataset. * indicates that the method were trained and tested by ourselves. The boldface figures denote the highest performance for each experiment.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>[51] 31.60 53.68 22.42(±0.19) 42.91(±0.23) 8.97(±0.13) 22.24(±0.20) 3.69(±0.14) 11.16(±0.11) 0.90(±0.04) 3.84(±0.11) Js-AGCN * [37] 34.39 57.04 23.06(±0.19) 43.41(±0.37) 9.13(±0.20) 21.80(±0.17) 3.81(±0.14) 11.22(±0.17) 0.92(±0.05) 3.92(±0.12) Js-AAGCN * [36] 35.66 58.27 27.13(±0.14) 48.55(±0.19) 11.77(±0.20) 26.61(±0.18) 4.81(±0.19) 13.38(±0.18) 1.06(±0.06) 4.06(±0.10) PeGCNtotal 33.78 56.24 33.34(±0.13) 55.84(±0.09) 32.45(±0.12) 54.78(±0.09) 30.90(±0.28) 53.37(±0.20) 24.04(±0.22) 45.41(±0.27)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison depending on the noise level using Kinetics-skeleton dataset. The boldface figures denote the best performances among the listed methods. indicates that the method were trained and tested by ourselves. The boldface figures denote the highest performance for each experiment.</figDesc><table /><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>A.1 Dimensional details for the kernels on the GCN module and the autoregressive module Dimentionality of each layer in PeGCN model including both GCN and autoregression modules. The N x (e.g. 3x or 2x) means that corresponding layer or block in solid-line is repeating N times. In filter column, first two figures are filter size and the last two figures are input and output dimension respectively. Note that, the output of FC-layer is 60 for NTU-RGB+D datset.</figDesc><table><row><cell></cell><cell></cell><cell>Layer</cell><cell>Filter</cell><cell cols="2">Strides Padding</cell></row><row><cell></cell><cell></cell><cell>6x Convolutional</cell><cell>1x1x3x16</cell><cell>(1,1)</cell></row><row><cell></cell><cell></cell><cell>3x Convolutional</cell><cell>1x1x3x64</cell><cell>(1,1)</cell></row><row><cell></cell><cell></cell><cell>Convolutional</cell><cell>1x1x3x64</cell><cell>(1,1)</cell></row><row><cell></cell><cell></cell><cell>Convolutional</cell><cell>9x1x64x64</cell><cell>(1,1)</cell><cell>(4,0)</cell></row><row><cell></cell><cell></cell><cell>6x Convolutional</cell><cell>1x1x64x16</cell><cell>(1,1)</cell></row><row><cell></cell><cell></cell><cell>3x Convolutional</cell><cell>1x1x64x64</cell><cell>(1,1)</cell></row><row><cell></cell><cell>3x</cell><cell>Convolutional</cell><cell>9x1x64x64</cell><cell>(1,1)</cell><cell>(4,0)</cell></row><row><cell></cell><cell></cell><cell>6x Convolutional</cell><cell>1x1x64x32</cell><cell>(1,1)</cell></row><row><cell></cell><cell></cell><cell>3x Convolutional</cell><cell>1x1x64x128</cell><cell>(1,1)</cell></row><row><cell cols="2">GCN main module</cell><cell>Convolutional Convolutional Convolutional 6x Convolutional 3x Convolutional</cell><cell>1x1x64x128 9x1x128x128 1,1,64,128 1x1x128x32 1x1x128x128</cell><cell>(1,1) (2,1) (1,1) (1,1) (2,1)</cell><cell>(4,0)</cell></row><row><cell></cell><cell>2x</cell><cell>Convolutional</cell><cell>9x1x128x128</cell><cell>(1,1)</cell><cell>(4,0)</cell></row><row><cell></cell><cell></cell><cell>6x Convolutional</cell><cell>1x1x128x64</cell><cell></cell></row><row><cell></cell><cell></cell><cell>3x Convolutional</cell><cell>1x1x128x256</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Convolutional</cell><cell>1x1x128x256</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Convolutional</cell><cell>9x1x256x256</cell><cell>(2,1)</cell><cell>(4,0)</cell></row><row><cell></cell><cell></cell><cell>Convolutional</cell><cell>1x1x128x256</cell><cell>(2,1)</cell></row><row><cell></cell><cell></cell><cell>6x Convolutional</cell><cell>1x1x256x64</cell><cell>(1,1)</cell></row><row><cell></cell><cell></cell><cell>3x Convolutional</cell><cell>1x1x256x256</cell><cell>(1,1)</cell></row><row><cell></cell><cell>2x</cell><cell>Convolutional</cell><cell>9x1x256x256</cell><cell>(1,1)</cell><cell>(4,0)</cell></row><row><cell>Autoregression</cell><cell>module</cell><cell>GRU FC-layer</cell><cell>1x256x256 1x256x60</cell><cell></cell></row><row><cell>Fig. 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on NTU-RGB+D dataset and Kinetics-skeleton dataset. '-' indicates that the result were not reported. * indicates that model is trained by ourselves and figures in parentheses means reported accuracy. The boldface figures denote the highest performance for each experiment.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>NTU-CS</cell><cell></cell><cell>NTU-CV</cell><cell></cell><cell cols="2">Kinetics-skeleton</cell></row><row><cell>Methods</cell><cell cols="2">Year Architecture</cell><cell cols="5">Top1 Top5 Top1 Top5 Top1</cell><cell>Top5</cell></row><row><cell>Fenture Enc [13]</cell><cell>2015</cell><cell>Hand</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>14.9</cell><cell>25.8</cell></row><row><cell>HBRNN [10]</cell><cell>2015</cell><cell>RNN</cell><cell>59.1</cell><cell>-</cell><cell>64.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Deep LSTM [34]</cell><cell>2016</cell><cell>LSTM</cell><cell>60.7</cell><cell>-</cell><cell>67.3</cell><cell>-</cell><cell>16.4</cell><cell>35.3</cell></row><row><cell>ST-LSTM [27]</cell><cell>2016</cell><cell>LSTM</cell><cell>69.2</cell><cell>-</cell><cell>77.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>STA-LSTM [40]</cell><cell>2017</cell><cell>LSTM</cell><cell>73.4</cell><cell>-</cell><cell>81.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VA-LSTM [55]</cell><cell>2017</cell><cell>LSTM</cell><cell>80.7</cell><cell>-</cell><cell>88.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TCN [20]</cell><cell>2017</cell><cell>CNN</cell><cell>74.3</cell><cell>-</cell><cell>83.1</cell><cell>-</cell><cell>20.3</cell><cell>40.0</cell></row><row><cell>Clips+CNN+MTLN [19]</cell><cell>2017</cell><cell>CNN</cell><cell>79.6</cell><cell>-</cell><cell>84.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Synthesized CNN [28]</cell><cell>2017</cell><cell>CNN</cell><cell>80.0</cell><cell>-</cell><cell>87.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3scale ResNet152 [23]</cell><cell>2017</cell><cell>CNN</cell><cell>85.0</cell><cell>-</cell><cell>92.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison depending on the noise level using Kinetics-skeleton dataset. The boldface figures denote the highest performance for each experiment. [37] 34.39 57.04 23.06(±0.19) 43.41(±0.37) 9.13(±0.20) 21.80(±0.17) 3.81(±0.14) 11.22(±0.17) 0.92(±0.05) 3.92(±0.12) Bs-AGCN * [37] 34.11 56.97 24.01(±0.21) 44.48(±0.24) 10.03(±0.14) 23.05(±0.21) 3.99(±0.15) 11.34(±0.15) 0.82(±0.08) 3.17(±0.07) 2s-AGCN * [37] 36.77 59.24 28.27(±0.13) 50.11(±0.12) 12.92(±0.12) 28.18(±0.25) 5.32(±0.18) 14.52(±0.16) 1.09(±0.06) 4.15(±0.10) PeGCNtotal 33.78 56.24 33.34(±0.13) 55.84(±0.09) 32.45(±0.12) 54.78(±0.09) 30.90(±0.28) 53.37(±0.20) 24.04(±0.22) 45.41(±0.27)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Noise-level</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>None</cell><cell>1</cell><cell></cell><cell>3</cell><cell></cell><cell>5</cell><cell></cell><cell>10</cell></row><row><cell></cell><cell>Top1 Top5</cell><cell>Top1</cell><cell>Top5</cell><cell>Top1</cell><cell>Top5</cell><cell>Top1</cell><cell>Top5</cell><cell>Top1</cell><cell>Top5</cell></row><row><cell>Js-AGCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Action: "take off a hat/cap"</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive predictive coding of speech signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Atal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1973" to="1986" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoderdecoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sensor fusion-based activity recognition for parkinson patients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bahrepour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meratnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Taghikhaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Havinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor Fusion-Foundation and Applications</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="171" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive importance sampling to accelerate training of a neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Senécal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="722" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<title level="m">Openpose: realtime multiperson 2d pose estimation using part affinity fields</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Investigation of different skeleton features for cnn-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predictive coding-i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Elias</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.1955.1055126</idno>
		<ptr target="https://doi.org/10.1109/TIT.1955.1055126" />
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="24" />
			<date type="published" when="1955-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Change detection from remotely sensed images: From pixel-based to object-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE conference on computer vision and pattern recognition workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolution for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning human pose models from synthesized data for robust rgb-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1545" to="1564" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6426</idno>
		<title level="m">A fast and simple algorithm for training neural probabilistic language models</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recognition of grasping motion based on modal space haptic information using dp pattern-matching algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mizoguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yashiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ohnishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2043" to="2051" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning graph convolutional network for skeleton-based human action recognition by neural searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04131</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition with multistream adaptive graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06971</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Richly activated graph convolutional network for action recognition with incomplete skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Part-based graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04983</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3-d posture and gesture recognition for interactivity in smart spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="178" to="187" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hallucinating idt descriptors and i3d optical flow features for action recognition with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8698" to="8708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Two-stream 3-d convnet fusion for action recognition in videos with arbitrary size and length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="634" to="644" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Driver drowsiness detection using condition-adaptive representation learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2018.2883823</idno>
		<ptr target="https://doi.org/10.1109/TITS.2018.2883823" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4206" to="4218" />
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Joint representation learning of appearance and motion for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Yow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1157" to="1170" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rgb-d-based action recognition datasets: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="86" to="105" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fusing geometric features for skeleton-based action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2330" to="2343" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Experiment: Source codes and pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Each model has multiple weight files depending on the three factors: 1) Dataset, 2) Evaluation protocol and, 3) Data-type</title>
		<ptr target="https://drive.google.com/open?id=1Q-S-JAJwURPH7cy9-h25Mo0p15w_ALsb" />
	</analytic>
	<monogr>
		<title level="m">All models used in our experiments are publicly available on github including our PeGCN model</title>
		<imprint/>
	</monogr>
	<note>Github links are listed in Table 1. We also provides all weight files of the models via Google drive. Details of each weight file are described in Table 2. Table 1: Publicly available source codes</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Method Github</title>
		<idno>3s-RA-GCN [41</idno>
		<ptr target="https://github.com/yfsong0709/RA-GCNv1PeGCNhttps://github.com/andreYoo/PeGCNs" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Note that, -symbol in Protocol column indicates that nothing is determined and * symbol indicates that models are trained by ourselves and the others are downloaded from official github page</title>
		<imprint/>
	</monogr>
	<note>Table 2: All weight files for each GCN method</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<title level="m">Kinetics-Skeleton -Joint ki aagcn joint -Bone</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<idno>ST-GCN [51] NTU-RGB+D</idno>
		<title level="m">Cross-View Joint st gcn.ntu-xview Cross-Subject Joint st gcn.ntu-xsub Kinetics-Skeleton -Joint st gcn.kinetics PB-GCN [43] NTU-RGB+D Cross-View Joint crossview weights Cross-Subject Joint crosssubject weights 2s-RA-GCN [41] NTU-RGB+D Cross-View Joint 3304 2s RA-GCN NTUcv.pth Cross-Subject Joint 3302 2s RA-GCN NTUcs.pth 3s-RA-GCN [41] NTU-RGB+D Cross-View Joint 3303 3s RA-GCN NTUcv.pth Cross-Subject Joint 3301 3s RA-GCN NTUcs.pth PeGCN NTU-RGB+D Cross-View Joint ntu cv magcn joint gcn ntu cv magcn joint ar Cross-Subject Joint ntu cs magcn joint gcn ntu cs magcn joint ar</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<idno>GCN 83.6 - 89.8 - - - AGC-LSTM(Joint&amp;Part) [38] 2019 GCN+LSTM 89.2 - 95.0 - - - AS-GCN [25] 2019</idno>
		<title level="m">Kinetics-Skeleton -Joint ki magcn joint gcn ki magcn joint gr DPRL+GCNN</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gcn-Nas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Joint&amp;Bone</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
