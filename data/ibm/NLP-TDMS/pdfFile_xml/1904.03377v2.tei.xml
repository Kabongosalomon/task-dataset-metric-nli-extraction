<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Blind Super-Resolution With Iterative Kernel Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
							<email>jinjingu@link.cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The School of Science and Engineering</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannan</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<email>wmzuo@hit.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<email>chao.dong@siat.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">ShenZhen Key</orgName>
								<orgName type="department" key="dep2">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep3">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Blind Super-Resolution With Iterative Kernel Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning based methods have dominated superresolution (SR) field due to their remarkable performance in terms of effectiveness and efficiency. Most of these methods assume that the blur kernel during downsampling is predefined/known (e.g., bicubic). However, the blur kernels involved in real applications are complicated and unknown, resulting in severe performance drop for the advanced SR methods. In this paper, we propose an Iterative Kernel Correction (IKC) method for blur kernel estimation in blind SR problem, where the blur kernels are unknown. We draw the observation that kernel mismatch could bring regular artifacts (either over-sharpening or over-smoothing), which can be applied to correct inaccurate blur kernels. Thus we introduce an iterative correction scheme -IKC that achieves better results than direct kernel estimation. We further propose an effective SR network architecture using spatial feature transform (SFT) layers to handle multiple blur kernels, named SFTMD. Extensive experiments on synthetic and real-world images show that the proposed IKC method with SFTMD can provide visually favorable SR results and the state-of-the-art performance in blind SR problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As a fundamental low-level vision problem, single image super-resolution (SISR) is an active research topic and has attracted increasingly attention. SISR aims to reconstruct the high-resolution (HR) image from its low-resolution (LR) observation. Since the seminal work of employing convolutional neural networks (CNNs) for SR <ref type="bibr" target="#b5">[6]</ref>, various deep learning based methods with different network architectures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40]</ref> and training strategies <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5]</ref> have been proposed to continuously improve the SR performance. Most of the existing advanced * This work was done when they were interns at SenseTime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LR image</head><p>ZSSR <ref type="bibr" target="#b26">[27]</ref> SR without kernel correction Iterative Kernel Correction (ours) <ref type="figure">Figure 1</ref>. SISR results on image "img 017" with SR factor 4. Before bicubic downsamping, the HR image is blurred by a Gaussian kernel with σ = 1.8 SR methods assume that the downsampling blur kernel is known and pre-defined, but the blur kernels involved in real applications are typically complicated and unavailable. As has been revealed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36]</ref>, learning-based methods will suffer severe performance drop when the pre-defined blur kernel is different from the real one. This phenomenon of kernel mismatch will introduce undesired artifacts to output images, as shown in <ref type="figure">Figure 2</ref>. Thus the problem with unknown blur kernels, also known as blind SR, has failed most of deep learning based SR methods and largely limited their usage in real-world applications. Most existing blind SR methods are model-based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>, which usually involve complicated optimization procedures. They predict the underlying blur kernel using self-similarity properties of natural images <ref type="bibr" target="#b22">[23]</ref>. However, their predictions are easily affected by input noises, leading to inaccurate kernel estimation. A few deep learning based methods have also tried to make progress for blind SR. For example, in CAB <ref type="bibr" target="#b24">[25]</ref> and SRMD <ref type="bibr" target="#b38">[39]</ref>, the net-work can take the blur kernel as an additional input and generate different results according to the provided kernel. They achieve satisfactory performance if the input kernel is close to the ground truth. However, these methods still cannot predict the blur kernel for every image on hand, thus are not applicable in real applications. Although deep learning based methods have dominated SISR, they have limited progress on blind SR problem.</p><p>In this paper, we focus on using deep learning methods to solve the blind SR problem. Our method stems from the observation that artifacts caused by kernel mismatch have regular patterns. Specifically, if the input kernel is smoother than the real one, then the output image will be blurry/oversmoothing. Conversely, if the input kernel is sharper than the correct one, then the results will be over-shapened with obvious ringing effects (see <ref type="figure">Figure 2</ref>). This asymmetry of kernel mismatch effect provides us an empirical guidance on how to correct an inaccurate blur kernel. In practical, we propose an Iterative Kernel Correction (IKC) method for blind SR based on predict-and-correct principle. The estimated kernel is iteratively corrected by observing the previous SR results, and gradually approaches the ground truth. Even the predicted blur kernel is slightly different from the real one, the output image can still get rid of those regular artifacts caused by kernel mismatch.</p><p>By further diving into the SR methods proposed for multiple blur kernels (i.e., SRMD <ref type="bibr" target="#b38">[39]</ref>), we find that taking the concatenation of image and blur kernel as input is not the optimal choice. To make a step forward, we employ spatial feature transform (SFT) layers <ref type="bibr" target="#b32">[33]</ref> and propose an advanced CNN structure for multiple blur kernels, namely SFTMD. Experiments demonstrate that the proposed SFTMD is superior to SRMD by a large margin. By combining the above components -SFTMD and IKC, we achieve state-of-the-art (SOTA) performance on blind SR problem.</p><p>We summarize our contributions as follows: (1) We propose an intuitive and effective deep learning framework for blur kernel estimation in single image super resolution. <ref type="bibr" target="#b1">(2)</ref> We propose a new non-blind SR network using the spatial feature transform layers for multiple blur kernels. We demonstrate the superior performance of the proposed nonblind SR network. <ref type="bibr" target="#b2">(3)</ref> We test the blind SR performance on both carefully selected blur kernels and real images. Extensive experiments show that the combination of SFTMD and IKC achieves the SOTA performance in blind SR problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Super-Resolution Neural Networks. In the past few years, neural networks have shown remarkable capability on improving SISR performance. Since the pioneer work of using CNN to learn the end-to-end mapping from LR to HR images <ref type="bibr" target="#b5">[6]</ref>, plenty of CNN architectures have been pro-posed for SISR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28]</ref>. In order to go deeper in network depth and achieve better performance, most of the existing high-performance SR networks have residual architecture <ref type="bibr" target="#b14">[15]</ref>. SRGAN <ref type="bibr" target="#b18">[19]</ref> first introduce residual blocks into SR networks. EDSR <ref type="bibr" target="#b19">[20]</ref> improve it by removing unnecessary batch normalization layer in residual block and expanding the model size. DenseSR <ref type="bibr" target="#b40">[41]</ref> present an effective residual dense block and ESRGAN <ref type="bibr" target="#b33">[34]</ref> further use a residual-in-residual dense block to improve the perceptual quality of SR results. Zhang et al. <ref type="bibr" target="#b39">[40]</ref> introduce the channel attention component in residual blocks. Some networks are specifically designed for the SR task in some special scenarios, e.g., Wang et al. <ref type="bibr" target="#b32">[33]</ref> use a novel spatial feature transform layer to introduce the semantic prior as an additional input of SR network. Moreover, Riegler et al. <ref type="bibr" target="#b24">[25]</ref> propose conditioned regression models can effectively exploit the additional kernel information during training and inference. SRMD <ref type="bibr" target="#b38">[39]</ref> propose a stretching strategy to integrate non-image degradation information in a SR network.</p><p>Blind Super-Resolution. Blind SR assume that the degradation kernels are unavailable. In recent years, the community has paid relatively less research attention to blind SR problem. Michaeli and Irani <ref type="bibr" target="#b22">[23]</ref> estimate the optimal blur kernel based on the property that small image patches will re-appear in images. There are also research works trying to employ deep learning in blind SR task. Yuan et al. <ref type="bibr" target="#b36">[37]</ref> propose to learn not only SR mapping but also the degradation mapping using unsupervised learning. Shocher et al. <ref type="bibr" target="#b26">[27]</ref> exploit the internal recurrence of information inside an image and propose an unsupervised SR method to super-resolve images with different blur kernels. They train a small CNN on examples extracted from the input image itself, the trained image-specific CNN is appropriate for super-resolving this image. Different from the previous works, our method employs the correlation between SR results and kernel mismatch. Our method uses the intermediate SR results to iteratively correct the estimation of blur kernels, thus provide artifact-free final SR results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>The blind super-resolution problem is formulated as follows. Mathematically, the HR image I HR and LR image I LR are related by a degradation model</p><formula xml:id="formula_0">I LR = (k ⊗ I HR ) ↓ s +n,<label>(1)</label></formula><p>where ⊗ denotes convolution operation. There are three main components in this model, namely the blur kernel k, the downsampling operation ↓ s and the additive noise n. In literature, the most widely adopted blur kernel is isotropic Gaussian blur kernel <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>. Besides, the anisotropic blur kernels also appear in some works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref>, which can be regarded as the combination of a motion blur and an isotropic blur kernel. For simplicity, we mainly focus on the isotropic blur kernel without motion effect in this paper. Following most recent deep learning based SR methods <ref type="bibr" target="#b38">[39]</ref>, we adopt the combination of Gaussian blur and bicubic downsampling. In real-world use cases, the LR images are often accompanied with additive noises. As in SRMD <ref type="bibr" target="#b38">[39]</ref>, we assume that the additive noise follows Gaussian distribution in real world application. Note that the formulation of blind SR in this paper is different with the previous works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref> . Although defined as blind SR problem, our method focuses on a limited variety of kernels and noise. But the kernel estimated according to our assumptions can handle most of the real world images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motivation</head><p>We then review the importance of using correct blur kernel during SISR based on the settings described above. In order to obtain the LR images I LR , the HR images I HR are first blurred by the isotropic Gaussian kernel with kernel width σ LR and then downsampled by bicubic interpolation. Assume that the mapping F(I LR , k) is a well-trained SR model with the kernel information as input (e.g., SRMD <ref type="bibr" target="#b38">[39]</ref>). Then the output image is artifact-free with correct kernel k. The blind SR problem is equivalent to finding the kernel k that helps SR model generate visual pleasing result I SR . A straightforward solution is to adopt a predictor function k = P(I LR ) that estimates k from the LR input directly. The predictor can be optimized by minimizing the l 2 distance as</p><formula xml:id="formula_1">θ P = arg min θ P k − P(I LR ; θ P ) 2 2 ,<label>(2)</label></formula><p>where θ P is the parameter of P. By employing the predictor function and the SR model together, we are able to build an end-to-end blind SR model. However, accurate estimation of k is impossible. As the inverse problem is ill-posed, there exists multiple candidates of k for a single input. Meanwhile, the SR models are very sensitive to the estimation error. If the inaccurate kernel is used for SR directly, then the final SR results will contain obvious artifacts. <ref type="figure">Figure 2</ref> shows the sensitivity of the SR results to kernel mismatch, where σ SR denotes the kernel width used for SR. As shown in the upper-right region of <ref type="figure">Figure 2</ref>, where the kernel used for SR are sharper than the real one (σ SR &lt; σ LR ), the SR results are oversmoothing and the the high frequency textures are significantly blurred. In the lower-left region of <ref type="figure">Figure 2</ref>, where the kernel used for SR are smoother than the correct one (σ SR &gt; σ LR ), the SR results show unnatural ringing artifacts caused by over-enhancing high-frequency edges. In contrast, the results on the diagonal, which use correct blur kernels, look natural without artifacts and blurring. The above phenomenon illustrates that the estimation error of k will be significantly magnified by the SR model, resulting in unnatural output images. To address the kernel mismatch problem, we propose to iteratively correct the kernel until we obtain an artifact-free SR results.</p><p>To correctly estimate k, we build a corrector function C that measures the difference between the estimated kernel and the ground truth kernel. In the core of our idea is to adopt the intermediate SR results. The corrector function can be obtained by minimizing the l 2 distance between the corrected kernel and the ground truth as</p><formula xml:id="formula_2">θ C = arg min θ C k − (C(I SR ; θ C ) + k ) 2 2 ,<label>(3)</label></formula><p>where θ C is the parameter of C and I SR is the SR result using the last estimated kernel. This corrector adjusts the estimated blur kernel based on the features of the SR image. After correction, the SR results using adjusted kernel are supposed to approach natural images with less artifacts. However, if we train our model with only one time of correction, the corrector may provide inadequate correction or over-correct the kernel, leading to unsatisfactory SR results. A possible solution is to use smaller correction steps that gradually refine the kernel until it reaches ground truth. When the SR result does not contain serious over-smoothing or over-sharpening effects, the corrector will make little changes to the estimated kernel to ensure convergence. Then we are able to get a high-quality SR image by iteratively applying kernel correction. Experiments also demonstrate our assumption. <ref type="figure">Figure 3</ref> shows the PSNR and SSIM results using different iteration numbers. It can be observed that correcting only once is not sufficient. When the number of iterations increases, both PSNR and SSIM increase gradually until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Proposed Method</head><p>Overall framework. The proposed Iterative Kernel Correction (IKC) framework consists of a SR model F, a predictor P and a corrector C, and the pseudo-code is shown in Algorithm 1. Suppose the LR image I LR is of size C × H × W , where C denotes the number of channels, H and W denote the height and width of the image. We assume that blur kernel is of size l × l and the kernel space is a l 2 -dimensional linear space. In order to save computation, we first reduce the dimensionality of the kernel space by principal component analysis (PCA). The kernels are projected onto a b-dimensional linear space by a dimension reduction matrix M ∈ R b×l <ref type="bibr" target="#b1">2</ref> . Thus we only need to perform estimation in this low dimensional space, which is more effective in calculation. The kernel after the dimension reduction is denoted by h, where h = M k, h ∈ R b . At the start of the algorithm, an initial estimation h 0 is given by the predictor function h 0 = P(I LR ), and then used to get the first SR result I SR 0 = F(I LR , h 0 ). After obtaining the initial estimation, we proceed to the correction phase of the estimated kernel. At the ith iteration, given the previous estimation h i−1 , the correcting update ∆h i , the new estimation h i and the new SR result I SR i can be written as</p><formula xml:id="formula_3">∆h i = C(I SR i , h i−1 ) (4) h i = h i−1 + ∆h i<label>(5)</label></formula><formula xml:id="formula_4">I SR i = F(I LR , h i ).<label>(6)</label></formula><p>After t iterations, the I SR t is the final output of IKC. Network architecture of SR model F. As the most successful SR method for multiple blur kernels, SRMD <ref type="bibr" target="#b38">[39]</ref> propose a simple yet efficient stretching strategy for CNN to process non-image input directly. SRMD stretches the input h into kernel maps H of size b × H × W , where all the elements of the ith map are equal to the ith element of h. SRMD takes the concatenated LR image and kernel maps of size (b+C)×H ×W as input. Then, a cascade of 3×3 convolution layers and one pixel-shuffle upsampling layer are applied to perform super-resolution. However, to exploit the kernel information, concatenating the image and the transformed kernel as input is not the only or best choice. On the one hand, the kernel maps do not actually contain the information of the image. Processing the kernel maps and the image at the same time with convolution operation will introduce interference that is not related to the image. Using this concatenation strategy with residual blocks can interfere with image processing, making it difficult to employ residual structure to improve performance. On the other hand, the influence of kernel information is only considered at the first layer. When applying the same strategy in a deeper network, the deeper layers are difficult to be affected by the kernel information input at the first layer. To address above problems, we proposed a new SR model for multiple kernels using spatial feature transform (SFT) layers <ref type="bibr" target="#b32">[33]</ref>, namely SFTMD. In SFTMD, the kernel maps influence the output of network by applying an affine transformation to the feature maps in each middle layer by SFT layers. This affine transformation is not involved in the process of input image directly, thus providing better performance. <ref type="figure">Figure 4</ref> illustrates the network architecture of SFTMD. We employ the high level architecture of SRResNet <ref type="bibr" target="#b18">[19]</ref> and extend it to handle multiple kernels by SFT layers. The SFT layer provides affine transformation for the feature maps F conditioned on the kernel maps H by a scaling and shifting operation:</p><formula xml:id="formula_5">SFT(F, H) = γ F + β,<label>(7)</label></formula><p>where γ and β is the parameters for scaling and shifting, present Hadamard product. The transformation parameters γ and β are obtained by small CNN. Suppose that the output feature maps of the previous layer F are of size and after the global residual connection. It is worth pointing out that the code maps are spatially uniform, thus the SFT layers do not actually provide spatial variability according to the code maps. This is different from its application in semantic super resolution <ref type="bibr" target="#b32">[33]</ref>. We only employ the transformation characteristic of SFT layers. Network architecture of predictor P and corrector C. The network designs of the predictor and corrector are shown in <ref type="figure">Figure 5</ref>. For the predictor P, we use four convolution layers with Leaky ReLU activations and a global average pooling layer. The convolution layers give the estimation of the kernel h spatially and form the estimation maps. Then the global average pooling layer gives the global estimation by taking the mean value spatially.</p><p>For the corrector C, we take not only the SR image I SR but also the previous estimation h as inputs. Similar to Eq.</p><p>(3), the new corrector can be obtained by solving the following optimization problem:</p><formula xml:id="formula_6">θ C = arg min θ C k − (C(I SR , h; θ C ) + k ) 2 2 .<label>(8)</label></formula><p>The input SR result is first processed to feature maps F SR by five convolution layers with Leaky ReLU activations. Note that the previous SR result may contain artifacts (e.g., ringing and blurry) caused by kernel mismatch, which can be extracted by these convolution layers. At the same time, we use two fully-connected layers with Leaky ReLU activations to extract the inner correlations of the previous kernel estimation. We then stretch the output vector f h to feature maps F h using the same strategy used in SFTMD. The F h and F SR are then concatenated to predict the ∆h. We use three convolution layers with kernel size 1 × 1 and Leaky ReLU activations to give the estimation for ∆h spatially. Same as the predictor, a global average pooling operation is used to get the global estimation of ∆h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Preparation and Network Training</head><p>We synthesize the training image pairs according to the problem formulation described in section 3.1. For the isotropic Gaussian blur kernels used for training, the kernel width ranges are set to [0.2, 2.0], [0.2, 3.0] and [0.2, 4.0] for SR factors 2, 3 and 4, respectively. We uniformly sample the kernel width in the above ranges. The kernel size is fixed to 21×21. When applying on real world images, we use the additive Gaussian noise with covariance σ = 15. We also provide noise-free version for comparison on the synthetic test images. The HR images are collected from DIV2K <ref type="bibr" target="#b0">[1]</ref> and Flickr2K <ref type="bibr" target="#b29">[30]</ref>, then the training set consists of 3450 high-quality 2K images. The training dataset is augmented with random horizontal flips and 90 degree rotations. All models are trained and tested on RGB channels.</p><p>The SFTMD and IKC are both trained on the synthetic training image pairs and their corresponding blur kernels. First, the SFTMD is pre-trained using mean square error (MSE) loss. We then train the predictor network and the corrector network alternately. The parameters of the trained SFTMD are fixed during training the predictor and the corrector. The order of training can refer to Algorithm 1. For every mini-batch data</p><formula xml:id="formula_7">{I LR i , I HR i , h i } N i=1 ,</formula><p>where N denotes the mini-batch size, we first update the parameters of the predictor according to Eq. (2). We then update the cor- <ref type="table">Table 1</ref>. Quantitative comparison of SRCNN-CAB <ref type="bibr" target="#b24">[25]</ref>, SRMDNF <ref type="bibr" target="#b38">[39]</ref> and the proposed SFTMD. The comparison is conducted using three different isotropic Gaussian kernels on Set5, Set14 and BSD100 dataset. The best two results are highlighted in red and blue colors. rector according to Eq. (8) with a fixed iteration number t = 7. For optimization, we use Adam <ref type="bibr" target="#b16">[17]</ref> with β 1 = 0.9, β 2 = 0.999 and learning rate 1 × 10 −4 . We implement our models with the PyTorch framework and train them using NVIDIA Titan Xp GPUs.</p><p>We also propose a test kernel set for the quantitative evaluation of blind SR methods, namely Gausssian8. As declared by the name, Gausssian8 consists eight selected isotropic Gaussian blur kernels for each SR factor 2, 3 and 4 (twenty four kernels in total). The ranges of kernel width are set to [0.80, 1.60], <ref type="bibr">[1.35, 2.40]</ref> and [1.80, 3.20] for SR factors 2, 3 and 4, respectively. The HR images are first blurred by the selected blur kernels and then downsampled by bicubic interpolation. By determining the blur kernels for testing, we can compare and analyze the performance of blind SR methods. Although it only contains isotropic Gaussian kernels, it can still be used to test the basic performance of a blind SR method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments of SFTMD</head><p>We evaluate the performance of the proposed SFTMD on different Gaussian kernels. The kernel settings are given in <ref type="table">Table 1</ref>. We compare the SFTMD with the SOTA nonblind SR methods SRCNN-CAB <ref type="bibr" target="#b24">[25]</ref> and SRMD <ref type="bibr" target="#b38">[39]</ref>. As SFTMD adopts SRResNet as the main network, which is different from SRMD and SRCNN-CAB, we provide two additional baselines that have same network structures but different concatenation strategies: (1) SRResNet with concatenating H at the first layer, (2) SFTMD with SFT layer replaced by direct concatenation 1 . <ref type="table">Table 1</ref> shows the quantitative comparison results. Comparing with the SOTA SR methods -SRCNN-CAB and SRMD, the proposed SFTMD achieves significantly better performance on all settings and dataset. Comparing with two additional baselines that all use SRResNet as the main network, SFTMD could also obtain the best results. This further demonstrated the effect of SFT layers. It is worth noting that directly concatenating H in SRResNet will cause severe performance drop. As the combination of direct concatenation strategy and residual structure will interfere with image processing and cause severe artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on Synthetic Test Images</head><p>We evaluate the performance of the proposed method on the synthetic test images. <ref type="figure" target="#fig_3">Figure 7</ref> shows the intermediate results during correction. As one can see that the SR results using the kernel estimated by the predictor directly (the initial prediction in <ref type="figure" target="#fig_3">Figure 7</ref>) are unsatisfactory and contain either blurry or ringing artifacts. As the number of iterations increases, artifacts and blurring are gradually alleviated. The quantitative results (PSNR) also prove the necessity of the iterative correction strategy. We can see at the 4th iteration, the SR results using corrected kernels are able to show good visual quality.</p><p>We then conduct thorough comparisons with the SOTA non-blind and blind SR methods using Gaussian8 kernels. We also provide the comparison with the solutions using the SOTA deblurring method. We perform blind debluring method Pan et al. <ref type="bibr" target="#b23">[24]</ref> before and after the non-blind SR method CARN <ref type="bibr" target="#b1">[2]</ref>. <ref type="table">Table 2</ref> shows the PSNR and SSIM <ref type="bibr" target="#b34">[35]</ref> results on five widely-used datasets. As one can see, despite the remarkable performance under bicubic downsampling setting, the non-blind SR methods suffer severe performance drop when the downsampling kernel is different from the predefined bicubic kernel. The ZSSR <ref type="bibr" target="#b26">[27]</ref> takes the effect of blur kernel into account, and provides better SR performance compared with non-blind SR methods. Performing blind deblurring on the LR images makes the SR images sharper, but lost in image quality The final SR results have severe distortion. Deblurring on the blurred super-resolved images provides better results, but fails to reconstruct textures and details. Although the SR results without kernel correction (denoted by "P+SFTMD") achieves comparable quantitative performance with the existing methods, the SR performance can still be greatly im-A+ <ref type="bibr" target="#b30">[31]</ref> CARN <ref type="bibr" target="#b1">[2]</ref> CARN + Pan et al. <ref type="bibr" target="#b23">[24]</ref> ZSSR <ref type="bibr" target="#b26">[27]</ref> P+SFTMD IKC (ours) <ref type="figure">Figure 6</ref>. SISR performance comparison of different methods with SR factor 4 and kernel width 1.8 on image "Img 050" from Urban100. <ref type="table">Table 2</ref>. Quantitative comparison of the SOTA SR methods and IKC method. The best two results are highlighted in red and blue colors, respectively. Note that the methods marked with "*" is not designed for blind SR, thus the comparison with these methods is unfair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Scale</head><p>Set5 <ref type="bibr" target="#b3">[4]</ref> Set14 <ref type="bibr" target="#b37">[38]</ref> BSD100 <ref type="bibr" target="#b20">[21]</ref> Urban100 <ref type="bibr" target="#b12">[13]</ref> Manga109 <ref type="bibr">[</ref>  proved by using the proposed IKC method. An example is shown in <ref type="figure">Figure 6</ref>. The PSNR values of different methods on different blur kernels are shown in <ref type="figure">Figure 9</ref>. As can be seen, when the kernel width becomes larger, the SR performance of the previous methods decreases. Meanwhile, the proposed IKC method achieves superior performance under all blur kernels.</p><p>To further show the generalization ability of the proposed IKC method, we test our method on another widelyused degradation setting <ref type="bibr" target="#b35">[36]</ref>, which involves Gaussian kernels and direct downsampler. When the downsampling  <ref type="figure">Figure 9</ref>. The PSNR performance of different methods on BSD100 <ref type="bibr" target="#b20">[21]</ref> with different kernel width. The test SR factor is 3.</p><p>for the kernels. IKC learns the relationship between the SR images and these features rather than the Gaussian kernel width. In <ref type="table" target="#tab_3">Table 3</ref>, we provide the comparison with the IKC method that adopts kernels parameterized by Gaussian kernel width. Experiments prove that the use of PCA helps to improve the generalization performance of IKC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on Real Images Set</head><p>Besides the above experiments on synthetic test images, we also conduct experiments on real images to demonstrate the effectiveness of the proposed IKC and SFTMD. Since there are no ground-truth HR images, we only provide the visual comparison. <ref type="figure">Figure 8</ref> shows the SISR results on real world image from the Historic dataset. For comparison, the A+ <ref type="bibr" target="#b30">[31]</ref> and CARN <ref type="bibr" target="#b1">[2]</ref> are used as the representative SR methods with bicubic downsampling, and ZSSR <ref type="bibr" target="#b26">[27]</ref> is used as the representative blind SR method. For a real-world image, the downsampling kernel is unknown and complicated, thus performance of the non-blind SR methods are severely affected. The SOTA blind method -ZSSR also fails to provide satisfactory results. In comparison, IKC provides artifact-free SR result with sharp edges.</p><p>We also compare the proposed IKC method with the non-blind SR method using 'hand-craft' kernel on realworld image 'Chip'. We super-resolve the LR image using SRMD with the 'hand-craft' kernel suggested by <ref type="bibr" target="#b38">[39]</ref>. They use a grid search strategy to find the kernel parameters with good visual quality. The visual comparison is shown in <ref type="figure">Figure 10</ref>. We can see that the result of SRMD has harper edges and higher contrast, but also looks a little artificial. At the same time, IKC could provide visual pleasing SR results automatically. Although the contrast of IKC result is not as high as SRMD result, it still provides sharp edges and more LR image ZSSR <ref type="bibr" target="#b26">[27]</ref> SRMD with hand-craft kernel IKC (Ours) <ref type="figure">Figure 10</ref>. SR results of the real image "Chip" with SR factor 4. The hand-craft kernel width suggested by SRMD is 1.5. natural visual effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this paper, we explore the relationship between blur kernel mismatch and the SR results, then propose an iterative blind SR method -IKC. We also propose SFTMD, a new SR network architecture for multiple blur kernels. In this paper, our experiments are mainly conducted on the isotropic kernels. However, the isotropic kernels don't seem to be applicable in some real world applications. As in most cases, there are some slightly motion blurs that affect the kernel. It is worth noting that the asymmetry of the kernel mismatch effect that IKC relies on can still be observed in the case of slightly motion blur (anisotropy blur kernels). For example, the artifacts and blur of a SR image in a certain direction is related to the width of the kernel in the same direction. This indicates that, by employing such asymmetry of the kernel mismatch in each direction, the IKC method can also be applied to more realistic cases with slightly motion blur, which will be our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>LR = 1 SR = 1 . 5 SR = 2 . 0 SR = 2 . 5 SR = 3 . 0 Figure 2 .</head><label>1152025302</label><figDesc>SR sensitivity to the kernel mismatch. Where σLR denotes the kernel used for downsampling and σSR denotes the kernel used for SR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 6 :</head><label>16</label><figDesc>Iterative Kernel CorrectionRequire: the LR image I LR Require: the max iteration number t 1: h 0 ← P(I LR ) (Initialize the kernel estimation) 2:I SR 0 ← F(I LR , h 0 ) (The initial SR result) 3: i ← 0 (Initialize counter) 4: while i &lt; t do 5: i ← i + 1 ∆h i ← C(I SR i−1 , h i−1 ) (Estimate the kernel error using the intermediate SR results) 7: h i ← h i−1 + ∆h i (Update kernel estimation) I LR , h i ) (Update the SR result) 9: return I SR t (Output the final SR result)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FFigure 4 .Figure 5 .</head><label>45</label><figDesc>C f × H × W , where C f is the number of feature maps, and the kernel maps are of size b × H × W . The CNN takes the concatenated feature maps and kernel maps (total size is (b + C f ) × H × W ) as input and output γ and β. We use SFT layers after all convolution layers in residual blocks The architecture of the proposed SFTMD network. The design of the proposed SFT layer is shown in pink box. The network architecture of the proposed predictor and corrector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>The intermediate SR results during kernel correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 3. The curves of PSNR and SSIM vs. iterations. The experiments are conducted using IKC method. The test set is Set14 and the SR factor is 4.</figDesc><table><row><cell></cell><cell>28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>27.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR(dB)</cell><cell>27.1 27.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell>0.73 0.74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>26.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>26.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.71</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>26.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Iterations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Iterations</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative performance of the proposed IKC method on other downsampling settings. , the LR images obtained by the same blur kernel are also different.Table 3shows the quantitative results of the proposed IKC method under different downsampling settings. The proposed IKC method has maintained its performance, which indicates that IKC is able to generalize to a downsampling setting that is inconsistent with the training settings. An important reason why the IKC method has such generalization ability is that IKC learns the kernel after PCA rather than the kernel parameterized by kernel width. PCA provides a feature representation SISR performance comparison of different methods with SR factor 4 on a real historic image '1967 Vietnam war protest'.</figDesc><table><row><cell>Method</cell><cell>Kernel Width</cell><cell cols="2">BSD100 [21] PSNR SSIM</cell><cell cols="2">BSD100 [21] PSNR SSIM</cell></row><row><cell>CARN [2]</cell><cell></cell><cell>26.05</cell><cell>0.6970</cell><cell>25.92</cell><cell>0.6601</cell></row><row><cell>ZSSR [27]</cell><cell></cell><cell>25.64</cell><cell>0.6771</cell><cell>25.64</cell><cell>0.6446</cell></row><row><cell>CARN [2]+Pan et al. [24] P+ SFTMD IKC, w/o PCA</cell><cell>2.0</cell><cell>25.71 23.42 26.85</cell><cell>0.7115 0.6812 0.7694</cell><cell>25.94 25.01 26.30</cell><cell>0.6804 0.7231 0.7812</cell></row><row><cell>IKC (ours)</cell><cell></cell><cell>27.06</cell><cell>0.7704</cell><cell>26.35</cell><cell>0.7838</cell></row><row><cell>CARN [2]</cell><cell></cell><cell>24.20</cell><cell>0.6066</cell><cell>24.53</cell><cell>0.5812</cell></row><row><cell>ZSSR [27]</cell><cell></cell><cell>24.19</cell><cell>0.6045</cell><cell>24.53</cell><cell>0.5796</cell></row><row><cell>CARN [2]+Pan et al. [24] P+ SFTMD IKC, w/o PCA</cell><cell>3.0</cell><cell>25.62 23.30 26.75</cell><cell>0.6678 0.6799 0.7685</cell><cell>25.52 24.41 26.28</cell><cell>0.6293 0.7214 0.7849</cell></row><row><cell>IKC (ours)</cell><cell></cell><cell>26.98</cell><cell>0.7694</cell><cell>26.58</cell><cell>0.7994</cell></row><row><cell>function is different</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Direct concatenation means concatenating the kernel maps with feature maps directly. This is different from the affine transformation in the SFT layer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast, accurate, and lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhyuk</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungkon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Ah</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="252" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind super-resolution using a learning-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Begin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on</title>
		<meeting>the 17th International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="85" to="89" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie Line Alberi-Morel</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">To learn image super-resolution, use a gan to learn how to do image degradation first</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate blur models vs. image priors in single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netalee</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Apartsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2832" to="2839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image super-resolution using gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A soft map framework for blind super-resolution image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lap-Pui</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="364" to="373" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Psf estimation using sharp edge prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Andrew P Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition (CVPR) workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Eighth IEEE International Conference on</title>
		<meeting>Eighth IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuma</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="21811" to="21838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonparametric blind super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deblurring images via dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditioned regression models for non-blind single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ruther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot super-resolution using deep internal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image superresolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Patch based blind image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="709" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Singleimage super-resolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised image superresolution using cycle-in-cycle generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
