<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforced Axial Refinement Network for Monocular 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chufan</forename><surname>Wu</surname></persName>
							<email>chufanwu15@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforced Axial Refinement Network for Monocular 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D Object Detection</term>
					<term>Refinement</term>
					<term>Reinforcement Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular 3D object detection aims to extract the 3D position and properties of objects from a 2D input image. This is an illposed problem with a major difficulty lying in the information loss by depth-agnostic cameras. Conventional approaches sample 3D bounding boxes from the space and infer the relationship between the target object and each of them, however, the probability of effective samples is relatively small in the 3D space. To improve the efficiency of sampling, we propose to start with an initial prediction and refine it gradually towards the ground truth, with only one 3d parameter changed in each step. This requires designing a policy which gets a reward after several steps, and thus we adopt reinforcement learning to optimize it. The proposed framework, Reinforced Axial Refinement Network (RAR-Net), serves as a post-processing stage which can be freely integrated into existing monocular 3D detection methods, and improve the performance on the KITTI dataset with small extra computational costs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past years, monocular 3D object detection has attracted increasing attentions in computer vision <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref>. For many practical applications such as autonomous driving <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18]</ref>, augmented reality <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref> and robotic grasping <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21]</ref>, high-precision 3D perception of surrounding objects is an essential prerequisite. Compared to 2D object detection, monocular 3D object detection can provide more useful information including orientation, dimension, and 3D spatial location. However, due to the increase in dimensionality, the 3D Intersection-over-Union (3D-IoU) evaluation criterion is much more strict than 2D-IoU, making monocular 3D object detection a very difficult problem. In some challenging scenarios, state-of-the-art methods can only achieve a 3D average precision (3D AP) of around 10% <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>There have been a variety of efforts on detecting the objects in 3D space from a single image, and two popular trends are using geometry constraints <ref type="figure">Fig. 1</ref>. Illustration of our idea that sequentially refines 3D detection using deep reinforcement learning. During the process, the 3D parameters are refined iteratively. In this example, we can see the trend that 3D-IoU gets improved as the 3D box gradually fits the object. Many intermediate steps are omitted here due to the limited space. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22</ref>] and depth estimation <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44]</ref>. Due to the lack of real 3D cues, these methods often suffer from the problem of foreshortening (for distant objects, a tiny displacement on the image plane can lead to a large shift in the 3D space), and thus fail to achieve high 3D-IoU rates between detection results and ground-truth. To make up for the loss of 3D information, recently researchers propose to use a sampling-based method <ref type="bibr" target="#b24">[25]</ref> to score the fitting degree between a sampled box and the object. However, in 3D space, the efficiency of sampling is very low and a randomly placed 3D box often has no overlap (3D-IoU is 0) to the target, which leads to inefficient learning. To this end, it is desirable to propose a method which can significantly increase the sampling efficiency.</p><p>In this paper, we ease this challenge by presenting a new framework called Reinforced Axial Refinement Network (RAR-Net), which, as illustrated in <ref type="figure">Fig. 1</ref>, iteratively refines the detected 3D object to the most probable direction. In this way, the probability of effective sampling (finding a positive example with a non-zero 3D-IoU) increases with iteration. This is a Markov Decision Process (MDP), which involves optimizing a strategy that gets a reward after multiple steps. We train the model using a Reinforcement Learning (RL) algorithm.</p><p>RAR-Net takes the current status as input, and outputs one refining action at a time. In each step, to provide the current detection information as auxiliary cues, we project it to an image of the same spatial resolution as the input image (each face of the box is painted in a specific color), concatenate this additional image to the original input, and feed the 6-channel input to the RAR-Net. This implicit way of embedding the 2D image and 3D information into the same feature space brings consistent accuracy gain. Overall, RAR-Net is optimized smoothly during training, in particular, with the help of abundant training data that are easily generated by simply jittering the ground-truth 3D box.</p><p>We conduct extensive experiments on the KITTI object orientation estimation benchmark, 3D object detection benchmark and bird's eye view benchmark. As a refinement step, RAR-Net works well upon four popular 3D detection baselines, improving the base detection accuracy by a large margin, while requiring relatively small extra computational costs. This implies its potential in real-world scenarios. In summary, our contributions are three-fold:</p><p>-To the best of our knowledge, this is the first work that applies deep RL to refine 3D parameters in an iterative manner. -We define the action space and state representation, and propose a data enhancement which embeds axial information and image contents. -RAR-Net is a plug-and-play refinement module. Experimental results on the KITTI dataset demonstrate its effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Monocular 3D Object Detection. Monocular 3D object detection aims to generate 3D bounding-boxes for objects from single RGB images. It is more challenging than 2D object detection due to the increased dimension and the absence of depth information. Early studies use handcrafted approaches trying to design efficient features for certain domain scenarios <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b8">9]</ref>. However, they suffer with the ability to generalize. Recently, researchers have developed deep learning based approaches aiming to solve this problem leveraging largely labeled data. One cut-in point is to use geometry constraints to make up for the lack of 3D information. Mousavian et al. <ref type="bibr" target="#b31">[32]</ref> present MultiBin architecture for orientation regression and compute the 3D translation using tight constraint. <ref type="bibr">Kundu et al. [20]</ref> propose a differentiable Render-and-Compare loss to supervise 3D parameters learning. Li et al. <ref type="bibr" target="#b21">[22]</ref> utilize surface features to explore the 3D structure information of the object. Apart from these pure geometry-based methods, there are some other methods which turn to the depth estimation to recover 3D information. One straightforward way is to first predict the depth map using the depth estimation module and then perform 3D detection using the estimated 3D depth <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b25">26]</ref>. Another way is to infer instance depth instead of global depth map <ref type="bibr" target="#b34">[35]</ref>, which does not require additional training data. Recently, Liu et al. <ref type="bibr" target="#b24">[25]</ref> propose to sample 3D bounding boxes from the space and introduce fitting degree to score the candidates. Brazil et al. <ref type="bibr" target="#b2">[3]</ref> design a 3D region proposal network called M3D-RPN to generate 3D object proposals in the space. However, the performance of these methods is still limited because of the low efficiency of sampling in the 3D space. Our work jumps out of the limitation of trending object detection modules by iteratively refining the box to the ground-truth. It greatly solves the issue when network cannot directly regress to the goal detection and achieves better result. Pose Refinement Methods. Our method belongs to the large category of coarse-to-fine learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>, which refines visual recognition in an iterative manner. The approaches most relevant to ours are the iterative 3D object pose refinement approaches in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b22">23]</ref>. Manhardt et al. <ref type="bibr" target="#b28">[29]</ref> train a deep neural network to predict a translational and rotational update for 6D model tracking. DeepIM <ref type="bibr" target="#b22">[23]</ref> aims to iteratively refine estimated 6D pose of objects given the initial pose estimation. They also see the limitation of direct regression of images. However, these methods require the CAD model of the objects for fine correction and cannot be used in autonomous driving directly. In our case, we do not require complex CAD models and optimize the whole pose refinement process using deep RL.</p><p>Deep RL. RL aims at maximizing a reward signal instead of trying to generate a representational hidden state like traditional supervised learning problem <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref>. Deep RL is the method of incorporating RL with deep learning. Due to the distinguished feature of delayed reward and the massive power of deep learning, deep RL has been widely used on decision making in goal-oriented problems like object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>, deformable face tracking <ref type="bibr" target="#b15">[16]</ref>, interaction mining <ref type="bibr" target="#b11">[12]</ref>, object tracking <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b37">38]</ref> and video face recognition <ref type="bibr" target="#b35">[36]</ref>. However, to our best knowledge, little work has been made in RL for pose refinement, especially in monocular 3D object detection. Our approach sees 3D parameter refinement problem as a multi-step decision-making problem by updating the 3D box using action from each step, which takes advantage of trial-and-error search in RL to achieve better result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The monocular 3D object detection task requires solving a 9-Degree-of-Freedom (9-DoF) problem including dimension, orientation and location using a single RGB image as input. In this paper, we focus on improving the detection accuracy in the context of autonomous driving, where the object can only rotate around the Y axis, so the orientation has only 1-DoF. Although many excellent methods have been proposed so far, the monocular 3D object detection accuracy is still below satisfactory. So, we formulate the refinement problem as follows: given an initial estimation (x,ŷ,ẑ,ĥ,ŵ,l,θ), the refinement model predicts a set of displacement values (∆x, ∆y, ∆z, ∆h, ∆w, ∆l, ∆θ). Then, a new estimation is computed as (x + ∆x,ŷ + ∆y,ẑ + ∆z,ĥ + ∆h,ŵ + ∆w,l + ∆l,θ + ∆θ) and fed into the refinement model again. After several iterations, the refinement model can generate more and more accurate estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline and the Curse of Sampling in 3D Space</head><p>Monocular 3D object detection is an ill-posed problem, i.e., to recover 3D perception from 2D data. Although some powerful models have been proposed for 3D understanding <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b2">3]</ref>, it is still difficult to build relationship between the depth-agnostic input image and the desired 3D location. To alleviate the information gap, researchers came up with an alternative idea that samples a number of 3D boxes from the space and asks the model to judge the IoU between the target object and each sampled box <ref type="bibr" target="#b24">[25]</ref>. Such models, sometimes referred to as a fitting network, produced significant improvement under sufficient training data and the help of extra (e.g., geometric) constraints. However, we point out that the above sampling-based approaches suffer a difficulty in finding 'effective samples' (those having non-zero overlap with the target) especially in the testing stage. This is mainly caused by the increased dimensionality: the probability that a randomly placed 3D box has overlap to a pre-defined object is much lower than that in the 2D scenario. For example, if we use a Gaussian distribution with a deviation of 1 meter, there is only a chance of 0.12 to place an effective sample on a car that is 5 meters away from the initial detection result. This situation even deteriorates with the distance becomes larger. That being said, unless the initial detection is sufficiently accurate, the sampling efficiency can be very low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Towards Higher Sampling Efficiency</head><p>To improve the sampling efficiency, a straightforward idea is to go towards a roughly correct direction and then perform sampling at a better place. For the same example of the car that is 5 meters behind the detection result, if we move the current detection result towards the back direction for 2 meters, the possibility of sampling a non-zero IoU box will increase to 0.63. Furthermore, with multi-step refinement, the 3D box can even converge to the ground-truth and sampling becomes unnecessary.</p><p>There are many moving options to choose, and we find that moving in only one direction at a time is the most efficient, because the training data collected in this way is the most concentrated (the output targets will not be scattered throughout the three-dimensional space). Most existing refinement models choose to optimize their objective function using one-step optimization, which learns to move from the initial estimate to the ground-truth directly. However, one-step optimization can barely achieve the global optimum, especially when there is more than one variable to be refined, because different variables can have an effect on each other. For example, refining the orientation first can help the model make better use of appearance information to refine to a more precise location. Two-stage cascaded refinement algorithm is another design choice, but it may bring considerable difficulties in algorithm design, especially in the way of defining different stages. Also, it is a challenging topic to prepare data for each stage, e.g. how to guarantee the training input fed into the second stage match the case in testing scenario.</p><p>Motivated by this concern, we choose to optimize the learning objective for the entire MDP instead of one step using RL-based framework which can support an arbitrary number of stages and the training procedure is elegant (few heuristic rules are required). Our approach starts from an initial estimate (x,ŷ,ẑ,ĥ,ŵ,l,θ), and outputs a refining operation at a time. The 3D-IoU of the predicted object is therefore improved as the refinement of the 3D parameters. <ref type="figure">Fig. 2</ref> shows our overall pipeline, Reinforced Axial Refinement Network (RAR-Net), where we first enhance the input information using a parameteraware module and then use a ResNet-101 <ref type="bibr" target="#b16">[17]</ref> backbone to output the action value (Q-value). Similar to <ref type="bibr" target="#b3">[4]</ref>, we also use the history vector to encode 10 past actions in order to stabilize search trajectories that might get stuck in repetitive cycles. We formulate the process of refining the 3D box from initial coarse estimate to the destination as an MDP and introduce an RL method for optimization. The goal is to predict a tight bounding-box with a high 3D-IoU.  <ref type="figure">Fig. 2</ref>. The proposed framework for monocular 3D object detection. It is an iterative algorithm optimized by RL. In each iteration, an input image is enhanced by a parameter-aware mask and fed into a deep network, which produces a Q-value for each action as output and the 3D box is refined according to -greedy policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Refining 3D Detection with Reinforcement Learning</head><p>In the RL setting, the optimal policy of selecting actions should maximize the sum of expected rewards R on a given initial estimated state S i . Since we do not have a priori knowledge about the optimal path to refine the initial predicted 3D bounding-box to the destination, we address the learning problem through standard DQN <ref type="bibr" target="#b30">[31]</ref>. This approach learns an approximate action value function Q(S i , A i ) for each action A i , and selects the action with the maximum value as the next action to be done at each iteration. In order to prevent falling into local optimum, we use -greedy policy, where there is certain possibility to choose random actions. The learning process iteratively updates the action-selection policy by minimizing the following loss function:</p><formula xml:id="formula_0">L(θ) = [R i + γ max Ai+1 Q(S i+1 , A i+1 ; θ −1 ) − Q(S i , A i ; θ)] 2 ,<label>(1)</label></formula><p>where γ is the discount factor, θ are the parameters of the Q-network, and θ −1 are the parameters of the target-Q-network, whose weights are kept frozen most of the time, but are updated with the Q-network's weights every few hundred iterations. We use [R i + γ max Ai+1 Q(S i+1 , A i+1 ; θ −1 )] to approximate the optimal target value, because the optimal action-value function obeys the Bellman equation:</p><formula xml:id="formula_1">Q (S i , A i ) = E Si+1 [R i + γ max Ai+1 Q (S i+1 , A i+1 )|S i , A i ].<label>(2)</label></formula><p>Under our refinement problem setting, for the output Q-value, we use a 15dimensional vector to represent 15 different refining operations and actions are chosen based on -greedy policy. Considering that continuous action space is too large and difficult to learn, we set the refinement value to be discrete during each iteration. In practice, we define the refinement value as a fixed ratio of the corresponding dimension of the object. We present the detailed settings of the definition of state, action, state transition and reward of our refinement framework for monocular 3D object detection as follows:</p><p>State: In this work, we define the state to include both the observation image patch and the projected 3D cuboid. Given an initial estimate of the object X = (x,ŷ,ẑ,ĥ,ŵ,l,θ), which is often the detection results of other monocular 3D object detection methods, we use a standard camera projection to obtain the top left point and bottom right point of the crop image patch:</p><formula xml:id="formula_2">(u min , v min , u max , v max ) = µ(X, K),<label>(3)</label></formula><p>where K ∈ R 3×4 is the camera intrinsic matrix and the function µ is the projection operation. To include more context information, we enlarge the patch regions by a factor of 1.2 in height and width. For the projected 3D cuboid, we crop in the same position as the image patch and use white color as the background. Therefore, our state is a 6-channal image patch:</p><formula xml:id="formula_3">S = [φ(u min , v min , u max , v max , I); P(X, K)],<label>(4)</label></formula><p>where I is the original image, P(X, K) is the projected 3D cuboid and φ(·) is the crop operation. Finally, S is resized to fit the input size of RAR-Net. Action: Our action set A consists of 15 refining operations, including a none operation indicating no refinement. These operations are related to the 3D parameters of the detections. For instance, the action +∆x will lead to a displacement along the width axis of the object with the value (∆x = δ ×ŵ), where δ is a fixed ratio. It is worth mentioning that there are two choices for the definition of our shifting actions, one is defined in the world coordinate system and the other is defined in the axial coordinate system of the object as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. If we need to move the object to the left in the world coordinate system, for the former definition, we have to predict the same moving action for cars with different orientations (appearances). But, if we use the latter definition, the shifting operation will be related to the orientation of the cars, thus turning a many-to-one mapping to one-to-one mapping and easing the training process. State Transition: Our state transition function T refines the predicted box of the objects from X i = (x,ŷ,ẑ,ĥ,ŵ,l,θ) to X i+1 = (x + ∆x,ŷ + ∆y,ẑ + ∆z,ĥ + ∆h,ŵ + ∆w,l + ∆l,θ + ∆θ). However, the moving direction is defined along the coordinate axes of the object, while the (x,ŷ,ẑ) is defined in the world coordinate system, so we need to transform the displacement value across two different coordinate systems. Denote the output displacement of RAR-Net as  (∆x , ∆y , ∆z ), which is defined in the axial coordinate system, we have:</p><formula xml:id="formula_4">     ∆x =∆z × cosθ + ∆x × sinθ ∆y =∆y ∆z =∆z × (− sinθ) + ∆x × cosθ<label>(5)</label></formula><p>Therefore, we can translate state S i to state S i+1 according to the output displacement value of RAR-Net. Reward: The reward function R reflects the detection accuracy improvement from state S i to S i+1 . Considering that increasing the 3D-IoU will have positive reward and decreasing the 3D-IoU will have negative reward, we define the reward function as:</p><formula xml:id="formula_5">R i =      + 1, if ∆IoU 3D &gt; 0 − 1, if ∆IoU 3D &lt; 0 sgn[(X i+1 − X i )(X − X i )], if ∆IoU 3D = 0<label>(6)</label></formula><p>where X is the ground-truth 3D parameters and ∆IoU 3D is the changes of 3D-IoU. When there is no overlap between the estimated and ground-truth boxes, we use the changes in 3D parameters as the reward signal. In addition, when we arrive at a none action or the end of the sequence, we set the reward to +3 for a successful refinement (IoU ≥ 0.7), and −3 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameter-aware Data Enhancement</head><p>In our iteration-based framework, two input sources are necessary, namely, an image patch which lies in the 2D image space (high-level image features), and the current detection result which lies in the 3D physical space (low-level geometry features). Provided that the desired output is strongly related to both information, it remains unclear how to combine both cues, in particular they come from two domains which are quite different from each other. Based on the above motivation, we propose to attach the refined result of last iteration into the input of current iteration. There are many options to achieve this goal, and the naive case is to concatenate the 3D parameters and the image feature in a late-fusion manner, but this practice can barely provide enough appearance cues. Another way is to project the 3D bounding-box on the input image patch <ref type="bibr" target="#b24">[25]</ref> or render the 3D object when 3D CAD models are available <ref type="bibr" target="#b19">[20]</ref>, but these methods may damage the original information since the projection result will obscure the original image.</p><p>To avoid loss of information while providing sufficient appearance cues, we propose to project the 3D bounding box on the 2D image plane, and draw different colors on each face of the projected cuboid. This idea is similar to <ref type="bibr" target="#b37">[38]</ref>. In order to prevent loss of depth information during the projection operation, we embed the instance depth into the intensity of the color as c , where c = c × 128 255 if z &gt; 50, and c = c × (1 − z 100 ) if z ≤ 50, c is the base RGB value shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, and z is the instance depth of the object. Thus, different appearance will represent different 3D parameters of the object. For example, we paint blue for the front face, so the blue cue can guide the model to learn the refining policy along the forward-backward axis. A sample projection is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. We concatenate the painted cuboid and the original image patch to construct a 6-channel input patch as the final input of our RAR-Net.</p><p>For the painting process, we use the OpenCV function fillConvexPoly to color each face of the projected cuboid. We also apply black to the edges of the projected cuboid to strengthen the boundary. Since some faces are invisible from the front view, we have to determine the visibility of each face. Denote the center of i-th face as C i , and the center of the 3D bounding box as C, the visibility of i-th face, V i , is determined by whether (0 − C)(C i − C) is greater than 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation Details</head><p>Training: We used the ResNet-101 as backbone, and changed the input size into 224 × 224 × 6, and the output size into 15. We trained the model from scratch. In order to speed up the RL process, we first performed supervised pre-training using one-step optimization where the model learns to perform the operation with the largest amount of correction. To create the training set, we added a jitter of Gaussian distribution to the 3D bounding boxes and each object leads to 300 training samples, whose projection is checked to be inside the image space. During the pre-training process, the model was trained with SGD optimizer using a start learning rate of 10 −2 with a batch size of 64. The model was trained for 15 epochs and the learning rate was decayed by 10 every 5 epochs. During RL, The model was trained with Adam optimizer using a start learning rate of 10 −4 with a batch size of 64 for 40000 iterations. We used memory replay <ref type="bibr" target="#b40">[41]</ref> with buffer size of 10 4 . The target Q-Network is updated for every 1000 iterations.</p><p>The for greedy policy is set to 0.5 and will decay exponentially towards 0.05. The discount factor γ is set to 0.9. Testing: we set the total refinement steps to 20, and during each step, we chose the action based on -greedy policy, which is to take actions either randomly or with the highest action-value. For each action, the refining stride was set to 0.05× corresponding dimensions. The for greedy policy is set to 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation</head><p>We evaluate our method on the real-world KITTI dataset <ref type="bibr" target="#b14">[15]</ref>, including the object orientation estimation benchmark, the 3D object detection benchmark, and the bird's eye view benchmark. There are 7481 training images and 7518 testing images in the dataset, and in each image, the object is annotated with 2D location, dimension, 3D location, and orientation. However, only the labels in the KITTI training set are released, so we mainly conduct controlled experiments in the training set. Results are evaluated based on three levels of difficulty, namely, Easy, Moderate, and Hard, which are defined according to the minimum bounding-box height, occlusion, and truncation grade. There are two commonly used train/val experimental settings: Chen et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> (val 1) and Xiang et al. <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> (val 2). Both splits guarantee that images from the training set and validation set are sampled from different videos.</p><p>We evaluate 3D object detection results using the official evaluation metrics from KITTI. 3D box evaluation is conducted on both two validation splits (different models are trained with the corresponding training sets). We focus our experiments on the car category as KITTI provides enough car instances for our method. Following the KITTI setting, we perform evaluation on the three difficulty regimes individually. In our evaluation, the 3D-IoU threshold is set to be 0.5 and 0.7 for better comparison. We compute the Average Orientation Similarity (AOS) for the object orientation estimation benchmark, the Average Precision (AP) for the bird's eye view boxes (which are obtained by projecting the 3D boxes to the ground plane), and the 3D Average Precision (3D AP) metric for evaluating the full 3D bounding-boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to the State-of-the-Arts</head><p>To demonstrate that our proposed refinement method's effectiveness, we use the 3D detection results from different state-of-the-art 3D object detectors including Deep3DBox <ref type="bibr" target="#b31">[32]</ref>, MonoGRNet <ref type="bibr" target="#b34">[35]</ref>, GS3D <ref type="bibr" target="#b21">[22]</ref> and M3D-RPN <ref type="bibr" target="#b2">[3]</ref> as the initial coarse estimates. These detection results are provided by the authors, except that we reproduce M3D-RPN by ourselves.</p><p>We first compare AOS with these baseline methods, and the results are shown in <ref type="table">Table 1</ref>. The 2D Average Precision (2D AP) is the upper bound of AOS by definition, and we can see that our refinement method can improve the baseline <ref type="table">Table 1</ref>. Comparisons of the Average Orientation Similarity (AOS, %) to baseline methods on the KITTI orientation estimation benchmark. (In each group, we also show the 2D Average Precision (2D AP) of 2D detection results, which is the upper bound of AOS).  even if the performance is already very close to the upper bound. Then we compare 2D AP in bird's view of our method with these published methods. As can be seen in <ref type="table" target="#tab_2">Table 2</ref>, our method improve the existing monocular 3D object detection methods for a large margin. For example, the AP of Deep3DBox in the setting of IoU = 0.7 gains a 4% improvement. We also notice that for different baselines, our improvements differ -for the lower baseline, the improvements are larger because they have more less perfect detection results. Similarly, we report a performance boost on 3D AP as shown in <ref type="table">Table 3</ref>. In addition, our method works better in the hard scenario that requires IoU = 0.7. <ref type="table">Table 4</ref> shows our results on the KITTI test set using M3D-RPN as baseline, which is consistent with the results in the validation set. We also tried to use D4LCN <ref type="bibr" target="#b10">[11]</ref> as a baseline, which used additional depth data for training, and we can still observe accuracy gain (0.51% AP) with a smaller step size (0.02). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Diagnostic Studies</head><p>In the ablation study we want to analyze the contributions of different submodule and different design choices of our framework. In <ref type="table" target="#tab_4">Table 5</ref>, We use the initial detection results of MonoGRNet <ref type="bibr" target="#b34">[35]</ref> as baseline. Discrete Output is to output a discrete refining choice instead of a continuous refining value. We also tried three different feature combining methods: Simple Fusion is the naive option which concatenates the current detection results parameters and the image feature vector, Direct Projection is to project the bounding box on the original image as <ref type="bibr" target="#b24">[25]</ref> did, and Parameter-aware means our parameter-aware module. We refer Axial Coordinate to the option of refining the location along the axial coordinate system rather than the world coordinate system. Single Action is to output one single refinement operation at a time rather than output all refinement operations for all the 3D parameters at the same time. RL is to optimize the model using RL. Final Model is our fully model with the best design choices.</p><p>Through comparing Discrete Output with Final Model, we find that directly regressing the continuous 3D parameters can easily lead to a failure in refinement and with controlled discrete refinement stride, the results can be much better. Also, we can see that Simple Fusion does not work well, which verifies that our image enhancement approach captures richer information. Besides, moving along the axial coordinate system and using the single refinement operation can also improve the performance and verify our arguments. Experiment also demonstrate that RL play an important role in boosting the performance further since it optimizes the whole refinement process. We notice that the number of steps and the refining stride have great impact to the final refinement results. So, during the test phase, we have tried different setting of steps and stride. With smaller strides and more steps, better performance can be achieved but with lager time cost. In addition, when the strides are too large, the initial 3D box of an object may jump to its neighboring object occasionally and some false positives can also be adjusted to overlap with an existing, true 3D box by accident. Since the moving stride and steps are also a part of the refinement policy, using RL to optimize them is feasible as well.</p><p>Last but not least, we visualize some refinement results in <ref type="figure" target="#fig_2">Fig. 4</ref>, where the initial 3D bounding box and the final refinement result are in shown with their 3D-IoU to ground-truth. We can see that our refinement method can refine the 3D bounding box from a coarse estimate to the destination where it can fit the object tightly. Apart from drawing the starting point and ending point of 3D detection boxes on 2D images, we also show some intermediate result for better understanding. During each iteration, our approach can output a refining operation to increase the detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computational Costs</head><p>We also compute the latency for our model. Our method achieves about 4% improvement compared to baseline, with a computation burden of 0.3s (10 steps), which is much smaller than the detection time cost: 2s (GS3D <ref type="bibr" target="#b21">[22]</ref>). Generally speaking, the cost is related to three aspects: (1) network backbone, (2) number of steps, (3) number of objects. For (1), using smaller backbone (such as ResNet-18) can further speed up the refinement process with some degraded performance. For (2), we can increase the refining stride of each step that will cause the number of steps to drop and further accelerate the refining stage, with the price of some imperfect correction. For (3), multiple objects in one image can be fed into the GPU as a batch and processed in parallel, so the inference time does not increase significantly compared to a single object. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have proposed a unified refinement framework called RAR-Net. In order to use multi-step refinement to increase the sampling efficiency, we formulate the entire refinement process as an MDP and use RL to optimize the model. At each step, to fuse two information sources from the image and 3D spaces into the same input, we project the current detection into the image space, which maximally preserves information and eases model design. quantitative and qualitative results demonstrate that our approach boost the performance of the state-of-the-art monocular 3D detectors with a small time cost. The success of our approach sheds light on applying indirect optimization to improve the data sampling efficiency in challenging vision problems. We believe that inferring 3D parameters from 2D cues will be a promising direction of a variety of challenges in the future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>(a) shows the world coordinate system, which is related to the camera pose and shared by all the objects. (b) shows the axial coordinate system for one sample object. We also illustrate how to generate the parameter-aware mask from a 3D object (best viewed in color). Each color indicates one fixed face. Only two faces are visible in this real example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Top 2 rows: Representative examples on which the proposed refinement method achieves significant improvement beyond the baseline detection results. The rightmost example is further detailed in the bottom 2 rows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparisons of 3D localization accuracy (AP, %) to state-of-the-arts methods on the KITTI bird's eye view benchmark. 46.50 32.01 39.15 28.71 33.46 14.34 20.00 12.52 16.44 11.36 13.40 +RAR-Net 38.31 48.90 34.01 39.91 29.70 35.16 18.47 24.29 16.21 19.23 14.10 15.92</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">IoU = 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IoU = 0.7</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Easy</cell><cell cols="2">Moderate</cell><cell cols="2">Hard</cell><cell cols="2">Easy</cell><cell cols="2">Moderate</cell><cell cols="2">Hard</cell></row><row><cell></cell><cell cols="12">val 1 val 2 val 1 val 2 val 1 val 2 val 1 val 2 val 1 val 2 val 1 val 2</cell></row><row><cell>Deep3DBox [32]</cell><cell>-</cell><cell>30.02</cell><cell>-</cell><cell>23.77</cell><cell>-</cell><cell>18.83</cell><cell>-</cell><cell>9.99</cell><cell>-</cell><cell>7.71</cell><cell>-</cell><cell>5.30</cell></row><row><cell>+RAR-Net</cell><cell cols="4">-33.12 -24.42</cell><cell>-</cell><cell>19.11</cell><cell cols="5">-14.38 -10.28 -</cell><cell>8.29</cell></row><row><cell cols="2">MonoGRNet [35] 53.91</cell><cell>-</cell><cell>39.45</cell><cell>-</cell><cell>32.84</cell><cell>-</cell><cell>24.84</cell><cell>-</cell><cell>19.27</cell><cell>-</cell><cell>16.20</cell><cell>-</cell></row><row><cell>+RAR-Net</cell><cell cols="4">54.01 -41.29 -</cell><cell>32.89</cell><cell>-</cell><cell cols="6">26.34 -23.15 -19.12 -</cell></row><row><cell cols="2">GS3D [22] 38.24 M3D-RPN [3] 56.92</cell><cell>-</cell><cell>43.03</cell><cell>-</cell><cell>35.86</cell><cell>-</cell><cell>27.56</cell><cell>-</cell><cell>21.66</cell><cell>-</cell><cell>18.01</cell><cell>-</cell></row><row><cell>+RAR-Net</cell><cell cols="4">57.12 -44.41 -</cell><cell>37.12</cell><cell>-</cell><cell cols="6">29.16 -22.14 -18.78 -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparisons of 3D detection accuracy (AP, %) with state-of-the-arts on the KITTI 3D object detection benchmark. 3D detection accuracy (AP, %) in the KITTI test set (in each group, the left accuracy is produced by M3D-RPN, and the right one by M3D-RPN+RAR-Net). /88.48 82.81/83.29 67.08/67.54 Bird 21.02/22.45 13.67/15.02 10.23/12.93 3D AP 14.76/16.37 9.71/11.01 7.42/9.52</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">IoU = 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IoU = 0.7</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Easy</cell><cell cols="2">Moderate</cell><cell cols="2">Hard</cell><cell cols="2">Easy</cell><cell cols="2">Moderate</cell><cell cols="2">Hard</cell></row><row><cell></cell><cell cols="12">val 1 val 2 val 1 val 2 val 1 val 2 val 1 val 2 val 1 val 2 val 1 val 2</cell></row><row><cell>Deep3DBox [32]</cell><cell>-</cell><cell>27.04</cell><cell>-</cell><cell>20.55</cell><cell>-</cell><cell>15.88</cell><cell>-</cell><cell>5.85</cell><cell>-</cell><cell>4.10</cell><cell>-</cell><cell>3.84</cell></row><row><cell>+RAR-Net</cell><cell cols="5">-28.92 -22.13 -</cell><cell cols="4">16.12 -14.25 -</cell><cell>9.90</cell><cell>-</cell><cell>6.14</cell></row><row><cell cols="2">MonoGRNet [35] 50.27</cell><cell>-</cell><cell>36.67</cell><cell>-</cell><cell>30.53</cell><cell>-</cell><cell>13.84</cell><cell>-</cell><cell>10.11</cell><cell>-</cell><cell>7.59</cell><cell>-</cell></row><row><cell>+RAR-Net</cell><cell cols="5">54.17 -39.71 -31.82</cell><cell>-</cell><cell cols="6">18.25 -14.40 -11.98 -</cell></row><row><cell>GS3D [22]</cell><cell cols="12">30.60 42.15 26.40 31.98 22.89 30.91 11.63 13.46 10.51 10.97 10.51 10.38</cell></row><row><cell>+RAR-Net</cell><cell cols="12">33.12 42.29 28.11 32.18 24.12 31.85 17.82 19.10 14.71 15.72 14.81 13.85</cell></row><row><cell cols="2">M3D-RPN [3] 50.24</cell><cell>-</cell><cell>40.01</cell><cell cols="2">-33.48</cell><cell>-</cell><cell>20.45</cell><cell>-</cell><cell>17.03</cell><cell>-</cell><cell>15.32</cell><cell>-</cell></row><row><cell>+RAR-Net</cell><cell cols="4">51.20 -44.12 -</cell><cell>32.12</cell><cell>-</cell><cell cols="6">23.12 -19.82 -16.19 -</cell></row><row><cell></cell><cell></cell><cell>Metirc</cell><cell cols="2">Easy</cell><cell cols="3">Moderate</cell><cell>Hard</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">AOS 88.38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation experiments on KITTI dataset (val 1, Easy, IoU = 0.7). The performance difference can be seen by comparing each column with the last column.</figDesc><table><row><cell>Module</cell><cell>Design Choices</cell><cell>Final Model</cell></row><row><cell>Discrete Output</cell><cell></cell><cell></cell></row><row><cell>Simple Fusion</cell><cell></cell><cell></cell></row><row><cell>Direct Projection</cell><cell></cell><cell></cell></row><row><cell>Parameter-aware</cell><cell></cell><cell></cell></row><row><cell>Axial Coordinate</cell><cell></cell><cell></cell></row><row><cell>Single Action</cell><cell></cell><cell></cell></row><row><cell>RL</cell><cell></cell><cell></cell></row><row><cell>3D AP</cell><cell>1.81 0.40 10.88 5.34 2.27 13.96</cell><cell>18.25</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Augmented reality meets computer vision: Efficient data generation for urban driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="961" to="972" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision-based intelligent vehicles: State of the art and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fascioli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Active object localization with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teulière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep optics for monocular depth estimation and 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Graphbit: Bitwise interaction mining via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">3d object detection and viewpoint estimation with a deformable 3d cuboid model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dual-agent deep reinforcement learning for deformable face tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Computer vision for autonomous vehicles: Problems, datasets and state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05519</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="421" to="436" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reinforcement learning improves behaviour from evaluative feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">445</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ojea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>RSS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep model-based 6d pose refinement in rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Reinforcement learning for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">From contours to 3d object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Payet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-view and 3d deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2232" to="2245" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Attention-aware deep reinforcement learning for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Soccer on your tabletop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning with iterative shift for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robotic grasping of novel objects using vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Driemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>López-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Subcategory-aware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Attentionnet: Aggregating weak directions for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Recurrent saliency transformation network: Incorporating multi-stage visual cues for small organ segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Action-decision networks for visual tracking with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Young Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
