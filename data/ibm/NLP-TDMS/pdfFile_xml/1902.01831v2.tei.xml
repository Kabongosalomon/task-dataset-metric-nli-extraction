<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face Alignment using a 3D Deeply-initialized Ensemble of Regression Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
							<email>rvalle@fi.upm.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
							<email>josemiguel.buenaposada@urjc.es</email>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valdés</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Universidad Complutense de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Baumela</surname></persName>
							<email>lbaumela@fi.upm.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Face Alignment using a 3D Deeply-initialized Ensemble of Regression Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face alignment algorithms locate a set of landmark points in images of faces taken in unrestricted situations. State-of-the-art approaches typically fail or lose accuracy in the presence of occlusions, strong deformations, large pose variations and ambiguous configurations. In this paper we present 3DDE, a robust and efficient face alignment algorithm based on a coarse-to-fine cascade of ensembles of regression trees. It is initialized by robustly fitting a 3D face model to the probability maps produced by a convolutional neural network. With this initialization we address self-occlusions and large face rotations. Further, the regressor implicitly imposes a prior face shape on the solution, addressing occlusions and ambiguous face configurations. Its coarse-to-fine structure tackles the combinatorial explosion of parts deformation. In the experiments performed, 3DDE improves the state-of-the-art in 300W, COFW, AFLW and WFLW data sets. Finally, we perform cross-dataset experiments that reveal the existence of a significant data set bias in these benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face alignment algorithms precisely locate a set of points of interest in the images of faces taken in unrestricted conditions. It has received much attention from the research community <ref type="bibr" target="#b15">(Jin and Tan, 2017)</ref> since it is a preliminary step for estimating 3D facial structure <ref type="bibr" target="#b44">(Zhao et al., 2016)</ref> and many other face image analysis problems such as verification and recognition <ref type="bibr">(Soltan-pour et al., 2017)</ref>, attributes estimation <ref type="bibr" target="#b1">(Bekios-Calfa et al., 2014)</ref> or facial expression recognition <ref type="bibr" target="#b24">(Martinez and Du, 2012)</ref>, to name a few. Present approaches typically fail or lose precision in the presence of occlusions, strong deformations produced by facial expressions, large pose variations and ambiguous configurations caused, for example, by strong make-up or the existence of other nearby faces.</p><p>Top performers in the most popular benchmarks are based on Convolutional Neural Networks (CNNs) and Ensemble of Regression Trees (ERT), see e.g., Tables 1, 2, 3, 4 and 5. The large effective receptive field of deep models <ref type="bibr" target="#b20">(Kowalski et al., 2017;</ref><ref type="bibr" target="#b23">Lv et al., 2017;</ref><ref type="bibr" target="#b37">Xiao et al., 2016;</ref><ref type="bibr" target="#b40">Yang et al., 2017;</ref><ref type="bibr" target="#b34">Wu et al., 2018</ref>) enable them to model context better and produce robust landmark estimations. However, in these models it is not easy to enforce facial shape consistency, something that limits their accuracy in the presence of occlusions and ambiguous facial configurations. ERT-based models <ref type="bibr" target="#b5">(Burgos-Artizzu et al., 2013;</ref><ref type="bibr" target="#b7">Cao et al., 2014;</ref><ref type="bibr" target="#b17">Kazemi and Sullivan, 2014;</ref><ref type="bibr" target="#b22">Lee et al., 2015;</ref><ref type="bibr" target="#b25">Ren et al., 2016)</ref>, on the other hand, are difficult to initialize, but may implicitly impose face shape consistency in their estimations <ref type="bibr" target="#b7">(Cao et al., 2014)</ref>. This increases their performance in occluded and ambiguous situations. They are also much more efficient than deep models and, as we demonstrate in our experiments, with a good initialization they are also very accurate.</p><p>In this paper we present the 3DDE (3D Deeply-initialized Ensemble) regressor, a robust and efficient face alignment algorithm based on a coarse-to-fine cascade of ERTs. It is a hybrid approach that inherits good properties of ERT, such as the ability to impose a face shape prior, and the robustness of deep models. It is initialized by robustly fitting a 3D face model to the probability maps produced by a CNN. With this initialization we tackle one of the main drawbacks of ERT, namely the difficulty in initializing the regressor in the presence of occlusions and large face rotations. On the other hand, the ERT implicitly imposes a prior face shape on the solution, addressing the shortcomings of deep models when occlusions and ambiguous face configurations are present. Finally, its coarse-to-fine structure tackles the combinatorial explosion of parts deformation, which is also a key limitation of approaches using shape constraints <ref type="bibr" target="#b7">(Cao et al., 2014)</ref>.</p><p>A preliminary version of our work appeared in <ref type="bibr" target="#b33">Valle et al. (2018)</ref>. Here we refine and extend it in several ways. First we improve the initialization by using a RANSAC-like procedure that increases its robustness in the presence of occlusions. We have also introduced early stopping and better data augmentation techniques for increasing the regularization when training both the ERT and the CNN. We also extend the evaluation including the newly released WFLW data base and a detailed ablation study. Finally, 3DDE may also be trained in presence of missing and occluded landmarks in the training set. This has enabled us to perform cross-dataset experiments that reveal the existence of significant data set bias that may limit the generalization capabilities of regressors trained on present data bases. To the best of our knowledge, this is the first time such a problem has been raised in the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Face alignment has been a topic of intense research for more than twenty years. Initial successful results were based on 2D and 3D generative approaches such as the Active Appearance Models (AAM) <ref type="bibr" target="#b8">(Cootes et al., 1998)</ref> or the 3D Morphable Models (3DMM) <ref type="bibr" target="#b3">(Blanz and Vetter, 2003)</ref>. Recent approaches are based on a cascaded combination of discriminative regressors.</p><p>In the earliest case these regressors are Random Ferns <ref type="bibr" target="#b11">(Dollar et al., 2010)</ref>, Ensembles of Regression Trees <ref type="bibr" target="#b6">(Cao et al., 2012)</ref> or linear models <ref type="bibr">la Torre, 2013, 2015)</ref>. Key ideas in this approach are indexing image description relative to the current shape estimate <ref type="bibr" target="#b11">(Dollar et al., 2010)</ref>, and the use of a regressor whose predictions lie on the subspace spanned by the training face shapes <ref type="bibr" target="#b7">(Cao et al., 2014)</ref>, this is the so-called Cascade Shape Regressor (CSR) framework. <ref type="bibr" target="#b17">Kazemi and Sullivan (2014)</ref> improved the original cascade framework by proposing a realtime ensemble of regression trees. <ref type="bibr" target="#b25">Ren et al. (2016)</ref> used locally binary features to boost the performance up to 3000 FPS. <ref type="bibr" target="#b5">Burgos-Artizzu et al. (2013)</ref> included occlusion estimation and decreased the influence of occluded landmarks. <ref type="bibr" target="#b28">Shen et al. (2014)</ref> refine the initial location of face landmarks using a random forest and SIFT features. <ref type="bibr">la Torre (2013, 2015)</ref> also use SIFT features and learn the linear regressor dividing the search space into individual regions with similar gradient directions. Overall, this set of approaches are very sensitive to the starting point of the regression process. For this reason an important part of recent work revolves around how to find good initializations <ref type="bibr" target="#b46">(Zhu et al., 2015</ref><ref type="bibr" target="#b45">(Zhu et al., , 2016</ref>. However, they are extremely efficient and may take advantage of implicit shape constraints <ref type="bibr" target="#b6">(Cao et al., 2012</ref><ref type="bibr" target="#b7">(Cao et al., , 2014</ref>.</p><p>The recent development of deep learning techniques has also impacted the face alignment field with the widespread use of CNN-based regressors. <ref type="bibr" target="#b30">Sun et al. (2013)</ref> were pioneers to apply a three-level CNN for locating landmarks. <ref type="bibr" target="#b43">Zhang et al. (2014)</ref> proposed a multi-task solution to deal with face alignment and attributes classification. <ref type="bibr" target="#b23">Lv et al. (2017)</ref> use global and local face parts regressors for fine-grained facial deformation estimation. <ref type="bibr" target="#b41">Yu et al. (2016)</ref> transform the landmarks rather than the input image for the refinement cascade. <ref type="bibr" target="#b32">Trigeorgis et al. (2016)</ref> and <ref type="bibr" target="#b37">Xiao et al. (2016)</ref> are the first approaches that fuse the feature extraction and regression steps of CSR into a recurrent neural network trained end-to-end. <ref type="bibr" target="#b20">Kowalski et al. (2017)</ref> and <ref type="bibr" target="#b40">Yang et al. (2017)</ref> use a global similarity transform to normalize landmark locations followed by a VGG-based and a Stacked Hourglass network respectively to regress the final shape. <ref type="bibr" target="#b34">Wu et al. (2018)</ref> derive face landmarks from boundary lines, which helps to remove the ambiguities in the landmark definition. Deep CNN models have large effective receptive fields that let them model context better and convey these approaches with a high degree of robustness to face rotation, scale, deformation and initialization. However, when used in a cascaded framework they may notably increase the computational requirements. Moreover, it is not clear how to impose facial shape consistency on the estimated set of landmarks. Hence, the regressor accuracy may be harmed in the presence of occlusions or ambiguities.</p><p>There is also an increasing number of works based on 3D face models. In the simplest case, they fit a mean model to the estimated image landmarks positions <ref type="bibr" target="#b19">(Kowalski and Naruniec, 2016)</ref> or jointly regress the pose and shape of the face <ref type="bibr" target="#b16">(Jourabloo et al., 2017;</ref><ref type="bibr" target="#b36">Xiao et al., 2017)</ref>. <ref type="bibr" target="#b47">Zhu et al. (2017)</ref> and <ref type="bibr" target="#b21">Kumar and Chellappa (2018)</ref> fit a 3DMM in a cascaded way. These approaches provide 3D pose information that may be used to estimate landmark self-occlusions or to train simpler regressors specialized in a given head orientation. However, building and fitting a 3D face model is a difficult task and the results of the full 3D approaches in current benchmarks are not as good as those described above.</p><p>Our proposal tries to leverage on the good properties of the three approaches described above. Using a CNN-based initialization we inherit the robustness of deep learning models. Like the simple 3D approaches we fit a rigid 3D face model to initialize the regressor and estimate the initial face orientation to address selfocclusions and ambiguities. Finally, we use a cascaded ERT within a coarse-to-fine framework to achieve accuracy and efficiency while avoiding the combinatorial explosion of independent parts deformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rigid pose computation</head><p>ERT-based regressors require a good initialization to converge. We propose the use of face landmarks location probability maps <ref type="bibr" target="#b2">(Belhumeur et al., 2011;</ref><ref type="bibr" target="#b9">Dantone et al., 2012;</ref><ref type="bibr" target="#b37">Xiao et al., 2016)</ref> to generate plausible shape initialization candidates. We define a UNet-like architecture <ref type="bibr" target="#b26">(Ronneberger et al., 2015;</ref><ref type="bibr" target="#b14">Honari et al., 2016)</ref>, with a loss function that handles missing landmarks. We train this CNN to obtain a set of probability maps, P(I), that model the position of each landmark in the input image (see <ref type="figure" target="#fig_0">Fig. 1</ref>). The maximum of each smoothed probability map determines our initial landmark positions. Note in <ref type="figure" target="#fig_0">Fig. 1</ref> that these predictions are sensitive to occlusions, ambiguities and may not be a valid face shape. Compared to typical CNN-based approaches, e.g., <ref type="bibr" target="#b40">Yang et al. (2017)</ref>, our CNN is much simpler, since we only require a rough estimation of landmark locations.</p><p>To start the ERT with a plausible face, we compute the initial shape by fitting a rigid 3D head model to the estimated 2D landmarks locations. To this end we use the softPOSIT algorithm proposed by <ref type="bibr" target="#b10">David et al. (2004)</ref> within a robust scheme. Unlike <ref type="bibr" target="#b33">Valle et al. (2018)</ref>, here we use a set of the distinct landmarks to establish the correspondences between the CNN predictions and the 3D face model. This avoids problems related to ambiguous landmarks around the jaw that do not correspond always to the same 3D points and produce wrong initializations, mainly in profile faces. Moreover, we have also implemented a RANSAC-like procedure, that runs softPOSIT several times with subsets of correspondences, to obtain a robust estimation (see Algorithm 1). </p><formula xml:id="formula_0">(xz) = L l=1 P l (I)[xz(l)] if p(xz) &gt; p * then p * = p(xz), R * = R, t * = t end if end for x 0 , v 0 = projectPoints(X, R * , t * ) Output: x 0 , v 0</formula><p>Let X ∈ R L×3 be the 3D coordinates of the L landmarks on the 3D face model, x ∈ R L×2 their 2D projections onto the image plane and v ∈ {0, 1} L their visibilities. We produce sub- sets of correspondences (x s , X s ) from the distinct landmarks shown in <ref type="figure" target="#fig_6">Fig. 8a</ref>, estimate the 3D face model pose (R, t) with softPOSIT and evaluate the goodness of each estimation as the sum of landmarks probabilities,</p><formula xml:id="formula_1">p(x z ) = L l=1 P l (I)[x z (l)],</formula><p>where x z (l) are the 2D coordinates of the l-th landmark and P l (I) is the probability map for landmark l. Finally, we select the rigid transformation (R, t) with highest p(x z ). As a result, we project the 3D model onto the image using the most likely estimated rigid transformation. This provides the ERT with a rough estimation of the scale, translation and 3D pose of the target face (see <ref type="figure" target="#fig_0">Fig. 1</ref>), and the visibility estimation of the self-occluded parts of the face. Let x 0 = g 0 (P(I), X) be the initial shape, the output of the initialization function g 0 after processing the input image I. With our initialization we enforce two key requirements for the convergence of the ERT. First, that x 0 lies on the face with an approximately correct 3D face pose. Second, that x 0 is a valid face shape. The latter guarantees that the predictions in the next step of the algorithm will also be valid face shapes <ref type="bibr" target="#b7">(Cao et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ERT-based non-rigid shape estimation</head><formula xml:id="formula_2">Let S = {s i } N i=1 be the set of training face shapes, where s i = (I i , x g i , v g i , w g i , x 0 i , v 0 i )</formula><p>. Each shape s i has its own training image, I i , ground truth shape, x g i , ground truth visibility label, v g i , annotated landmark label, w g i ∈ {0, 1} L , initial shape, x 0 i , and visibilities, v 0 i , for training the ERT regressor. In our implementa-tion we use shape-indexed features <ref type="bibr" target="#b22">(Lee et al., 2015)</ref>, φ(P(I i ), x t i , w g i ), that depend on the current shape x t i of the landmarks in image I i and whether they are annotated or not, w g i . We divide the regression process into a maximum of T stages. We learn an ensemble of K regression trees for the t-th stage,</p><formula xml:id="formula_3">C t (f i ) = x t−1 i + K k=1 g k (f i ), where f i = φ(P(I i ), x t−1 i , w g i</formula><p>) and x j are the coordinates of the landmarks estimated in j-th stage. To train the ERT we use the N training shapes in S to generate an augmented training set of samples, S A , and a validation set, S V , with cardinality N A = |S A | and N V = |S V | respectively. The total number of samples is N T = N A + N V . Instead of using a fixed number of stages, like <ref type="bibr" target="#b33">Valle et al. (2018)</ref>, we stop training when the validation error stops improving. In this way the regressor has a variable number of stages. We compute the initialization for each sample using the 3D projections produced by g 0 (see generated initializations in <ref type="figure">Fig. 2</ref>). We also improve the data augmentation used in <ref type="bibr" target="#b33">Valle et al. (2018)</ref>. To this end we add random noise to the yaw, pitch and roll angles, of the rotation matrix R * estimated with g 0 , to generate new training initializations for each sample in S A .</p><p>Following et al. <ref type="bibr" target="#b5">Burgos-Artizzu et al. (2013)</ref> and <ref type="bibr" target="#b17">Kazemi and Sullivan (2014)</ref>, we attach to each landmark in S the binary labels {v, w} ∈ {0, 1} that model respectively whether it is visible and annotated. We learn these labels in the ERT together with the landmark location. Each initial shape is progressively refined by estimating a shape and visibility incre-</p><formula xml:id="formula_4">ments C v t (φ(P(I i ), x t−1 i , w g i )) where x t−1 i</formula><p>represents the current shape of the i-th sample (see Algorithm 2). C v t is trained to minimize only the landmark position errors but on each tree leaf, in addition to the mean shape, we also output the mean of all training shapes visibilities, v g i , that belong to that node. We define</p><formula xml:id="formula_5">A t−1 = {(x t−1 i , v t−1 i )} N A i=1 and V t−1 = {(x t−1 i , v t−1 i )} N V i=1</formula><p>as the set of all current shapes and corresponding visibility vectors for all training and validation data, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Training an Ensemble of Regression Trees</head><p>Input: S, T // Generate an augmented training set of samples SA, SV = dataAugmentation(S) repeat // Extract training (FA) and validation (FV ) fea-</p><formula xml:id="formula_6">tures FA ∪ FV = {fi} N T i=1 = {φ(P(Ii), x t−1 i , w g i )} N T i=1 // Apply Algorithm 3 using training samples C v t = learnCoarseToFineRegressor(SA, FA, At−1, K, P ) // Update validation samples Vt = Vt−1 + {C v t (fi)} N V i=1 // Increase P when N M E({x t i , x g i } N A i=1 ) &lt; N M E({x t i , x g i } N V i=1 ) // Compute validation error improvement ∆ε = N M E({x t−1 i , x g i } N V i=1 ) − N M E({x t i , x g i } N V i=1 ) until t &gt; T or ∆ε &lt; 1% Output: {C v t } T * t=1 // T * is the last trained stage</formula><p>Compared with conventional ERT approaches, our ensemble is simpler. It will require fewer trees because we only have to estimate the nonrigid face deformation, since the 3D rigid component has already been estimated in the previous step. In the following we describe the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Initial shapes for regression</head><p>The selection of the starting point in the ERT is fundamental to reach a good solution. The simplest choice is the mean of the ground truth training shapes,x 0 = N i=1 x g i /N . However, <ref type="figure">Figure 2</ref>: The 8 worst initial shapes for the 300W private test set produced by g 0 (CNN+3D).</p><p>such a poor initialization leads to wrong alignment results in test images with large pose variations. Alternative strategies run the ERT several times with different initializations <ref type="bibr" target="#b5">(Burgos-Artizzu et al., 2013)</ref>, initialize with other ground truth shapes x 0 i ← x g j where i = j (Kazemi and Sullivan, 2014), or randomly deform the initial shape <ref type="bibr" target="#b20">(Kowalski et al., 2017)</ref>.</p><p>In our approach we initialize the ERT using the algorithm described in section 3.1, that provides a robust pose and a valid shape for initialization (see <ref type="figure">Fig. 2</ref>). Hence, the ERT only needs to estimate the non-rigid deformation component of the face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feature Extraction</head><p>ERT efficiency depends on the feature extraction step. In general, descriptor features such as SIFT used by <ref type="bibr" target="#b38">Xiong and la Torre (2013)</ref> and <ref type="bibr" target="#b46">Zhu et al. (2015)</ref> improve face alignment results, but have higher computational cost compared to simpler features such as plain pixel value differences <ref type="bibr" target="#b7">(Cao et al., 2014;</ref><ref type="bibr" target="#b5">Burgos-Artizzu et al., 2013;</ref><ref type="bibr" target="#b17">Kazemi and Sullivan, 2014;</ref><ref type="bibr" target="#b25">Ren et al., 2016)</ref>. In our case, a simple feature suffices, since shape landmarks are close to their ground truth location.</p><p>We use the probability maps P(I) to extract features for the cascade. To this end, we select a landmark l and its associated probability map P l (I). The feature is computed as the difference between two pixels values in P l (I) from a FREAK descriptor pattern <ref type="bibr" target="#b0">(Alahi et al., 2012)</ref> around l, similar to those in <ref type="bibr" target="#b22">Lee et al. (2015)</ref>. However, ours are defined on the probability maps, P(I), instead of the image, I. We let the training algorithm select the most informative landmark and pair of pixels in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Learn a coarse-to-fine regressor</head><p>To train the t-th stage regressor, C v t , we fit an ERT. Thus, the goal is to sequentially learn a series of weak learners to greedily minimize the regression loss function:</p><formula xml:id="formula_7">Lt(SA, FA, At−1) = N A i=1 ||w g i (x g i − x t−1 i − K k=1 g k (fi))|| 2 (1)</formula><p>where is the Hadamard product. There are different ways of minimizing Equation 1. <ref type="bibr" target="#b17">Kazemi and Sullivan (2014)</ref> present a general framework based on Gradient Boosting for learning an ensemble of regression trees. <ref type="bibr" target="#b22">Lee et al. (2015)</ref> establish an optimization method based on Gaussian Processes also learning an ensemble of regression trees but outperforming previous literature by reducing the overfitting. In our approach we adopt a Gradient Boosting scheme (see Algorithm 3).</p><p>A crucial problem when training a global face landmark regressor is the lack of examples showing all possible combinations of face parts deformations. Hence, these regressors quickly overfit and generalize poorly to combinations of part deformations not present in the training set. To address this problem we introduce the coarse-tofine ERT architecture.</p><p>The goal is to be able to cope with combinations of face part deformations not seen during training. A single monolithic regressor is not able to estimate these local deformations (see the difference between monolithic and coarse-to-fine NME curves in <ref type="figure">Fig. 6a</ref>). Our algorithm is agnostic in the number of parts and stages of the coarse-to-fine estimation. Algorithm 3 details the training of P face parts regressors (each one with a subset of the landmarks) to build a coarseto-fine regressor. Note that A k−1 in this context is the shape and visibility vectors from the last regressor output (e.g., the previous part regressor or a previous full stage regressor). In our implementation the coarse-to-fine scheme has two stages. The coarse stage has one part, P = 1, that involves all landmarks and K 1 trees. The fine stage has ten parts, P = 10, left/right eyebrow, left/right eye, nose, top/bottom mouth, left/right ear and chin (see <ref type="figure">Fig. 3</ref>), with K 2 trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Training P parts regressors</head><p>Input: SA, FA, At−1, ν, K, P for k=1 to K do for p=1 to P do // Compute residuals: // is the Hadamard product // (p) selects elements of vectors in that part</p><formula xml:id="formula_8">{r k i (p) = w g i (p) (x g i (p) − x k−1 i (p))} N A i=1 g p k = fitRegressionTree({r k i (p)} N A i=1 ,FA(p))</formula><p>// Update samples with the regression tree estimation, // ν, shrinkage factor to scale each tree contribution <ref type="figure">Figure 3</ref>: The P = 10 face parts of 300W, COFW, AFLW and WFLW data bases in the fine stage of our coarse-to-fine ERT.</p><formula xml:id="formula_9">A k (p) = A k−1 (p) + ν · {g p k (fi(p))} N A i=1 end for end for Output: {C p } P p=1 , being C p = {g p k } K k=1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Fit a regression tree</head><p>The training objective for the k-th regression tree is to minimize the sum of squared residuals, taking into account the annotated landmark labels:</p><formula xml:id="formula_10">E k = N A i=1 ||r k i || 2 = N A i=1 ||w g i (x g i − x k−1 i )|| 2 (2)</formula><p>We learn each regression binary tree by recursively splitting the training set into the left (l) and right (r) child nodes. The tree node split function is designed to minimize E k from Equation 2 in the selected landmark. To train a regression tree node we randomly generate a set of candidate split functions, each of them involving four parameters θ = (τ, p 1 , p 2 , l), where p 1 and p 2 are pixels coordinates on a fixed FREAK structure around the l-th landmark coordinates in x k−1 i . The feature value corresponding to θ for the i-th training sample is</p><formula xml:id="formula_11">f i (θ) = P l (I i )[p 1 ] − P l (I i )[p 2 ]</formula><p>, the difference of probability values in the maps for the given landmark. Finally, we compute the split function thresholding the feature value, f i (θ) &gt; τ .</p><p>Given N ⊂ S A the set of training samples at a node, fitting a tree node for the k-th tree, consists of finding the parameter θ that minimizes</p><formula xml:id="formula_12">E k (N , θ) arg min θ E k (N , θ) = arg min θ b∈{l,r} s∈N θ,b ||r k s − µ θ,b || 2<label>(3)</label></formula><p>where N θ,l and N θ,r are, respectively, the samples sent to the left and right child nodes due to the decision induced by θ. The mean residual µ θ,b for a candidate split function and a subset of training data is given by</p><formula xml:id="formula_13">µ θ,b = 1 |N θ,b | s∈N θ,b r k s<label>(4)</label></formula><p>Once we know the optimal split each leaf node stores the mean residual, µ θ,b , as the output of the regression for any example reaching that leaf. We also output the mean visibility of the samples reaching the tree leaf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To train and evaluate our proposal, we perform experiments with 300W, COFW, AFLW and WFLW that are considered the most challenging public data sets:</p><p>• 300W. It provides 68 manually annotated landmarks, <ref type="bibr" target="#b27">Sagonas et al. (2016)</ref>. We follow the most established approach and divide the 300W annotations into 3148 training and 689 testing images (public competition). Evaluation is also performed on the 300W private competition using the previous 3837 images as training and 600 newly updated images as testing set. • AFLW. It provides a collection of 25993 in-the-wild faces, with 21 facial landmarks annotated depending on their visibility, <ref type="bibr" target="#b18">Koestinger et al. (2011)</ref>. We have found several annotations errors and, consequently, removed these faces from our experiments. From the remaining faces we randomly choose 19312 images for training/validation and 4828 instances for testing.</p><p>• WFLW. It consists of 7500 extremely challenging training and 2500 testing faces divided into six subgroups, pose, expression, illumination, make-up, occlusion and blur, with 98 fully manual annotated landmarks, <ref type="bibr" target="#b34">Wu et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation</head><p>We use the Normalized Mean Error (NME) as a metric to measure the shape estimation error</p><formula xml:id="formula_14">N M E = 100 N N i=1 1 ||w g i ||1 L l=1 w g i (l) · xi(l) − x g i (l) di<label>(5)</label></formula><p>It computes the mean euclidean distance between the ground-truth and estimated landmark positions normalized by d i . We report our results using different values of d i : the ground truth distance between the eye centers (pupils), the ground truth distance between the outer eye corners (corners) and the ground truth bounding box size (height).</p><p>In addition, we also compare our results using Cumulative Error Distribution (CED) curves.</p><p>We calculate AU C ε as the area under the CED curve for images with an NME smaller than ε and F R ε as the failure rate representing the percentage of testing faces with NME greater than ε. We use precision/recall percentages to compare occlusion prediction.</p><p>To train our algorithm we shuffle the training set of each data base and split it into 90% trainset and 10% validation-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>All experiments have been carried out with the settings described in this section. For each data set, we train from scratch the CNN selecting the model parameters with lowest validation error. We crop faces using the ground truth bounding boxes annotations enlarged by 30%. We generate different training samples in each epoch by applying random in plane rotations between ±45 • , scale changes by ±15% and translations by ±5% of bounding box size, randomly mirroring images horizontally and generating random rectangular occlusions. We use Adam stochastic optimization with β 1 = 0.9, β 2 = 0.999 and = 1e −8 parameters. We train until convergence with an initial learning rate α = 0.001. When validation error levels out for 10 epochs, we multiply the learning rate by decay = 0.05. In the CNN the cropped input face is reduced from 160×160 to 1×1 pixels gradually dividing by half their size across B = 8 branches applying a stride 2 convolution with kernel size 2×2 1 . We apply batch normalization after each convolution. All layers contain 68 filters to describe the required landmark features. We apply a Gaussian filter with σ = 33 to the output probability maps to stabilize the initialization, g 0 .</p><p>We train the coarse-to-fine ERT with the Gradient Boosting algorithm <ref type="bibr" target="#b13">(Hastie et al., 2009</ref>). It requires a maximum of T = 20 stages of K = 50 regression trees per stage. The depth of trees is set to 4. The number of tests to choose the best split parameters, θ, is set to 200. We resize each image to set the face size to 160×160 pixels. For feature extraction, the FREAK pattern diameter is reduced gradually in each stage (i.e., in the last stages the pixel pairs for each feature are closer). We generate Z = 25 initializations in the robust softPOSIT scheme of g 0 . We augment the shapes of each face training image to create a set, S A , of at least N A = 60000 samples to train the cascade. To avoid overfitting we use a shrinkage factor ν = 0.1 and subsampling factor η = 0.5 in the ERT. Our regressor triggers the coarse-to-fine strategy once the training error is below the validation error, e.g., t = 5 in <ref type="figure">Fig. 6a</ref>.</p><p>Training the CNN and the coarse-to-fine ensemble of trees takes 48 hours using a NVidia GeForce GTX 1080Ti (11GB) GPU and an dual Intel Xeon Silver 4114 CPU at 2.20GHz (2×10 cores/20 threads, 128 GB of RAM) with a batch size of 32 images. At runtime our method process test images on average at a rate of 12.5 FPS, where the CNN takes 75 ms and the ERT 5 ms per face image using C++, Tensorflow and OpenCV libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments using public code</head><p>Published results in the literature are sometimes not fully comparable. In this section we use publicly available code to ensure a fair comparison between 3DDE and DCFE <ref type="bibr" target="#b33">(Valle et al., 2018)</ref>, LAB <ref type="bibr" target="#b34">(Wu et al., 2018)</ref>, DAN <ref type="bibr" target="#b20">(Kowalski et al., 2017)</ref>, RCN <ref type="bibr" target="#b14">(Honari et al., 2016)</ref>, cGPRT <ref type="bibr" target="#b22">(Lee et al., 2015)</ref>, RCPR (Burgos-Artizzu et al., 2013) and ERT <ref type="bibr" target="#b17">(Kazemi and Sullivan, 2014)</ref> with the same settings (including same training, valida-tion and bounding boxes), in different benchmarks: 300W public, 300W private, COFW and WFLW. Note that LAB <ref type="bibr" target="#b34">(Wu et al., 2018)</ref> only provides a trained model for the WFLW data set. In addition, DAN <ref type="bibr" target="#b20">(Kowalski et al., 2017)</ref> provides code using 68 landmarks, for this reason we only report results in 300W. In <ref type="figure" target="#fig_4">Fig. 4</ref> we plot the CED curves for all data bases. In the legend we provide the AU C and F R values for each algorithm.</p><p>The selected algorithms are representative of the three main families of solutions: a) ensembles of regression trees (cGPRT, RCPR, ERT), b) CNN-based approaches <ref type="figure">(LAB, DAN, RCN)</ref> and c) mixed approaches with deep nets and ensembles of regression trees (3DDE, DCFE). Overall, 3DDE is better than any other providing a public implementation in the literature. We improve over our preliminary algorithm, <ref type="bibr">DCFE Valle et al. (2018)</ref>, because of the better 3D initialization and regularization (see a complete analysis in section 4.5). In general we are able to improve by a large margin other ERT methods as RCPR, ERT or cGPRT because of the better initialization and the robust features provided by the CNN. We also outperform RCN (without any denoising model), a CNN architecture like the one used in 3DDE. Even DAN and LAB, that implement a cascade of CNN regressors, can not compete with the regularization obtained by using the cascade of ERT in 3DDE (see <ref type="figure" target="#fig_4">Fig. 4</ref>). The fact that the largest margin is in COFW reflects the importance of the implicit shape model in our cascade to address occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments using published results</head><p>In this section we compare 3DDE with other methods in the literature by using their published results. Since our method is able to train with unannotated landmarks and visibilities, we are able to train and evaluate all data sets in the literature.</p><p>First we test our method against the 300W benchmark. Our approach obtains the best overall performance in the indoor and outdoor subsets of the private competition (see <ref type="table" target="#tab_3">Table 2</ref>) and in the full subset of the 300W public test set (see <ref type="table" target="#tab_2">Table 1</ref>). This is due to the excellent accuracy achieved by the coarse-to-fine ERT scheme enforcing valid face shapes and the deep robust features extracted from the CNN. In the challenging subset of the 300W public competition, SHN <ref type="bibr" target="#b40">(Yang et al., 2017)</ref> gets better results than 3DDE. This is due to 3DDE failing to estimate good landmark probability maps for images with large scale variations. Our method exhibits superior capability in handling typical cases in the data base, since we achieve the best NME full set results in <ref type="bibr">300W public, 4.39, and in 300W private, 3.73.</ref> We may assess the improvement achieved by the 3D initialization and the coarse-to-fine ERT by comparing the results of 3DDE in the full subset of 300W, 4.39, with Honari's RCN using the denoising model <ref type="bibr" target="#b14">(Honari et al., 2016)</ref>, 5.41. It roughly represents a 19% improvement in the inter-pupils NME. <ref type="table" target="#tab_5">Table 3</ref> compares the performance of our model using the COFW data set. This is the standard to evaluate occlusions. 3DDE obtains the best results, NME 5.11, establishing a new state-of-the-art. This shows the importance of the face shape model implicit in the cascade of  <ref type="bibr" target="#b6">(Cao et al., 2012)</ref> 5.  <ref type="bibr" target="#b42">(Zhang and Hu, 2018)</ref> 5.42 -11.80 -6.67 ---ERT <ref type="bibr" target="#b17">(Kazemi and Sullivan, 2014)</ref> ----6.40 ---LBF <ref type="bibr" target="#b25">(Ren et al., 2016)</ref> 4.95 -11.98 -6.32 ---cGPRT <ref type="bibr" target="#b22">(Lee et al., 2015)</ref> ----5.71 ---CFSS <ref type="bibr" target="#b46">(Zhu et al., 2015)</ref> 4.73 -9.98 -5.76 -49.87 5.08 DDN <ref type="bibr" target="#b41">(Yu et al., 2016)</ref> ----5.65 ---TCDCN <ref type="bibr" target="#b43">(Zhang et al., 2014)</ref> 4.80 -8.60 -5.54 ---MDM <ref type="bibr" target="#b32">(Trigeorgis et al., 2016)</ref> ------52.12 4.21 3DDFA <ref type="bibr" target="#b47">(Zhu et al., 2017)</ref> 5.09 -8.07 -5.63 ---RCN <ref type="bibr" target="#b14">(Honari et al., 2016)</ref> 4.67 -8.44 -5.41 ---DAN <ref type="bibr" target="#b20">(Kowalski et al., 2017)</ref> 4.42 3.19 7.57 5.24 5.03 3.59 55.33 1.16 TSR <ref type="bibr" target="#b23">(Lv et al., 2017)</ref> 4.36 -7.56 -4.99 ---RAR <ref type="bibr" target="#b37">(Xiao et al., 2016)</ref> 4.    ERT to cope with severe occlusions. In terms of landmark visibility estimation, we have obtained better precision with an overall better recall than the best previous approach, DCFE. Again, the regularization together with the new initialization contributes to improve DCFE.</p><p>In <ref type="table" target="#tab_6">Table 4</ref> we show the results of our evaluation with AFLW. This is a challenging data set not only because of its size and the large variability of face poses, but also because of the large number of samples with occluded landmarks, that are unannotated. Although the results in <ref type="table" target="#tab_6">Table 4</ref> are not strictly comparable, because each paper uses its own train and test subsets, we get an NME of 2.06 with the full 21 landmarks set. Again, it is a new state-of-the-art, since most competing approaches do not use the two most difficult landmarks, each located in one earlobe (see 19 landmarks results in <ref type="table" target="#tab_6">Table 4</ref>). We have also evaluated 3DDE without the two earlobe landmarks. In this case we get an NME of 2.01, the best reported result.</p><p>Finally, we have also evaluated 3DDE with the newly released WFLW data set <ref type="bibr" target="#b34">(Wu et al., 2018)</ref>. In enables us to evaluate different sources of variability (i.e., expressions, illumination, make-up, occlusions and blur). In <ref type="table" target="#tab_8">Table 5</ref> we provide the results of various competing methods <ref type="bibr" target="#b34">(Wu et al., 2018)</ref>, normalized by the eye corners distance. 3DDE outperforms its competitors in all the WFLW subsets by a large margin. We hypothesize that the reason for this is that the hybrid approach in 3DDE can be trained with less samples that some of its most prominent competitors and at the same time provide a very accurate face shape (see <ref type="figure">Fig. 5</ref>). Moreover, we Method pupils occlusion N M E AU C8 F R8 precision/recall RCPR <ref type="bibr" target="#b5">(Burgos-Artizzu et al., 2013)</ref> 8.50 --80/40 TCDCN <ref type="bibr" target="#b43">(Zhang et al., 2014)</ref> 8.05 ---RAR <ref type="bibr" target="#b37">(Xiao et al., 2016)</ref> 6.03 ---DAC-CSR  6.03 ---Wu et al. <ref type="bibr" target="#b35">(Wu and Ji, 2015)</ref> 5.93 --80/49.11 SHN <ref type="bibr" target="#b40">(Yang et al., 2017)</ref> 5.6 ---PCD-CNN <ref type="bibr" target="#b21">(Kumar and Chellappa, 2018)</ref>    <ref type="bibr" target="#b46">(Zhu et al., 2015)</ref> 3.92 -CCL <ref type="bibr" target="#b45">(Zhu et al., 2016)</ref> 2.72 -DAC-CSR  2.27 -Binary-CNN <ref type="bibr" target="#b4">(Bulat and Tzimiropoulos, 2017)</ref> -2.85 PCD-CNN <ref type="bibr" target="#b21">(Kumar and Chellappa, 2018)</ref> -2.40 TSR <ref type="bibr" target="#b23">(Lv et al., 2017)</ref> 2.17 -DCFE <ref type="bibr" target="#b33">(Valle et al., 2018)</ref> 2.12 2.17 3DDE 2.01 2.06 Figure 5: First row shows LAB <ref type="bibr" target="#b34">(Wu et al., 2018)</ref> results, second row 3DDE results. We report the corresponding NME normalized by the eye corners distance. Blue and green colours represent ground truth and predictions respectively.</p><p>achieve the best AU C in all subsets, which determines that 3DDE is the best approach under all capture conditions (easy/frontal and difficult/profile) including all subsets that contain several types of difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation study</head><p>3DDE is based on three key ideas: 3D initialization, a cascaded ERT regressor operating on probabilistic CNN features and a coarse-to-fine scheme. In this section we analyze the contribution of each one to the overall performance of our algorithm.</p><p>In <ref type="table" target="#tab_11">Table 6</ref> we show the results obtained by different configurations of our framework when evaluated on WFLW. We have chosen WFLW in our study because it allows the analysis of results stratified by different types of difficulties (i.e., facial expressions, large poses, illumination changes, etc.). In this case, since there are many profile faces, we use the height as normalization for the NME. So, the numerical values are not di-rectly comparable to those in <ref type="table" target="#tab_8">Table 5</ref>. MS stands for "mean shape initialization" of the ERT. 3D means to initialize the ERT with the procedure in section 3.1. SE denotes using plain gray level features for the ERT whereas DE denotes using probability maps produced by the CNN to train the ERT. Finally CF stands for using the coarseto-fine scheme.</p><p>When combined with the cascaded ERT, the 3D initialization is key to achieve top overall performance, see CNN+MS+DE vs CNN+3D+DE in the full subset. The reason for this is that, in the 3D case, the initialization takes care of the rigid component of face pose so that the ERT cascade only models non-rigid deformations. Moreover, the projection of the 3D face model is a correct 2D shape, a requirement for the ERT to converge to a valid face shape <ref type="bibr" target="#b7">(Cao et al., 2014)</ref>. Of course, the 3D initialization is fundamental to achieve good performance in presence of large face rotations. So, it provides the largest improvement in the pose subset.</p><p>The use of CNN probability maps improves the NME in the full data set in about 20% (see CNN+3D+SE vs CNN+3D+DE). The large receptive fields of CNNs are specially helpful in challenging situations, specifically those in the pose and occlusion subsets.</p><p>The coarse-to-fine strategy in our cascaded ERT provides significative local improvements in difficult cases, with rare facial part combinations (see <ref type="figure">Fig. 6a</ref>). For this reason, the largest gain of CNN+3D+DE+CF vs CNN+3D+DE occurs in the expressions subset. Although this strategy provides improvements in all the data base subsets, the actual NME differences are washed out when averaged over the number of landmarks in the face and the number of images in the subset. They may be appreciated by looking into specific data subsets or samples (see <ref type="figure">Fig. 6a</ref>), such</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Full</head><p>Pose Expression Illumination Make-up Occlusion Blur corners corners corners corners corners corners corners N M E AU C 10 F R 10 N M E AU C 10 F R 10 N M E AU C 10 F R 10 N M E AU C 10 F R 10 N M E AU C 10 F R 10 N M E AU C 10 F R 10 N M E AU C 10 F R 10 ESR <ref type="bibr">(Cao et al., 2012) 11.13 27.74 35.24 25.88 1.77 90.18 11.47 19.81 42.04 10.49 29.53 30.80 11.05 24.85 38.84 13.75 19.46 47.28 12.20 22.04 41.40 SDM (Xiong et al.)</ref> 10.29 30.02 <ref type="bibr">29.40 24.10 2.26 84.36 11.45 22.93 33.44 9.32 32.37 26.22 9.38 31.25 27.67 13.03 20.60 41.85 11.28 23.98 35.32 CFSS (Zhu et al., 2015</ref><ref type="bibr">) 9.07 36.59 20.56 21.36 6.32 66.26 10.09 31.57 23.25 8.30 38.54 17.34 8.74 36.91 21.84 11.76 26.88 32.88 9.96 30.37 23.67 LAB (Wu et al., 2018</ref>   as the left eyebrow/eye location improvement in <ref type="figure">Fig. 6b and 6c</ref> (best viewed after zoom-in). Finally, we analyze the NME distribution produced by the rigid initialization and the final 3DDE model (see <ref type="figure">Fig. 7</ref>). Using the model trained for the WFLW experiment, we align the 2500 test samples of WFLW and plot the distribution of NMEs, produced both with the CNN+3D regressor (softPOSIT result) and the full CNN+3D+DE+CF regressor (3DDE result). The values of percentiles 10 and 90 of the NME distribution are 3.71 and 6.87 for the CNN+3D regressor and 1.03 and 3.32 for the CNN+3D+DE+CF one. So, on average, the full regressor reduces in about 60% the NME achieved by the rigid initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Cross-dataset evaluation</head><p>In this section we perform cross-dataset experiments to evaluate the quality of present benchmarks and the generalization of the regressors trained on them. Here we benefit from the fact that 3DDE may be trained in a semi-supervised way, i.e., using data sets with missing or unlabeled landmarks. To this end we select 24 distinct facial landmarks (see <ref type="figure" target="#fig_6">Fig. 8a</ref>). We consider them distinct because they may be accurately located by a human annotator. We train and evaluate 3DDE respectively with the training and test sets of each data base. We have also performed one more experiment training 3DDE with the training sets of all data bases and evaluating it successively with the tests sets of each of them, we denote this experiment with label All.</p><p>In <ref type="table" target="#tab_13">Table 7</ref> we show the results of our evaluation. The smallest data base, COFW, has the worst cross-dataset results. On the other hand, the data set with greatest diversity, WFLW, has the best results. Moreover, the model All, trained with the training sets of all data bases, is able to improve, in all cross-dataset experiments, the models trained in a single data set. However, the most prominent outcome of this experiment is that we always achieve the best result when training with the train subset of the same data base. This holds even when compared against the model trained with all data sets, confirming the existence of the so-called "data set bias" in current benchmarks <ref type="bibr" target="#b31">(Torralba and Efros, 2011)</ref>.</p><p>In a final experiment we use model All to evaluate the NME of each landmark using the test sets of all data sets (see <ref type="figure" target="#fig_6">Fig. 8b</ref>). The landmarks with highest NME are those related to the ears, the bottom of the mouth and the chin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have introduced 3DDE, a robust face alignment method that leverages on good properties of CNNs, cascade of ERT and 3D face models. The CNN provides robust landmark estimations    We use the height as normalization for the NME.   with weak face shape enforcement. The ERT is able to enforce the face shape and achieve better accuracy in landmark detection, but it only converges with a good initialization. Finally, 3D models exploit face orientation information to improve self-occlusion estimation. 3DDE is initialized by robustly fitting a 3D face model to the probability maps produced by the CNN. The 3D model enables 3DDE to handle self-occlusions and successfully deal with both frontal and profile faces. Once initialized, the cascade of ERT only models the non-rigid component of face motion. It provides various benefits, namely, it enforces shape consistency, may be trained with unlabeled landmarks, estimate landmark visibility and efficiently parallelize the execution of the regression trees within each stage. We have additionally introduced a coarse-to-fine scheme within the cascade of ERT that is able to deal with the combinatorial explosion of local parts deformation. In this case, the usual monolithic ERT will perform poorly when fitting faces with combinations of facial part deformations not present in the training set. This is a fundamental limitation of implicit shape models addressed by 3DDE.</p><formula xml:id="formula_15">N M E AU C 4 F R 4 N M E AU C 4 F R 4 N M E AU C 4 F R 4 N M E AU C 4 F R 4 N M E AU C 4 F R 4 N M E AU C 4 F R 4 N M E AU C 4 F R 4<label>CNN+3D+SE</label></formula><p>In the experiments we have shown that 3DDE improves, as far as we know, the state-of-theart performance in 300W, COFW, AFLW and WFLW data sets. In our ablation analysis we have shown that all the components of the system critically contribute to the final result.</p><p>The availability of large annotated data sets has encouraged research in this area with important performance improvements in recent years. However, as shown in <ref type="figure" target="#fig_8">Fig. 9</ref>, this problem is still far from being completely solved. A critical question here is whether the models trained with present data sets will generalize to the situations present in real-life operation. The cross-dataset experiments performed reveal the existence of a significant data set bias in present benchmarks that limit the generalization of models trained with them. So, further work in this direction is required to improve the performance of present face alignment algorithms.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Initialization algorithm (g 0 ) Input: P(I), X // Select coordinates of maximum probability {x(l) = arg max(P l (I))} L l=1 p * = 0 for z=1 to Z do // Select subset from distinct landmarks xs, Xs = chooseLandmarksSubset(x, X) // Compute projection matrix between xs, Xs R, t = softPOSIT(xs, Xs) // Project 3D face model using previous matrix xz, vz = projectPoints(X, R, t) // Evaluate the goodness of the initialization p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>3DDE framework diagram. GS, Max and RANSAC+POSIT represent the Gaussian smoothing filter, the maximum of each probability map and the robust 3D pose estimation respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>COFW. This benchmark, presented in Burgos-Artizzu et al. (2013) focuses on occlusion. Commonly, there are 1345 training faces in total. The testing set is made of 507 images. The annotations include the landmark positions and the binary occlusion labels for 29 points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Cumulative error distribution curves sorted by AUC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Example of a monolithic ERT regressor vs. our coarse-to-fine approach. (a) NME evolution through the stages in the cascade (left plot, 8 mouth landmarks for all test images in the expression subset; right plot, all 98 landmarks in one image). (b) predicted shape and zoom-in with a monolithic regressor. (c) predicted shape and zoom-in with our coarse-to-fine approach. Sample distribution of NMEs produced by the CNN+3D and 3DDE regressors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Location of distinct face landmarks and the NME related to each landmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Representative results considered errors using 3DDE in 300W, COFW, AFLW and WFLW testing subsets. Blue colour represents ground truth, green and red colours point out visible and non-visible shape predictions respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Error of face alignment methods on the 300W public test set.</figDesc><table><row><cell></cell><cell></cell><cell>Indoor</cell><cell></cell><cell cols="2">Outdoor</cell><cell></cell><cell></cell><cell>Full</cell></row><row><cell>Method</cell><cell cols="2">corners</cell><cell></cell><cell cols="2">corners</cell><cell></cell><cell></cell><cell>corners</cell></row><row><cell></cell><cell cols="9">N M E AU C8 F R8 N M E AU C8 F R8 N M E AU C8 F R8</cell></row><row><cell>ESR (Cao et al., 2012)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">32.35 17.00</cell></row><row><cell>cGPRT (Lee et al., 2015)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">41.32 12.83</cell></row><row><cell>CFSS (Zhu et al., 2015)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">39.81 12.30</cell></row><row><cell cols="2">MDM (Trigeorgis et al., 2016) -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">-5.05 45.32 6.80</cell></row><row><cell>DAN (Kowalski et al., 2017)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">-4.30 47.00 2.67</cell></row><row><cell>SHN (Yang et al., 2017)</cell><cell>4.10</cell><cell>-</cell><cell cols="2">-4.00</cell><cell>-</cell><cell cols="2">-4.05</cell><cell>-</cell><cell>-</cell></row><row><cell>DCFE (Valle et al., 2018)</cell><cell cols="9">3.96 52.28 2.33 3.81 52.56 1.33 3.88 52.42 1.83</cell></row><row><cell>3DDE</cell><cell cols="9">3.74 53.932.00 3.71 53.95 2.66 3.73 53.94 2.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Error of face alignment methods on the 300W private test set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">300W public</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">300W private</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.0</cell><cell cols="4">3DDE (46.90) (6.10)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell cols="4">3DDE (53.94) (2.33)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4 0.6 0.8 Images proportion</cell><cell cols="4">DCFE (45.71) (7.26) cGPRT (39.08) (14.08) DAN (39.00) (8.27) RCN (38.56) (11.61) ERT (31.23) (23.08) RCPR (28.91) (21.48)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4 0.6 0.8 Images proportion</cell><cell cols="4">DCFE (52.43) (1.83) DAN (46.96) (2.67) RCN (43.71) (2.50) cGPRT (41.32) (12.83) ERT (32.20) (19.00)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0 0.0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4 NME</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>0 0.0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4 NME</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">COFW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WFLW</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">3DDE (38.19) (6.51)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">3DDE (55.45) (5.04)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4 0.6 0.8 Images proportion</cell><cell cols="3">DCFE (35.86) (7.30) RCN (28.89) (13.61) ERT (21.11) (30.77)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4 0.6 0.8 Images proportion</cell><cell cols="3">LAB (53.26) (7.56) RCN (50.30) (6.88) ERT (36.82) (23.48)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0 0.0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4 NME</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>0 0.0</cell><cell>2</cell><cell></cell><cell>4</cell><cell>NME</cell><cell>6</cell><cell>8</cell><cell></cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Error of face alignment methods on COFW.</figDesc><table><row><cell></cell><cell cols="2">19 landmarks 21 landmarks</cell></row><row><cell>Method</cell><cell>height</cell><cell>height</cell></row><row><cell></cell><cell>N M E</cell><cell>N M E</cell></row><row><cell>PIFAS (Jourabloo et al., 2017)</cell><cell>-</cell><cell>4.45</cell></row><row><cell>CFSS</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Error of face alignment methods on AFLW.</figDesc><table><row><cell>6.407</cell><cell>20.798</cell><cell>24.565</cell><cell>16.859</cell></row><row><cell>3.829</cell><cell>13.105</cell><cell>6.719</cell><cell>8.168</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>5.27 53.23 7.56 10.24 23.45 28.83 5.51 49.51 6.37 5.23 54.33 6.73 5.15 53.94 7.77 6.79 44.90 13.72 6.32 46.30 10.74 3DDE 4.68 55.44 5.04 8.62 26.40 22.39 5.21 51.75 5.41 4.65 56.02 3.86 4.60 55.36 6.79 5.77 46.92 9.37 5.41 49.57 6.72</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Error of face alignment methods on WFLW.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>2.52 41.10 11.56 3.53 24.08 28.83 2.90 33.22 15.92 2.53 41.85 10.45 2.59 39.08 15.53 3.06 31.10 22.14 2.91 33.98 15.78 CNN+MS+DE 2.23 49.77 7.04 3.33 35.13 17.79 2.56 45.15 8.91 2.17 49.29 5.87 2.33 46.85 9.70 2.69 40.33 12.90 2.53 42.71 9.57 CNN+3D+DE 2.03 51.14 5.47 2.68 39.55 11.96 2.21 46.66 7.96 2.11 50.09 5.01 2.13 48.57 7.28 2.56 40.83 12.36 2.40 43.84 8.27 CNN+3D+DE+CF 2.01 51.67 5.20 2.63 39.90 10.73 2.15 48.19 5.73 2.06 50.79 4.87 2.12 49.05 7.28 2.54 40.94 12.22 2.39 43.93 8.02</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Ablation study. MS and 3D represent the 2D mean shape and 2D projections of the 3D mean face respectively. SE and DE represent the type of features used in the cascade being simple grayscale features and deep probability maps features respectively. The CNN+3D+DE+CF row represents the full 3DDE approach results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Cross-dataset experiment using only distinct landmarks to compute NME normalized by height.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">3D deeply-initialized EnsembleIn this section we present 3DDE. It consists of two main steps: CNN-based rigid face pose computation and ERT-based non-rigid face deformation estimation, both shown inFig. 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">5×5 images are reduced to 2×2 pixels applying a kernel size of 3×3</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: The authors gratefully acknowledge funding from the Spanish Ministry of Economy and Competitiveness, project TIN2016-75982-C2-2-R. They also thank the anonymous reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FREAK: fast retina keypoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust gender recognition by exploiting facial attributes dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bekios-Calfa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="228" to="234" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3726" to="3734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2887" to="2894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="484" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time facial feature detection using conditional regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2578" to="2585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Softposit: Simultaneous pose and correspondence determination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dementhon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="259" to="284" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1078" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic attentioncontrolled cascaded shape regression exploiting training data augmentation and fuzzy-set sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3681" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recombinator networks: Learning coarse-to-fine feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5743" to="5752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face alignment in-thewild: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pose-invariant face alignment with a single CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face alignment using k-cluster regression forests with weighted splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1567" to="1571" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep alignment network: A convolutional neural network for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2034" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disentangling 3d pose in a dendritic CNN for unconstrained 2d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face alignment using cascade gaussian process regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4204" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deep regression architecture with twostage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3691" to="3700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A model of the perception of facial expressions of emotion by humans: Research overview and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1589" to="1608" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face alignment via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1233" to="1245" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: database and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exemplar-based human action pose correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leyvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1053" to="1066" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of local feature methods for 3D face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soltanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boufama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A deeply-initialized coarseto-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valdés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="609" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection under significant head poses and occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent 3D-2D dual learning for large-pose facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1642" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentive-refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2664" to="2673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2025" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep deformation network for object landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="52" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exemplar-based cascaded stacked auto-encoder networks for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast and precise face alignment and 3D shape reconstruction from a single 2D image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision Workshops</title>
		<meeting>European Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="590" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Change</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3409" to="3417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face alignment in full pose range: A 3D total solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="78" to="92" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
