<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text Readability Assessment for Second Language Learners</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-18">18 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The ALTA Institute Computer Laboratory University of Cambridge Cambridge</orgName>
								<address>
									<postCode>CB3 0FD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Kochmar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The ALTA Institute Computer Laboratory University of Cambridge Cambridge</orgName>
								<address>
									<postCode>CB3 0FD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The ALTA Institute Computer Laboratory University of Cambridge Cambridge</orgName>
								<address>
									<postCode>CB3 0FD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text Readability Assessment for Second Language Learners</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-18">18 Jun 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the task of readability assessment for the texts aimed at second language (L2) learners. One of the major challenges in this task is the lack of significantly sized level-annotated data. For the present work, we collected a dataset of CEFR-graded texts tailored for learners of English as an L2 and investigated text readability assessment for both native and L2 learners. We applied a generalization method to adapt models trained on larger native corpora to estimate text readability for learners, and explored domain adaptation and self-learning techniques to make use of the native data to improve system performance on the limited L2 data. In our experiments, the best performing model for readability on learner texts achieves an accuracy of 0.797 and P CC of 0.938.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Developing reading ability is an essential part of language acquisition. However, finding proper reading materials for training language learners at a specific level of proficiency is a demanding and timeconsuming task for English instructors as well as the readers themselves. To automate the process of reading material selection and the assessment of reading ability for non-native learners, a system that focuses on text readability analysis for L2 learners can be developed. Such a system enhances many pedagogical applications by supporting readers in their second language education.</p><p>Text readability, which has been formally defined as the sum of all elements in textual ma-terial that affect a reader's understanding, reading speed, and level of interest in the material <ref type="bibr" target="#b7">(Dale and Chall, 1949)</ref>, is influenced by multiple variables. These may include the style of writing, its format and organization, reader's background and interest as well as various contextual dimensions of the text, such as its lexical and syntactic complexity, level of conceptual familiarity, logical sophistication and so on.</p><p>The choice of the criteria to measure readability often depends upon the need and characteristics of the target readers. Most of the studies so far have evaluated text difficulty as judged by native speakers, despite the fact that text comprehensibility can be perceived very differently by L2 learners. In the case of L2 learners, due to the difference in the pace of language acquisition, the focus in readability measures often differs from that for native readers. For example, the grammatical aspects of readability usually contribute more to text comprehensibility for L2 learners than the conceptual cognition difficulty of the reading material <ref type="bibr" target="#b11">(Heilman et al., 2007)</ref>. A system that is tailored towards learner's perception of reading difficulty can produce more accurate estimation of text reading difficulty for non-native readers and thus better facilitate language learning.</p><p>One of the major challenges for a data-driven approach to text readability assessment for L2 learners is that there is not enough significantly sized, properly annotated data for this task. At the same time, text readability assessment in general has been previously studied by many researchers and there are a number of existing corpora aimed at native speakers that can be used. To address the problem, we compiled a collection of texts that are tailored for L2 learners' readability and looked at several approaches to make use of existing native data to estimate readability for L2 learners.</p><p>In sum, the contribution of our work is threefold. First, we develop a system that produces state-ofthe-art estimation of text readability, exploit a range of readability measures and investigate their predictive power. Second, we focus on readability for L2 learners of English and present a level-graded dataset for non-native readability analysis. Third, we explore methods that help to make use of the existing native corpora to produce better estimation of readability when there is not enough data aimed at L2 learners. Specifically, we apply a generalization method to adapt models trained on native data to estimate text readability for learners, and explore domain adaptation and self-training techniques to improve system performance on the data aimed at L2 learners. To the best of our knowledge, these approaches have not been applied in readability experiments before. The best performing model in our experiments achieves an accuracy (ACC) of 0.797 and Pearson correlation coefficient (P CC) of 0.938.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Automated Readability Assessment</head><p>Many previous studies on text readability assessment have used machine learning based approaches, which enable investigation of a broader set of linguistic features. <ref type="bibr" target="#b21">Si and Callan (2001)</ref> and Collins- <ref type="bibr" target="#b4">Thompson and Callan (2004)</ref> were among the early works on statistical readability assessment. They applied unigram language models and naïve Bayes classification to estimate the grade level of a given text. Experiments showed that the language modelling approach yields better results in terms of accuracy than the traditional readability formulae, such as the the Flesch-Kincaid score <ref type="bibr" target="#b13">(Kincaid et al., 1975)</ref>. <ref type="bibr" target="#b19">Schwarm and Ostendorf (2005)</ref> extended this method to multiple language models. They combined traditional reading metrics with statistical language models as well as some basic parse tree features and then applied an SVM classifier. <ref type="bibr" target="#b11">Heilman et al. (2007;</ref><ref type="bibr" target="#b11">2008)</ref> expanded the feature set to include certain lexical and grammatical features extracted from parse trees while using a linear regression model to predict the grade level. <ref type="bibr" target="#b18">Pitler and Nenkova (2008)</ref> and <ref type="bibr" target="#b10">Feng et al. (2010)</ref> were the first to introduce discourse-based features into the framework. The experiments with discourse features demonstrated promising results in predicting the readability level of text for both classification and regression approaches. <ref type="bibr" target="#b12">Kate et al. (2010)</ref> looked at both the effect of the feature choice and the machine learning framework choice on performance, and found that the improvement resulting from changing the framework is smaller than that from changing the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Readability Assessment for L2 Learners</head><p>Most previous work on readability assessment is directed at predicting reading difficulty for native readers. Several efforts in developing automated readability assessment that take L2 learners into consideration have emerged since 2007. <ref type="bibr" target="#b11">Heilman et al. (2007)</ref> tested the effect of grammatical features for both L1 (first language) and L2 readers and found that grammatical features play a more important role in L2 readability prediction than in L1 readability prediction. <ref type="bibr" target="#b23">Vajjala and Meurers (2012)</ref> combined measures from Second Language Acquisition research with traditional readability features and showed that the use of lexical and syntactic features for measuring language development of L2 learners has a substantial positive impact on readability classification. They observed that lexical features perform better than syntactic features, and that the traditional features have a good predictive power when used with other features. <ref type="bibr" target="#b20">Shen et al. (2013)</ref> developed a language-independent approach to automatic text difficulty assessment for L2 learners. They treated the task of reading level assessment as a discriminative problem and applied a regression approach using a set of features that they claim to be language-independent. However, most of these studies have used textual data annotated with the readability levels for native speakers of English rather than L2 learners specifically.</p><p>While the majority of work on automated readability assessment are for English, studies on L2 readability in other languages, including French (François and Fairon, 2012), Portuguese <ref type="bibr" target="#b0">(Branco et al., 2014)</ref>, and Swedish   <ref type="bibr">(Pilán et al., 2015)</ref>, are also emerging. These studies generally use textbook materials with readability levels assigned by publishers or language instructors.</p><p>Overall, study of automatic readability analysis for L2 learners is still in its early stages, mainly due to the lack of available well-labelled data annotated with the readability levels for L2 learners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Native Data: the WeeBit Corpus</head><p>Among the existing publicly available corpora, the WeeBit corpus created by <ref type="bibr" target="#b23">Vajjala and Meurers (2012)</ref> is one of the largest datasets for readability analysis. The WeeBit corpus is composed of articles targeted at readers of different age groups from two sources, the Weekly Reader magazine and the BBC-Bitesize website. Within the dataset, the Weekly Reader data consists of texts covering ageappropriate non-fictional content for four grade levels, corresponding to children of ages between 7-8, 8-9, 9-10 and 10-12 years old. The BBC-Bitesize website data is targeted at two grade levels, for ages between 11-14 and 14-16. The two datasets are merged to form the WeeBit corpus, with the targeted ages used to assign readability levels.</p><p>A copy of the original WeeBit corpus was obtained from the authors <ref type="bibr" target="#b23">(Vajjala and Meurers, 2012)</ref>. The texts are webpage documents stored in raw HTML format. We have identified that some texts contain broken sentences or extraneous content from the webpages, such as copyright declaration and links, that correlate with the target labels in a way which is likely to artificially boost performance on the task and would not generalize well to other datasets. To avoid that, we re-extracted texts from the raw HTML and discarded text documents that do not contain proper reading passages. <ref type="table" target="#tab_1">Table 1</ref> shows the distribution of texts in the modified dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">L2 Data: the Cambridge Exams dataset</head><p>Most work on readability assessment has been done on native corpora with age-specific reading levels <ref type="bibr" target="#b19">(Schwarm and Ostendorf, 2005;</ref><ref type="bibr" target="#b10">Feng et al., 2010)</ref>. Such texts are aimed not at L2 learners but rather at native-speaking children of different ages. Therefore, the level annotation in such texts is arrived at using criteria different from those that are relevant for L2 readers. The lack of significantly sized L2 level-annotated data raises a problem for readability analysis aimed at L2 readers. To tackle this, we created a dataset with texts tailored for L2 learners' readability specifically.</p><p>We have collected a dataset composed of reading passages from the five main suite Cambridge English Exams (KET, PET, FCE, CAE, CPE). 1 These five exams are targeted at learners at A2-C2 levels of the Common European Framework of Reference (CEFR) <ref type="bibr">(Council of Europe, 2001)</ref>. <ref type="bibr">2</ref> The documents are harvested from all the tasks in the past reading papers for each of the exams. The Cambridge English Exams are designed for L2 learners specifically and the A2-C2 levels assigned to each reading paper can be treated as the level of reading difficulty of the documents for the L2 learners. 3 Table 2 shows the number of documents at each CEFR level across the dataset. The data will be available at http://www.cl.cam.ac.uk/˜mx223/cedata.html.</p><p>Experimenting on the language testing data annotated with the L2 learner readability levels is one of the contributions of this research. Most previous work on readability assessment for English have relied on the data annotated with readability levels aimed at native speakers. In this work, we use language testing data with the levels assigned based on L2 learner levels, and we believe that this level annotation is more appropriate for text readability assessment for L2 learners than using texts with the level annotation aimed at native speakers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Readability Measures</head><p>This section describes the range of linguistic features explored and the machine learning framework applied to the WeeBit data that constitute a general readability assessment system. The set of features used in our experiments is an extension to those used in previous work <ref type="bibr" target="#b10">(Feng et al., 2010;</ref><ref type="bibr" target="#b18">Pitler and Nenkova, 2008;</ref><ref type="bibr" target="#b23">Vajjala and Meurers, 2012;</ref><ref type="bibr">Vajjala and Meurers, 2014)</ref>, and their predictive power for reading difficulty assessment is investigated in our experiments. We have extended the feature set with the EVP-based features, GRbased complexity measures and the combination of language modeling features that have not been applied to readability assessment before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Features</head><p>Traditional Features The traditional features are easy-to-compute representations of superficial aspects of text. The metrics that are considered include: the number of sentences per text, average and maximum number of words per sentence, average number of characters per word, and average number of syllables per word. Two popular readability formulas are also included: the Flesch-Kincaid score <ref type="bibr" target="#b13">(Kincaid et al., 1975)</ref> and the Coleman-Liau readability formula <ref type="bibr" target="#b3">(Coleman and Liau, 1975)</ref>.</p><p>Lexico-semantic Features Vocabulary knowledge is one of the most important aspects of reading comprehension (Collins-Thompson, 2014). Lexicosemantic features provide information about the difficulty or familiarity of vocabulary in the text.</p><p>A widely used lexical measure is the type-token ratio (TTR), which is the ratio of the number of unique word tokens (referred to as types) to the total number of word tokens in a text. However, the conventional TTR is influenced by the length of the text. Root TTR and Corrected TTR, which take the logarithm and square root of the text length instead of the direct word count as denominator, can produce a more unbiased representation and are included in the experiment.</p><p>Part of speech (POS) based lexical variation and lexical density measures <ref type="bibr" target="#b14">(Lu, 2011)</ref> are also examined. Lexical variation is defined as the typetoken ratio of lexical items such as nouns, adjectives, verbs, adverbs and prepositions. Lexical density is defined as the proportion of the five classes of lexical items in all word tokens. The percentage of content words (nouns, verbs, adjectives and adverbs) and function words (all the remaining POS types) are two other indicators of lexical density. <ref type="bibr" target="#b23">Vajjala and Meurers (2012;</ref> reported in their readability classification experiment that the proportion of words in the text that are found in the Academic Word List is one of the most predictive measures among all the lexical features they considered. The Academic Word List <ref type="bibr" target="#b6">(Coxhead, 2000)</ref> is comprised of words that frequently occur across all topic ranges in an academic text corpus. The proportion of academic vocabulary words in the text can be viewed as another measure of lexical complexity.</p><p>A similar but more refined approach to estimate lexical complexity is based on the use of the English Vocabulary Profile (EVP). <ref type="bibr">4</ref> The EVP is an online vocabulary resource that contains information about which words and phrases are acquired by learners at each CEFR level. It is collected from the Cambridge Learner Corpus (CLC), a collection of examination scripts written by learners from all over the world <ref type="bibr" target="#b1">(Capel, 2012)</ref>. It provides a more fine-grained lexical complexity measure that captures the relative difficulty of each word by assigning the word difficulty to one of the six CEFR levels. Additionally, the EVP indicates the word difficulty for L2 learners rather than native speakers, which makes it more informative in non-native readability analysis. In our experiments, the proportion of words at each CEFR level is calculated and added to the feature set.</p><p>Parse Tree Syntactic Features A number of syntactic measures based on the RASP parser output <ref type="bibr">(Briscoe et al., 2006)</ref> are used to describe the grammatical complexity of text, including average parse tree depth, and average number of noun, verb, adjective, adverb, prepositional phrases and clauses per sentence.</p><p>Grammatical relations (GR) between constituents in a sentence may also affect the judgement of syntactic difficulty. <ref type="bibr" target="#b26">Yannakoudakis (2013)</ref> applied 24 GR-based complexity measures in essay scoring and showed good results. These complexity measures capture the grammatical sophistication of the text through the representation of the distance between the sentence constituents. For instance, these measures calculate the longest/average distance in the GR sets generated by the parser and the average/maximum number of GRs per sentence. A set of 24 GR-based measures used by <ref type="bibr" target="#b26">Yannakoudakis (2013)</ref> are generated by RASP for each sentence. We take the average of these measures across the text to incorporate the GR-related aspect of its syntactic difficulty.</p><p>Other types of complexity measures that are derived from the parser output include: cost metric, which is the total number of parsing actions performed for generating the parse tree; ambiguity of the parse, and so on. A total number of 114 non-GR based complexity measures are extracted. These complexity measures are averaged across the text and used to model finer details of the syntactic difficulty of the text.</p><p>Language Modeling Features Statistical language modeling (LM) provides information about distribution of word usage in the text and is in fact another way to describe the lexical dimension of readability. To avoid over-fitting to the WeeBit data, two types of language modeling based features are extracted using the SRILM toolkit <ref type="bibr" target="#b22">(Stolcke, 2002)</ref>:</p><p>(1) word token n-gram models, with n ranging from 1 to 5, trained on the British National Corpus (BNC), and (2) POS n-grams, with n ranging from 1 to 5, trained on the five levels in the WeeBit corpus itself. The LMs are used to score the text with loglikelihood and perplexity.</p><p>Discourse-based Features Discourse features measure the cohesion and coherence of the text. Three types of discourse-based features are used.</p><p>(1) Entity density features Previous work by <ref type="bibr" target="#b10">Feng et al. (2009;</ref><ref type="bibr" target="#b10">2010)</ref> has shown that entity density is strongly associated with text comprehension. An entity set is a union of named entities and general nouns (including nouns and proper nouns) contained in a text, with overlapping general nouns removed. Based on this, 9 en-tity density features, including the total number of all/unique entities per document, the average number of all/unique entities per sentence, percentage of named entities per sentence/document, percentage of named entities in all entities, percentage of overlapping nouns removed, and percentage of unique named entities in all unique entities, are calculated.</p><p>(2) Lexical chain features Lexical chains model the semantic relations among entities throughout the text. The lexical chaining algorithm developed by Galley and <ref type="bibr">McKeown (2003)</ref> is implemented. The semantically related words for the nouns in the text, including synonyms, hypernyms, and hyponyms, are extracted from the WordNet <ref type="bibr" target="#b15">(Miller, 1995)</ref>. Then for each pair of the nouns in the text, we check whether they are semantically related. Finally, lexical chains are built by linking semantically related nouns in text. A set of 7 lexical chain-based features are computed, including total number of lexical chains per document, total number of lexical chains normalized with text length, average/maximum lexical chain length, average/maximum lexical chain span, and the number of lexical chains that span more than half of the document. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3) Entity grid features</head><p>Another entity-based approach to measure text coherence is the entity grid model introduced by <ref type="bibr" target="#b0">Barzilay and Lapata (2008)</ref>. They represented each text by an entity grid, which is a two-dimensional array that captures the distribution of discourse entities across text sentences. Each grid cell contains the grammatical role of a particular entity in the specified sentence: whether it is a subject (S), object (O), neither a subject nor an object (X), or absent from the sentence (-). A local entity transition is defined as the transition of the grammatical role of an entity from one sentence to the following sentence. In our experiments, we used the Brown Coreference Toolkit v1.0 <ref type="bibr" target="#b9">(Eisner and Charniak, 2011)</ref> to generate the entity grid for the documents. The probabilities of the 16 types of local entity transition patterns are calculated to represent the coherence of the text.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation and Evaluation</head><p>In our experiments, we cast readability assessment as a supervised machine learning problem. In particular, a pairwise ranking approach is adopted and compared with a classification method. We believe that the reading difficulty of text is a continuous rather than discrete variable. Text difficulty within a level can also vary. Instead of assigning an absolute level to the text, treating readability assessment as a ranking problem allows prediction of the relative difficulty of pairs of documents, which captures the gradual nature of readability better. Because of this, we hypothesize that the ranking model can generalize better to unseen texts and texts with different level annotation. Support vector machines (SVM) have been used in the past for readability assessment by many researchers and have consistently yielded better results when compared to other statistical models for the task <ref type="bibr" target="#b12">(Kate et al., 2010)</ref>. We use the LIB-SVM toolkit <ref type="bibr" target="#b2">(Chang and Lin, 2011)</ref> to implement both multi-class classification and pairwise ranking. Five-fold cross validation is used for evaluation. We report two popular performance metrics, accuracy (ACC) and Pearson correlation coefficient (P CC), and use pairwise accuracy to evaluate ranking models. Pairwise accuracy is defined as the percentage of instance pairs that the model ranked correctly. It should be noted that accuracy and pairwise accuracy are not directly comparable. Thus, P CC is introduced to compare the results of the classification and the ranking models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>In predicting the text reading difficulty on the WeeBit data, the best result is achieved with a combination of all features and a classification model, with ACC=0.803 and P CC=0.900. We performed ablation tests and found that all feature sets have contributed to the overall model performance. Although there have been readability assessment studies on similar datasets, the results obtained in our experiments are not directly comparable to those. One of the major reasons is the modifications that we have made to the corpus (as discussed in Section 3.1). <ref type="bibr" target="#b23">Vajjala and Meurers (2012)</ref> reported that a multilayer perceptron classifier using three traditional metrics alone yielded an accuracy of 70.3% on their version of the WeeBit corpus. Their final system achieved a classification accuracy of 93.3% on the five-class corpus. Nonetheless, the best system in our experiments yields results competitive to most existing studies. For reference, <ref type="bibr" target="#b10">Feng et al. (2010)</ref> reported an accuracy of 74.01% using a combination of discourse, lexical and syntactic features for readability classification on their Weekly Reader Corpus and an accuracy of 63.18% when using all feature sets described in <ref type="bibr" target="#b19">Schwarm et al. (2005)</ref>.</p><p>Comparing the classification and the ranking models, we note that the results of the two models vary across feature sets and none of the two models is consistently better than the other. When all features are combined, the classification model outperforms the ranking one. It suggests that a ranking model is not necessarily the best model in predicting readability overall when trained and tested on the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Readability Assessment on L2 Data</head><p>So far we have studied the effect of various readability measures on the task of readability assessment and built two different types of models to predict text difficulty. However, the WeeBit corpus consists of texts aimed at native speakers of different ages rather than at L2 readers. Although there are certain similarities concerning reading comprehension between these two groups, the perceived difficulty of texts can be very different due to the difference in the pace and stages of language acquisition. Since the goal of our research is to automatically detect readability levels for language learners, it would be more helpful to work with data that are directly annotated with reading difficulty for L2 learners.</p><p>Ideally, it would be good to train a model di-   rectly on text annotated with L2 levels and then use this model to estimate readability for the new texts. However, the Cambridge Exams data we have compiled is relatively small, and the model trained on it will likely not generalize well. Therefore, we examined several approaches to make use of the WeeBit corpus for readability assessment on the L2 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Generalization Experiment</head><p>First, we tested the generalization ability of the classification and ranking models trained on the WeeBit corpus on the Cambridge Exams data to see if it is possible to directly apply the models trained on native data to L2 data. <ref type="table" target="#tab_7">Table 4</ref> reports the results.</p><p>In the case of the multi-class classification model, the accuracy dropped greatly when the model is applied to the L2 dataset, while the correlation remained relatively high. Looking at the confusion matrix of the classifier's predictions on the L2 data (see <ref type="table" target="#tab_8">Table 5</ref>), we notice that most of the documents in the L2 data are classified into the higher levels of WeeBit by the model. This is because, on average, the Cambridge Exams texts are more difficult than the WeeBit corpus ones which are generally targeted at children of young ages. Thus, the mismatch between the targeted levels has led to poor generalization of the classification model.</p><p>In contrast, for the ranking model, both evaluation measures are relatively unharmed when the model is applied to the L2 data. It shows that, when generalizing to an unseen dataset, the estimation produced by the ranking model is able to maintain a high pairwise accuracy and correlation with the ground truth.</p><p>We believe that this is because the ranking model does not try to band the documents into one of the levels on a different basis of difficulty annotation. Instead, pairwise ranking captures the relative reading difficulty of the documents, and therefore the resulting ranked positions of the documents are closer to the ground truth compared to the classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Mapping Ranking Scores to CEFR Levels</head><p>From the generalization experiment we can conclude that ranking is more accurate in predicting the CEFR levels of unseen learner texts than classification. Therefore, it is more appropriate to make use of the more informative ranking scores produced by the ranking model to learn a function that bands the scores into CEFR levels.</p><p>In learning the mapping function, we adopted a five-fold cross-validation approach. We split the Cambridge Exams dataset into five cross validation folds, with approximately equal number of documents at each level in each fold. A mapping function that converts ranking scores into CEFR levels is learnt from training folds and then tested on the validation fold in each run. The final results are averaged across the runs.</p><p>We compared three groups of methods to learn the mapping function.</p><p>(1) Regression and rounding: A regression function is learnt from the ranking scores and the ground truth labels on the training part of the dataset and then applied to the validation part. The mapped CEFR prediction is then rounded to its closest integer and clamped to range <ref type="bibr">[1,</ref><ref type="bibr">5]</ref>. Both linear regression and polynomial regression models are considered. The intuition behind using polynomial functions instead of a simple linear function for mapping is that the correlation of ranking scores and CEFR levels is not necessarily linear so a non-linear function might be more suitable for this task.</p><p>(2) Learning the cut-off boundary: We learn a separation boundary that bands the ranking scores to levels by maximizing the accuracy of such separation. For instance, we consider the ranked documents as a list with descending readability, with their ranking scores following the same order. If we could find a suitable cut-off boundary between each two adjacent levels in the list, then every docu-  <ref type="table">Table 6</ref>: Results of mapping ranking scores to CEFR levels ment above the boundary would fall into the higher level, and all documents below the boundary into the lower level. In this way, the ranked documents are banded into five levels with four separation boundaries learnt.</p><p>(3) Classification on the ranking scores: The task can also be addressed as a classification problem. The ranking scores can be considered as a single dimensional feature and CEFR levels as the target value. Here, two approaches are adopted and compared, logistic regression and a linear SVM. As a matter of fact, the SVM approach can be considered as a variation of learning a separation boundary, as it tries to find an optimal decision boundary between the classes. <ref type="table">Table 6</ref> shows the results of the three mapping methods.</p><p>Among the three approaches for mapping ranking scores to CEFR levels (regression-based, separation boundary-based, and classification-based), the classification ones showed better results than the others in terms of accuracy. Though not as high in accuracy as the SVM, a polynomial mapping function 6 also yielded very good results in terms of P CC. Compared to the other two methods, the separation boundary-based approach performs better than a linear regression function but fails to match the polynomial regression and classification-based methods. Nonetheless, all three approaches considerably outperformed the naive generalization of the classification model from the WeeBit corpus to the Cambridge Exams data. These improvements are statistically significant at p&lt;0.05 level. 7 6 A 4th order polynomial function is adopted because it yields better results compared to other orders. <ref type="bibr">7</ref> Throughout this paper, we test significance using t-test for ACC and Williams' test <ref type="bibr" target="#b25">(Williams, 1959)</ref>   Another way to make use of the native data is to treat the task as a domain adaptation problem, where the WeeBit corpus is taken as the source domain, and the L2 data as the target domain. The idea behind this is to use out-of-domain training data to boost the performance on limited in-domain data. EasyAdapt <ref type="bibr" target="#b8">(Daumé III, 2007)</ref> is one of the best performing domain adaptation algorithms. It has previously been applied to essay scoring and showed good results <ref type="bibr" target="#b16">(Phandi et al., 2015)</ref>. In a two domain case, EasyAdapt expands the input feature space from R F to R 3F , and then applies two mapping functions Φ S (x) = x, x, 0 and Φ T (x) = x, 0, x on source domain data and target domain data input vectors respectively. Here, 0 = 0, ...0 ∈ R F is the zero vector. In this manner, the instance feature vectors from the WeeBit corpus and Cambridge Exams datases are augmented to three times their original dimensionality. The augmented feature space captures both general and domain specific information and is thus capable of generalizing source domain knowledge to facilitate estimation on the target domain. As there is a mismatch between the levels on native and L2 data, the pairwise ranking algorithm needs to be adapted to ensure that the preference pairs are only created from the same domain. A five-fold cross-validation is used as in previous experiments. <ref type="table" target="#tab_11">Table 7</ref> shows the results of applying EasyAdapt with the ranking model. For comparison, we also present the results obtained when we apply the model trained on the native data to the L2 data directly, and the results obtained when we train the ranking model on the L2 data only. We can see that ranking with EasyAdapt outperforms the naive generalization approach significantly (p&lt;0.05), but it does not beat the results obtained when training a model on L2 data directly.</p><p>After applying the ranking model with Levels 1 2 3 4 5 A2 11 3 0 0 0 B1 2 9 0 1 0 B2 0 0 13 0 2 C1 0 0 2 9 2 C2 0 0 0 4 10 EasyAdapt, the ranking scores can be converted to CEFR levels using the same methods as described in Section 5.2. The best mapped CEFR estimation is achieved with a linear SVM classifier on the ranking score, reaching an ACC of 0.707 and P CC of 0.899. Compared to the naive generalization of the classification model from native to L2 data, the mapped estimation is less influenced by the mismatch between difficulty levels in the two domains (see <ref type="table" target="#tab_12">Table 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Using Self-training to Enhance the Classification Model</head><p>In addition to the domain adaptation, we experimented with self-training to boost the performance on the limited L2 data with the native data. To the best of our knowledge, neither of the approaches has been applied to readability assessment before. Self-training is a commonly used semi-supervised machine learning algorithm that aims to use the large amount of unlabelled data to help build a better classifier on a small amount of labeled data <ref type="bibr" target="#b27">(Zhu, 2005)</ref>. When using native data to boost model performance on L2 data with self-training, the L2 data is regarded as labeled instances, and the native data as unlabeled ones. A model is trained on the L2 data and then used to score the native data. The most confident K instances as well as their labels are added to the training set. Then the model is re-trained and the procedure is repeated. A five-fold cross-validation is used in evaluation as before.</p><p>We have experimented with a grid search on K's and the number of iterations, and found out that whatever the choice of the parameters is, the model performance degrades with self-training when the unlabeled instances are added blindly to all levels of the L2 dataset. Taking into account the mismatch in the difficulty levels between the native and L2 texts, we adapted the algorithm to add the unlabeled data only to the lower three levels of the L2 dataset. The Type ACC PCC L2 data only 0.785 0.924 self-training 0.797 0.938 <ref type="table">Table 9</ref>: Results of self-training best result is achieved with K=10 and 9 iterations, with 270 texts added in total (as shown in <ref type="table">Table 9</ref>). It seems reasonable to compare the results of this approach to those obtained with a model that is trained directly on the L2 data. Hence, we include the results of this model in <ref type="table">Table 9</ref> for comparison.</p><p>The results show that self-training can significantly (p&lt;0.05) help estimating readability for L2 texts by including a certain amount of unlabeled data (in this case, the native data) in training. However, the range of the reading difficulty covered by the unlabeled data may influence the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We investigated text readability assessment for both native and L2 learners. We collected a dataset with text tailored for language learners' readability and explored methods to adapt models trained on larger existing native corpora in estimating text reading difficulty for learners. In particular, we developed a system that achieves state-of-the-art performance in readability estimation, with ACC=0.803 and P CC=0.900 on native data, and ACC=0.785 and P CC=0.924 on L2 data, using a linear SVM. We compared a ranking model against the classification model for the task and showed that although a ranking model does not necessarily outperform a classification one in readability assessment on the same data, it is more accurate when generalizing to an unseen dataset. Following this, we showed that, by applying a ranking model and then learning a mapping function, the model trained on the native data can be applied to estimate the CEFR levels of unseen text effectively. This model achieves an accuracy of 0.622 and P CC of 0.864, and considerably outperforms the naive generalization of the classification model, which achieves an accuracy of 0.233 and P CC of 0.730.</p><p>In addition, we experimented with domain adaptation and self-training approaches to make use of the more plentiful native data to produce better estimation of readability when the L2 data is limited.</p><p>When treating the native data as a source domain and L2 data as a target domain, applying the EasyAdapt algorithm for ranking achieves an accuracy of 0.707 and P CC=0.899. The best result is achieved by using self-training to include native data as unlabelled data in training the classification model, with ACC=0.797 and P CC=0.938.</p><p>Future work will focus on the improvement of readability assessment framework for L2 learners and the identification of the optimal feature set that can generalize well to unseen text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of documents in the original and modifiedWeeBit corpus</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.of text 14.75 19.48 38.07 45.76 39.97    </figDesc><table><row><cell>Exams</cell><cell>KET</cell><cell>PET</cell><cell>FCE</cell><cell>CAE</cell><cell>CPE</cell></row><row><cell>targeted level</cell><cell>A2</cell><cell>B1</cell><cell>B2</cell><cell>C1</cell><cell>C2</cell></row><row><cell># of docs</cell><cell>64</cell><cell>60</cell><cell>71</cell><cell>67</cell><cell>69</cell></row><row><cell>avg. len</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics for the Cambridge English Exams data</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Classification and ranking results on the WeeBit corpus with feature sets grouped by their type</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Generalization results of the classification and ranking</figDesc><table><row><cell cols="2">models trained on native data applied to language testing data</cell></row><row><cell cols="2">Levels 1 2 3 4 5</cell></row><row><cell>A2</cell><cell>4 0 55 4 1</cell></row><row><cell>B1</cell><cell>0 0 24 6 30</cell></row><row><cell>B2</cell><cell>0 1 1 4 65</cell></row><row><cell>C1</cell><cell>0 0 0 3 64</cell></row><row><cell>C2</cell><cell>0 0 0 0 69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Confusion matrix of the classification model on the</figDesc><table><row><cell>language testing data</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>for P CC.</figDesc><table><row><cell></cell><cell cols="2">pairwise ACC PCC</cell></row><row><cell>EasyAdapt</cell><cell>0.933</cell><cell>0.905</cell></row><row><cell>native data only</cell><cell>0.913</cell><cell>0.880</cell></row><row><cell>L2 data only</cell><cell>0.943</cell><cell>0.913</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Results of domain adaptation from native to language</figDesc><table><row><cell>testing data</cell></row><row><cell>5.3 Domain Adaptation from Native to L2 Data</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Confusion matrix of the mapped estimation after EasyAdapt application on one of the cross-validation folds</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.cambridgeenglish.org 2 The CEFR determines foreign language proficiency at six levels in increasing order: A1 and A2, B1 and B2, C1 and C2.3  We are aware that the type of the task may also have an effect on the reading difficulty of the texts, but this is ignored at this stage.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.englishprofile.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The length of a chain is the number of entities contained in the chain. The span of a chain is the distance between the indexes of the first and the last entities in the chain.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Cambridge Assessment for their assistance in the collection of the language testing data. We would like to express our gratitude to Sowmya Vajjala and Detmar Meurers for sharing the WeeBit corpus with us. We are also grateful to the reviewers for their useful comments. We thank Lucy Cavendish College for their support in the publication.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Assessing automatic text classification for interactive language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata ; António</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Branco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Interactive presentation sessions</title>
		<editor>Briscoe et al.2006] Ted Briscoe, John Carroll, and Rebecca Watson</editor>
		<meeting>the COLING/ACL on Interactive presentation sessions</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="77" to="80" />
		</imprint>
	</monogr>
	<note>2014 International Conference on</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Completing the English Vocabulary Profile: C1 and C2 vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><surname>Capel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">English Profile Journal</title>
		<imprint>
			<biblScope unit="issue">e1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">and Lin2011</title>
		<imprint>
			<publisher>TIST</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A computer readability formula designed for machine scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liau1975] Meri</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ta</forename><forename type="middle">Lin</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="284" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computational assessment of text readability: A survey of current and future research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Callan2004] Kevyn</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>Collins-Thompson2014] Kevyn Collins-Thompson</editor>
		<meeting>North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="97" to="135" />
		</imprint>
	</monogr>
	<note>A Language Modeling Approach to Predicting Reading Difficulty</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Common European Framework of Reference for Languages: Learning, Teaching, Assessment</title>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
		<respStmt>
			<orgName>of Europe2001] Council of Europe</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Averil</forename><surname>Coxhead</surname></persName>
		</author>
		<title level="m">A new academic word list. TESOL Quarterly</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="213" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The concept of readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chall1949] Edgar</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanne</forename><forename type="middle">S</forename><surname>Chall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elementary English</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extending the entity grid with entityspecific features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lijun Feng, Martin Jansche, Matt Huenerfauth, and Noémie Elhadad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1486" to="1488" />
		</imprint>
	</monogr>
	<note>Improving word sense disambiguation in lexical chaining</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining lexical and grammatical features to improve readability measures for first and second language texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Heilman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>Heilman et al.2008] Michael Heilman, Kevyn Collins-Thompson, and Maxine Eskenazi</editor>
		<meeting>North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
	<note>Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to predict readability using diverse linguistic features</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="546" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy enlisted personnel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kincaid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Corpus-Based Evaluation of Syntactic Complexity Measures as Indices of College-Level ESL Writers&apos; Language Development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TESOL Quarterly</title>
		<imprint>
			<biblScope unit="page" from="36" to="62" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flexible Domain Adaptation for Automated Essay Scoring Using Correlated Linear Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A readable read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Ildikó Pilán, Sowmya Vajjala, and Elena Volodina. To appear in Research in Computing Science</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revisiting readability: A unified framework for predicting text quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
	<note>Pitler and Nenkova2008</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reading level assessment using support vector machines and statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">E</forename><surname>Ostendorf2005</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Schwarm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Language-Independent Approach to Automatic Text Difficulty Assessment for Second-Language Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations</title>
		<meeting>the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A statistical model for scientific readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Callan2001] Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="574" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Spoken Language Processing</title>
		<meeting>the International Conference on Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On improving the accuracy of readability classification using insights from second language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Vajjala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="163" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Readability assessment for text simplification: From analysing documents to identifying sentential simplifications</title>
	</analytic>
	<monogr>
		<title level="m">Sowmya Vajjala and Detmar Meurers</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="194" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">James</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Williams</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
			<publisher>Wiley</publisher>
			<biblScope unit="volume">14</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Automated assessment of English-learner writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<idno>UCAM-CL-TR-842</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<idno>1530</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Computer Sciences, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
