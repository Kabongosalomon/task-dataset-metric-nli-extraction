<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When AWGN-based Denoiser Meets Real Noises</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
							<email>yuqian2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP Group</orgName>
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Megvii Research 4 Stony Brook</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Megvii Research 4 Stony Brook</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP Group</orgName>
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFP Group</orgName>
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">When AWGN-based Denoiser Meets Real Noises</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Discriminative learning based image denoisers have achieved promising performance on synthetic noises such as Additive White Gaussian Noise (AWGN). The synthetic noises adopted in most previous work are pixel-independent, but real noises are mostly spatially/channel-correlated and spatially/channel-variant. This domain gap yields unsatisfied performance on images with real noises if the model is only trained with AWGN. In this paper, we propose a novel approach to boost the performance of a real image denoiser which is trained only with synthetic pixel-independent noise data dominated by AWGN. First, we train a deep model that consists of a noise estimator and a denoiser with mixed AWGN and Random Value Impulse Noise (RVIN). We then investigate Pixel-shuffle Down-sampling (PD) strategy to adapt the trained model to real noises. Extensive experiments demonstrate the effectiveness and generalization of the proposed approach. Notably, our method achieves state-of-theart performance on real sRGB images in the DND benchmark among models trained with synthetic noises. Codes are available at https://github.com/yzhouas/PD-Denoising-pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>As a fundamental task in image processing and computer vision, image denoising has been extensively explored in the past several decades even for downstream applications <ref type="bibr" target="#b20">(Zhou, Liu, and Huang 2018;</ref>. Traditional methods including the ones based on image filtering <ref type="bibr" target="#b2">(Dabov et al. 2008)</ref>, low rank approximation <ref type="bibr" target="#b4">(Gu et al. 2014;</ref><ref type="bibr" target="#b12">Xu et al. 2017;</ref><ref type="bibr" target="#b14">Yair and Michaeli 2018)</ref>, sparse coding <ref type="bibr" target="#b2">(Elad and Aharon 2006)</ref>, and image prior (Ulyanov, Vedaldi, and Lempitsky 2017) have achieved satisfactory results on synthetic noise such as Additive White Gaussian Noise (AWGN). Recently, deep CNN has been applied to this task, and discriminative-learning-based methods such as DnCNN <ref type="bibr" target="#b16">(Zhang et al. 2017a</ref>) outperform most traditional methods on AWGN denoising.</p><p>Unfortunately, while these learning-based methods work well on the same type of synthetic noise that they were <ref type="figure">Figure 1</ref>: Basic idea of the proposed adaptation method: Pixel-shuffle Down-sampling (PD). Spatially-correlated real noise (Left) is broken into spatially-variant pixelindependent noise (Middle) to approximate spatially-variant Gaussian noise <ref type="bibr">(Right)</ref>. Then an AWGN-based denoiser can be applied to such real noise accordingly. trained on, their performance degrades rapidly on real images, showing poor generalization ability in real world applications. This indicates that these data-driven denoising models are highly domain-specific and non-flexible to transfer to other noise types beyond AWGN. To improve model flexibility, the recently-proposed FFDNet <ref type="bibr" target="#b18">(Zhang, Zuo, and Zhang 2018</ref>) trains a conditional non-blind denoiser with a manually adjusted noise-level map. By giving high-valued uniform maps to FFDNet, only over-smoothed results can be obtained in real image denoising. Therefore, blind denoising of real images is still very challenging due to the lack of accurate modeling of real noise distribution. These unknown real-world noises are much more complex than pixel-independent AWGN. They can be spatially-variant, spatially-correlated, signal-dependent, and even device-dependent.</p><p>To better address the problem of real image denoising, current attempts can be roughly divided into the following categories: (1) realistic noise modeling <ref type="bibr" target="#b12">(Shi Guo 2018;</ref><ref type="bibr" target="#b0">Brooks et al. 2019;</ref><ref type="bibr" target="#b0">Abdelhamed, Timofte, and Brown 2019)</ref>, (2) noise profiling such as multi-scale <ref type="bibr">(Lebrun, Colom, and Morel 2015a;</ref><ref type="bibr" target="#b14">Yair and Michaeli 2018)</ref>, multi-channel <ref type="bibr" target="#b12">(Xu et al. 2017</ref>) and regional based <ref type="bibr">(Liu et al. 2017) settings, and (3)</ref> data augmentation techniques such as the adversariallearning-based ones ). Among them, CBD-Net (Shi Guo 2018) achieves good performance by modeling the realistic noise using the in-camera pipeline model proposed in <ref type="bibr" target="#b6">(Liu et al. 2008)</ref>. It also trains an explicit noise estimator and sets a larger penalty for under-estimated noise. The network is trained on both synthetic and real noises, but it still cannot fully characterize real noises. <ref type="bibr" target="#b0">Brooks et al. (Brooks et al. 2019</ref>) used prior statistics stored in the raw data of DND to augment the synthetic RGB data, but it does not prove the generalization of the model on other real noises.</p><p>In this work, from a novel viewpoint of real image blind denoising, we seek to adapt a learning-based denoiser trained on pixel-independent synthetic noises to unknown real noises. As shown in <ref type="figure">Figure 1</ref>, we assume that real noises differ from pixel-independent synthetic noises dominantly in spatial/channel-variance and correlation <ref type="bibr">(Stanford 2015)</ref>. This difference results from in-camera pipeline like demosaicing <ref type="bibr" target="#b19">(Zhou et al. 2019)</ref>. Based on this assumption, we first propose to train a basis denoising network using mixed AWGN and RVIN. Our flexible basis net consists of an explicit noise estimator followed by a conditional denoiser. We demonstrate that this fully-convolutional nets are actually efficient in coping with pixel-independent spatially/channel-variant noises. Second, we propose a simple yet effective adaptation strategy, Pixel-shuffle Downsampling(PD), which employs the divide-and-conquer idea to handle real noises by breaking down the spatial correlation.</p><p>In summary, our main contributions include: • We propose a new flexible deep denoising model (trained with AWGN and RVIN) for both blind and non-blind image denoising. We also demonstrate that such fully convolutional models trained on spatially-invariant noises can handle spatially-variant noises. • We adapt the AWGN-RVIN-trained deep denoiser to real noises by applying a novel strategy called Pixelshuffle Down-sampling (PD). Spatially-correlated noises are broken down to pixel-wise independent noises. We examine and overcome the proposed domain gap to boost real denoising performance. • The proposed method achieves state-of-the-art performance on DND benchmark and other real noisy RGB images among models trained only with synthetic noises. Note that our model does not use any images or prior meta-data from real noise datasets. We also show that with the proposed PD strategy, the performance of some other existing denoising models can also be boosted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Discriminative Learning based Denoiser. Denoising methods based on CNNs have achieved impressive performance on removing synthetic Gaussian noise. Burger et al. <ref type="bibr" target="#b1">(Burger, Schuler, and Harmeling 2012)</ref> proposed to apply multi-layer perceptron (MLP) to denoising task. In <ref type="figure">(</ref>  <ref type="bibr" target="#b18">(Zhang, Zuo, and Zhang 2018)</ref>, the author proposed a non-blind denoising by concatenating the noise level as a map to the noisy image. By manually adjusting noise level to a higher value, FFDNet demonstrates a spatial-invariant denoising on realistic noises with over-smoothed details. Blind Denoising on Real Noisy Images. Real noises of CCD cameras are complicated and are related to optical sensors and in-camera process. Specifically, multiple noise sources like photon noise, read-out noise etc. and processing including demosaicing, color and gamma transformation introduce the main characteristics of real noises: spatial/channel correlation, variance, and signal-dependence. To approximate real noise, multiple types of synthetic noise are explored in previous work, including Gaussian-Poisson <ref type="bibr" target="#b9">Liu, Tanaka, and Okutomi 2014)</ref>, Gaussian Mixture Model (GMM) (Zhu, Chen, and Heng 2016), incamera process simulation <ref type="bibr" target="#b6">(Liu et al. 2008;</ref><ref type="bibr" target="#b12">Shi Guo 2018)</ref> and GAN-generated noises (Chen et al. 2018), to name a few. CBDNet (Shi Guo 2018) first simulated real noise and trained a subnetwork for noise estimation, in which spatialvariance noise is represented as spatial maps. Besides, multichannel <ref type="bibr" target="#b12">(Xu et al. 2017;</ref><ref type="bibr" target="#b12">Shi Guo 2018)</ref> and multi-scale (Lebrun, Colom, and Morel 2015a; Yu and Koltun 2015) strategy were also investigated for adaptation. Different from all the aforementioned works which focus on directly synthesizing or simulating noises for training, in this work, we apply AWGN-RVIN model and focus on pixel-shuffle adaptation strategy to fill in the gap between pixel-independent synthetic and pixel-correlated real noises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology Basis Noise Model</head><p>The basis noise model is mixed AWGN-RVIN. Noises in sRGB images are no longer approximated Gaussian-Poisson Noises as in the raw sensor data mainly due to gamma transform, demosaicing, and other interpolations etc.. In <ref type="figure" target="#fig_0">Figure 2</ref>,  we follow <ref type="bibr" target="#b6">(Liu et al. 2008</ref>) pipeline to synthesize noisy images, and plot the Noise Level Functions (NLFs) (noise variance as a function of image intensity) before (first row) and after (second row) the Gamma Correction transform and demosaicing. From left to right, the Gamma factor increases. It shows that in RGB images, clipping effect and other nonlinear transforms will greatly influence the originally linear noise variance-intensity relationship in raw sensor data, even change the noise mean. Tough complicated, for a more general case than Gaussian-Poisson noises of modeling different nonlinear transforms, real noises in RGB can still be locally approximated as AWGN <ref type="bibr" target="#b18">(Zhang, Zuo, and Zhang 2018;</ref><ref type="bibr">Lee 1980;</ref><ref type="bibr" target="#b13">Xu, Zhang, and Zhang 2018)</ref>. In this paper, we thus assume the RGB noises to be approximated spatiallyvariant and spatially-correlated AWGN.</p><p>Adding RVIN for training aims at explicitly resolving the defective pixels caused by dead pixels of camera hardware or long exposure frequently appearing in most night-shot images. We generate AWGN, RVIN and mixed AWGN-RVIN following PGB <ref type="bibr" target="#b12">(Xu et al. 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basis Model Structure</head><p>The architecture of the proposed basis model is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. The proposed blind denoising model G consists of a noise estimator E and a follow-up non-blind denoiser R. Given a noisy observation y i = F(x i ), where F is the noise synthetic process, and x i is the noise-free image, the model aims to jointly learn the residual G(y i ) ≈ v i = y i −x i , and it is trained on paired synthetic data (y i , v i ). Specifically, the noise estimator outputs E(y i ) consisting of six pixel-wise noise-level maps that correspond to two noise types, i.e., AWGN and RVIN, across three channels (R, G, B). Then y i is concatenated with the estimated noise level maps E(y i ) and fed into the non-blind denoiser R. The denoiser then outputs the noise residual G(y i ) = R(y i , E(y i )). Three objectives are proposed to supervise the network training, including the noise estimation (L e ), blind (L b ) and non-blind (L nb ) image denoising objectives, defined as,</p><formula xml:id="formula_0">L e = 1 2N N i=1 ||E(y i ; Θ E ) − e i || 2 F ,<label>(1)</label></formula><formula xml:id="formula_1">L b = 1 2N N i=1 ||R(y i , E(y i ; Θ E ); Θ R ) − v i || 2 F , (2) L nb = 1 2N N i=1 ||R(y i , e i ; Θ R ) − v i || 2 F ,<label>(3)</label></formula><p>where Θ E and Θ R are the trainable parameters of E and R. e i is the ground truth noise level maps for y i , consisting of e iAW GN and e iRV IN . For AWGN, e iAW GN is represented as the even maps filled with the same standard deviation values ranging from 0 to 75 across R,G,B channels. For RVIN, e iRV IN is represented as the maps valued with the corrupted pixels ratio with upper-bound set to 0.3. We further normalize e i to range [0,1]. Then the full objective can be represented as a weighted sum of the above three losses,</p><formula xml:id="formula_2">L = αL e + βL b + γL nb ,<label>(4)</label></formula><p>in which α, β and γ are hyper-parameters to balance the losses, and we set them to be equal for simplicity. The proposed model structure can perform both blind and non-blind denoising simultaneously, and the model is more flexible in interactive denoising and result adjustment. Explicit noise estimation also benefits noise modeling and disentanglement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel-shuffle Down-sampling (PD) Adaptation</head><p>Pixel-shuffle Down-sampling. Pixel-shuffle (Shi et al. 2016) down-sampling is defined to create the mosaic by sampling the images with stride s. Compared to other downsampling methods like linear interpolation, bi-cubic interpolation, and pixel area relation, the pixel-shuffle and nearestneighbour down-sampling on noisy image would not influence the real noise distribution. Besides, pixel-shuffle also benefits image recovery by preserving the original pixels from the images compared to others. These two advantages yield the two stages of PD strategy: adaptation and refinement.</p><p>Adaptation. Learning-based denoiser trained on AWGN is not robust enough to real noises due to domain difference.</p><p>To adapt the noise model to real noise, here we briefly analyze and justify our assumption on the difference between real noises and Gaussian noise: spatial/channel variance and correlation.  Suppose a noise estimator is robust, which means it can accurately estimate the exact noise level, for a single AWGN-corrupted image, pixel-shuffle down-sampling will neither influence the AWGN variance nor the estimation values, when the sample stride is small enough to preserve the textural structures. When extending it to real noise case, we have an interesting hypothesis: as we increase the sample stride of pixel-shuffle, the estimation values of specific noise estimators will first fluctuate and then keep steady for a couple of stride increment. This assumption is feasible because pixel-shuffle will break down the spatial-correlated noise patterns to pixel-independent ones, which can be approximated as spatial-variant AWGN and adapted to those estimators.</p><p>We justify this hypothesis on both <ref type="bibr" target="#b8">(Liu, Tanaka, and Okutomi 2013)</ref> and our proposed pixel-wise estimator. As shown in <ref type="figure">Figure 1</ref>, we randomly cropped a patch of size 200 × 200 from a random noisy image y in SIDD <ref type="bibr" target="#b0">(Abdelhamed, Lin, and Brown 2018)</ref>. We add AWGN with std = 35 to its noise-free ground truth x. After pixel-shuffling both y and AWGN-corrupted x, starting from stride s = 2, the noise pattern of y demonstrates expected pixel independence. Using <ref type="bibr" target="#b8">(Liu, Tanaka, and Okutomi 2013)</ref>, the estimation result for x is unchanged in <ref type="figure" target="#fig_3">Figure 4</ref> (a) (Left), but the one for y in <ref type="figure" target="#fig_3">Figure 4</ref> (a) (Right) first increases and begins to keep steady after stride s = 2. It is consistent with the visual pattern and our hypothesis.</p><p>One assumption of <ref type="bibr" target="#b8">(Liu, Tanaka, and Okutomi 2013)</ref> is that the noise is additive and evenly distributed across the image. For spatial-variant signal-dependent real noises, our pixel-wise estimator has its superiority. To make statistics of spatial-variant noise estimation values, we extract the three AWGN channels of noise map E AW GN (y i ) ∈ R W ×H×3 , where W and H are width and height of the input image, and compute the normalized 10-bin histograms h s ∈ R 10×3 across each channel when the stride is s. We introduce the changing factor r s to monitor the noise map distribution changes as the stride s increases,</p><formula xml:id="formula_3">r s = E c ||h sc − h (s+1)c || 2 2 ,<label>(5)</label></formula><p>where c is the channel index. We then investigate the difference of r s sequence between AWGN and realistic noises. Specifically, we randomly select 50 images from CBSD68 (Roth and Black 2009) and add random-level AWGN to them. For comparison, we randomly pick up 50 image patches of 512 × 512 from DND benchmark. In <ref type="figure" target="#fig_3">Figure 4 (b)</ref>, r s sequence remains closed to zero for all AWGN-currupted images (Left <ref type="figure">figure)</ref>, while for real noises r α demonstrates an abrupt drop when s = 2. It indicates that the spatialcorrelation has been broken from s = 2.</p><p>The above analysis inspires the proposed adaptation strategy based on pixel-shuffle. Intuitively, we aim at finding the smallest stride s to make the down-sampled spatialcorrelated noises match the pixel-independent AWGN. Thus we keep increasing the stride s until r s drops under a threshold τ . We run the above experiments on CBSD68 for 100 iterations to select the proper generalized threshold τ . After averaging the maximum r of each iteration, we empirically set τ = 0.008.</p><p>PD Refinement. <ref type="figure" target="#fig_4">Figure 5</ref> shows the proposed Pixelshuffle Down-sampling (PD) refinement strategy: (1) Compute the smallest stride s, which is 2 in this example and more digital camera image cases, to match AWGN following the adaptation process, and pixel-shuffle the image into mosaic y s ; (2) Denoise y s using G; (3) Refill each subimage with noisy blocks separately and pixel-shuffle upsample them; (4) Denoise each refilled image again using G and average them to obtain the 'texture details' T ; (5) Combine the over-smoothed 'flat regions' F to refine the final result.</p><p>As summarized in <ref type="bibr" target="#b6">(Liu et al. 2008)</ref>, the goals of noise removal include preserving texture details and boundaries, smoothing flat regions, and avoiding generating artifacts. Therefore, in the above step-(5), we propose to further refine the denoised image with the combination of 'texture details' T and 'flat regions' F . 'Flat regions' can be obtained from over-smoothed denoising results generated by lifting the noise estimation levels. In this work, given a noisy observation y, the refined noise maps are defined as,</p><formula xml:id="formula_4">E(P D(y))(i, j) = max i,j E(P D(y))(i, j), i ∈ [1, W ], j ∈ [1, H]. (6)</formula><p>Consequently, the 'flat region' is defined as F = P U (R(P D(y),Ê(P D(y)))), where PD and PU are pixelshuffle downsampling and upsampling. The final result is obtained by kF + (1 − k)T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Implementation Details</head><p>In this work, the structures of the sub-network E and R follow DnCNN <ref type="bibr" target="#b16">(Zhang et al. 2017a</ref>) of 5 layers and 20 layers. For grayscale image experiments, we also follow DnCNN to crop 50 × 50 patches from 400 images of size 180 × 180. For color image model, we crop 50 × 50 patches with stride 10 from 432 color images in the Berkeley segmentation dataset (BSD) (Roth and Black 2009). The training data ratio of single-type noises (either AWGN or RVIN) and mixed noises (AWGN and RVIN) is 1:1. During training, Adam optimizer is utilized and the learning rate is set to 10 −3 , and batch size is 128. After 30 epochs, the learning rate drops to 10 −4 and the training stops at epoch 50.</p><p>To evaluate the algorithm on synthetic noise (AWGN, mixed AWGN-RVIN and spatially-variant Gaussian), we utilize the benchmark data from BSD68, Set20 <ref type="bibr" target="#b12">(Xu et al. 2016</ref>) and CBSD68 (Roth and Black 2009). For realistic noise, we test it on RNI15 (Online 2015a), DND benchmark <ref type="bibr" target="#b11">(Plötz and Roth 2017)</ref>, and self-captured night photos. We evaluate the performance of the algorithm in terms of PSNR and SSIM. Qualitative performance for denoising is also presented, with comparison to other state-of-the-arts. Mixed AWGN and RVIN. Our model follows similar structure of DnCNN and FFDNet <ref type="bibr" target="#b18">(Zhang, Zuo, and Zhang 2018)</ref>, so its performance on single-type AWGN removal is also similar to them. We thus evaluate our model on eliminating mixed AWGN and RVIN on Set20 as in <ref type="bibr" target="#b12">(Xu et al. 2016)</ref>. We also compare our method with other baselines, including BM3D ) and WNNM <ref type="bibr" target="#b4">(Gu et al. 2014)</ref> which are non-blind Gaussian denoisers anchored with a specific noise level estimated by the approach provided in <ref type="bibr" target="#b8">(Liu, Tanaka, and Okutomi 2013)</ref>. Besides, we include the PGB <ref type="bibr" target="#b12">(Xu et al. 2016</ref>) denoiser that is designed for mixed AWGN and RVIN. The result of the blind version of DnCNN-B, trained by the same strategy as our model, is also presented for reference. The comparison results are shown in <ref type="table" target="#tab_1">Table 1</ref>, from which we can see the proposed method achieves the best performance. Compared to DnCNN-B, for complicated mixed noises, our model explicitly disentangles different noises. It benefits the conditional denoiser to differentiate mixed noises from other types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation with Synthetic Noise</head><p>Signal-dependent Spatially-variant Noise. We conduct experiments to examine the generalization ability of fully convolutional model on signal-dependent noise model <ref type="bibr" target="#b12">(Shi Guo 2018;</ref><ref type="bibr" target="#b9">Liu, Tanaka, and Okutomi 2014)</ref>. Given a clean image x, the noises in the noisy observation y contain both signal-dependent components with variance xσ 2 s and independent components with variance σ 2 c . <ref type="table" target="#tab_2">Table 2</ref> shows that for non-blind model like BM3D and FFDNet, only scalar noise estimator <ref type="bibr" target="#b8">(Liu, Tanaka, and Okutomi 2013)</ref> is applied, thus they cannot well cope with the spatially-variant cases. In this experiment, DnCNN-B is the original blind model trained on AWGN with σ ranged between 0 and 55. It shows that spatially-variant Gaussian noises can still be handled by fully convolutional model trained with spatially-invariant AWGN <ref type="bibr" target="#b18">(Zhang, Zuo, and Zhang 2018)</ref>. Compared to DnCNN-B, the proposed network explicitly estimates the pixel-wise map to make the model more flexible and possible for real noise adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation with Real RGB Noise</head><p>Qualitative Comparisons. Some qualitative denoising results on DND are shown in <ref type="figure">Figure 6</ref>. The compared results of DND are all directly obtained online from the original submissions of the authors. The methods we include for the comparison cover blind real denoisers (CBDNet, NI (Online 2015b) and NC <ref type="bibr" target="#b5">(Lebrun, Colom, and Morel 2015b)</ref>), blind Gaussian denoisers (CDnCNN-B) and non-blind Gaussian denoisers (CBM3D, WNNM <ref type="bibr" target="#b4">(Gu et al. 2014)</ref>, and FFD-Net). From these example denoised results, we can observe that some of them are either noisy (as in DnCNN and WNNM), or spatially-invariantly over-smoothed (as in FFD-Net). CBDNet performs better than others but it still suffers from blur edges and uncleaned background. Our proposed method (PD) achieves a better spatially-variant denoising performance by smoothing the background while preserving the textural details in a full blind setting.</p><p>Quantitative Results on DND Benchmark. The images in the DND benchmark are captured by digital camera and demosaiced from raw sensor data, so we simply set the stride number s = 2. We follow the submission guideline of DND  <ref type="bibr" target="#b19">(Zhao, Lam, and Lun 2019)</ref> and CBDNet, achieved promising performance on DND, but they are all finetuned on real noisy images, or use prior knowledge in the meta-data of DND <ref type="bibr" target="#b0">(Brooks et al. 2019)</ref>. For fair comparison, we select some representative conventional methods(MCWNNM, EPLL, TWSC, CBM3D), and learning-based methods trained only with synthetic noises. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Models trained on AWGN (DnCNN, TNRD, MLP) perform poorly on real RGB noises mainly due to the large gap between AWGN and real noise. CBDNet improves the results significantly by training the deep networks with artificial realistic noise model. Our AWGN-RVIN-trained model with PD refinement achieves much better results (+0.83dB) than CBD-Net trained only with synthetic noises, and also boosts the performance of other AWGN-based methods (+PD). Compared to the base model, the proposed adaptation methods improve the performance on real noises by 5.8 dB. Note that our model is only trained on synthetic noises, and does not utilize any prior data of DND.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study on Real RGB Noise</head><p>Adding RVIN. Training models with mixed AWGN and RVIN noises will benefit the removal of dead or overexposure pixels in real images. For comparison, We train another model only with AWGN, and test it on real noisy night photos. An example utilizing the full pipeline is shown in <ref type="figure" target="#fig_5">Figure 7</ref>, in which it demonstrates the superiority of the existence of RVIN in the training data. Even though model trained with AWGN can also achieve promising denoising performance, it is not effective on dead pixels.</p><p>Stride Selection. We apply different stride numbers while refining the denoised results, and compare the visual quality in <ref type="figure" target="#fig_6">Figure 8 (a)(b)</ref>. For arbitrary given sRGB images, the  <ref type="bibr" target="#b12">(Xu et al. 2017)</ref> 37.38 0.929 EPLL <ref type="bibr" target="#b21">(Zoran and Weiss 2011)</ref> 33.51 0.824 TWSC  37.93 0.940 MLP <ref type="bibr" target="#b1">(Burger, Schuler, and Harmeling 2012)</ref>   stride number can be computed using our adaptation algorithm with the assistance of noise estimator. In our experiments, the selected stride is the smallest s that r s &lt; τ . Small stride number will treat large noise patterns as textures to preserve, as shown in <ref type="figure" target="#fig_6">Figure 8 (b)</ref>. While using large stride number tends to break the textural structures and details. Interestingly, as shown in <ref type="figure" target="#fig_6">Figure 8 (b)</ref>, the texture of the fabric is invisible while applying s &gt; 2.  Image Refinement Process. The ablation on the refinement steps is shown in <ref type="figure" target="#fig_6">Figure 8</ref> (c)(d) and <ref type="table" target="#tab_6">Table 4</ref>, in which we compare the denoised results of I (i.e. directly pixelshuffling upsampling after step <ref type="formula">(2)</ref>), DI (i.e. denoising I using G), and Full (i.e. the current whole pipeline). It shows that both I and DI will form additional visible artifacts, while the whole pipeline smooths out those artifacts and has the best visual quality.</p><p>Blending Factor k. Due to the ambiguity nature of fine texture and mid-frequent noises, human perception intervene on the denoising level is inevitable. k is this parameter introduced as a 'linear' adjustment of denoising level for a more flexible and interactive user operation. Using blending factor k is more stable and safe to preserve the spatiallyvariant details than directly adjusting the estimated noise level like CBDNet. In <ref type="figure" target="#fig_7">Figure 9</ref>, as k increases, the denoised results tend to be over-smoothed. This is suitable for images with more background patterns. However, smaller k will preserve more fine details which are applicable for images with more foreground objects. In most cases, users can simply set k to 0 to obtain the most detailed textures recovery and visually plausible results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we revisit the real image blind denoising from a new viewpoint. We assumed the realistic noises are spatially/channel -variant and correlated, and addressed adaptation from AWGN-RVIN noises to real noises. Specifically, we proposed an image blind and non-blind denoising network trained on AWGN-RVIN noise model. The network consists of an explicit multi-type multi-channel noise es- timator and an adaptive conditional denoiser. To generalize the network to real noises, we investigated Pixel-shuffle Down-sampling (PD) refinement strategy. We showed qualitatively that PD behaves better in both spatially-variant denoising and details preservation. Results on DND benchmark and other realistic noisy images demonstrated the newly proposed model with the strategy are efficient in processing spatial/channel variance and correlation of real noises without explicit modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Noise Level Function (NLFs) (noise variance as a function of image intensity) before (first row) and after (second row) gamma transform and demosaicing. Gamma factor is 0.39, 1.38 and 2.31 from the left to right column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Structure of the proposed blind denoising model. It consists of a noise estimator E and a follow-up non-blind denoiser R. The model aims to jointly learn the image residual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) As the stride increases, Left: Estimated noise level on AWGNcorrupted image. Right: Estimated noise level on real noisy images.(b) Left: Changing factor rs on AWGN-corrupted images of CBSD68 and Right: on real noisy images of DND. Different color lines represent different image samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Influence of Pixel-shuffle on noise patterns and noise estimation algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Pixel-shuffle Down-sampling (PD) refinement strategy with s = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Denoised performance of models trained with AWGN in (b) and mixed AWGN-RVIN in (c). During testing, k = 0 and s = 2.(a) Noisy image (b) Denoised. (c) Noisy Image (d) Denoised.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>(a)(b):Denoised performance of different stride s when k = 0, and (c)(d): Ablation study on refinement. s = 2 and k = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Ablation study on merging factor k, and s = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of PSNR results on mixture of Gaussian noise (AWGN) and Impulse noise (RVIN) removal performance on Set20.</figDesc><table><row><cell cols="2">(σ, r) BM3D WNNM PGB DnCNN-B Ours-NB Ours-B</cell></row><row><cell>(10, 0.15) 25.18 25.41 27.17 32.09</cell><cell>32.43 32.37</cell></row><row><cell>(10, 0.30) 21.80 21.40 22.17 29.97</cell><cell>30.47 30.32</cell></row><row><cell>(20, 0.15) 25.13 23.57 26.12 29.52</cell><cell>29.82 29.76</cell></row><row><cell>(20, 0.30) 21.73 21.40 21.89 27.90</cell><cell>28.41 28.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of PSNR results on Signal-dependent Noises on CBSD68.</figDesc><table><row><cell cols="5">(σs, σc) BM3D FFDNet DnCNN-B CBDNet Ours-B</cell></row><row><cell>(20, 10) 29.09</cell><cell>28.54</cell><cell>34.38</cell><cell>33.04</cell><cell>34.75</cell></row><row><cell>(20, 20) 29.08</cell><cell>28.70</cell><cell>31.72</cell><cell>29.77</cell><cell>31.32</cell></row><row><cell>(40, 10) 23.21</cell><cell>28.67</cell><cell>32.08</cell><cell>30.89</cell><cell>32.12</cell></row><row><cell>(40, 20) 23.21</cell><cell>28.80</cell><cell>30.32</cell><cell>28.76</cell><cell>30.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">: Comparison of PSNR and SSIM on DND Bench-</cell></row><row><cell cols="2">mark. PD: Pixel-suffle Down-sampling Strategy. Among all</cell></row><row><cell>models trained only with synthetic data.</cell><cell></cell></row><row><cell>Method</cell><cell>PSNR SSIM</cell></row><row><cell>MCWNNM</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on refinement steps.</figDesc><table><row><cell cols="5">Model (s=1) (s=3, Full) (s=2,I) (s=2,DI) (s=2,Full)</cell></row><row><cell>PSNR 32.60</cell><cell>37.90</cell><cell>37.00</cell><cell>37.20</cell><cell>38.40</cell></row><row><cell>SSIM 0.7882</cell><cell>0.9349</cell><cell cols="2">0.9339 0.9361</cell><cell>0.9452</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on real image denoising: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brown ; Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07396</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="11036" to="11045" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schuler</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing: Algorithms and Systems, Neural Networks, and Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing: Algorithms and Systems VI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6812</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
		</imprint>
	</monogr>
	<note>Image restoration by sparse 3d transform-domain collaborative filtering</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Practical poissonian-gaussian noise modeling and fitting for single-image raw-data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1737" to="1754" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. [Lebrun, Colom, and Morel 2015a] Lebrun, M.; Colom, M.; and Morel, J.-M. 2015a. Multiscale image blind denoising</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3149" to="3161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Refined filtering of image noise using local statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colom</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>The noise clinic: a blind image denoising algorithm</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic estimation and removal of noise from a single image</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="314" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">When image denoising meets highlevel vision tasks: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04284</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single-image noise level estimation for blind denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanaka</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5226" to="5237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Practical signal-dependent noise parameter estimation from a single noisy image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanaka</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4361" to="4371" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang ;</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Niknejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02867.[Online2015a]Online.2015a</idno>
		<ptr target="https://ni.neatvideo.com/" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS. [Niknejad, Bioucas-Dias, and Figueiredo</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Class-specific poisson denoising by patch-based importance sampling. Online 2015b] Online. 2015b. [online] available</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dn-resnet: Efficient deep residual network for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06766</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>CVPR. [Ren, El-Khamy, and</editor>
		<meeting><address><addrLine>Santhanam, Morariu, and Davis</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">205</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Generalized deep image to image regression. Shi et al. 2016. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In CVPR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-channel weighted nuclear norm minimization for real color image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shi ; Shi Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Z W Z L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1807.04686</idno>
		<idno>Xu et al. 2016</idno>
		<ptr target="https://web.stanford.edu/group/vista/cgi-bin/wiki/index.php/DemosaickingandDenoising" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>Stanford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5232" to="5239" />
		</imprint>
	</monogr>
	<note type="report_type">Deep image prior. arXiv preprint</note>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A trilateral weighted sparse coding scheme for real-world image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04364</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-scale weighted nuclear norm image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multiscale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10343</idno>
		<idno>Zhang et al. 2017a</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Path-restore: Learning network path selection for image restoration</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ffdnet: Toward a fast and flexible solution for cnn based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang ; Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhancement of a cnn-based denoiser based on spatial and spectral analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Lun ; Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10085" to="10086" />
		</imprint>
	</monogr>
	<note>2019 IEEE International Conference on Image Processing</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Survey of face detection on low-quality images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="769" to="773" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>and Weiss</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
