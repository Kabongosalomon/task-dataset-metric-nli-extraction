<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Srivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural IR models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their generalization capabilities. To address this, and to allow researchers to more broadly establish the effectiveness of their models, we introduce BEIR (Benchmarking IR), a heterogeneous benchmark for information retrieval. We leverage a careful selection of 17 datasets for evaluation spanning diverse retrieval tasks including open-domain datasets as well as narrow expert domains. We study the effectiveness of nine state-of-the-art retrieval models in a zero-shot evaluation setup on BEIR, finding that performing well consistently across all datasets is challenging.</p><p>Our results show BM25 is a robust baseline and Reranking-based models overall achieve the best zero-shot performances, however, at high computational costs. In contrast, Denseretrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. In this work, we extensively analyze different retrieval models and provide several suggestions that we believe may be useful for future work. BEIR datasets and code are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many real-world NLP problems rely on a practical and efficient retrieval component as a first step to find relevant information. Examples are opendomain question-answering <ref type="bibr">(Chen et al., 2017)</ref>, claim-verification <ref type="bibr" target="#b29">(Thorne et al., 2018)</ref>, and duplicate question detection <ref type="bibr">(Zhang et al., 2015)</ref>. Traditionally, retrieval has been dominated by lexical approaches like TF-IDF or BM25 <ref type="bibr" target="#b21">(Robertson and Zaragoza, 2009</ref>). However, these approaches suffer from what is known as lexical gap <ref type="bibr">(Berger et al., 2000)</ref> and only retrieve documents that contain the keywords also present within the query. Further, queries and documents are treated in a bag-ofwords manner which does not take word ordering into consideration.</p><p>Recently, deep learning and in particular pretrained Transformer models like <ref type="bibr">BERT (Devlin et al., 2018)</ref> have became popular in the information retrieval space . They overcome the lexical gap by mapping queries and documents to a dense vector <ref type="bibr">(Guo et al., 2016;</ref><ref type="bibr">Karpukhin et al., 2020;</ref><ref type="bibr">Guu et al., 2020;</ref><ref type="bibr">Gao et al., 2020;</ref><ref type="bibr">Liang et al., 2020;</ref><ref type="bibr" target="#b8">Ma et al., 2021)</ref>. The relevant documents for a given query are then retrieved using (approximate) nearest neighbor search <ref type="bibr">(Johnson et al., 2017)</ref>.</p><p>Another widely used approach involves reranking documents from the output of a first-stage retrieval system <ref type="bibr" target="#b13">(Nogueira et al., 2019a</ref><ref type="bibr" target="#b11">Nogueira and Cho, 2020;</ref><ref type="bibr">Khattab and Zaharia, 2020)</ref>. While dense retrieval approaches try to overcome the (potential) lexical gap, re-ranking approaches aim to create a better comparison of the retrieved documents. Different approaches can also be combined together <ref type="bibr">(Ding et al., 2020;</ref><ref type="bibr">Gao et al., 2020;</ref><ref type="bibr" target="#b7">Luan et al., 2021)</ref>.</p><p>Previous approaches were commonly trained on rather large datasets like the Natural Questions (NQ) dataset  containing around 133k training examples or the MS-MARCO dataset <ref type="bibr" target="#b10">(Nguyen et al., 2016)</ref> with more than 500k training examples. Existing approaches have been shown to perform well when evaluated in-domain or for similar tasks <ref type="bibr" target="#b11">(Nogueira and Cho, 2020;</ref><ref type="bibr">Karpukhin et al., 2020;</ref><ref type="bibr">Ding et al., 2020)</ref>. However, large training corpora are not available for most tasks and domains. As creating a large training corpus can be expensive, it is not feasible to create such for most tasks and domains. Hence, in most scenarios, we apply retrieval models in a zero-shot setup, i.e. pre-trained models are applied out-of-the-box across new tasks and domains. In <ref type="figure">Figure 1</ref>: An overview of the diverse tasks and datasets present in <ref type="bibr">BEIR.</ref> this crucial setting, it remains unclear whether and how well these retrieval models transfer or generalize to new tasks or domains.</p><p>In this work, we establish a new heterogeneous benchmark for Information Retrieval called BEIR (Benchmarking IR) consisting of a broad range of domains and tasks. Through BEIR, we systematically study the zero-shot generalization capabilities of multiple neural retrieval approaches. Existing benchmarks <ref type="bibr" target="#b40">(Guo et al., 2020;</ref><ref type="bibr">Petroni et al., 2020)</ref> have issues of a comparatively narrow evaluation focusing either only on a single task or on a certain domain. BEIR overcomes these shortcomings by covering 9 diverse retrieval tasks with 17 datasets including a diverse set of domains, query and document types.</p><p>We use BEIR to evaluate nine diverse state-ofthe-art retrieval approaches. From our analysis, we find that no approach consistently outperforms all others. Further, we notice that the in-domain performance of a model does not correlate with its generalization capabilities: models fine-tuned with the same training data might generalize differently in <ref type="bibr">BEIR.</ref> In BEIR, we observe a trade-off between the zero-shot performances and the computational cost: computationally expensive re-ranking models overall perform the best. More computationally efficient dense models can, depending on the task and domain, substantially underperform traditional lexical models like BM25. In fact, BM25 is the third best performing approach on our benchmark, beaten only by neural re-ranking approaches.</p><p>With BEIR, we take an important step towards broadly establishing and improving the zero-shot capabilities of neural retrieval systems. Our benchmark attempts to help the community better to understand when a retrieval system could work or fail for a certain domain or task. Further, our bench-mark attempts to help researchers develop a universal neural retriever that performs well across diverse tasks and domains.</p><p>We publicly release BEIR and an integration of various retrieval models in a well-documented, easy to use and extensible open-source implementation. The implementation is designed to allow easy integration of new tasks, datasets and retrieval models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>To our knowledge, BEIR is the first zero-shot information retrieval benchmark. Existing benchmarks do not evaluate retrieval in a zero-shot setting in depth, they either focus over a single task, small corpora or on a certain domain. This setting hinders for investigation of model generalization across diverse set of domains and task types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Existing Benchmarks</head><p>MultiReQA <ref type="bibr" target="#b40">(Guo et al., 2020)</ref> consists of eight Question-Answering (QA) datasets and evaluates sentence-level answer retrieval given a question.MultiReQA only tests a single task (question to answer-sentence retrieval) and five out of eight datasets are on retrieval from Wikipedia. Further, MultiReQA evaluates retrieval over rather small corpora: six out of eight tasks have less than 100k candidate sentences, which benefits dense retrieval over lexical as shown previously in . KILT <ref type="bibr">(Petroni et al., 2020)</ref> consists of five knowledge-intensive tasks including a total of eleven datasets. The tasks involve retrieval, but it is not the primary task. Further, KILT retrieves documents only from Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Retrieval</head><p>Information retrieval is the process of searching and returning relevant documents for a query from a collection. In our paper, we focus on text retrieval and use document as a cover term for text of any length in the given collection and query for the user input, which can be of any length as well.</p><p>Traditionally, lexical approaches like TF-IDF and BM25 <ref type="bibr" target="#b21">(Robertson and Zaragoza, 2009</ref>) have dominated textual information retrieval. Recently, there is a strong interest in using neural networks to improve or replace these lexical approaches. In this section, we highlight a few neural-based approaches and we refer the reader to  for a detailed survey on recent developments in neural retrieval.</p><p>Retriever-based Lexical approaches suffer from the lexical gap <ref type="bibr">(Berger et al., 2000)</ref>. To overcome this, earlier techniques proposed to improve lexical retrieval systems with neural networks. Doc2query <ref type="bibr" target="#b14">(Nogueira et al., 2019b</ref>) utilized a deep encoder-decoder architecture and generated possible queries for a given document. These queries were appended to the document. DeepCT (Dai and Callan, 2020) utilized BERT to learn relevant terms in a document and repeats those terms at the end of the document, to achieve a higher termfrequency weight. More recently, dense retrieval approaches have been proposed to map queries and documents in a shared, dense vector space <ref type="bibr">(Gillick et al., 2018)</ref>. Here, a dual-encoder neural architecture based on pre-trained Transformers showed strong performance for various tasks <ref type="bibr" target="#b40">(Guo et al., 2020;</ref><ref type="bibr">Karpukhin et al., 2020;</ref><ref type="bibr">Liang et al., 2020;</ref><ref type="bibr" target="#b8">Ma et al., 2021)</ref>. This dense approach was recently extended by hybrid lexical-dense approaches which aims to combine the strengths of both approaches <ref type="bibr">(Gao et al., 2020;</ref><ref type="bibr" target="#b24">Seo et al., 2019;</ref><ref type="bibr" target="#b7">Luan et al., 2021)</ref>. In contrast to the dense approaches, SPARTA (Zhao et al., 2020) learns sparse representations by combining contextualized and noncontextualized embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reranking-based</head><p>Neural reranking approaches use the output of a first-stage retrieval system, often BM25, and re-ranks the documents based on the estimated relevancy between the query and the document. Significant improvement in performance was achieved with the crossattention mechanism of BERT <ref type="bibr" target="#b11">(Nogueira and Cho, 2020)</ref>. A disadvantage of reranking with cross-attention is the high computational overhead <ref type="bibr" target="#b19">(Reimers and Gurevych, 2019)</ref>. In contrast, ColBERT (Khattab and Zaharia, 2020) computes contextualized embeddings independently for queries and documents and uses an efficient maximum-similarity operation for the relevance estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BEIR Benchmark</head><p>The selection of diverse datasets is an integral part of a benchmark. Including a diverse set of challenging datasets is essential to infer broadly the applicable findings from the observed benchmark results. Furthermore, the benchmark should cover challenging tasks to support future work in that area <ref type="bibr">(Gehrmann et al., 2021)</ref>.</p><p>To collect the tasks and datasets with the desired properties, a selection methodology is crucial. For BEIR, the methodology is motivated by the following three factors:</p><p>(1) Diversity in Tasks Information retrieval is a broad task and the lengths of queries and indexed documents can differ extremely between tasks. Sometimes, queries are short, like a keyword, while in other cases, they can be long like a complete news article. Similarly, indexed documents can sometimes be long, and for other tasks, short like a tweet. In BEIR, we aim to include vastly different tasks with different properties for queries and documents.</p><p>(2) Diversity in Domains Information retrieval systems are applied in all types of textual-based domains. From broad and generic domains, like Wikipedia, to highly specialized ones such as scientific publications of a specialised field. Hence, we include tasks or datasets which provide a representation of real-world problems and are diverse ranging from generic to specialized domains.</p><p>(3) Challenging Datasets The difficulty of a dataset has to be sufficient. If a task is easily solved by any algorithm, it will not be useful to differentiate the various models used for evaluation. We evaluate several datasets based on existing model performances and literature. We select tasks which we believe are challenging and are not yet fully solved with existing approaches and where model performances could be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Tasks and Datasets</head><p>Following the selection criteria, we include seventeen English 1 and evaluation datasets that span over nine heterogeneous tasks. As the majority of the evaluated approaches have been trained on the MSMARCO dataset, we also report performances on this dataset, but don't include the outcome in our zero-shot comparison. <ref type="figure">Figure 1</ref> shows each task, dataset and domain together with the type of query and document. Table 1 shows detailed statistics of the tasks and datasets in BEIR. A majority of the retrieval tasks are binary, i.e. a search result can be relevant or irrelevant, and a few contain fine-grained relevancy judgements. Some datasets have few (&lt; 2) annotated relevant documents for a question, while others contain multiple relevant documents. Not all datasets include a training or a development split denoting the practical importance for zero-shot retrieval techniques and model performances.</p><p>In Appendix C, we motivate each task and dataset included within the benchmark. We would refer the reader to this section for a detailed overview on the tasks and datasets in the benchmark. We additionally provide downloadable links <ref type="table" target="#tab_9">(Table 6</ref>) and examples <ref type="table" target="#tab_11">(Table 8</ref>) for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset and Diversity Analysis</head><p>As shown in <ref type="table">Table 1</ref>, the datasets present in BEIR are from various domains including Wikipedia, scientific publications, Twitter, News, online user communities, and more. To measure the domain similarity between the datasets, we compute a pairwise weighted Jaccard similarity (Ioffe, 2010) on unigram word overlap between the document collections. For understanding how the similarity score was calculated, refer to Appendix D. The pairwise results can be found in <ref type="figure" target="#fig_4">Figure 5</ref> in the appendix. We use these pairwise similarity scores and cluster the datasets using the force-directed placement algorithm. The result is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Nodes close in this graph have a high word overlap, while nodes far away in the graph have a low overlap. From <ref type="figure" target="#fig_4">Figure 5</ref>, we observe a rather low weighted Jaccard word overlap between the different domains, indicating that BEIR is a challenging benchmark and to perform well on it, retrieval approaches must generalize well to diverse text types.  <ref type="bibr">berg et al., 2008)</ref>. We color datasets differently for different domains.</p><p>We now describe our experimental setup which we use to benchmark different state-of-the-art retrieval architectures and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Retrieval Models</head><p>In our work, we include a broad spectrum of retrieval models which we believe cover diverse settings and architectures. We focus on selecting models based on its performance, contemporaneity and popularity. We try to include publicly available pre-trained checkpoints from a practical point of view, which could lead to different model configurations. Unless specified otherwise, we concatenate the title and passage together with a single whitespace across the models. We group the models based on their retrieval architecture: we select one lexical, one sparse, six dense and two re-ranking models, leading to a total of nine different retrieval approaches. Besides the included ones, in future, models can be easily incorporated and compared within the benchmark. All model checkpoints can be found in <ref type="table" target="#tab_10">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Lexical Model</head><p>BM25 <ref type="bibr" target="#b21">(Robertson and Zaragoza, 2009</ref>) is a commonly-used bag-of-words retrieval function based on token-matching between two highdimensional sparse vectors with TF-IDF token weights. Due to its strong and efficient performance, it is the de-facto industrial standard. We use the implementation available on Elasticsearch 2 with the default settings. We index the title (if available) and passage of each document as separate text fields for retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Sparse Model</head><p>SPARTA (Zhao et al., 2020) computes the similarity between the non-contextualized word piece embeddings from BERT and the contextualized word piece embeddings for a given document. Due to the finite size of 30k non-contextualized embeddings, the similarity scores can be pre-computed for a given document, which results in a 30k dimensional sparse vector. As the original implementation is not publicly available, we re-implement the approach. We fine-tune a DistilBERT <ref type="bibr" target="#b23">(Sanh et al., 2020</ref>) model on MSMARCO <ref type="bibr" target="#b10">(Nguyen et al., 2016)</ref> and use sparse-vectors with 2,000 non-zero entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Dense Models</head><p>USE-QA <ref type="bibr" target="#b40">(Yang et al., 2020</ref>) is a two-tower biencoder encoding the query and document separately. The model has been fine-tuned on multiple online question-answering forums and the SNLI <ref type="bibr">(Bowman et al., 2015)</ref> corpus. The documents are encoded as a sentence-level input with additional context information and are retrieved using cosinesimilarity. We utilize the English USE-QA model 3 . We substitute the title as the input; passage as the surrounding context for the original response encoder. For datasets without document titles, we substitute the passage in both the fields.</p><p>DPR (Karpukhin et al., 2020) is a two-tower biencoder. The model uses [CLS] token pooling and retrieves similar documents using dot-product. We experimented and found the Multi model 4 to perform better compared to the NQ model fine-tuned only on Natural Questions . The Multi-DPR model was fine-tuned on four QA datasets: (NQ; <ref type="bibr">Kwiatkowski et al., 2019), (TriviaQA;</ref><ref type="bibr">Joshi et al., 2017)</ref>, (WebQuestions; <ref type="bibr">Berant et al., 2013) and (CuratedTREC;</ref><ref type="bibr">Baudiš and Šedivỳ, 2015)</ref>. The model requires a title for retrieval and concatenates the title and passage together with a [SEP] token. However, we keep the title field empty for evaluation if unavailable for a dataset.</p><p>ANCE <ref type="bibr" target="#b39">(Xiong et al., 2020</ref>) is a siamese biencoder encoding the query and document together in a common embedding space. The model uses [CLS] token pooling and retrieves similar documents using dot-product. ANCE constructed negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which in parallel was updated to select negative training instances during fine-tuning of the model. We use the open-sourced RoBERTa <ref type="bibr" target="#b6">(Liu et al., 2019)</ref> model 5 fine-tuned on MSMARCO <ref type="bibr" target="#b10">(Nguyen et al., 2016)</ref> for 600K steps.</p><p>SBERT <ref type="bibr" target="#b19">(Reimers and Gurevych, 2019</ref>) is a siamese bi-encoder using mean pooling for encoding and cosine-similarity for retrieval. We fine-tune a DistilBERT <ref type="bibr" target="#b23">(Sanh et al., 2020)</ref>   <ref type="table">Table 2</ref>: In-domain and Zero-shot retrieval performances on BEIR datasets. All scores denote nDCG@10. The best retrieval performance on a given dataset is marked in bold, and the second best performance is underlined. Corresponding Recall@100 performances can be found in <ref type="table">Table 9</ref>. ‡ indicates the in-domain performances.</p><p>together with hard-negatives as provided by MS-MARCO. Then, for all train queries we retrieve the top-20 most similar documents from a collection of 3 million MSMARCO passages. We use an ELECTRA (Clark et al., 2020) cross-encoder model fine-tuned on MSMARCO (section 4.1.4) to filter out false positives and only keep (query, passage) pairs with a relevancy score below 0.1. We use these pairs as additional hard negatives and fine-tune the model for 10 more epochs.</p><p>GenQ Similar to past work <ref type="bibr">(Liang et al., 2020;</ref><ref type="bibr" target="#b8">Ma et al., 2021)</ref>, we propose an unsupervised domain-adaption approach for dense retrieval models using synthetic queries. First, we fine-tune a T5-base <ref type="bibr" target="#b18">(Raffel et al., 2020)</ref> model to generate queries given a passage. We use the MSMARCO dataset and train for 2 epochs. Then, for a target corpus we generate 5 queries for each document using a combination of top-k and nucleus-sampling (top-k: 25; top-p: 0.95). Due to resource constraints, we cap the maximum number of target documents in each dataset to 100K. We found the T5 model to perform better compared to BART <ref type="bibr" target="#b3">(Lewis et al., 2020)</ref> and our decoding setting better as compared to beam-search. For retrieval, we continue to fine-tune the SBERT model (section 4.1.3) on the synthetic queries and document pairs. Note, this creates an independent model for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Reranking Models</head><p>BM25 + CE We retrieve the top-100 relevant documents using BM25 and rerank them using a ELECTRA cross-encoder model <ref type="bibr">(Clark et al., 2020)</ref>. The model has been fine-tuned on MS-MARCO <ref type="bibr" target="#b10">(Nguyen et al., 2016)</ref> using the approach described by <ref type="bibr" target="#b11">(Nogueira and Cho, 2020)</ref>.</p><p>ColBERT (Khattab and Zaharia, 2020) independently encodes the query and the document into a bag of token embeddings. The model first top-k retrieves candidates through approximate nearest neighbor search using faiss 7 <ref type="bibr">(Johnson et al., 2017)</ref> and then reranks those by computing the sum of maximum cosine similarity between the query and document token embeddings. We use the provided implementation 8 and retrieve the top-100 relevant documents using faiss. For reranking, we use a -6 -6 -10 -10 -11 -14 -15 -16</p><p>Figure 3: Comparison of zero-shot neural retrieval performances with BM25. Only Reranking-based models, i.e., BM25+CE and ColBERT outperform BM25 on more than half of the evaluation datasets in BEIR.</p><p>bert-base-uncased model fine-tuned on the MS-MARCO dataset for 300K steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metric</head><p>Depending upon the nature and requirements of real-world applications, retrieval tasks can be either be precision or recall focused. To obtain comparable results across models and datasets in BEIR, we argue that it is important to leverage a single evaluation metric that can be computed comparably across all tasks. Decision support metrics such as Precision and Recall which are both rank unaware are not suitable. Binary rank-aware metrics such as MRR (Mean Reciprocal Rate) and MAP (Mean Average Precision) fail to evaluate tasks with graded relevance judgements. We find that Normalised Cumulative Discount Gain (nDCG@k) provides a good balance suitable for both tasks involving binary and graded relevance judgements. <ref type="bibr">9</ref> We refer the reader to <ref type="bibr" target="#b38">Wang et al. (2013)</ref> for understanding the theoretical advantages of the metric. For our experiments, we utilize the Python interface of the official TREC evaluation tool 10 (Van Gysel and de Rijke, 2018) and compute nDCG@10 for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Zero-Shot Evaluation Results</head><p>In <ref type="table">Table 2</ref>, we report the nDCG@10 results for the retrieval approaches on a single in-domain dataset (MSMARCO) and 17 zero-shot datasets. <ref type="figure">Figure 3</ref> shows on how many datasets the neural approaches outperform (green) and underperform (red) our BM25 baseline in the zero-shot evaluation setup.</p><p>Overall, we observe that in-domain performance is not a good indicator of the generalization capability as discussed below:</p><p>Lexical Model Although BM25 underperforms neural approaches in the in-domain evaluation setting, the zero-shot results indicate that it is a strong baseline for generalization. It even achieves the best performance in 4 out of 17 tasks.</p><p>Sparse Model SPARTA outperforms BM25 by a large margin on the in-domain dataset, but performs poorly on BEIR datasets. It beats BM25 only for 3 out of 17 datasets with strong performance drops for other datasets.</p><p>Dense Models Dense retrieval approaches perform well only for certain datasets, while for others, there are strong performance drops compared to BM25. DPR and USE-QA perform poorly on nearly all the datasets. In contrast, SBERT and ANCE perform well for 6 and 7 datasets respectively. As we will show in section 7, these approaches fine-tuned on the MSMARCO dataset perform well if the domain is similar to the training dataset. Interestingly, GenQ, which further fine-tunes SBERT on synthetically generated query data is able to capture vital domain information. It outperforms SBERT on specialised domains such as scientific, finance or StackExchange domains. However, it underperforms SBERT on tasks over broad domains such as Wikipedia. We intuitively believe this is due to catastrophic forgetting during fine-tuning of GenQ with synthetic queries.</p><p>Reranking Models The experiments show strong results for both the re-ranking strategies: BM25 + CE and ColBERT. Both techniques achieve the best performances. They both outperform BM25 in 11 out of 17 datasets. This indicates re-ranking methods involving a cross-encoder generalize well to unseen datasets and domains, which is in line with the findings of other recent work <ref type="bibr">(Akkalyoncu Yilmaz et al., 2019;</ref><ref type="bibr" target="#b22">Rücklé et al., 2020;</ref><ref type="bibr" target="#b28">Thakur et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Retrieval Latency and Index Sizes</head><p>In this section, we compare different retrievers from a practical viewpoint. Models need to potentially compare a single query against millions of documents, hence, a high computational speed for retrieving results in real-time is desired. Precomputation and storing documents in indexes is vital at inference or during deployment on production systems. For our comparison, we randomly sample 1M documents from DBPedia <ref type="bibr">(Hasibi et al., 2017)</ref>. We evaluate retrieval for dense models using exact-search with an exception for ColBERT where   <ref type="table" target="#tab_3">Table 3</ref>.</p><p>Retrieval Latency The good generalization capabilities of the both reranking approaches come at the cost of high computational overhead, being slowest at inference. High latency of 0.4 and 0.6 seconds are neither desirable nor practical for many applications. In contrast, dense retrieval models are about 20-30x faster compared to the reranking models, enabling real-time responses. On CPU, the simpler model architectures such as SPARTA and BM25 dominate in terms of speed. Note, we did not specifically speed-optimize the implementations and used them as provided. With further optimizations, latency can potentially be reduced for all approaches, however the observed behaviour of the approaches will generally be the same.</p><p>Index Sizes Besides speed, the size of the index is a critical factor as well since it is usually stored in memory. Out of all the models, BM25 requires the smallest memory to store the index. SPARTA has the second largest index sizes, as it stores a 2000 dimensional sparse vector for each document, and ColBERT has the largest index size, as it stores a 128 dimensional dense vector for each document token. Index size is especially relevant for larger document collections: ColBERT stores an index of 900GB size for BioASQ with around 15M documents, while BM25 uses 18GB to store its index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>This section attempts to analyse a few retrieval techniques in depth and uncover interesting insights observed in our zero-shot experiments.</p><p>Dataset Annotation Biases The biggest performance drops in both dense and re-ranking techniques compared to BM25 are found in BioASQ, Signal-1M (RT) and Tóuche-2020 datasets. One possible reason lies in the construction of the datasets itself. In BioASQ 11 , document candidates are retrieved for annotation via term-matching with boosting tags <ref type="bibr" target="#b30">(Tsatsaronis et al., 2015)</ref>. Further, the annotation depth is shallow (approximately 5 relevant articles per query), whereas the total number of documents is around 15M, which favours lexical term-matching systems like BM25 <ref type="bibr">(Liang et al., 2020)</ref>. Creation of Signal-1M (RT), involved retrieving tweets for a query using 8 different retrieval systems for annotation. 7 out of these 8 techniques relied upon term-matching signals and we suspect this causes a bias towards lexical annotated documents <ref type="bibr" target="#b27">(Suarez et al., 2018)</ref>. With Tóuche-2020, we believe one of the reasons lies in the skewed length distribution of relevant documents as shown in <ref type="figure" target="#fig_2">Figure 4</ref>. All neural approaches have limitations with document lengths as they have limit of 512 word pieces. On the other hand BM25 greatly profits from longer documents as more keywords occur in them thereby increasing the chances of lexical overlap. Domain Shift The MSMARCO <ref type="bibr" target="#b10">(Nguyen et al., 2016)</ref> dataset is used for fine-tuning six out of the nine retrieval models. Similar to Shah et al.  and the target datasets present in BEIR. We calculate the overlap using a Weighted Jaccard Similarity score (Ioffe, 2010). From <ref type="table" target="#tab_5">Table 4</ref>, we observe that dense models are unable to generalize on target datasets with a small weighted Jaccard similarity score with MSMARCO, thereby under-performing BM25 on these datasets. To analyse the correlation, we calculate the Spearman's rank correlation (ρ) between the difference in performances of the dense and lexical-based models ranked accordingly from high to low unigram overlap. <ref type="bibr">12</ref> We achieve a correlation score (ρ) of 0.77 for SBERT and 0.71 for ANCE denoting a high correlation between domain overlap and the performance of dense retrievers.</p><p>Dot Product vs. Cosine Similarity Dense models require a similarity function to retrieve relevant documents for a given query within an embedding space. This similarity function is also used during training dense models with the InfoNCE (van den Oord et al., 2019) loss:</p><formula xml:id="formula_1">L q = − log exp(τ · sim(q, d + )) n i=0 exp(τ · sim(q, d i ))</formula><p>using n in-batch negatives for each query q and a scaling factor τ . where d + denotes the relevant (positive) document for query q. Commonly used similarity functions (sim(q, d)) are cosine- <ref type="bibr">12</ref> We drop BioASQ, Signal-1M (RT) and Tòuche-2020 datasets in the calculation of Spearman's rank correlation due to annotation biases as discussed in section 7. similarity (used by USE-QA and SBERT) and dotproduct (used by ANCE and DPR).</p><p>As far as we know, the differences between the two similarity functions were not systematically studied before. The performances of SBERT and ANCE with in-domain evaluation on MSMARCO are nearly identical. However, in the zero-shot evaluation setup, the two models can perform vastly different: on TREC-COVID, ANCE outperforms SBERT by 17.2 points, whereas SBERT outperforms ANCE by 9.7 points on HotpotQA.</p><p>We re-ran the training of SBERT (section 4.1.3) and only changed the similarity function from cosine-similarity to dot-product. As shown in Table 5 and <ref type="table" target="#tab_13">Table 10</ref> in the appendix, we observe significant performance differences for some datasets. For TREC-COVID, the model trained with dotproduct achieves the biggest improvement with 15.3 points, while for a majority of other datasets, it performs worse than the cosine-similarity model.</p><p>We observe that these (nearly) identical models retrieve documents with vastly different lengths as shown in the violin plots in <ref type="table" target="#tab_7">Table 5</ref> and <ref type="figure" target="#fig_5">Figure 6</ref> in the appendix. For all datasets, we find the cosine-similarity model to prefer shorter documents over longer ones. This is especially severe for TREC-COVID: a large fraction of the scientific papers (approx. 42k out of 171k) consist only of publication titles without an abstract. The cosinesimilarity model prefers retrieving these documents. In contrast, the dot-product model primarily retrieves longer documents, i.e., publications with an abstract. Cosine-similarity uses vectors of unit length, thereby having no notion of the encoded text length. In contrast, for dot-product, longer documents can result in vectors with higher magnitudes which can yield higher similarity scores for a query.</p><p>As we previously saw in <ref type="figure" target="#fig_2">Figure 4</ref>, relevance scores are not uniformly distributed over document lengths: for some datasets, longer documents are annotated with higher relevancy scores, while in others, shorter documents are. This can be either due to the annotation process, e.g., the candidate selection method prefers short or long documents, or due to the task itself, where shorter or longer documents could be more relevant to the user information need. Hence, it can be more advantageous to train a model with either cosine-similarity or dot-product depending upon the nature and needs of the specific task.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work, we presented BEIR: a heterogeneous benchmark for information retrieval. In contrast to the existing benchmarks, we provided a broader selection of target tasks ranging from narrow expert domains to open domain datasets. We included 9 different retrieval tasks spanning 17 diverse datasets. BEIR marks the first large scale benchmark for the evaluation of zero-shot transfer capabilities of retrieval models to a wide range of settings.</p><p>We broadly studied the effectiveness of nine different retrieval models on BEIR. We show that BM25 is a strong baseline for zero-shot evaluation. We demonstrated, that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Overall, we demonstrate Reranking-based models generalize well across all the datasets, at the cost of slower retrieval times. Dense models outperform BM25 when a high domain overlap is present between the training dataset and the evaluated target dataset. Dense models, similar to BM25, are fast in retrieval and produce lightweight indexes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Limitations of BEIR</head><p>Even though we cover a wide range of tasks and domains in BEIR, no benchmark is perfect and has its limitations. Making those explicit is a critical point in understanding the results on the benchmark and, for future work, to improve up-on the benchmark.</p><p>Multilingual Tasks Currently, our benchmark is focused on English tasks. We would like to extend BEIR to include multi-and cross-lingual tasks and models.</p><p>Long Document Retrieval Most of our tasks have average document lengths up-to a few hun-dred words roughly equivalent to a few paragraphs. Including tasks that require the retrieval of longer documents would be highly relevant. However, as transformer-based approaches often have a length limit of 512 word pieces, a fundamental different setup would be required to compare approaches.</p><p>Multi-factor Search Until now, we focused on pure textual search in BEIR. In many real-world applications, further signals are used to estimate the relevancy of documents, such as PageRank <ref type="bibr" target="#b15">(Page et al., 1999)</ref>, recency <ref type="bibr">(Dong et al., 2010)</ref>, authority score <ref type="bibr" target="#b0">(Kleinberg, 1999)</ref> or user-interactions such as click-through rates <ref type="bibr" target="#b17">(Radlinski et al., 2008)</ref>. The integration of such signals in the tested approaches is often not straight-forward and is an interesting direction for research.</p><p>Multi-field Retrieval Retrieval can often be performed over multiple fields. For example, for scientific publication we have the title, the abstract, the document body, the authors list, and the journal name. So far we focused only on datasets that have one or two fields.</p><p>Personalised Search Document relevancy can vary with a user personal interest or preference <ref type="bibr">(Ghorab et al., 2013)</ref>. We currently rank relevant documents from a single global annotated relevant or not-relevant label. In future, we would like to incorporate personal preferences within the benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Future Work</head><p>In this work, we presented BEIR together with results and analysis for nine different retrieval methods. However, BEIR offers plenty more opportunities for future work. Some of the most interesting are: Understanding why approaches generalize differently across tasks and how to improve the zero-shot performance for approaches? Further, it would be interesting to be able to predict when a certain pre-trained model performs well and when it does not? As we have shown, re-ranking approaches generally perform better than other approaches, but come at a higher computational cost: Understanding how to reduce the computational overhead while maintaining the good generalization capability would be of high practical value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work has been supported by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 03VP02540 (Ar-gumenText), by the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1 and grant GU 798/17-1) and has been funded by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE. We would like to thank Tilman Beck, Kexin Wang and Jorge Cardona for their feedback. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head><p>In this Appendix, we mention the following sections in detail: Training and in-domain evaluation (B), descriptions of all zero-shot tasks and datasets in BEIR (C). We further explain how we calculate weighted jaccard similarity (D) and capped recall at k (E) metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training and In-domain Evaluation</head><p>We use the MSMARCO Passage Ranking dataset <ref type="bibr" target="#b10">(Nguyen et al., 2016)</ref>, which contains 8.8M Passages and an official training set of 532,761 querypassage pairs for fine-tuning for a majority of retrievers. The dataset contains queries from Bing search logs with one text passage from various web sources annotated as relevant. We find the dataset useful for training, in terms of covering a wide variety of topics and providing the highest number of training pairs. It has been extensively explored and used for fine-tuning dense retrievers in recent works <ref type="bibr" target="#b11">(Nogueira and Cho, 2020;</ref><ref type="bibr">Gao et al., 2020;</ref><ref type="bibr">Ding et al., 2020)</ref>. We use the official MSMARCO development set for our in-domain evaluation which has been widely used in prior research <ref type="bibr" target="#b11">(Nogueira and Cho, 2020;</ref><ref type="bibr">Gao et al., 2020;</ref><ref type="bibr">Ding et al., 2020)</ref>. It has 6,980 queries. Most of the queries have only 1 document judged relevant; the labels are binary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Zero-shot Evaluation Tasks</head><p>Following the selection criteria mentioned in section 3, we include 17 evaluation datasets that span over 9 heterogeneous tasks. Each dataset contains a document corpus denoted by T and test queries for evaluation denoted by Q. We additionally provide website-links in <ref type="table" target="#tab_9">Table 6</ref> and intuitive examples in <ref type="table" target="#tab_11">Table 8</ref>. We now summarise each task and describe each dataset included for evaluation in BEIR:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Bio-Medical Information Retrieval</head><p>Bio-medical information retrieval is the task of searching relevant scientific documents such as research papers or blogs for a given scientific query in the biomedical domain <ref type="bibr">(Jiang and Zhai, 2007)</ref>.</p><p>We consider a scientific query as input and retrieve bio-medical documents as output.</p><p>TREC-COVID <ref type="figure" target="#fig_0">(Voorhees et al., 2021)</ref> is an ad-hoc search challenge based on the CORD-19 dataset containing scientific articles related to the COVID-19 pandemic <ref type="bibr" target="#b36">(Wang et al., 2020)</ref>. We include the July 16, 2020 version of CORD-19 13 as corpus T and use the final cumulative judgements of the original task as queries Q.</p><p>NFCorpus (Boteva et al., 2016) contains natural language queries harvested from NutritionFacts 14 (NF). We use the original splits provided alongside all content sources from NF (videos, blogs, and Q&amp;A posts) as queries Q and annotated medical documents from PubMed 15 as corpus T.</p><p>BioASQ <ref type="bibr" target="#b30">(Tsatsaronis et al., 2015)</ref> Task 8b is a biomedical semantic QA challenge. We use the original train and test splits provided in Task 8b as queries Q and collect around 15M articles from PubMed provided in Task 8a as our corpus T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Question Answering</head><p>Retrieval in open domain question answering (Chen et al., 2017) is the task of retrieving the correct answer for a question, without a predefined location for the answer. In open-domain tasks, model must retrieve over an entire knowledge source (such as Wikipedia). We consider the question as input and the passage containing the answer as output.</p><p>Natural Questions  contains Google search queries and documents with paragraphs and answer spans within Wikipedia articles. We did not use the NQ version from ReQA (Ahmad et al., 2019) as it focused on queries having a short answer. As a result, we parsed the HTML of the original NQ dataset and include more complex development queries that often require a longer passage as answer compared to ReQA. We filtered out queries without an answer, or having a table as an answer, or with conflicting Wikipedia pages. We retain 2,681,468 passages as our corpus T and 3452 test queries Q from the original dataset.</p><p>HotpotQA <ref type="bibr">(Yang et al., 2018)</ref> contains questions which require reasoning over multiple paragraphs to find the correct answer. We include the fullwiki task setting: utilizing processed Wikipedia passages as corpus T. We held out randomly sampled 5447 queries from training as development split. We use the original task's development set as our test queries Q.</p><p>FiQA-2018 <ref type="bibr" target="#b9">(Maia et al., 2018)</ref> Task 2 consists of opinion-based QA. We include financial data by crawling StackExchange posts under the Investment topic from 2009-2017 as our corpus T. We randomly sample out 500 and 648 as queries Q from the training split as development and test splits respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Tweet Retrieval</head><p>Twitter is a popular micro-blogging website on which people post real-time messages (i.e. tweets) about their opinions on a variety of topics and discuss current issues. We consider a news headline as input and retrieve relevant tweets as output.</p><p>Signal-1M Related Tweets <ref type="bibr" target="#b27">(Suarez et al., 2018)</ref> includes 97 news articles from the Signal-1M dataset (Corney et al., 2016) as queries Q. We construct our twitter corpus T by manually scraping tweets from given tweet-ids using Tweepy 16 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 News Retrieval</head><p>TREC-NEWS <ref type="bibr" target="#b26">(Soboroff et al., 2019)</ref> 2019 track involves background linking: Given a news headline, we retrieve relevant news articles that provide important context or background information. We include the shared task data as our test queries Q and the TREC Washington Post 17 as our corpus T. We convert the original exponential gain relevant judgements to linear labels for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Argument Retrieval</head><p>Argument retrieval is the task of ranking argumentative texts in a collection of focused arguments (output) in order of their relevance to a textual query (input) on different topics.</p><p>ArguAna Counterargs Corpus <ref type="bibr" target="#b35">(Wachsmuth et al., 2018)</ref> involves the retrieval of the best counterargument to an argument. It contains pairs of argument and counterargument scraped from the online debate portal 18 as corpus T. We consider the original arguments present in the original tasks test split as our queries Q.</p><p>Touchè-2020 (Bondarenko et al., 2020) Task 1 is a conversational argument retrieval task. We use the conclusion as title and premise for arguments present in args.me 19 <ref type="bibr" target="#b34">(Wachsmuth et al., 2017)</ref> as corpus T. We include the shared task data as our test queries Q. The original relevance judgements (qrels) file also included negative judgements (-2), but for simplicity we substitute them as zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Duplicate Question Retrieval</head><p>Duplicate question retrieval is the task of identifying duplicate questions asked in community question answering (cQA) forums. A given query is the input and the duplicate questions are the output.</p><p>CQADupStack <ref type="figure" target="#fig_0">(Hoogeveen et al., 2015)</ref> is a benchmark dataset for community questionanswering (cQA) research. The corpus T comprises of queries from 12 different StackExchange subforums. We utilize the predefined test split for our queries Q. We evaluate each subforum separately and report the overall mean score.</p><p>Quora Duplicate Questions dataset identifies whether two questions are duplicates. Quora originally released containing 404,290 question pairs. We add transitive closures to the original dataset. Further, we split it into train, dev, and test sets with a ratio of about 85%, 5% and 10% of the original pairs. We remove all overlaps between the splits and ensure that a question in one split of the dataset does not appear in any other split to mitigate the transductive classification problem <ref type="bibr">(Ji et al., 2010)</ref>. We finally achieve 522,931 unique queries as our corpus T and 5,000 dev and 10,000 test queries Q respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 Entity Retrieval</head><p>Entity retrieval involves retrieving unique Wikipedia pages to entities mentioned in the query. This is crucial for tasks involving Entity Linking (EL). The entity-bearing query is the input and the entity abstract and title are retrieved as output. <ref type="bibr">-Entity-v2 (Hasibi et al., 2017)</ref> is an established entity retrieval dataset. It contains a set of heterogeneous entity-bearing queries Q and retrieves entities from the english part of DBpedia corpus T from October 2015. We randomly sample out 67 queries from the test split as development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DBPedia</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8 Citation Prediction</head><p>Citations are a key signal of relatedness between scientific papers <ref type="bibr" target="#b36">(Cohan et al., 2020)</ref>. In this task, the model attempts to retrieve cited papers (output) for a given query paper title (input). <ref type="bibr" target="#b36">(Cohan et al., 2020)</ref> contains a corpus T of 30K held-out pool of scientific papers. We consider the original direct-citations task as our retrieval task. It includes 1k papers as queries Q and 5 relevant papers and 25 (randomly selected) uncited papers for each query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCIDOCS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.9 Fact Checking</head><p>Fact checking verifies a claim against a big collection of evidence <ref type="bibr" target="#b29">(Thorne et al., 2018)</ref>. The task requires knowledge about the claim and reasoning over multiple documents. We consider the claim as input and the relevant document passage verifying the claim as output.</p><p>FEVER <ref type="bibr" target="#b29">(Thorne et al., 2018)</ref> The Fact Extraction and VERification dataset is collected to facilitate the automatic fact checking. We utilize the original paper splits as queries Q and retrieve evidences from the pre-processed Wikipedia Pages (June 2017 dump) as our corpus T.</p><p>Climate-FEVER (Diggelmann et al., 2020) is a dataset for verification of real-world climate claims. We include the claims as queries Q and retrieve evidences from the same FEVER Wiki corpus T.</p><p>SciFact <ref type="bibr" target="#b36">(Wadden et al., 2020)</ref> retrieves abstracts from the research literature containing evidence for a given scientific claim. We use the original dev split from the task containing 300 queries as our test queries Q, and include documents from the original scientific corpus T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Weighted Jaccard Similarity</head><p>The weighted Jaccard similarity J(S, T ) (Ioffe, 2010) is intuitively calculated as the unique word overlap for all words present in both the datasets. More formally, the normalized frequency for an unique word k in a dataset is calculated as the frequency of word k divided over the sum of frequencies of all words in the dataset. S k is the normalized frequency of word k in the source dataset S and T k for the target dataset T respectively. The weighted Jaccard similarity between S and T is defined as:</p><formula xml:id="formula_2">J(S, T ) = k min(S k , T k ) k max(S k , T k )</formula><p>where the sum is over all unique words k present in datasets S and T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Capped Recall@k Score</head><p>Recall at k is calculated as the fraction of the relevant documents that are successfully retrieved within the top k extracted documents. More formally, the R@k score is calculated as:</p><formula xml:id="formula_3">R@k = 1 |Q| |Q| i=1 | max k (A i ) ∩ A i | |A i |</formula><p>where Q is the set of queries, A i is the set of relevant documents for the ith query, and A i is a scored list of documents provided by the model, from which top k are extracted.</p><p>However measuring recall can be counterintuitive, if a high number of relevant documents (&gt; k) are present within a dataset. For example, consider a hypothetical dataset with 500 relevant documents for a query. Retrieving all relevant documents would produce a maximum R@100 score = 0.2, which is quite low and unintuitive. To avoid this we cap the recall score (R_cap@k) at k for datasets if the number of relevant documents for a query greater than k. It is defined as:</p><formula xml:id="formula_4">R_cap@k = 1 |Q| |Q| i=1 | max k (A i ) ∩ A i | min(k, |A i |)</formula><p>where the only difference lies within the denominator where we compute the minimum of k and |A i |, instead of |A i | present in the original recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC-COVID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BioASQ</head><p>NFCorpus   Climate-FEVER Sea level rise is now increasing faster than predicted due to unexpectedly rapid ice melting.</p><p>&lt;Title&gt; Sea level rise &lt;Paragraph&gt; A sea level rise is an increase in the volume of water in the world 's oceans, resulting in an increase in global mean sea level. The rise is usually attributed to global climate change by thermal expansion of the water in the oceans and by melting of Ice sheets and glaciers...  <ref type="table">Table 9</ref>: In-domain and zero-shot retrieval performance on BEIR datasets. Scores denote Recall@100. The best retrieval performance on a given dataset is marked in bold, and the second best performance is underlined. ‡ indicates in-domain retrieval performance. indicates the capped recall score: R_cap@100 (Appendix E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSMARCO TREC-COVID BioASQ NFCorpus</head><p>Cosine-Sim.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>A 2D-visualization of the pairwise weighted jaccard similarity scores (Figure 5) using the forcedirected placement algorithm with NetworkX (Hag</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Annotated relevant document lengths (in words) for Tóuche-2020 (Bondarenko et al., 2020). Majority of the highly relevant documents are greater than 300-350 words providing a better score with BM25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>0.17 0.15 0.33 0.22 0.3 0.36 0.29 0.37 0.29 0.16 0.15 0.13 0.44 0.89 0.24 0.17 0.21 0.2 0.15 0.22 0.32 0.29 0.23 0.26 0.16 0.16 0.22 0.24 0.23 0.26 0.22 0.16 0.18 0.18 0.15 0.52 0.8 0.27 0.2 0.24 0.24 0.18 0.25 0.78 0Heatmap showing pairwise weighted jaccard similarity (Ioffe, 2010) on unigram word overlap between the document collections for all datasets in BEIR..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Violin plots(Hintze and Nelson, 1998)  of document lengths for top-10 retrieved hits for SBERT (section 4.1.3) trained with either cosine-similarity or dot-product. The x-axis shows the retrieved document length, capped at a max length of 512 words. The dashed lines within the plot denotes the corresponding (25%, 50% and 75%) quantiles. The plot shows the cosine-similarity (top, blue) model across all datasets retrieves on average shorter length documents compared to the dot product model (bottom, orange).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>model 6 on MS-MARCO<ref type="bibr" target="#b10">(Nguyen et al., 2016)</ref>. During fine-tuning, we include in-batch negatives(Gillick et al., 2018)   </figDesc><table><row><cell>Model (→)</cell><cell>Lexical</cell><cell>Sparse</cell><cell></cell><cell cols="2">Dense / Neural</cell><cell></cell><cell></cell><cell cols="2">Reranking</cell></row><row><cell>Dataset (↓)</cell><cell>BM25</cell><cell cols="2">SPARTA USE-QA</cell><cell>DPR</cell><cell cols="5">ANCE SBERT GenQ BM25+CE ColBERT</cell></row><row><cell>MSMARCO</cell><cell>0.218</cell><cell>0.351  ‡</cell><cell>0.259</cell><cell>0.177</cell><cell>0.388  ‡</cell><cell>0.389  ‡</cell><cell>0.389  ‡</cell><cell>0.384  ‡</cell><cell>0.425  ‡</cell></row><row><cell>TREC-COVID</cell><cell>0.616</cell><cell>0.538</cell><cell>0.528</cell><cell>0.332</cell><cell>0.654</cell><cell>0.482</cell><cell>0.554</cell><cell>0.667</cell><cell>0.677</cell></row><row><cell>BioASQ</cell><cell>0.514</cell><cell>0.351</cell><cell>0.093</cell><cell>0.127</cell><cell>0.306</cell><cell>0.295</cell><cell>0.351</cell><cell>0.489</cell><cell>0.474</cell></row><row><cell>NFCorpus</cell><cell>0.297</cell><cell>0.301</cell><cell>0.252</cell><cell>0.189</cell><cell>0.237</cell><cell>0.257</cell><cell>0.293</cell><cell>0.303</cell><cell>0.305</cell></row><row><cell>NQ</cell><cell>0.310</cell><cell>0.398</cell><cell>0.180</cell><cell>0.474  ‡</cell><cell>0.446</cell><cell>0.450</cell><cell>0.360</cell><cell>0.516</cell><cell>0.524</cell></row><row><cell>HotpotQA</cell><cell>0.601</cell><cell>0.492</cell><cell>0.258</cell><cell>0.391</cell><cell>0.456</cell><cell>0.513</cell><cell>0.497</cell><cell>0.701</cell><cell>0.593</cell></row><row><cell>FiQA-2018</cell><cell>0.239</cell><cell>0.198</cell><cell>0.264</cell><cell>0.112</cell><cell>0.295</cell><cell>0.258</cell><cell>0.284</cell><cell>0.326</cell><cell>0.317</cell></row><row><cell>Signal-1M (RT)</cell><cell>0.388</cell><cell>0.252</cell><cell>0.241</cell><cell>0.155</cell><cell>0.249</cell><cell>0.261</cell><cell>0.257</cell><cell>0.308</cell><cell>0.274</cell></row><row><cell>TREC-NEWS</cell><cell>0.371</cell><cell>0.258</cell><cell>0.340</cell><cell>0.161</cell><cell>0.382</cell><cell>0.367</cell><cell>0.369</cell><cell>0.430</cell><cell>0.393</cell></row><row><cell>ArguAna</cell><cell>0.441</cell><cell>0.279</cell><cell>0.048</cell><cell>0.175</cell><cell>0.415</cell><cell>0.429</cell><cell>0.517</cell><cell>0.313</cell><cell>0.233</cell></row><row><cell>Tóuche-2020</cell><cell>0.605</cell><cell>0.231</cell><cell>0.252</cell><cell>0.127</cell><cell>0.284</cell><cell>0.249</cell><cell>0.226</cell><cell>0.378</cell><cell>0.275</cell></row><row><cell>CQADupStack</cell><cell>0.316</cell><cell>0.257</cell><cell>0.236</cell><cell>0.153</cell><cell>0.296</cell><cell>0.306</cell><cell>0.339</cell><cell>0.348</cell><cell>0.350</cell></row><row><cell>Quora</cell><cell>0.742</cell><cell>0.630</cell><cell>0.767</cell><cell>0.248</cell><cell>0.852</cell><cell>0.855</cell><cell>0.853</cell><cell>0.778</cell><cell>0.854</cell></row><row><cell>DBPedia</cell><cell>0.288</cell><cell>0.314</cell><cell>0.238</cell><cell>0.263</cell><cell>0.281</cell><cell>0.339</cell><cell>0.320</cell><cell>0.380</cell><cell>0.392</cell></row><row><cell>SCIDOCS</cell><cell>0.156</cell><cell>0.126</cell><cell>0.104</cell><cell>0.077</cell><cell>0.122</cell><cell>0.133</cell><cell>0.148</cell><cell>0.154</cell><cell>0.145</cell></row><row><cell>FEVER</cell><cell>0.648</cell><cell>0.596</cell><cell>0.546</cell><cell>0.562</cell><cell>0.669</cell><cell>0.670</cell><cell>0.641</cell><cell>0.793</cell><cell>0.771</cell></row><row><cell>Climate-FEVER</cell><cell>0.179</cell><cell>0.082</cell><cell>0.113</cell><cell>0.148</cell><cell>0.198</cell><cell>0.205</cell><cell>0.220</cell><cell>0.246</cell><cell>0.184</cell></row><row><cell>SciFact</cell><cell>0.620</cell><cell>0.582</cell><cell>0.312</cell><cell>0.318</cell><cell>0.507</cell><cell>0.531</cell><cell>0.592</cell><cell>0.524</cell><cell>0.671</cell></row><row><cell>AVERAGE</cell><cell>0.419</cell><cell>0.346</cell><cell>0.280</cell><cell>0.233</cell><cell>0.391</cell><cell>0.388</cell><cell>0.401</cell><cell>0.447</cell><cell>0.437</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Estimated retrieval latency for a single query and index sizes for 1 million documents in DBPedia. Ranked from best to worst zero-shot performance. Lower the latency and size is better.</figDesc><table><row><cell>we followed the original implementation (Khattab</cell></row><row><cell>and Zaharia, 2020). For both reranking approaches</cell></row><row><cell>we evaluate performances for top-100 hits. CPU</cell></row><row><cell>performances were measured with an 8 core Intel</cell></row><row><cell>Xeon Platinum 8168 CPU @ 2.70GHz. GPU per-</cell></row><row><cell>formances were measured using a single Nvidia</cell></row><row><cell>Tesla V100, CUDA 11.0 and cuDNN. Results are</cell></row><row><cell>shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Domain shift analysis with MSMARCO (Nguyen et al., 2016). Datasets are sorted from high to low unigram weighted Jaccard similarity (Ioffe, 2010). ANCE and SBERT outperform BM25 (green) more of- ten when MSMARCO and the target dataset share a high domain overlap.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Signal-1M (RT)</cell><cell>FEVER</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell cols="2">SBERT (Method) Cosine-Sim. Dot-Prod.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell cols="2">100 200 300 400 500</cell></row><row><cell cols="2">TREC-COVID</cell><cell cols="2">Signal-1M (RT)</cell><cell>FEVER</cell><cell></cell></row><row><cell>Cosine-0.482</cell><cell>0.635</cell><cell>0.261</cell><cell>0.243</cell><cell>0.670</cell><cell>0.685</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Violin plots (Hintze and Nelson, 1998) of</cell></row><row><cell>document lengths for the top-10 retrieved hits and</cell></row><row><cell>nDCG@10 scores using SBERT (section 4.1.3) model</cell></row><row><cell>trained with either cosine similarity (blue, top) or dot</cell></row><row><cell>product (orange, bottom).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Demian Gholipour, Artem Sokolov, and Stefan Riezler. 2016. A full-text learning to rank dataset for medical information retrieval. In Proceedings of the 38th European Conference on Information Retrieval (ECIR 2016), pages 716-722. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,</figDesc><table><row><cell>Zhilin Yang, pages 2369-2380, Brussels, Belgium. Association</cell></row><row><cell>Amin Ahmad, Noah Constant, Yinfei Yang, and Daniel Cer. 2019. ReQA: An evaluation for end-to-end an-swer retrieval models. In Proceedings of the 2nd Workshop on Machine Reading for Question Answer-ing, pages 137-146, Hong Kong, China. Association for Computational Linguistics. Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Cross-domain mod-eling of sentence-level evidence for document re-trieval. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP 2019), pages 3490-3496. Association for Computational Linguistics. Petr Baudiš and Jan Šedivỳ. 2015. Modeling of the question answering task in the yodaqa system. In In-ternational Conference of the Cross-Language Eval-uation Forum for European Languages, pages 222-228. Springer. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan-guage Processing, pages 1533-1544, Seattle, Wash-ington, USA. Association for Computational Lin-guistics. Adam Berger, Rich Caruana, David Cohn, Dayne Fre-itag, and Vibhu Mittal. 2000. Bridging the lexical chasm: statistical approaches to answer-finding. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 192-199. Alexander Bondarenko, Maik Fröbe, Meriem Be-loucif, Lukas Gienapp, Yamen Ajjour, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, Martin Potthast, and Matthias Hagen. 2020. Overview of Touché 2020: Argument Re-trieval. In Working Notes Papers of the CLEF 2020 Evaluation Labs, volume 2696 of CEUR Workshop Proceedings. for Computational Linguistics. Yun Zhang, David Lo, Xin Xia, and Jian-Ling Sun. 2015. Multi-factor duplicate question detection in stack overflow. Journal of Computer Science and Technology, 30(5):981-997. Vera Boteva, In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Compu-tational Linguistics. Danqi Chen, Adam Fisch, Jason Weston, and Antoine Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learn-ing Representations. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. 2020. SPECTER: Document-level representation learning using citation-informed transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2270-2282, Online. Association for Computational Linguistics. Davind Corney, Dyaa Albakour, Miguel Martinez, and Samir Moussa. 2016. What do a million news ar-ticles look like? In Proceedings of the First In-ternational Workshop on Recent Trends in News In-formation Retrieval co-located with 38th European Conference on Information Retrieval (ECIR 2016), pages 42-47. Zhuyun Dai and Jamie Callan. 2020. Context-aware term weighting for first stage passage retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Infor-mation Retrieval, SIGIR '20, page 1533-1536, New York, NY, USA. Association for Computing Machin-ery. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language under-standing. arXiv preprint arXiv:1810.04805. Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-lian, Massimiliano Ciaramita, and Markus Leippold. 2020. Climate-fever: A dataset for verification of real-world climate claims. puting symposium, pages 1-8. Sergey Ioffe. 2010. Improved consistent sampling, weighted minhash and l1 sketching. In 2010 IEEE International Conference on Data Mining, pages 246-255. IEEE. Ming Ji, Yizhou Sun, Marina Danilevsky, Jiawei Han, and Jing Gao. 2010. Graph regularized transduc-tive classification on heterogeneous information net-works. In Machine Learning and Knowledge Dis-covery in Databases, pages 570-586, Berlin, Heidel-berg. Springer Berlin Heidelberg. Jing Jiang and ChengXiang Zhai. 2007. An empiri-cal study of tokenization strategies for biomedical information retrieval. Information Retrieval, 10(4-5):341-363. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale dis-tantly supervised challenge dataset for reading com-prehension. In Proceedings of the 55th Annual Meet-ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Van-couver, Canada. Association for Computational Lin-guistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Lin-guistics. Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-cient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR Conference on Research Bordes. 2017. Reading Wikipedia to answer open-Tiancheng Zhao, Xiaopeng Lu, and Kyusong Lee. domain questions. In Proceedings of the 55th An-2020. Sparta: Efficient open-domain question an-nual Meeting of the Association for Computational Proceedings of the 20th Australasian document com-tional Linguistics. set for community question-answering research. In 1879, Vancouver, Canada. Association for Computa-Baldwin. 2015. Cqadupstack: A benchmark data Linguistics (Volume 1: Long Papers), pages 1870-Doris Hoogeveen, Karin M Verspoor, and Timothy swering via sparse transformer matching retrieval.</cell></row><row><cell>and Development in Information Retrieval, SIGIR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Original dataset website (link) for all datasets present in BEIR. SBERT (Cos-Sim.) https://huggingface.co/sentence-transformers/msmarco-distilbert-base-v3 SBERT (Dot-Prod.) https://huggingface.co/sentence-transformers/msmarco-distilbert-base-dot-prod-v3 ELECTRA (CE) https://huggingface.co/sentence-transformers/ce-ms-marco-electra-base SPARTA https://huggingface.co/BeIR/sparta-msmarco-distilbert-base-v1 ANCE https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/msmarco-roberta-base-ance-fristp.zip DPR (Query) https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/facebook-dpr-question_encoder-multiset-base.zip DPR (Context) https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/facebook-dpr-ctx_encoder-multiset-base.zip ColBERT https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/models/ColBERT/msmarco.psg.l2.zip</figDesc><table><row><cell>Model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Neural model and checkpoints used for evaluation in BEIR. Passiflora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned, white fleshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, while others list the fruits as being bitter and inedible. assiflora herbertiana. A rare passion fruit native to Australia... Origin of the COVID-19 virus has been intensely debated in the community...BioASQWhat is the effect of HMGB2 loss on CTCF clustering &lt;Title&gt; HMGB2 Loss upon Senescence Entry Disrupts Genomic Organization and Induces CTCF Clustering across Cell Types. &lt;Paragraph&gt; Processes like cellular senescence are characterized by complex events giving rise to heterogeneous cell populations. However, the early molecular events driving this cascade remain elusive.... Titanium Dioxide Nanoparticles in Food and Personal Care Products &lt;Paragraph&gt; Titanium dioxide is a common additive in many food, personal care, and other consumer products used by people, which after use can enter the sewage system, and subsequently enter the environment as treated effluent discharged to surface waters or biosolids applied to agricultural land, or incinerated wastes... Tobacco advertising &lt;Paragraph&gt; The first calls to restrict advertising came in 1962 from the Royal College of Physicians, who highlighted the health problems and recommended stricter laws...HotpotQAStockely Webster has paintings hanging in what home (that serves as the residence for the Mayor of New York)?&lt;Title&gt; Stokely Webster &lt;Paragraph&gt; Stokely Webster(1912 -2001)  was best known as an American impressionist painter who studied in Paris. His paintings can be found in the permanent collections of many museums, including the Metropolitan Museum of Art in New York, the National Museum...&lt;Paragraph&gt; PEG is Price/Earnings to Growth. It is calculated as Price/Earnings/Annual EPS Growth. It represents how good a stock is to buy, factoring in growth of earnings, which P/E does not. Obviously when PEG is lower, a stock is more undervalued, which means that it is a better buy, and more likely... Senate launches bill to remove immunity for websites hosting illegal content, spurred by Backpage.com &lt;Paragraph&gt; The legislation, along with a similar bill in the House, sets the stage for a battle between Congress and some of the Internet's most powerful players, including Google and various free-speech advocates, who believe that Congress shouldn't regulate Web content or try to force websites to police themselves more rigorously...ArguAnaSexist advertising is subjective so would be too difficult to codify. Effective advertising appeals to the social, cultural, and personal values of consumers. Through the connection of values to products, services and ideas, advertising is able to accomplish its goal of adoption...&lt;Title&gt; media modern culture television gender house would ban sexist advertising &lt;Paragraph&gt; Although there is a claim that sexist advertising is to difficult to codify, such codes have and are being developed to guide the advertising industry. These standards speak to advertising which demeans the status of women, objectifies them, and plays upon stereotypes about women which harm women and society in general.Earlier the Council of Europe was mentioned, Denmark, Norway and Australia as specific examples of codes or standards for evaluating sexist advertising which have been developed. America should support blanket amnesty for illegal immigrants. &lt;Paragraph&gt; Undocumented workers do not receive full Social Security benefits because they are not United States citizens " nor should they be until they seek citizenship legally. Illegal immigrants are legally obligated to pay taxes... Combing head and tail in a single call via pipe &lt;Paragraph&gt; On a regular basis, I am piping the output of some program to either 'head' or 'tail'. Now, suppose that I want to see the first AND last 10 lines of piped output, such that I could do something like ./lotsofoutput | headtail... The New York Trilogy &lt;Paragraph&gt; The New York Trilogy is a series of novels by Paul Auster. Originally published sequentially as City of Glass (1985), Ghosts (1986) and The Locked Room (1986), it has since been collected into a single volume. SCIDOCS CFD Analysis of Convective Heat Transfer Coefficient on External Surfaces of Buildings &lt;Title&gt; Application of CFD in building performance simulation for the outdoor environment: an overview &lt;Paragraph&gt; This paper provides an overview of the application of CFD in building performance simulation for the outdoor environment, focused on four topics... True Underdog Story &lt;Paragraph&gt; DodgeBall: A True Underdog Story is a 2004 American sports comedy film written and directed by Rawson Marshall Thurber and starring Vince Vaughn and Ben Stiller. The film follows friends who enter a dodgeball tournament...</figDesc><table><row><cell>Dataset</cell><cell>Query</cell><cell>Relevant-Document</cell></row><row><cell cols="3">MSMARCO &lt;Paragraph&gt; TREC-COVID what fruit is native to australia what is the origin of COVID-19 &lt;Title&gt; Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using Artificial</cell></row><row><cell cols="3">Intelligence &lt;Paragraph&gt; NFCorpus Titanium Dioxide &amp; Inflammatory Bowel Dis-ease &lt;Title&gt; NQ when did they stop cigarette advertising on tele-vision? &lt;Title&gt; FiQA-2018 What is the PEG ratio? How is the PEG ratio</cell></row><row><cell></cell><cell>calculated? How is the PEG ratio useful for</cell><cell></cell></row><row><cell></cell><cell>stock investing?</cell><cell></cell></row><row><cell>Signal-1M (RT)</cell><cell>Genvoya, a Gentler Anti-HIV Cocktail, Okayed</cell><cell>&lt;Paragraph&gt; All people with #HIV should get anti-retroviral drugs: @WHO, by @kkelland via</cell></row><row><cell></cell><cell>by EU Regulators</cell><cell>@Reuters_Health #AIDS #TasP</cell></row><row><cell cols="3">TREC-NEWS &lt;Title&gt; Tóuche-2020 Websites where children are prostituted are im-mune from prosecution. But why? Should the government allow illegal immigrants to become citizens? &lt;Title&gt; CQADupStack Command to display first few and last few lines of a file &lt;Title&gt; Quora How long does it take to methamphetamine out &lt;Paragraph&gt; How long does it take the body to get rid of methamphetamine?</cell></row><row><cell></cell><cell>of your blood?</cell><cell></cell></row><row><cell cols="3">DBPedia &lt;Title&gt; FEVER Paul Auster novels DodgeBall: A True Underdog Story is an Amer-&lt;Title&gt; DodgeBall: A</cell></row><row><cell></cell><cell>ican movie from 2004</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Examples of queries and relevant documents for all datasets included in BEIR. (&lt;Title&gt;) and (&lt;Para-graph&gt;) are used to distinguish the title separately from the paragraph within a document in the table above. These tokens were not passed to the respective models.</figDesc><table><row><cell>Model (→)</cell><cell>Lexical</cell><cell>Sparse</cell><cell></cell><cell cols="2">Dense / Neural</cell><cell></cell><cell></cell><cell cols="2">Reranking</cell></row><row><cell>Dataset (↓)</cell><cell>BM25</cell><cell cols="2">SPARTA USE-QA</cell><cell>DPR</cell><cell cols="5">ANCE SBERT GenQ BM25+CE ColBERT</cell></row><row><cell>MSMARCO</cell><cell>0.621</cell><cell>0.793  ‡</cell><cell>0.720</cell><cell>0.552</cell><cell>0.852  ‡</cell><cell>0.847  ‡</cell><cell>0.847  ‡</cell><cell>0.621  ‡</cell><cell>0.865  ‡</cell></row><row><cell>TREC-COVID</cell><cell>0.447</cell><cell>0.409</cell><cell>0.339</cell><cell>0.212</cell><cell>0.457</cell><cell>0.344</cell><cell>0.442</cell><cell>0.447</cell><cell>0.464</cell></row><row><cell>BioASQ</cell><cell>0.716</cell><cell>0.351</cell><cell>0.145</cell><cell>0.256</cell><cell>0.463</cell><cell>0.466</cell><cell>0.577</cell><cell>0.716</cell><cell>0.645</cell></row><row><cell>NFCorpus</cell><cell>0.196</cell><cell>0.243</cell><cell>0.273</cell><cell>0.208</cell><cell>0.232</cell><cell>0.235</cell><cell>0.272</cell><cell>0.196</cell><cell>0.254</cell></row><row><cell>NQ</cell><cell>0.753</cell><cell>0.787</cell><cell>0.605</cell><cell>0.880  ‡</cell><cell>0.836</cell><cell>0.858</cell><cell>0.845</cell><cell>0.753</cell><cell>0.912</cell></row><row><cell>HotpotQA</cell><cell>0.757</cell><cell>0.651</cell><cell>0.398</cell><cell>0.591</cell><cell>0.578</cell><cell>0.637</cell><cell>0.624</cell><cell>0.757</cell><cell>0.748</cell></row><row><cell>FiQA-2018</cell><cell>0.505</cell><cell>0.446</cell><cell>0.601</cell><cell>0.342</cell><cell>0.581</cell><cell>0.540</cell><cell>0.609</cell><cell>0.505</cell><cell>0.603</cell></row><row><cell>Signal-1M (RT)</cell><cell>0.376</cell><cell>0.270</cell><cell>0.266</cell><cell>0.162</cell><cell>0.239</cell><cell>0.263</cell><cell>0.271</cell><cell>0.376</cell><cell>0.283</cell></row><row><cell>TREC-NEWS</cell><cell>0.401</cell><cell>0.262</cell><cell>0.381</cell><cell>0.215</cell><cell>0.398</cell><cell>0.367</cell><cell>0.395</cell><cell>0.401</cell><cell>0.367</cell></row><row><cell>ArguAna</cell><cell>0.931</cell><cell>0.893</cell><cell>0.454</cell><cell>0.751</cell><cell>0.937</cell><cell>0.945</cell><cell>0.979</cell><cell>0.931</cell><cell>0.914</cell></row><row><cell>Tóuche-2020</cell><cell>0.463</cell><cell>0.257</cell><cell>0.257</cell><cell>0.171</cell><cell>0.307</cell><cell>0.300</cell><cell>0.273</cell><cell>0.463</cell><cell>0.309</cell></row><row><cell>CQADupStack</cell><cell>0.588</cell><cell>0.521</cell><cell>0.546</cell><cell>0.403</cell><cell>0.579</cell><cell>0.596</cell><cell>0.672</cell><cell>0.588</cell><cell>0.624</cell></row><row><cell>Quora</cell><cell>0.948</cell><cell>0.896</cell><cell>0.978</cell><cell>0.470</cell><cell>0.987</cell><cell>0.989</cell><cell>0.989</cell><cell>0.948</cell><cell>0.989</cell></row><row><cell>DBPedia</cell><cell>0.384</cell><cell>0.411</cell><cell>0.281</cell><cell>0.349</cell><cell>0.319</cell><cell>0.403</cell><cell>0.396</cell><cell>0.384</cell><cell>0.461</cell></row><row><cell>SCIDOCS</cell><cell>0.346</cell><cell>0.297</cell><cell>0.255</cell><cell>0.219</cell><cell>0.269</cell><cell>0.296</cell><cell>0.334</cell><cell>0.346</cell><cell>0.344</cell></row><row><cell>FEVER</cell><cell>0.908</cell><cell>0.843</cell><cell>0.850</cell><cell>0.840</cell><cell>0.900</cell><cell>0.914</cell><cell>0.920</cell><cell>0.908</cell><cell>0.934</cell></row><row><cell>Climate-FEVER</cell><cell>0.383</cell><cell>0.227</cell><cell>0.346</cell><cell>0.390</cell><cell>0.445</cell><cell>0.448</cell><cell>0.517</cell><cell>0.383</cell><cell>0.444</cell></row><row><cell>SciFact</cell><cell>0.838</cell><cell>0.863</cell><cell>0.631</cell><cell>0.727</cell><cell>0.816</cell><cell>0.849</cell><cell>0.892</cell><cell>0.838</cell><cell>0.878</cell></row><row><cell>AVERAGE</cell><cell>0.587</cell><cell>0.523</cell><cell>0.463</cell><cell>0.430</cell><cell>0.566</cell><cell>0.572</cell><cell>0.603</cell><cell>0.587</cell><cell>0.613</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod.</figDesc><table><row><cell>0.389</cell><cell>0.389</cell><cell>0.482</cell><cell>0.635</cell><cell>0.295</cell><cell>0.280</cell><cell>0.257</cell><cell>0.267</cell></row><row><cell>NQ</cell><cell></cell><cell cols="2">HotpotQA</cell><cell cols="2">FiQA-2018</cell><cell cols="2">Signal-1M (RT)</cell></row><row><cell>0.450</cell><cell>0.442</cell><cell>0.513</cell><cell>0.477</cell><cell>0.258</cell><cell>0.255</cell><cell>0.261</cell><cell>0.243</cell></row><row><cell cols="2">TREC-NEWS</cell><cell>ArguAna</cell><cell></cell><cell cols="2">Tóuche-2020</cell><cell>Quora</cell><cell></cell></row><row><cell>Cosine-0.389</cell><cell>0.389</cell><cell>0.482</cell><cell>0.635</cell><cell>0.295</cell><cell>0.280</cell><cell>0.855</cell><cell>0.833</cell></row><row><cell cols="2">CQADupStack</cell><cell>DBPedia</cell><cell></cell><cell cols="2">SCIDOCS</cell><cell>FEVER</cell><cell></cell></row><row><cell>Cosine-0.306</cell><cell>0.283</cell><cell>0.339</cell><cell>0.315</cell><cell>0.133</cell><cell>0.113</cell><cell>0.670</cell><cell>0.685</cell></row><row><cell></cell><cell></cell><cell cols="2">Climate-FEVER</cell><cell>SciFact</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.205</cell><cell>0.169</cell><cell>0.531</cell><cell>0.511</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>nDCG@10 scores for SBERT (section 4.1.3) trained with either cosine-similarity (Cosine-Sim.) or dotproduct (Dot-Prod.). For few datasets, we find the dot-product model to outperform cosine-sim. model by a large margin. For a majority of datasets, we find the cosine-sim. model to marginally outperform the dot-product model.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">MSMARCO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TREC-COVID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BioASQ</cell><cell></cell><cell></cell><cell>NFCorpus</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell>SBERT (Method) Cosine-Sim. Dot-Prod.</cell></row><row><cell>0</cell><cell>50</cell><cell cols="2">100 NQ</cell><cell>150</cell><cell>200</cell><cell></cell><cell>0</cell><cell></cell><cell>100</cell><cell cols="2">200 HotpotQA 300</cell><cell>400</cell><cell>500</cell><cell></cell><cell>0</cell><cell>100</cell><cell cols="3">200 FiQA-2018 300</cell><cell>400</cell><cell>500</cell><cell>0</cell><cell>100</cell><cell>200 Signal-1M (RT) 300</cell><cell>400</cell><cell>500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell>SBERT (Method) Cosine-Sim. Dot-Prod.</cell></row><row><cell>0</cell><cell>100</cell><cell cols="3">200 TREC-NEWS 300</cell><cell>400</cell><cell>500</cell><cell>0</cell><cell></cell><cell>100</cell><cell cols="2">200 ArguAna 300</cell><cell>400</cell><cell>500</cell><cell></cell><cell>0</cell><cell>100</cell><cell cols="3">200 Touche-2020 300</cell><cell>400</cell><cell>500</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15 Quora</cell><cell>20</cell><cell>25</cell></row><row><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell>SBERT (Method) Cosine-Sim. Dot-Prod.</cell></row><row><cell>0</cell><cell>100</cell><cell cols="3">200 CQADupStack 300</cell><cell>400</cell><cell>500</cell><cell>0</cell><cell></cell><cell>100</cell><cell cols="2">200 DBPedia 300</cell><cell>400</cell><cell>500</cell><cell></cell><cell>0</cell><cell>100</cell><cell cols="3">200 SCIDOCS 300</cell><cell>400</cell><cell>500</cell><cell>0</cell><cell>5 10 15 20 25 30 35 40 FEVER</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell>SBERT (Method) Cosine-Sim. Dot-Prod.</cell></row><row><cell>0</cell><cell>100</cell><cell cols="5">200 climate-FEVER 300 SBERT (Method) 400 500 Cosine-Sim. Dot-Prod.</cell><cell>0.8 1.0</cell><cell>0</cell><cell></cell><cell cols="4">50 Climate-FEVER 100 150 SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell></cell><cell>0</cell><cell>100</cell><cell cols="2">200 SciFact 300 SciFact</cell><cell cols="3">400 SBERT (Method) 500 Cosine-Sim. Dot-Prod. SBERT (Method) Cosine-Sim. Dot-Prod.</cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell><cell>500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>100</cell><cell>200</cell><cell cols="2">300</cell><cell>400</cell><cell>500</cell><cell cols="2">0.0 0.0 0</cell><cell>0.2 100</cell><cell>0.4 200</cell><cell>0.6 300</cell><cell>0.8 400</cell><cell>1.0 500</cell><cell>0</cell><cell>0</cell><cell>100 100</cell><cell>200 200</cell><cell>300 300</cell><cell></cell><cell>400 400</cell><cell>500 500</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We find it also necessary to benchmark multilingual and cross-lingual tasks in BEIR, which we keep for future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.elastic.co</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://tfhub.dev/google/universal-sentence-encoderqa/3 4 https://huggingface.co/transformers/model_doc/dpr.html 5 https://github.com/microsoft/ANCE 6 https://www.sbert.net/docs/pretrained_models.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Note that we additionally provide other metrics like Precision, Recall and MAP (Mean Average Precision) in BEIR. 10 https://github.com/cvangysel/pytrec_eval</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Please see details at http://participants-area. bioasq.org/general_information/Task8b/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">https://www.semanticscholar.org/cord19 14 https://nutritionfacts.org 15 https://pubmed.ncbi.nlm.nih.gov</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">https://www.tweepy.org 17 https://ir.nist.gov/wapo/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/324133.324140</idno>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<title level="m">Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics</title>
		<editor>Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1612</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cicero Nogueira dos Santos, Ramesh Nallapati, Zhiheng Huang, and Bing Xiang. 2020. Embeddingbased zero-shot retrieval through query generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<title level="m">Pretrained transformers for text ranking: Bert and beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<title level="m">Sparse, dense, and attentional representations for text retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Zero-shot neural passage retrieval via domain-targeted synthetic question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Korotkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Www&apos;18 open challenge: Financial opinion mining and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macedo</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manel</forename><surname>Zarrouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
		<idno type="DOI">10.1145/3184558.3192301</idno>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference 2018, WWW &apos;18</title>
		<meeting><address><addrLine>Republic and Canton of Geneva, CHE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1941" to="1942" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">660</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<title level="m">Passage re-ranking with bert</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Document ranking with a pretrained sequence-to-sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.63</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Multi-stage document ranking with bert</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Document expansion by query prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web. Technical Report 1999-66</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<idno>number = SIDL-WP-1999-0120</idno>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note type="report_type">Stanford InfoLab. Previous</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<title level="m">Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2020. Kilt: a benchmark for knowledge intensive language tasks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How does clickthrough data reflect retrieval quality?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhu</forename><surname>Kurup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
		<idno type="DOI">10.1145/1458082.1458092</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The curse of dense low-dimensional information retrieval for large index sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000019</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MultiCQA: Zero-shot transfer of selfsupervised text matching models on a massive scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)</title>
		<meeting>The 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2471" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time open-domain question answering with dense-sparse phrase index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1436</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4430" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation for duplicate question detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darsh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1131</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1056" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Trec 2019 news track overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A data collection for evaluating the retrieval of related tweets to news articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dyaa</forename><surname>Albakour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Esquivel</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-319-76941-7_76</idno>
	</analytic>
	<monogr>
		<title level="m">40th European Conference on Information Retrieval Research (ECIR 2018)</title>
		<meeting><address><addrLine>Grenoble, France, March</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="780" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Augmented sbert: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An overview of the bioasq large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Michael R Alvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergios</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Representation learning with contrastive predictive coding</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pytrec_eval: An extremely fast python interface to trec_eval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Van Gysel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Trec-covid: Constructing a pandemic information retrieval test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasmeer</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3451964.3451965</idno>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">54</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Building an Argument Search Engine for the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Al-Khatib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamen</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorel</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janek</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Workshop on Argument Mining</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Retrieval of the best counterargument without prior topic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fact or fiction: Verifying scientific claims</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7534" to="7550" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoganand</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Reas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Katsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dewey</forename><surname>Murdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devvret</forename><surname>Rishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oren Etzioni</title>
		<imprint>
			<publisher>Chris Wilhelm</publisher>
		</imprint>
	</monogr>
	<note>and Sebastian Kohlmeier. 2020. Cord-19: The covid-19 open research dataset</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A theoretical analysis of ndcg ranking measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on learning theory (COLT 2013)</title>
		<meeting>the 26th annual conference on learning theory (COLT 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multilingual universal sentence encoder for semantic retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jax</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Hernandez Abrego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dataset</forename><surname>Website</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Link</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<ptr target="https://microsoft.github.io/msmarco/TREC-COVIDhttps://ir.nist.gov/covidSubmit/index.htmlNFCorpushttps://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/BioASQhttp://bioasq.orgNQhttps://ai.google.com/research/NaturalQuestionsHotpotQAhttps://hotpotqa.github.ioFiQA-2018https://sites.google.com/view/fiqa/Signal-1M(RT)https://research.signal-ai.com/datasets/signal1m-tweetir.htmlTREC-NEWShttps://trec.nist.gov/data/news2019.htmlArguAnahttp://argumentation.bplaced.net/arguana/data" />
	</analytic>
	<monogr>
		<title level="j">MSMARCO</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
