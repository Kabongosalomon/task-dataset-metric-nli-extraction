<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Reasoning Graph for Activity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingran</forename><surname>Zhang</surname></persName>
							<email>jrzhang339@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Future Multimedia School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>610051</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
							<email>fumin.shen@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Future Multimedia School of Computer Science and Engineering University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<postCode>610051</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
							<email>xing.xu@uestc.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Center for Future Multimedia School of Computer Science and Engineering University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<postCode>610051</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Heng</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Center for Future Multimedia School of Computer Science and Engineering University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<postCode>610051</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Center for Future Multimedia School of Computer Science and Engineering University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<postCode>610051</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Reasoning Graph for Activity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite great success has been achieved in activity analysis, it still has many challenges. Most existing work in activity recognition pay more attention to design efficient architecture or video sampling strategy. However, due to the property of fine-grained action and long term structure in video, activity recognition is expected to reason temporal relation between video sequences. In this paper, we propose an efficient temporal reasoning graph (TRG) to simultaneously capture the appearance features and temporal relation between video sequences at multiple time scales. Specifically, we construct learnable temporal relation graphs to explore temporal relation on the multi-scale range. Additionally, to facilitate multi-scale temporal relation extraction, we design a multihead temporal adjacent matrix to represent multi-kinds of temporal relations. Eventually, a multi-head temporal relation aggregator is proposed to extract the semantic meaning of those features convolving through the graphs. Extensive experiments are performed on widely-used large-scale datasets, such as Something-Something and Charades, and the results show that our model can achieve state-of-the-art performance. Further analysis shows that temporal relation reasoning with our TRG can extract discriminative features for activity recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As one of the important video analysis tasks, activity recognition has attracted significant attention from the academic community in computer vision. Thanks to the efficiency of deep learning techniques and larger datasets in computer vision, the action recognition performance has been remarkably boosted. Two kinds of architectures are widely adopted: (1) 2 dimensional convolution neural networks (2D ConvNets, 2D CNNs) for capturing framelevel features <ref type="bibr" target="#b41">[42]</ref>; (2) 3 dimensional convolution neural  <ref type="figure">Figure 1</ref>. The example of temporal relation helps to classify and distinguish an activity (all the samples are originated from Something-Something V2 dataset). The three kinds of activities are (a) throwing something into the air and letting it fall; (b) throwing something into the air and catching it; (c) throwing something. We cannot distinguish one activity from the other two with a single frame. The inference of the temporal relation between the sequence is needed for deducing what has happened and classifying an activity.</p><p>network (3D ConvNets, 3D CNNs) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[19]</ref> or recurrent neural networks (RNNs) <ref type="bibr" target="#b5">[6]</ref> for modeling temporal context information. Afterward, a natural idea to boost the recognition performance based on the above architectures is to applying two-stream <ref type="bibr" target="#b29">[30]</ref> based framework which has heterogeneous inputs for action recognition. However, those approaches cannot capture the underlying structure which is characterized by transformations and temporal relations rather than the appearance of certain entities <ref type="bibr" target="#b52">[53]</ref>. Therefore, they are very inefficient for activity recognition, because the activity can be characterized by the temporal evolution of arXiv:1908.09995v1 [cs.CV] 27 Aug 2019 appearance governed by motion. Consider three kinds of frequently occurred activity in ordinary daily life as shown in <ref type="figure">Figure 1</ref>. Those activities cannot be recognized without reasoning about short or long-term temporal relations. An ordinary activity typically consists of several temporal relations at a multi-scale time-span. As the example shown in <ref type="figure">Figure 1</ref> (a) and (b), the activity "throwing" contains the short-term relation like "pillow throwing" and "falling", and long-term relation like "keeping it falling" or "catching it", i.e., activity often equipped with specific spatial patterns as well as multi-scale temporal structure. Hence, the ability to accurately capture relevant relation between temporal sequence and perform temporal reasoning is crucial for understanding activity in video. It is expected to discover temporal semantic knowledge of a video beyond the appearance of objects in the frames over short and long-range temporal dependencies. Therefore, to effectively exploit such relation, it is required to develop an effective deep model that has the capacity to cope with temporal semantic meaning for activity understanding. Several works <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b52">[53]</ref> have attempted to explore the relation between video sequences. They apply multi-layer perceptron (MLP) which may have limited capacity for learning similarity of sequence object to investigate the relation between video sequence object. Additionally, they could only explore shortrange temporal dependencies relation and cannot capture the transformable temporal states over time.</p><p>In this paper, we propose a temporal reasoning graph (TRG) network by directly performing temporal state relation reasoning in the graph to address the problem of capturing the transformable temporal states and different time-scale action dependencies. Notably, graph convolution networks (GCNs) <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b54">[55]</ref> are impressively powerful tools to analyze relations, but they typically have identified adjacent relation of different objects. Our proposed TRG approach tackles the key problem of how to represent the temporal graph connection structure. Inspired by the temporal relation network <ref type="bibr" target="#b52">[53]</ref> which defines pairwise temporal relation as a composite function and exploits MLP to capture their semantic dependency, in this paper, the proposed TRG approach presents pairwise relation as a component of a learnable adjacent matrix to address the ambiguous temporal state relations of a video sequence. Additionally, the multihead temporal relation graphs which discover temporal relations on a multi-scale range based on the adjacent matrix are constructed in our approach. Specifically, rather than convolving solely on the temporal sequence in the consecutive order, our proposed TRG approach defines specific relation graphs to explore the temporal relations of activity in both short-term and long-term temporal dependencies. Furthermore, the temporal instances in the graph have multirelation between each other, like causality or adjacency, etc. Therefore, the multi-head temporal adjacent matrix is devised to investigate such relations. Subsequently, temporal relation reasoning can be performed to deduce multi-kinds of relation for an activity. To further exploit the temporal relation in the reasoning graph and investigate a semantic meaning of those relations, our proposed TRG approach designs a relation aggregator to fuse those multi-kinds of relation. The whole activity recognition procedure in our proposed TRG approach is illustrated in <ref type="figure" target="#fig_4">Figure 2</ref>. Specially, ConvNets for spatial feature extraction of sampling video frames, our proposed TRG approach for temporal relation reasoning and the final fully connection for activity classification. Note that, in later experiment, we prove that our proposed TRG is flexible to be plugged into any stage of the off-the-shelf architectures, e.g., ResNet <ref type="bibr" target="#b9">[10]</ref> and Inception <ref type="bibr" target="#b11">[12]</ref>.</p><p>To further enhance the spatial-temporal features learning for activity recognition, we concatenate the spatial features extracted by ConvNets with the semantic temporal relation features. In this way, we can extract the spatial features of each frame and capture long-range temporal relation in videos. We conduct extensive experiments on three largescale data sets: Something-Something V1 <ref type="bibr" target="#b8">[9]</ref>, V2 <ref type="bibr" target="#b15">[16]</ref> and Charades <ref type="bibr" target="#b28">[29]</ref>. Both three datasets are extremely challenging, even for a human, as we cannot infer the activity by only the object or background in the frame. The experiments on the two datasets have demonstrated the significant improvements over state-of-the-art approaches and the importance of temporal reasoning in activity recognition.</p><p>Our major contributions are summarized as follows:</p><p>• We propose a novel temporal reasoning graph module for activity recognition which is the first attempt at temporal relation reasoning with graph, to our best knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We construct multi-head graph representation for multi-kinds temporal relation reasoning of a video sequence with a variant span and scale between the sequence in a long-range video.</p><p>• A semantic aggregator is developed to learn the importance of sequence state in different graphs and fuse the multi-kinds temporal relation features.</p><p>The rest of this paper is organized as follows: we briefly introduce related work in activity recognition literature, visual relation reasoning as well as graph convolution networks in Section 2. In section 3, the proposed temporal reasoning graph (TRG) is elaborated in detail. Section 4 presents the experimental results and further analysis of the model and results. Finally, the conclusion is given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Activity Recognition. Activity recognition has been widely studied in recent years. An active research which devotes to the design of deep networks for video representation learning has been trying to devise effective CNNs architectures <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Karpathy et al. <ref type="bibr" target="#b13">[14]</ref> attempted to design a deep network that stacks CNNs based frame-level features in a fixed size and then conduct spatiotemporal convolutions for video-level features learning. However, the results were not satisfying, which implied the difficulty of CNNs in capturing motion information of the video. Later, many works in this genre leverage CNNs   <ref type="figure" target="#fig_4">Figure 2</ref>. The overall architecture of our proposed temporal graph reasoning network. We apply ConvNets to extract video features. Afterward, we perform graph convolution on the temporal sequence with the learnable multi-head adjacent matrix and aggregate the semantic temporal features with the temporal relation aggregator. Finally, the spatial and temporal features are concatenated into a compact form for the final activity.</p><p>trained on frames to extract low-level features and then perform high-level temporal integration of those features using pooling <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, high-dimensional feature encoding <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, or recurrent neural networks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b49">[50]</ref>.</p><p>To explore long-term temporal relationships of video for learning a more robust representation, recently, the convolution neural network and long short-term memory (CNNs-LSTM) frameworks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b45">[46]</ref> were applied by stacking LSTM network to connect frame-level representation. They do have yielded an improvement for modeling temporal dynamics of convolution features in videos. However, this genre using CNNs as an encoder and RNN as a decoder of the video would lose low-level temporal context which is essential for action recognition. These works implied the importance of temporal information for action recognition and the incapability of CNNs to capture such information.</p><p>To exploit the temporal information, some studies resorted to the use of the 3D convolution kernel. Another efficient way to extract temporal features was to precomputing the optical flow using traditional optical flow estimation methods and training a separate CNNs to encode the precomputed optical flow, which is a kind of escape from temporal modeling but effective in motion features extraction. There are still several important issues with existing CNNs for action recognition: 1) CNNs has limited capacity for learning long temporal dependency, 2) it's difficult for CNNs to capture the temporal transformation with complex physical properties. To address these issues, we propose an efficient unit that applies graph convolution networks to learn temporal relation, which is much more efficient than convolving dense frames. Meanwhile, the model can construct a temporal graph for representing temporal relation. It flexibly incorporates temporal reasoning and spatial transformation with existing architectures.</p><p>Visual Relation Reasoning. Reasoning about the relation between instance over time in the video is critical for activity recognition <ref type="bibr" target="#b52">[53]</ref>. In addition, modeling relations between vision objects have become a popular problem in computer vision <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b50">[51]</ref>. The most straightforward visual reasoning task is object relation reasoning in CLEVR benchmark <ref type="bibr" target="#b12">[13]</ref>, and significant efforts have been devoted to a variety of traditional visual tasks with pairwise relationship reasoning. In the face clustering problem, several works attempted to modeling pair-wise relationships for face graph generation <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b48">[49]</ref>. To reason between different instances in visual question answering tasks, Xiong et al. <ref type="bibr" target="#b46">[47]</ref> proposed a graph matching module for investigating such relation. In sketch-based action recognition, several works showed that modeling interactions of sketch joint can achieve excellent performance <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Here, we show that explicitly exploiting the various and multi-scale temporal relations in videos can boost activity recognition accuracy.</p><p>Graph Convolution Networks (GCNs). GCNs is a powerful tool for modeling the graph instance relation. Spatial based GCNs <ref type="bibr" target="#b54">[55]</ref> which is the generalization of CNNs to graphs and perform manually-defined convolution on the graph is good at dealing with graph-structured data. Due to their convincing performance and high interpretability of modeling object relationships, GCNs has been widely applied in many computer vision task which needs to explore the relation of different vision instance. In terms of applications, existing works has led to considerable performance improvement by using GCNs in traditional computer vision tasks <ref type="bibr" target="#b1">[2]</ref>, for example, skeleton-based action recognition <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b47">[48]</ref>, link prediction <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b48">[49]</ref>, semi-supervised classification <ref type="bibr" target="#b14">[15]</ref>, hashing <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b55">[56]</ref>, person-reid <ref type="bibr" target="#b23">[24]</ref>, and multi-label image recognition <ref type="bibr" target="#b2">[3]</ref>, and etc.</p><p>In our work, we exploit GCNs that is built by stacking multiple layers of graph convolutions with a multihead adjacent matrix to capture temporal relations at multiple time scales. Moreover, our proposed method explicitly models the temporal interactions by building a temporal reasoning graph, which can be inflexible injected to the existing backbone. We show the temporal reasoning graph can efficient reason between temporal semantic instance for gaining activity classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we illustrate the framework of our proposed architectures showed in <ref type="figure" target="#fig_4">Figure 2</ref>, i.e., we will give detailed descriptions of how we build the temporal graph to investigate temporal relation. Firstly, the definition of the problem is given. Secondly, the detail of the construction of the temporal relation graph is described. Thirdly, the way how we implement convolution through the temporal graph is described in detail. Fourthly, the way we aggregate the semantic meaning of the multi-head temporal graph we constructed in Section 3.2 is presented. Finally, the way we model the spatiotemporal features and classify the activity is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>Formally, we have extracted a sequence of features in a video as X = {x 1 , x 2 , . . . , x T }, where T denotes the number of sequence or time of the video. If not specified, x i ∈ R C×H×W represents a frame feature map, C is the feature channels, and H, W represent the feature height and width, respectively. Here, we will apply the graph convolution neural network to explore the relation between temporal sequence X for activity recognition. Let G = (X , E) denote a graph of temporal sequence, where X is the set of T video frame object and E is the set of temporal relation edges between the video sequence. The neighbor set of a node x i is N (x i ), and the nodes at adjacent time steps are connected with the temporal edge. The time series of a video can be easy obtained, but the semantic meaning of the edge e = (x i , x j ) ∈ E between the node is still ambiguous. Attention mechanisms have been proven as a powerful tool in deep learning studies. We exploit the attention mechanism to investigate the semantic meaning of the edge e. More details about how we mine temporal relation will be elaborated in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Relation Graph Construction</head><p>In our work, we apply the temporal relation graph to form a hierarchical representation of the video sequence. We define the pairwise temporal relation and measure the similarity of the temporal feature to construct the temporal relation graph. We use the definition of temporal relation function as:</p><formula xml:id="formula_0">e ij = g θ (x i , x j ) ,<label>(1)</label></formula><p>where g θ is the similarity function of different frames. Note that g θ has many formats, typically expressing as following:</p><formula xml:id="formula_1">(1) sum: g θ (x, y) = V T tanh (x + y); (2) dot product: g θ (x, y) = x T y; and (3) bilinear: g θ (x, y) = x T W 1 y.</formula><p>More specifically, two sequence objects that one of the object instances may be predicted by the other one will have close connection and a high confidence edge.</p><p>In the temporal relation graph, we connect pairs of the semantically related frame features together. In order to obtain sufficient expressive power to transform the input frame feature into higher-level features, at least one transformation function which can project the feature sequences into a space for similarity measure is required. Thus, the new temporal relation function is:</p><formula xml:id="formula_2">e ij = g θ (Wx i , Wx j ) ,<label>(2)</label></formula><p>where w is the features transformation function parameter which can be learned via backpropagation. Therefore, a shared transformation, parameterized by a weight variable W, is applied to every frame feature. By adding such transformation function, we could learn the adjacent matrix which represents the correlations between different temporal feature across the frame of the temporal graph at each single feedforward step. After computing the correlations coefficient of the adjacent matrix, we perform normalization across each row of the adjacent matrix for easily comparable across different temporal features. We adopt the softmax function for normalization as:</p><formula xml:id="formula_3">a ij = sof tmax(e ij ) = exp (e ij ) T k=1 exp (e ik ) .<label>(3)</label></formula><p>The learnable adjacent matrix A expresses as A = (a ij ) T ×T . Here, to update the similarity measure of temporal relation at each step, we implement the adjacent matrix learning block by a single-layer feedforward neural network. An implementation example of element calculation in the adjacent matrix learning block is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>We should notice that the temporal-based neighbors of each node may have many kinds of relations which show different importance in learning temporal feature. In order to obtain sufficient representational power to capture the underlying relation of the temporal sequence, we design a mechanism, named multi-head adjacent matrix, to explore different semantic attributes.</p><p>Multi-head adjacent matrix. Inspired by transformer <ref type="bibr" target="#b35">[36]</ref> and graph attention networks <ref type="bibr" target="#b36">[37]</ref>, we construct the multi-head adjacent matrix to explore the power of temporal relation and stabilize the learning process of sequence object correlation coefficients. Specifically, N independent coefficient updating procedure execute the temporal relation function of Equation 3: where k ∈ {1, 2, . . . , N }. All the video temporal graph adjacent matrix denote as</p><formula xml:id="formula_4">a k ij = sof tmax e k ij = sof tmax g θ W k x i , W k x j ,<label>(4)</label></formula><formula xml:id="formula_5">: × Shared ℎ { } 1 × T, × 1 { } × × , × × : × ℎ × × , × × T, 1 × 1 T, 1</formula><formula xml:id="formula_6">A = A 1 , A 2 , . . . , A N , where A k = (a k ij ) T ×T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Graph Convolution Neural Network</head><p>Once the temporal graphs are built, we can exploit graph convolution for temporal relation reasoning. Temporal graph convolution takes a video sequence as input, performs computations over the sequence, and returns a new sequence, i.e., the graph convolutions allow us to compute the response of a node based on its neighbors dened by our multihead graph. So, for a specific target object x i in the video sequence, it aggregates features from all neighbor objects according to the edge weight defined as previous.</p><p>To exploring temporal reasoning on the temporal relation graph, we apply the Graph Convolution Networks proposed in <ref type="bibr" target="#b36">[37]</ref> to process the frame feature. The outputs of the GCNs are updated features of each frame node, which can be aggregated together for video classification. If we perform multi-head adjacent matrix updating mechanisms, the next state sequence feature will be:</p><formula xml:id="formula_7">z n i = σ T j=1 a n ij W n x j ,<label>(5)</label></formula><p>where z n i is output semantic feature map reasoning through graph, σ (·) is typically nonlinear activation function relu and W n is transformation matrix implemented by standard convolution in feature domain. Note that the filter weight W n in each graph is shared everywhere on feature sequences, because it is irrelevant to the location of the feature sequences. So after constructing multi-head temporal relation adjacent matrix A and apply graph convolution, we can obtain N kinds of sequence state feature, denoted as</p><formula xml:id="formula_8">z 1 i , z 2 i , . . . , z N i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-head Temporal Relation Aggregator</head><p>The multi-head adjacent matrix is exploited to discover the multi-kinds of relations between the sequence nodes. The main reason for designing the multi-head temporal relation aggregator is to reflect the different importance of the semantic relation. In short, the aggregator which exploits a mechanism similar to self-attention for temporal graph pooling.</p><p>Generally, every sequence instance in a heterogeneous graph contains multiple types of semantic information, i.e., semantic-specific sequence features extracted from one graph can only reflect temporal relation from one aspect. To learn a more comprehensive video feature, we need to fuse multiple semantics which can be revealed by multi-head adjacent matrix. To address the problem of semantic fusion in a heterogeneous graph, we propose a multi-head temporal relation aggregator to automatically learn the importance of sequence state in different graphs and fuse them. The operation of the temporal relation aggregator is illustrated in <ref type="figure">Figure 4</ref>. To investigate the importance of different sequence states z k i , k ∈ 1, 2, . . . , N which are updated based on different temporal graph adjacent matrix, we define an aggregator function as following to automatically learn the importance,</p><formula xml:id="formula_9">z i = f a z 1 i , z 2 i , . . . , z N i .<label>(6)</label></formula><p>To implement the function f a , inspired by squeeze-andexcitation networks <ref type="bibr" target="#b10">[11]</ref>, we first use global pooling as follows to extract the global semantic meaning of each state in the different graph,</p><formula xml:id="formula_10">z 1 i , z 2 i , . . . , z N i = pooling z 1 i , z 2 i , . . . , z N i ,<label>(7)</label></formula><p>where z k i ∈ R 1×1×1 . Then, the importance of each sequence state, denoted as β j , is shown as follows:</p><formula xml:id="formula_11">β j = relu W z j i ,<label>(8)</label></formula><p>the weight for each z j i can be obtained by normalizing the above importance coefficient β j through all temporal graphs using the softmax function,</p><formula xml:id="formula_12">β i = exp (β i ) N j=1 exp (β j ) .<label>(9)</label></formula><p>With the learned weight as coefficients, we can fuse these semantic-specific temporal features z k i to obtain the final feature z i as follows:</p><formula xml:id="formula_13">z i = N n=1 β n · z n i .<label>(10)</label></formula><p>Finally, the output of a video sequence features from the temporal graph convolution are aggregated. With the designed aggregator, the video sequence features after processed contain the semantic meaning of the multi-head temporal graph. For simplicity, the temporal graph convolution together with multi-head temporal relation aggregator operation are summarized as follows:</p><formula xml:id="formula_14">Z = f a σ N k=1 A n XW n ,<label>(11)</label></formula><p>where A n represents one of the multi-head adjacency graphs with T × T shape, and X is the input features map of all the sequence in the graph with shape T × C × H × W . W n is the spatial transformation matrix. Here, for computation simplicity, we exploit convolution kernel conv 3×3 with one padding and one stride to represent W n , so W n is a matrix of the layer with shape C ×C ×3×3. Note that the transformation matrix W n in each graph are shared everywhere on feature sequences because it's irrelevant to the location of the feature sequences. The final output features map Z still is a tensor with shape T × C × H × W . The temporal graph convolution operation and temporal relation aggregator can be stacked into multiple layers.</p><p>:</p><formula xml:id="formula_15">× × × 1 × 1 × 1 × × , × × × 1 × 1 × 1 × 1 × 1 × 1</formula><p>Semantic modeling aggregation × × × <ref type="figure">Figure 4</ref>. The detail operation of the multi-head temporal relation aggregator. The semantic modeling mainly aims at extracting comprehensive information of the feature sequences in heterogeneous graphs. The aggregation is exploited to fuse the feature sequences with the global semantic modeling information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Spatial and Temporal Modeling</head><p>ConvNets has been proved a powerful tool for capturing the basic visual concept in recent years <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b30">[31]</ref>, but it has a poor capacity of understanding the relation and importance of those spatial units in an image or a sequence for classification. From above, the temporal graph convolution is designed to extract such sequence relations of the spatial features. So we apply residual connection operation <ref type="bibr" target="#b9">[10]</ref> to fuse the spatial features extracted by ConvNets with the temporal relation features extracted by temporal graph convolution for activity recognition.</p><p>After the video sequence spatial features X extracted by ConvNets are obtained and the temporal relation features Z are extracted by temporal graph convolution, we begin to model the spatial and temporal feature extraction by connecting the X and Z as follows:</p><formula xml:id="formula_16">H = σ (X + Z) ,<label>(12)</label></formula><p>where "+" is the residual concatenation operation, H is the concatenated spatial-temporal features, followed by a non-linearity function σ (·), typically ReLU. The process of spatial and temporal modeling procedure is summarized as Algorithm 1.</p><p>Algorithm 1: The process of spatial and temporal modeling procedure in our proposed TRG. Input : The spatial features sequence of a video X = {x 1 , x 2 , . . . , x T }. Output: The final spatial and temporal representation H.</p><formula xml:id="formula_17">1 for x i ∈ {x 1 , x 2 , . . . , x T } do 2 for k = 1 . . . N do 3</formula><p>Calculate the graph node relation weight a k ij using Eq. 4;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Calculate the graph node feature z k i using Eq. 5; Calculate the semantic weight β j of j th node in i th feature sequences using Eq. 9; 10 end <ref type="bibr" target="#b10">11</ref> Calculate the aggregate feature z i using Eq. 10; 12 end <ref type="bibr" target="#b12">13</ref> Concatenate the semantic-specific temporal feature Z = {z 1 , z 2 , . . . , z T }; <ref type="bibr" target="#b13">14</ref> Fuse spatial and temporal feature to form H using Eq. 12; 15 Back propagation and update parameters in TRG; <ref type="bibr" target="#b15">16</ref> return H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Activity Classification with Temporal Reasoning Graph</head><p>The temporal reasoning graph provides a solid unit to work with existing architectures. As illustrated in <ref type="figure" target="#fig_4">Figure  2</ref>, the input is processing via spatial transformation and temporal graph reasoning to form the video representation, and then the representation is fed into a classifier to generate activity label. To this end, the whole framework can be trained in an end-to-end manner. For single-label activity recognition tasks, we apply cross-entropy loss when training, while binary sigmoid loss is used for multi-label activity recognition task. The loss function is given as: L = L 1 , for single-class classification; L 2 , for multi-class classification.</p><p>where L 1 = −s g + log C j=1 exp(s j ) , s j is the confidence scores of class j, g is the ground true label and C is the number of classes, and L 2 = C j=1 − (y j log(δ(s j )) + (1 − y j )log(1 − δ(s j ))), δ() is sigmoid function and y i = 1 when classes i is the ground true label of the sample and 0 otherwise.</p><p>Using the above objective function, the parameters of the whole framework are optimized with the standard minibatch stochastic gradient descent (SGD) algorithm with Nesterov momentum which is widely adopted in activity recognition task <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the effectiveness of our temporal reasoning graph, we perform extensive experiments on three benchmark datasets for activity recognition: Something-Something V1 <ref type="bibr" target="#b8">[9]</ref>, V2 <ref type="bibr" target="#b15">[16]</ref> and Charades <ref type="bibr" target="#b28">[29]</ref>. We first introduce the two datasets and implementation detail. Then, we compare our method with existing methods. Afterward, we conduct ablation studies to dig into the effect of the component and factor. In addition, we further analyze the temporal scaling strategy, where to place our temporal reasoning graph and recognition performance with different backbone. Finally, we provide the visualization of class activation map (CAM) <ref type="bibr" target="#b53">[54]</ref> of the intermediate features and apply t-SNE <ref type="bibr" target="#b3">[4]</ref> to visualize the distribution of feature representation learned by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets Description</head><p>Something-Something-V1 <ref type="bibr" target="#b8">[9]</ref>: The dataset is a large collection of densely-labeled video clips that allow machine learning models to develop a fine-grained understanding of basic actions. It contains 198,499 short video clips across 174 labels of simple textual descriptions based on templates, 86,017 of which are training videos, 11,522 are validation videos and 10,960 are testing videos. Each video has a duration ranging from 2 seconds to 6 seconds. The inference of the video in this dataset requires extracted features that are capable of representing the physical relation of the objects.</p><p>Something-Something-V2 <ref type="bibr" target="#b15">[16]</ref>: The dataset is twice as many videos as V1, collected by workers with humans performing pre-defined basic actions of everyday objects. Each video in the dataset equips with object annotations in addition to the video label and has a duration ranging from 2 to 6 seconds. In total, It has 318,572 annotations involving 30,408 unique objects and contains 220,847 videos across 174 class, with 168913 of which are training videos, 24,777 are validation videos and 27,157 are testing videos, i.e., it is split into train, validation, and test-sets in a ratio of 8:1:1.</p><p>Charades <ref type="bibr" target="#b28">[29]</ref>: The dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. Following the standard split, it has 7,986 training video and 1,863 validation video.</p><p>These datasets provide us a large number of samples to investigate our model in video activity understanding and commonsense reasoning for daily human activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Detail</head><p>Training. We adopt the training strategy proposed in TSN <ref type="bibr" target="#b41">[42]</ref>, like data enhancement and partial batch normalization, etc. The frames sampled from a video were first input to 2D or 3D ConvNets for spatial feature extraction. Then we apply the temporal reason graph to learn the highlevel semantic meaning based on the sequence spatial features. All the 2D or 3D spatial feature extraction processes are originated from <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> for a fair comparison.</p><p>We employ the PyTorch framework <ref type="bibr" target="#b16">[17]</ref> in this paper for Networks building, and all networks are trained on two GeForce GTX Titan X GPU with a total 24G memory. All the input images are resized to 224 × 224 with the backbone of Inception <ref type="bibr" target="#b11">[12]</ref> and ResNet <ref type="bibr" target="#b9">[10]</ref> and 299 × 299 with the backbone of Inception-V3 <ref type="bibr" target="#b31">[32]</ref> followed by the dataset processing strategy of <ref type="bibr" target="#b41">[42]</ref>. The network is trained with a mini-batch stochastic gradient descent optimizer for model training, and the initial learning rate here is 0.001 which will reduce by a factor 10 after 50 epochs. It has a decay rate of 5 × 10 −4 , and momentum 0.9 to update Network parameters. The whole training procedure takes 100 epochs. For Something-Something V2, the epoch number is halved because the duration of its videos is shorter. All the learnable Parameters in the temporal graph are implemented by a convolution layer followed by a batchnorm <ref type="bibr" target="#b11">[12]</ref> and relu layer <ref type="bibr" target="#b7">[8]</ref>.</p><p>Test. When comparing with previous state-of-the-art models, we followed the rule that uses the same number of frames for both training and testing to make a direct comparison. Following the common practice, the inference process is conducted by sampling 10 clips and 2 clips from a video along its temporal axis for Charades and Something-Something dataset respectively and the prediction scores are averaged over whole clips. By the way, we also rescale the shorter side to 256 pixels for each frame while maintaining the aspect ratios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Protocols</head><p>Every video in Something-Something-V1 and V2 datasets is assigned to a single classes and the distribution of classes over the test set is almost uniform. The ground truth labels of the videos are g i , i = 1, . . . , N , where N is the number of samples in the test set. Following <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b52">[53]</ref>, we apply top1 and top5 precision to evaluate the performance of several methods in this datasets. The methods produce a list of at most 5 action classes labels, l i,j , j = 1, . . . , 5, in the descending order of confidence for each video i. The topk (k = 1, . . . , 5) precision is defined as: topK = 1 N N i K j=1 I(l i,j , g i ), where I(x, y) = 1 if x = y and 0 otherwise. Under this circumstance, the top1 precision is when K set to 1 and K set to 5 for top5.</p><p>As Charades is a multi-label action classification dataset, following <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b52">[53]</ref>, we exploit the standard mean average precision (mAP) to measure the performance of several different methods. Here, mAP is defined as the mean of all classes' average precision (AP), where AP is formulated as AP = 1 N K k=1 P (k) × rel(k), where N is the number of positive samples in the test set, P (k) is the precision of the top K test samples, and rel(k) is an indicator function equaling 1 if the item at rank k is a positive sample, 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Compared Methods</head><p>We compare the performance of our TRG with the following state-of-the-art methods:</p><p>• C3D (ICCV 2017) <ref type="bibr" target="#b8">[9]</ref>: A baseline model provided in Something-Something dataset exploits multi-layer 3D convolution <ref type="bibr" target="#b32">[33]</ref> to evaluate the dataset.</p><p>• MultiScale TRN (ECCV 2018) <ref type="bibr" target="#b52">[53]</ref>: An improved temporal aggregation method that applies multiple full connection layers to reasoning the relation of temporal features extracted by 2D-CNNs.</p><p>• I3D (CVPR 2017) <ref type="bibr" target="#b0">[1]</ref>: The methods inflate the 2D convolution filters to the 3D convolution filters, which can be transferred to many existing architectures.</p><p>• NL (CVPR 2018) <ref type="bibr" target="#b42">[43]</ref>: Non-local block can be plugged into an existing backbone to capture longterm dependencies.</p><p>• GCNs (ECCV 2018) <ref type="bibr" target="#b43">[44]</ref>: The model first applies faster rcnn to extract object features in the video and then use GCNs to reasoning the relation between those semantic objects.</p><p>• ECO (ECCV 2018) <ref type="bibr" target="#b57">[58]</ref>: An end-to-end architecture injects 3D CNNs at the top of 2D CNNs architecture.</p><p>• TrajectoryNet (NIPS 2018) <ref type="bibr" target="#b51">[52]</ref>: Trajectory convolution, a replacement operation of temporal convolution, is proposed to integrate feature along the temporal dimension.  <ref type="bibr" target="#b29">[30]</ref>: The method designs a spatial and temporal stream for action recognition. The spatial stream extracts spatial feature from a single frame in a video, whilst the temporal stream extracts temporal feature from the pre-computed dense optical flow.</p><p>• Asyn-TF (CVPR 2017) <ref type="bibr" target="#b26">[27]</ref>: The model attempts to reason over various aspects of activity that includes objects, actions, and intentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results and Discussion</head><p>The results on Something-Something <ref type="bibr" target="#b8">[9]</ref> and Charades <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> are shown in <ref type="table">Table 1</ref> and 2 respectively. "# Frames" in the table denotes the number of frames used as input by the method, whilst "# Params" denotes the model parameters of the method, and "FLOPs" (floating point operations) is a type of measurement of the model computation complexity. For a fair comparison, we conduct a series of experiments with different input frames and different backbone to evaluate our model. For all of these datasets, we use the standard evaluation protocol provided by the authors. Note that we only exploit video frames as input without complementary information, like hand-crafted features-IDT <ref type="bibr" target="#b37">[38]</ref> or optical flow field. <ref type="table">Table 1</ref> shows all the results on Something-Something V1 and V2 validation set. Considering that they are finegrained activity understanding datasets, in which deformation or motion serves as a crucial cue, those recent methods design their model from different aspects. But they didn't take the long-term temporal relation into account. Compared with Multiscale TRN which also attempt to reason about the temporal state of a video sequence, our work surpasses by a large margin 4.1% on Something-Something V1 and 2.5% on Something-Something V2 with the same experimental settings. The comparison with the work <ref type="bibr" target="#b43">[44]</ref> which also apply graph convolution to reason the object proposal extracted by faster-rcnn <ref type="bibr" target="#b19">[20]</ref> is impressive. Here, we gain 3.7% accuracy improvement with this work. Since this approach considers the object-level relation in different frames, it first use faster-rcnn to extract object-level features, which may be harmful to their model setting if the model extracts negative object or missing some key object. Additionally, their computation cost is higher that our method. The margin may benefit from our multi-head adjacent matrix and semantic relation aggregator. Based on Inception-V3 architecture, our temporal reasoning graph widens the advantage over previous models considerably, bringing overall performance to 49.8% on Something-Something V1 and 61.3% on Something-Something V2. Compare with the most accurate method TrajectorNet which convolves through trajectory path, our method can still yield improvement. Although TrajectoryNet can work with a relatively small backbone to achieve a high performance, it has to apply MotionNet <ref type="bibr" target="#b56">[57]</ref> to pre-produce trajectory as the performance of TrajectoryNet highly depends on the quality of the trajectory. Those results indicate that temporal reasoning with our graph model can capture semantic features for activity recognition.</p><p>In addition, the evaluation results on the Charades dataset, a multi-label activity recognition case, are shown ResNet-101 32 <ref type="bibr" target="#b34">35</ref>.5 NL I3D <ref type="bibr" target="#b42">[43]</ref> ResNet-101 32 37.5 NL I3D + GCNs <ref type="bibr" target="#b43">[44]</ref> ResNet-50 32 37.5 I3D + GCNs <ref type="bibr" target="#b43">[44]</ref> ResNet-101 32 39.1 NL I3D + GCNs <ref type="bibr" target="#b43">[44]</ref> ResNet in <ref type="table">Table 2</ref>. We present mAP values of those methods in this dataset. Our temporal reasoning graph achieves competitive recognition performance with the non-local inflated 3D CNNs (I3D) plus graph convolution model. Not only can our model get higher performance than <ref type="bibr" target="#b43">[44]</ref>, but also the complexity of our model is very low. Since we need not detect objects from the frames, our model has a faster inference speed with only 0.3 seconds at a time for one video (32 frames). <ref type="table">Table 1</ref> and 2 also compares the inuence of architecture and the number of sampled frames. As observing from the table, the accuracy degraded with fewer frames partly due to important parts of the action may be missing with fewer samples. However, training with more frames has a higher computation cost. The evaluation performance of TRG, even with just 8 frames, is still much better than most approaches in this literature, since our model takes into account the relationship between these 8 instants in the video, even if they are far apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Further Analysis</head><p>In this subsection, we mainly explore the effectiveness of temporal reasoning graph block. To analyze the TRG in-depth, we evaluate the different factors which matter in action representation or be essential in our TRG. Component Analysis of The Model. To study the contribution of different model parts, we also train ablated versions of our model separately, i.e., without temporal reasoning in the video context. We first evaluate the effect of using the multi-head adjacent matrix for graph convolution in the Inception, Inception-V3 and ResNet-50 with I3D technology architectures as demonstrated in <ref type="table">Table 1</ref>. In the table, "w/o temporal graph" means without temporal graph, whilst "temporal graph with" means replacing temporal relation aggregator with a specific operation. Firstly, without temporal graphs, we apply temporal average pooling among features map as three baselines. In addition, for the 2D CNNs case, we also test plugging 3D convolution layer behind the end of each branch (i.e., inception3a to inception5b and Mixed5b to Mixed7c). Finally, for exploring the effect of the multi-head temporal relation aggregator. We replace the aggregator with an element-wise average operation and feature concatenation operation among the sequence depth dimension. All the experiments here are conducted with only 8 input frames.</p><p>We can obtain that the reported baselines typically underperform the proposed model. The performance which yields improvement with our model demonstrates that temporal reasoning with multi-kinds and multi-scale sequence instance relation helps to capture fine grain action for recognition and boosts the performance. We speculate this is because the temporal graph considerably explores the semantic information of a video sequence and the temporal relation aggregator is able to select more informative relation features for activity recognition.</p><p>Head Number of The Graph. To analyze the effect of the different numbers of the multi-head adjacent matrix on recognition performance, We perform a serial of experiments on Something Something V2 and Charades datasets with the different head numbers. Here, we only sample 8 frames for experiments. As shown in <ref type="figure" target="#fig_5">Figure 5</ref>, we observe that building multiple multi-head temporal adjacent matrix lead to conspicuous gain compared with only one head adjacent matrix. The model is able to further boost accuracy from 49.7% to 53.3% with Inception backbone, 50.2% to 54.7% with Inception-V3 backbone on Something-Something V2, and improve the mAP from 27.2% to 32.3% with Inception backbone, 30.3% to 35.2% with Inception-V3 backbone on Charades dataset. The intuition behind this is that two temporal objects have different kinds of relations. When we deduce an action from a specific video, some kinds of relations may be enhanced and the others may be restricted by the relation aggregator. Although the adjacent relation weight values in the matrix learned with different head encode different semantic information, another observation that the recognition performance is saturated when keep increasing the number of the head indicated that the accumulation of the encoded semantic information with endless has little contribution and may cause confusion for relational reasoning. Place to Inject Temporal Reasoning Graph. To figure out in which way the temporal reasoning graph helps in video context interaction, we conduct experiments of plugging temporal reasoning graphs in different places of the CNNs backbone. Furthermore, in this part, we will demonstrate how to push the temporal reasoning graph to the 2D or 3D CNNs backbone.  Here, we will apply vanilla Inception as backbone-V3 for 2D CNNs case, whereas, ResNet-50 with I3D technique as backbone for 3D CNNs case. Theoretically, the temporal reasoning graph described above can be plugged at any level of the networks to enhance the temporal sequence interaction. In detail, we will study the effect of the different place where choose to build temporal reasoning graph. We push three graphs into the top or bottom of the multibranch convolution networks. More specifically, we consider injecting the temporal reasoning graph into the output layers from mixed_5b to mixed_5d at bottom-level and from mixed_6a to mixed_6c for at bottom-level for Inception-V3. As a comparison, we consider processing output from mixed_7a to mixed_7c with our temporal reasoning graph for Inception network at the top-level. Similar processing will be applied to ResNet-50 from res2_2 to res2_4, res3_2 to res3_4 or res5_2 to res5_4.</p><p>Results are shown on <ref type="figure" target="#fig_7">Figure 6</ref> with 8 frames input case and the performance with top-level processing are higher. We speculate that output feature maps of late ConvNets layers contain abundant semantic information of the spatial objects. With the temporal reasoning of that semantic information, the global contextual good for activity representation is perfectly modeling for recognition. Study on Temporal Scale. The videos have a variable number of frames, and the activity in the video lasts seconds to even minutes in different datasets. To copy with the problem of action contextual information ranging from seconds to even minutes, we analyze the impact of different temporal stride and temporal spanning for sampling from the original video. By the way, all the experiments were conducted with Inception, and ResNet-50, a 3D model processed the same with <ref type="bibr" target="#b42">[43]</ref>, architectures.</p><p>The Uniformly sparse and global sampling strategy are exploited to select comprehensive frames from the entire video. Temporal pooling, except 3D CNNs case, process frames independently and their scores are aggregated only in the end. Consequently, the performance stays almost the same when they change the number of samples, which demonstrates that only with sparse sampling strategy does not really help to learn the long-range temporal context. Whereas, our model pays more attention to the temporal interaction of the video sequence. The performance indicates that our model really benefits from long-term temporal reasoning.</p><p>By contrast, we also apply dense sampling strategy with a fixed temporal stride 4, which means that this strategy covers a temporal range of 5 seconds at most in our experiments on Charades datasets (24 FPS). Uniformly spare sampling with large temporal striding, in fact, hurts the performance of 3D CNNs model. Temporal convolution kernel might not be suitable for long-range patterns, because long-range contextual patterns are more diverse and changeable, and include challenging scene cuts. On the other hand, large temporal striding with plugging temporal reasoning graphs steadily improves performance.</p><p>Temporal reasoning sufficiently exploits the comprehensive information from the entire video or several seconds clip, since our approach modeling the relation of changeable pattern in a video sequence both with short and long-range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom</head><p>Mid Top  Effect of Different Backbone. To measure the importance of context interaction of the temporal features in different backbones, we can plug our temporal reasoning graph into existing 2D CNNs and 3D CNNs architectures and inspect its activity recognition performance. For this, three temporal reasoning graphs were injected into the existing backbone at the top with almost the same setting. For fair comparison with existing methods, the 2D architectures, BnInception <ref type="bibr" target="#b11">[12]</ref> and InceptionV3 <ref type="bibr" target="#b31">[32]</ref>, are stemmed from TSN <ref type="bibr" target="#b41">[42]</ref>; and the 3D architectures, ResNet-50 <ref type="bibr" target="#b9">[10]</ref> and ResNet-101, are processed the same with non-local <ref type="bibr" target="#b42">[43]</ref> using I3D <ref type="bibr" target="#b0">[1]</ref> technique, which inflate the 2D convolution weights into 3D convolution. All the results are listed in <ref type="table">Table 1</ref>. Model Complexity. Our approach has multi-head temporal graphs for temporal reasoning and a temporal semantic aggregator for features fusion, and the model can be deployed to the existing backbone. It has perfect scalability and provides a balance between performance and complexity. To figure out the time complexity of our model, we report the floating point operations per seconds (FLOPs) in a single clip to demonstrate the cost (shown in <ref type="table">Table 1</ref>). The additional cost and parameters induced by our module are mainly contained in the convolution kernels of the temporal similarity calculation and graph convolution parameters, followed by temporal semantic relation aggregation. More precisely, the parameters introduced by our temporal reasoning graph is l N l C 2 l + 9N l C 2 l + N 2 l , where l denotes which stage we plug our block into the backbone, C l represents the dimension of the output channels, and N l denotes the numbers of the graph in this stage. The first term of the equation calculates the parameters in temporal similarity measure, the second term of the equation calculates the parameters in the spatial transformation of graph convolution, and the last term of the equation calculates the parameters in the temporal relation aggregator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Visualization</head><p>CAM Visualization. To get further insights into what our network learns, we visualize the CAM <ref type="bibr" target="#b53">[54]</ref> on Something-Something V2 dataset. CAM can visualize the most discriminative parts which will highlight the interesting region when classifying an activity of a clip. We randomly sample three video clips from the validation set and evaluate them by the trained model. To understand the primitives our model used for representing actions and visualize interesting class information, we show the output of CAM on the select samples. The results are shown in <ref type="figure" target="#fig_8">Figure 7</ref>. The highlight regions that correspond to the receptive field give us some insight into what the model cares about and indicating that spatio-temporal features are learned effectively.</p><p>In <ref type="figure" target="#fig_8">Figure 7</ref>, we also list the top-3 prediction probability scores of the three examples. We observe that the model can perfectly recognize the first two cases, but it fails in the last example. For this kind of fine-grained action, the model still has trouble in classification. The score of this case, shown in the figure, predicts almost equally. However, from the prediction, we can not definitely declare that the result is wrong.  Visualization of The Learned Representation Distribution. To visualize the distribution of the learned features, we apply the t-SNE tool (Principle Component Analysis for dimension reduction with 5000 iterations) for embedding the sequence representation extracted from our model with a different adjacent head number. The experiment was conducted on Something-Something V2 dataset with 20 randomly selected class which around 3K samples. The distribution difference between <ref type="figure" target="#fig_9">Figure 8</ref> (a) and (b), (c) reveals that the model can learn more discriminative embedding with the temporal reasoning graph. Moreover, as shown in <ref type="figure" target="#fig_9">Figure   8</ref> (b) and (c), through the multi-head adjacent matrix, the proposed model could separate sample points into several semantically discriminative clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this work, we presented a novel temporal reasoning graph module that captures the temporal relation of a video sequence to address the long-range temporal dependencies for activity recognition. Meanwhile, we proposed the multihead adjacent matrix to investigate the multi-kinds relations of a video sequence. Additionally, a multi-head temporal relation aggregator was designed to automatically learn the importance of different sequence state in different graphs for comprehensive video-level feature learning. Benefiting from these two novel module designing, the proposed model can capture multi-kinds temporal relation with different scale and temporal span. We evaluated the proposed model on Something-Something and Charades datasets and established competitive results compared with existing methods.</p><p>We hope the proposed temporal reasoning graph module will boost performance on various video understanding tasks. Furthermore, we plan to investigate the power of GCNs in video context correspondence learning with a selfsupervised manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>An illustration of the temporal similarity calculation operation in the adjacent matrix. The feature in a video sequence is shown in the tensor format. The transformation weight is implemented by 1 × 1 convolution kernel, and "⊗" denotes matrix multiplication. All the features are convolved by the convolution kernel after proper reshaping. The temporal similarity is computed by the softmax operation implemented after the features multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>• 2 -</head><label>2</label><figDesc>stream (NIPS 2014)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The performance of different head numbers of the adjacent matrix with Inception or Inception-V3 architecture on Something-Something V2 (left) and Charades (right) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>The performance on Charades dataset with different position to add temporal reasoning graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Pushing something so that it slightly moves (37.1%)Pushing something onto something (25.8%)Moving something upPlugging something into something (7.1%)Pushing something from right to left (99.8%)Pushing something with something (0.05%)Pushing something from left to roghtMoving something close to something (0.04%)Removing something, revealing something behind (88.7%) Uncovering something (11.2%)Removing something, revealing something behindPicking something up (0.06%) Visualization of "CAM"<ref type="bibr" target="#b53">[54]</ref> generated by our model. The maps highlight the discriminative region for action classification. The color in red denotes high importance for recognition. We also list the top-3 prediction scores of those examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8</head><label>8</label><figDesc>. t-SNE visualization of the video representation on Something-Something V2 validation set. The representations are extracted based on Inception architecture with different settings: average pooling temporal features, 3 head adjacent matrix, and 6 head adjacent matrix. Different colors denote different action class and each point represents an embedding of a video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 .TABLE 2 .</head><label>12</label><figDesc>COMPARISON OF STATE-OF-ART METHODS ON THE SOMETHING-SOMETHING V1 AND V2 DATASETS. WE ONLY REPORT THE TOP1 RECOGNITION ACCURACY USING RGB FRAMES AS INPUT WITHOUT OPTICAL FLOW. COMPARISON OF STATE-OF-ART METHODS IN TERMS OF ON THE CHARADES DATASET. "x/y" IN THE THIRD COLUMN OF INPUT FRAMES REPRESENTS THE INPUT HAS x SAMPLES RGB IMAGE AND y SAMPLES OPTICAL FLOW IMAGE. OTHER METHODS ONLY TAKE RGB MODALITY INTO ACCOUNT.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Backbone</cell><cell></cell><cell cols="2"># Frames # Params</cell><cell>FLOPs</cell><cell cols="3">V1 top-1 top-5 top-1 top-5 V2</cell></row><row><cell>C3D [9]</cell><cell></cell><cell>VGG16</cell><cell></cell><cell>60</cell><cell>23.3M</cell><cell>349.4G</cell><cell>27.2</cell><cell>-</cell><cell>47.7</cell><cell>77.3</cell></row><row><cell cols="2">MultiScale TRN [53]</cell><cell>Inception</cell><cell></cell><cell>8</cell><cell>18.3M</cell><cell>16.4G</cell><cell>34.4</cell><cell>-</cell><cell>48.8</cell><cell>-</cell></row><row><cell>I3D [44]</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>32</cell><cell>28.0M</cell><cell>153G</cell><cell>41.6</cell><cell>72.2</cell><cell>-</cell><cell>-</cell></row><row><cell>I3D + GCNs [44]</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>32</cell><cell>55.1M</cell><cell>158G</cell><cell>43.3</cell><cell>75.1</cell><cell>-</cell><cell>-</cell></row><row><cell>NL I3D [44]</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>32</cell><cell>35.1M</cell><cell>168G</cell><cell>44.3</cell><cell>75.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">NL I3D + GCNs [44]</cell><cell>ResNet-50</cell><cell></cell><cell>32</cell><cell>62.2M</cell><cell>303G</cell><cell>46.1</cell><cell>76.8</cell><cell>-</cell><cell>-</cell></row><row><cell>ECOen [58]</cell><cell cols="3">Inception+3DResNet-18</cell><cell>92</cell><cell>150.0M</cell><cell>267G</cell><cell>46.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TrajectoryNet [52]</cell><cell></cell><cell>S3DResNet-18</cell><cell></cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>47.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TRG</cell><cell></cell><cell>Inception</cell><cell></cell><cell>8</cell><cell>14.1M</cell><cell>16.2G</cell><cell>38.5</cell><cell>68.4</cell><cell>51.3</cell><cell>78.8</cell></row><row><cell>TRG</cell><cell></cell><cell>Inception</cell><cell></cell><cell>16</cell><cell>14.1M</cell><cell>32.1G</cell><cell>45.9</cell><cell>74.9</cell><cell>56.7</cell><cell>79.9</cell></row><row><cell>TRG</cell><cell></cell><cell>Inception</cell><cell></cell><cell>32</cell><cell>14.1M</cell><cell>64.5G</cell><cell>47.5</cell><cell>80.2</cell><cell>58.3</cell><cell>86.2</cell></row><row><cell>TRG</cell><cell></cell><cell>Inception-V3</cell><cell></cell><cell>8</cell><cell>28.3M</cell><cell>47.4G</cell><cell>41.3</cell><cell>73.4</cell><cell>52.5</cell><cell>80.6</cell></row><row><cell>TRG</cell><cell></cell><cell>Inception-V3</cell><cell></cell><cell>16</cell><cell>28.3M</cell><cell>94.6G</cell><cell>47.2</cell><cell>78.9</cell><cell>59.2</cell><cell>86.4</cell></row><row><cell>TRG</cell><cell></cell><cell>Inception-V3</cell><cell></cell><cell>32</cell><cell>28.3M</cell><cell>187.3G</cell><cell>49.7</cell><cell>85.3</cell><cell>61.3</cell><cell>91.4</cell></row><row><cell>TRG</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>8</cell><cell>28.4M</cell><cell>33.6G</cell><cell>41.5</cell><cell>74.7</cell><cell>53.8</cell><cell>81.2</cell></row><row><cell>TRG</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>16</cell><cell>28.4M</cell><cell>66.4G</cell><cell>48.1</cell><cell>80.4</cell><cell>59.8</cell><cell>87.4</cell></row><row><cell>TRG</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>32</cell><cell>28.4M</cell><cell>132.2G</cell><cell>49.5</cell><cell>86.1</cell><cell>62.2</cell><cell>90.3</cell></row><row><cell>Methods</cell><cell>Backbone</cell><cell cols="2"># Frames mAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2-Stream [27]</cell><cell>VGG16</cell><cell>1/20</cell><cell>18.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2-Stream +LSTM [27]</cell><cell>VGG16</cell><cell>1/20</cell><cell>17.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Asyn-TF [27]</cell><cell>VGG16</cell><cell>1</cell><cell>18.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MultiScale TRN [53]</cell><cell>Inception</cell><cell>8</cell><cell>25.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I3D [1]</cell><cell>Inception</cell><cell>32</cell><cell>32.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I3D [43]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 .</head><label>3</label><figDesc>ABLATION STUDY OF BASIC MODULE OF OUR MODEL ON SOMETHING-SOMETHING V2 DATASET.</figDesc><table><row><cell>Backbone</cell><cell>Module</cell><cell cols="2">top1 top5</cell></row><row><cell></cell><cell>AvgPool w/o temporal graph</cell><cell>37.3</cell><cell>68.9</cell></row><row><cell></cell><cell>3DConv w/o temporal graph</cell><cell>42.4</cell><cell>73.2</cell></row><row><cell>Inception</cell><cell>temporal graph with concatenation</cell><cell>48.5</cell><cell>77.4</cell></row><row><cell></cell><cell>temporal graph with element-wise Avg</cell><cell>49.3</cell><cell>78.2</cell></row><row><cell></cell><cell>TRG (full model)</cell><cell>51.3</cell><cell>78.8</cell></row><row><cell></cell><cell>AvgPool w/o temporal graph</cell><cell>39.5</cell><cell>70.1</cell></row><row><cell></cell><cell>3DConv w/o temporal graph</cell><cell>45.1</cell><cell>76.5</cell></row><row><cell>Inception-V3</cell><cell>temporal graph with concatenation</cell><cell>50.4</cell><cell>78.4</cell></row><row><cell></cell><cell>temporal graph with element-wise Avg</cell><cell>51.2</cell><cell>79.2</cell></row><row><cell></cell><cell>TRG (full model)</cell><cell>52.5</cell><cell>80.6</cell></row><row><cell></cell><cell>AvgPool w/o temporal graph</cell><cell>46.7</cell><cell>74.1</cell></row><row><cell>ResNet-50</cell><cell>temporal graph with concatenation temporal graph with element-wise Avg</cell><cell>50.6 51.9</cell><cell>77.6 79.5</cell></row><row><cell></cell><cell>TRG (full model)</cell><cell>53.8</cell><cell>81.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 .</head><label>4</label><figDesc>THE PERFORMANCE OF DIFFERENT VIDEO SAMPLING STRATEGY ON CHARADES DATASETS.</figDesc><table><row><cell>Sampling strategy</cell><cell>Methods</cell><cell>Backbone</cell><cell cols="2"># Frames mAP</cell></row><row><cell></cell><cell>Temporal pooling</cell><cell>Inception</cell><cell>16</cell><cell>30.4</cell></row><row><cell></cell><cell>Temporal pooling</cell><cell>Inception</cell><cell>32</cell><cell>31.6</cell></row><row><cell></cell><cell>TRG</cell><cell>Inception</cell><cell>16</cell><cell>30.7</cell></row><row><cell>Sparsely</cell><cell cols="2">TRG Temporal pooling ResNet-50 Inception</cell><cell>32 16</cell><cell>33.5 30.5</cell></row><row><cell></cell><cell cols="2">Temporal pooling ResNet-50</cell><cell>32</cell><cell>32.3</cell></row><row><cell></cell><cell>TRG</cell><cell>ResNet-50</cell><cell>16</cell><cell>34.1</cell></row><row><cell></cell><cell>TRG</cell><cell>ResNet-50</cell><cell>32</cell><cell>36.8</cell></row><row><cell></cell><cell>Temporal pooling</cell><cell>Inception</cell><cell>16</cell><cell>30.1</cell></row><row><cell></cell><cell>Temporal pooling</cell><cell>Inception</cell><cell>32</cell><cell>31.3</cell></row><row><cell></cell><cell>TRG</cell><cell>Inception</cell><cell>16</cell><cell>29.8</cell></row><row><cell>Densely</cell><cell cols="2">TRG Temporal pooling ResNet-50 Inception</cell><cell>32 16</cell><cell>32.7 31.9</cell></row><row><cell></cell><cell cols="2">Temporal pooling ResNet-50</cell><cell>32</cell><cell>33.8</cell></row><row><cell></cell><cell>TRG</cell><cell>ResNet-50</cell><cell>16</cell><cell>35.8</cell></row><row><cell></cell><cell>TRG</cell><cell>ResNet-50</cell><cell>32</cell><cell>38.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported in part by the National Natural Science Foundation of China under grants No. 61502081, 61602089, 61632007 and the Sichuan Science and Technology Program 2018GZDZX0032, 2019ZDZX0008 and 2019YFG0003.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep temporal linear encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2329" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>international conference on learning representations</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gharbieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09235</idno>
		<title level="m">Fine-grained video classification and captioning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attentive relational networks for mapping images to scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3957" to="3966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in neural information processing systems</title>
		<meeting>the Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep asymmetric pairwise hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1522" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised discrete hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H. Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="486" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Non-local graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07694</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Asynchronous temporal fields for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5650" to="5659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Actor and observer: Joint modeling of first and third-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in neural information processing systems</title>
		<meeting>the Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<title level="m">Convnet architecture search for spatiotemporal feature learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the neural information processing systems</title>
		<meeting>the neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. Proceeding on the international conference on learning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning discriminative video representations using adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="685" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video representation learning using discriminative pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1149" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedongs of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>eedongs of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1430" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision</title>
		<meeting>the european conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="413" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Linkage based face clustering via graph convolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1117" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling spatialtemporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual query answering by entity-attribute graph matching and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8357" to="8366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to cluster faces on an affinity graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="2298" to="2360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3107" to="3115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Trajectory convolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the neural information processing systems</title>
		<meeting>the neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2204" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Graph neural networks: A review of methods and applications. arXiv: Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph convolutional network hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Hidden two-stream convolutional networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="363" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="713" to="730" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
