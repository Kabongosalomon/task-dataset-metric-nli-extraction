<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
							<email>yaohantao@ict.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of CAS</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<email>wgao@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>qitian@cs.utsa.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at San Antonio</orgName>
								<address>
									<postCode>78249-1604</postCode>
									<settlement>San Antonio</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3123266.3123279</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Person Re-Identi cation</term>
					<term>Global-Local-Alignment Descriptor</term>
					<term>Re- trieval Framework</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The huge variance of human pose and the misalignment of detected human images signi cantly increase the di culty of person Re-Identi cation (Re-ID). Moreover, e cient Re-ID systems are required to cope with the massive visual data being produced by video surveillance systems. Targeting to solve these problems, this work proposes a Global-Local-Alignment Descriptor (GLAD) and an e cient indexing and retrieval framework, respectively. GLAD explicitly leverages the local and global cues in human body to generate a discriminative and robust representation. It consists of part extraction and descriptor learning modules, where several part regions are rst detected and then deep neural networks are designed for representation learning on both the local and global regions. A hierarchical indexing and retrieval framework is designed to eliminate the huge redundancy in the gallery set, and accelerate the online Re-ID procedure. Extensive experimental results show GLAD achieves competitive accuracy compared to the state-of-theart methods. Our retrieval framework signi cantly accelerates the online Re-ID procedure without loss of accuracy. Therefore, this work has potential to work better on person Re-ID tasks in real scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Person Re-Identi cation (Re-ID) targets to probe and return images containing the identical query person from a gallery set. Because of its promising applications in video surveillance and public security, person Re-ID has drawn more and more attention in recent years. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the appearance of a person image can be easily a ected by various factors like camera viewpoint, human pose, illumination, occlusion, etc. Those make identifying a speci c person from the large-scale gallery set a challenging task. To tackle this Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org.  challenge, most of person Re-ID works focus on two stages, i.e., descriptor learning and distance metric learning. Descriptor learning aims to learn a discriminative descriptor to represent the appearances of di erent persons. Distance metric learning is designed to reduce the distance among descriptors of images containing the same person. Traditional descriptor learning methods usually extract rigid local invariant features. Su ering from the huge variance of human pose and camera viewpoint, these descriptors are not robust enough and often fail to identify person. Most of distance metric learning methods take a pair of pedestrian images as input, thus correspond to the high complexity. Inspired by the success of Convolutional Neural Networks (CNN) in large-scale visual classication, latest works start to design deep learning algorithms and have achieved signi cant improvements.</p><p>Most of deep learning based works learn descriptors from the whole pedestrian images. Such descriptors thus depict the global cues but may lose crucial details. This problem has been noticed in previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, where the researchers divide the whole pedestrian image into several xed-length strips. By learning descriptors on these strips rather than the whole image, these methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> explicitly embed more detailed local cues. Experiments show such descriptors substantially outperform the global descriptors. However, xed-length strips would be sensitive to the pose variance and person misalignment. As illustrated in <ref type="figure" target="#fig_1">Fig.  1</ref>, misalignment commonly exists in detected pedestrian images. Therefore, more delicate ways should be designed to overcome the pose variance and misalignment issues.</p><p>Aiming to solve the above issues, we propose the Global-Local-Alignment Descriptor (GLAD), which is generated with two modules, i.e., part extraction and descriptor learning. Part extraction module utilizes Deeper Cut <ref type="bibr" target="#b12">[13]</ref> to estimate four human key points that are robust for various poses and camera viewpoints. Three coarse part regions, i.e., head, upper-body, and lower-body are hence generated based on the estimated key points. To explicitly embed part cues in the learned representation, a CNN composed of four sub-networks is proposed in descriptor learning module. Those sub-networks share several convolutional layers and are designed to learn descriptors on three part regions and the global image, respectively. During the training stage, the shared convolution layers can be e ciently optimized by multiple learning tasks on di erent body regions to avoid over tting. After network training, we feed the three part regions and global image into the neural network to extract four descriptors, which are nally concatenated as the GLAD. Therefore, GLAD contains both the global and local cues, thus is potential to be more discriminative. GLAD also could be more robust to the pose variance and image misalignment issues because the human body is divided in a more meaningful way.</p><p>Person Re-ID is commonly solved as a classi cation or distance metric learning problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>, which suffer from massive computations of classi er training and the high time complexity for online pairwise matching. To make our Re-ID system scalable on large-scale datasets, we regard person Re-ID as a ne-grained pedestrian retrieval task, and focus on designing a more e cient indexing and retrieval framework. In person Re-ID gallery sets, each person would has multiple samples. This implies data redundancy and could be optimized by indexing strategies. Our indexing algorithm is thus designed to group samples of the same person into one unit. Speci cally, we propose a Two-fold Divisive Clustering algorithm (TDC) to group di erent samples of one person together through dividing samples of di erent persons in a greedy manner. Finally, a descriptor is generated to depict the visual cues of each group. The online Re-ID procedure can be regarded as a two-folder retrieval, where the coarse retrieval retrieves image groups, and a ne retrieval is then conducted to get a precise image ranklist. In other words, we need not match the query person image against each gallery image during retrieval procedure. So, our retrieval strategy can e ectively speed up the online Re-ID.</p><p>Although there are many deep learning based person Re-ID works, our work di ers from them in the aspects of introducing a more e cient online Re-ID strategy and considering delicate part cues. Zheng et al. <ref type="bibr" target="#b42">[43]</ref> also propose a pose invariant embedding framework to solve the misalignment issue. Ten ne-grained parts are extracted by estimating human key points. These parts are rst normalized by a ne transformation, then are combined to compose a global pose invariant image. The nal representation is hence extracted from the standard pose image. Therefore, the representation in <ref type="bibr" target="#b42">[43]</ref> is not learned explicitly on local parts and still belongs to the global representation. Moreover, as shown in our experiments, ne-grained part extraction is easily a ected by image noises, pose and viewpoint variances. For instance, arms can be invisible due to occlusion or pose changes. Our experiments also show mandatory detection of ne-grained parts, e.g., arms, results in noisy part regions and degrades the Re-ID performance. Extensive experimental results on three public datasets show our GLAD and retrieval framework present competitive accuracy and e ciency compared to the state-of-the-art methods. Our method also presents substantial advantages on automatically detected pedestrian images. Therefore, we conclude this work has potential to be more robust and e ective in real scenarios, and our contribution is valuable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This work is related with deep learning based person Re-ID and human part detection for person Re-ID. The following parts brie y review several works on these two categories, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Learning based Person Re-ID</head><p>Deep learning shows remarkable performance in computer vision and multimedia tasks and has become the main stream method for person Re-ID. Current deep learning based person Re-ID methods can be divided into two categories based on the usage of deep neural network, i.e., feature learning and distance metric learning. Feature learning networks aim to learn a robust and discriminative feature to represent pedestrian images. Cheng et al. <ref type="bibr" target="#b5">[6]</ref> propose a multi-channel parts based network to learn a discriminative feature with an improved triplet loss. Wu et al. <ref type="bibr" target="#b36">[37]</ref> discover hand-crafted feature is complementary with CNN feature. They thus divide one image into ve xed-length part regions. For each part region, a histogram descriptor is generated and concatenated with the full body CNN feature. Su et al. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> propose a semi-supervised attribute learning framework to learn binary attribute features. In <ref type="bibr" target="#b44">[45]</ref>, identi cation model and veri cation model are combined to learn a discriminative representation. In <ref type="bibr" target="#b37">[38]</ref>, a new dropout algorithm is designed for feature learning on a multi-domain dataset, which is generated by combining several existing datasets.</p><p>Siamese network is commonly used to learn better distance metrics between the input image pair. Yi et al. <ref type="bibr" target="#b39">[40]</ref> propose a siamese network composed of three components, i.e., CNN, connection function, and cost function, respectively. Similar with <ref type="bibr" target="#b5">[6]</ref>, several xed-length part regions are divided and trained independently. In <ref type="bibr" target="#b35">[36]</ref>, an end-to-end siamese network is proposed. By utilizing small lters, the network goes deeper and obtains a remarkable performance. Ahmed et al. <ref type="bibr" target="#b0">[1]</ref> design a new layer to capture local relationships between input image pair. In <ref type="bibr" target="#b19">[20]</ref>, comparative attention network is proposed to adaptively compare the similarity between images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Human Part Detection for Person Re-ID</head><p>Human parts provide important local cues of human appearance. Therefore, it is natural to design part detection algorithms for person Re-ID in some early person Re-ID works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Motivated by the symmetry and asymmetry properties of human body, Farenzena et al. <ref type="bibr" target="#b7">[8]</ref> propose to detect salient part regions by the perceptual principles of symmetry and asymmetry. In <ref type="bibr" target="#b6">[7]</ref>, Cheng et al. propose a pictorial structure algorithm to detect parts. In <ref type="bibr" target="#b2">[3]</ref>, deformable part model <ref type="bibr" target="#b8">[9]</ref> is utilized to detect six body parts. Most of recent deep learning based methods directly divide pedestrian images into xed-length regions and have not paid much attention in leveraging part cues <ref type="bibr" target="#b16">[17]</ref>. Recently, Zheng et al. <ref type="bibr" target="#b42">[43]</ref> adopt the convolution pose machines <ref type="bibr" target="#b34">[35]</ref> to detect ne-grained body parts and then generate a standard pose image, which is hence utilized to generate descriptors. Therefore, the representation <ref type="bibr" target="#b42">[43]</ref> is not learned explicitly on local parts. Also, ne-grained part extraction is expensive and could be easily a ected by image noises, pose and viewpoint variance. Those factors would degrade the Re-ID accuracy and e ciency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>Given a probe image p, person Re-ID targets to identify and return images containing the identical person in p from a set of gallery images {( 1 , l 1 ), ( 2 , l 2 ), ..., ( N , l N )}, where i and l i denote the ith gallery image and its person ID label, respectively. Person Re-ID can be tackled by classifying those gallery images <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>, or by an image retrieval procedure, i.e., ranking those images based on a descriptor and a distance metric d(f p , f i ), where f represents the generated image descriptor, and d(·) denotes the distance between probe image and gallery image. The returned ranklist of N images could be denoted as {r 1 , r 2 , ..., r N }, where r i is the sorted index of image i . Under the retrieval formulation, the objective function of person Re-ID can be summarized as Eq. <ref type="formula" target="#formula_0">(1)</ref>, i.e.,</p><formula xml:id="formula_0">min N i=1 r i I(l p , l i ), I(l p , l i ) = 1 l p = l i 0 l p l i ,<label>(1)</label></formula><p>where l p is the person ID label of the probe image p. Compared with person classi cation, treating person Re-ID as a retrieval task has potential to better cope with large-scale data and present improved generalization ability to unseen samples. Therefore, the retrieval formulation may work better in real scenarios, because the probe persons commonly do not exist in the training set. Under the retrieval formulation, person Re-ID consists of two critical steps: 1) robust and discriminative descriptor generation, and 2) e cient image similarity computation and ranking.</p><p>Targeting to deal with the image misalignment and pose variance issues, we present Global-Local-Alignment Descriptor in Sec. 4. Most of previous Re-ID works focus on descriptor generation, and has not paid much attention to e cient gallery image indexing and ranking. In Sec. 5, we propose an e cient indexing and retrieval framework that makes person Re-ID using GLAD more e cient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GLOBAL-LOCAL-ALIGNMENT DESCRIPTOR</head><p>The framework of GLAD extraction is summarized in <ref type="figure" target="#fig_2">Fig. 2</ref>. It can be observed that, we rst detect several body parts from an input person image, then learn descriptors from both the global and local regions. Through detecting more subtle body parts, GLAD has potential to be robust to the misalignment and would gain more discriminative power by explicitly embedding global and local cues. In the following, we present the part extraction module and descriptor learning module, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Part Extraction</head><p>Body part extraction has been studied by many pose estimation works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. However, the pedestrian images in person Re-ID are taken in unconstrained environment, and are easily affected by occlusions, viewpoint changes, and pose variances. Those factors make it di cult to detect ne-grained parts. For example, either the left or right arms cannot be detected in side view images of pedestrian. Mandatory detection of such parts results in noisy part regions and may degrade the Re-ID performance. The above issues motivate us to consider parts that could be easily and reliably detected under various viewpoint and pose changes. Speci cally, we utilize Deeper Cut <ref type="bibr" target="#b12">[13]</ref> to estimate only four key points, i.e., upper-head, neck, right-hip, left-hip, respectively on the pedestrian image. As shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, based on those four key points, we can coarsely divide a pedestrian image into three part regions: head, upper-body, and lower-body, respectively. The head region can be located based on upper-head point and neck point. Suppose the size of person image is H × W and the coordinates of upper-head point and neck point are (x 1 , 1 ) and (x 2 , 2 ), we crop the head region B h with Eq. (2), i.e.,</p><formula xml:id="formula_1">B h = [(x c − w/2, 1 − α), (x c + w/2, 2 + α)], w = 2 − 1 + 2 · α, x c = (x 1 + x 2 )/2,<label>(2)</label></formula><p>where the B h is located by coordinates of the upper-left and bottomright points. α is a parameter controlling the overlap between neighboring parts regions. α is experimentally set as 15 for the 512 × 256 sized person image.</p><p>Suppose the coordinates of left-hip and right-hip points are (x 3 , 3 ) and (x 4 , 4 ), the upper-body region B ub and the lower-body region B lb can be captured in similar way with Eq. (3), i.e.,</p><formula xml:id="formula_2">B ub = [(0, 2 − 2 · α), (W − 1, c + 2 · α)], B lb = [(0, c − 2 · α), (W − 1, H − 1)], c = ( 3 + 4 )/2,<label>(3)</label></formula><p>Examples of detected keypoints and part regions are illustrated in <ref type="figure" target="#fig_4">Fig. 3</ref>. It can be observed that, the four key points, i.e., upperhead, neck, right-hip, left-hip are more robust to pose and viewpoint changes than the other keypoints. The three part regions hence could be reliably extracted. Because the keypoints on human foot are not stable, it is di cult to con rm the bottom coordinate of lower-body region. We thus simply set the bottom of the image as  the bottom of lower-body region. More extensive evaluations on the validity of part extraction will be given in Sec. 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Descriptor Learning</head><p>Existing deep neural networks such as AlexNet <ref type="bibr" target="#b15">[16]</ref>, GoogLeNet <ref type="bibr" target="#b27">[28]</ref>, VGGNet <ref type="bibr" target="#b22">[23]</ref>, and ResNet <ref type="bibr" target="#b11">[12]</ref> have been utilized to learn descriptors on the global image for person Re-ID. To explicitly leverage global and local cues for descriptor learning, we propose a four-stream CNN. As illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>, the proposed network includes one sub-network for global descriptor learning and three sub-networks for part descriptor learning, respectively. These sub-networks share the identical structure and can be initialized by exiting network structures and parameters. Speci cally, our network is modi ed and initialized from GoogLe-Net <ref type="bibr" target="#b27">[28]</ref> by replacing its fully connected layers with two convolutional layers as classi er. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, we call the rst convolutional layer as feature layer because it is used for feature extraction. The latter convolutional layer directly produces C feature maps corresponding to C classes in the training set. Therefore, we call those feature maps as con dence maps, which essentially show the classi cation con dences.</p><p>Based on the con dence maps, we apply Global Average Pooling (GAP) to generate the classi cation score for each class. GAP averages the responses on each two-dimensional feature map, i.e.,</p><formula xml:id="formula_3">S c = 1 X × Y X x =1 Y =1 M c (x, ),<label>(4)</label></formula><p>where S c denotes the classi cation score of the c-th class, and M c (x, ) is the response value at location of (x, ) on the con dence map corresponding to the c-th class. X and Y are the width and height of the con dence map, respectively. Following GAP, softmax loss function is used to compute the network loss. This updated architecture removes the fully connected layers and shows several advantages for feature learning. 1) It has fewer parameters, thus could better avoid over tting on small training sets. 2) Without fully connected layers, it accepts images with arbitrary scales as input. We thus could resize the input image into larger scales to allow the neural network capture more detailed cues. Experiment result shows our network generates more discriminative feature than many existing algorithms.</p><p>For the global descriptor learning, the input image is the original image with scale resized to 512 × 256. For descriptor learning on head, the head region is resized to 96 × 96 as the network input. For upper-body and lower-body sub-networks, the input size is set as 224 × 256, respectively. These sub-networks are trained in di erent classi cation tasks, i.e., each task aims to classify the global or local input regions into correct person classes. As illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>, instead of training the four sub-networks alone, we train them together with sharing weights in convolution layers. This optimizes the convolutional layers in di erent tasks and hence better avoids over tting. We evaluate this strategy in Sec. 6.3.</p><p>During testing, we use the feature maps produced by feature layer to generate descriptors. Suppose M channels of feature maps are generated in the feature layer, we nally generate an M dimensional feature descriptor by GAP on each feature map. The descriptors extracted on four regions are concatenated as the nal GLAD, i.e.,</p><formula xml:id="formula_4">f GLAD = [f G ; f h ; f ub ; f lb ],<label>(5)</label></formula><p>where f GLAD denotes the nal GLAD. f G represents the learned feature on the global image, f h , f ub and f lb are descriptors generated from the three sub-networks, respectively. Therefore, GLAD is an 4 × M dimensional vector, which explicitly contains global and local cues. We experimentally set M as 1024, which generates an 4096-dim GLAD. By only detecting robust coarse part regions, GLAD seeks a reasonable trade-o between part detection accuracy and robustness to misalignment and pose changes. Therefore, GLAD would be more robust to misalignment issues than global features. Moreover, GLAD is trained with multiple losses computed on di erent regions. This essentially enforces the network to focus on di erent parts and learn discriminative feature for each of them. This training strategy has potential to learn more discriminative features than previous deep features, which may get over tted to the most discriminative parts on the training set and ignores the others. Detailed evaluations on GLAD will be presented in Sec. 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RETRIEVAL FRAMEWORK</head><p>Based on the GLAD, we proceed to propose a hierarchical indexing and retrieval framework illustrated in <ref type="figure" target="#fig_5">Fig. 4</ref>. As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, the o ine indexing stage clusters similar images into the same group. This is motivated by the fact that, each person has multiple samples in the gallery set, e.g., one person can be recorded for multiple times by di erent cameras. Carefully grouping these samples together thus reduces the data redundancy and improves the online retrieval e ciency. Moreover, sample grouping is potential to signi cantly improve the accuracy of person Re-ID. For each person in the gallery set, the generated group may contain both his/her samples that can be easily identi ed and samples that can be hardly identi ed. Those hard samples thus can be retrieved together with the easy samples as one group during online retrieval. Therefore, e cient o ine grouping algorithms should be designed.</p><p>There are many ways to cluster the images into groups <ref type="bibr" target="#b41">[42]</ref>. Because the number of identities in person Re-ID gallery is unknown, it is hard to set the group number manually and makes clustering methods like K-Means [11] not optimal for this task. We thus propose a clustering method, called Two-fold Divisive Clustering (TDC) that does not need to manually specify the group number. Similar with H-LDC <ref type="bibr" target="#b33">[34]</ref>, TDC is a greedy strategy that divides images in galley into groups and ensures samples in each group share strong similarity with each other. For TDC, the group dissimilarity degree measurement is de ned as</p><formula xml:id="formula_5">D dis = 1 N × (N − 1) N i=1 N j=1 dis( i , j ),<label>(6)</label></formula><p>where D dis denotes the dissimilarity degree within a group, and N is the number of images in the group. dis(·) represents the squared Euclidean distance between two images in the group, i.e., i and j . TDC is conducted to divide the image gallery into groups in a greedy manner, and nally ensures the dissimilarity degree within each group below a threshold θ . Details of TDC are summarized in Algorithm 1. Compared with K-Means related methods, TDC does not require the pre-de ned cluster number and only has one parameter, i.e., the dissimilarity degree threshold θ . Our experiments in Sec. 6.6 show that this parameter could be easily tuned. Our current work computes dis( i , j ) with GLAD. More discriminative features could be leveraged to further improve the quality of groups. This will be studied in our future work.</p><p>After o ine clustering images into groups, we generate a group descriptor to depict the visual appearance of each group. We generate the group descriptor wth Eq. <ref type="formula" target="#formula_6">(7)</ref>, i.e.,</p><formula xml:id="formula_6">f G (i) = 1 N N j=1 f G LAD j (i),<label>(7)</label></formula><p>where f G (i) denotes the i-th dimension of group descriptor f G . N is the number of samples in the group, and f G LAD j (i) is the ith dimension in GLAD of the j-th sample. For every group, we can get an 4096-dim feature descriptor. To speed up the similarity computation, we reduce the dimensionality of f G into 128 with PCA for fast online retrieval. if ∃D dis * &gt; θ then <ref type="bibr">6:</ref> for each G * with D dis * &gt; θ do <ref type="bibr">7:</ref> Choose the furthest two samples ( l , r ) in G * 8:</p><formula xml:id="formula_7">G Count +1 = {}, G Count +2 = {} 9:</formula><p>for j ∈ G * do <ref type="bibr">10:</ref> if dis( j , l ) &lt; dis( j , r ) then 11: end if 23: end while As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, the online retrieval rst retrieves image groups. GLAD is rst extracted from the query, then is converted into an 128-dim vector with PCA. The 128-dim feature is used to retrieve relevant image groups. Because the number of image groups is signi cantly smaller than the number of images, this procedure can be e ciently nished. After the coarse retrieval, the top K relevant groups are selected for ne retrieval, i.e., the original 4096-dim GLAD is used to rank the images contained in the K groups to get a precise image ranklist. We experimentally set K as 100. The two stages are performed to rst quickly narrow-down the search space, then re ne the initial result, respectively. Thus, they are combined to improve both the Re-ID e ciency and accuracy. This retrieval framework is evaluated in Sec. 6.6.</p><formula xml:id="formula_8">G Count +1 ← G Count +1 ∪ j 12: else 13: G Count +2 ← G Count</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS 6.1 Datasets</head><p>We evaluate the proposed methods on three widely used person Re-ID datasets, i.e., Market1501 <ref type="bibr" target="#b43">[44]</ref>, CUHK03 <ref type="bibr" target="#b16">[17]</ref>, and VIPeR <ref type="bibr" target="#b9">[10]</ref>.</p><p>Market1501 <ref type="bibr" target="#b43">[44]</ref> is composed of 1,501 identities automatically detected from six cameras. The dataset clearly de nes and splits training and testing sets. The training set contains 12,936 images of 751 identities. 19,732 images of 750 identities are included in the testing set. Market1501 is a large-scale dataset and is designed for pedestrian retrieval task. Therefore, mean Average Precision (mAP) is also used to evaluate person Re-ID algorithms.</p><p>CUHK03 <ref type="bibr" target="#b16">[17]</ref> consists of 1,467 identities captured from two cameras. Automatically detected images by pedestrian detector and human labeled bounding boxes are both provided. On average, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation Details</head><p>We use Ca e <ref type="bibr" target="#b13">[14]</ref> to implement the neural networks. To estimate keypoints for GLAD extraction, we use Deeper Cut <ref type="bibr" target="#b12">[13]</ref> model pre-trained on the MPII human pose dataset <ref type="bibr" target="#b1">[2]</ref>. During GLAD learning procedure, an initial learning rate is set as 0.001, and is divided by 2 after every 20,000 iterations. Fine-tuning is applied on the target training set to avoid over tting. On Market1501 and CUHK03, we train our network with 100,000 iterations. On VIPeR, we combine VIPeR training set with the training sets of CUHK03 and Market1501, then train the neural network on this mixed dataset with 100,000 iterations. All experiments are conducted on a server equipped with GeForce GTX 1080 GPU, Intel i7 CPU, and 32GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation on Descriptor Learning</head><p>Our descriptor is learned by a four-stream neural network on the global and local regions. The four neural networks are trained with shared parameters in convolutional layers. We thus rst compare di erent feature fusion and training strategies to test the validity of our descriptor learning method. The experimental results on Market1501 are shown in <ref type="table" target="#tab_0">Table 1</ref>. In <ref type="table" target="#tab_0">Table 1</ref>, WO/S denotes training without parameter sharing, and W/S denotes training with shared parameters. "Global" denotes f G extracted by our four stream network on the global image. "Upper+Lower body" fuses f ub and f lb . "Head+Upper+Lower body" fuses features on three local regions, and "GLAD" fuses the four descriptors in Eq. (5).</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, it is obvious that, sharing parameters during training substantially boosts the performance of learned descriptors. This might be because, the shared convolution kernels are forced to learn both global and local cues, thus are trained with more samples and could better avoid over tting. We also observe that, the global feature performs better than features on local regions. This might be because the global image contains more visual cues thus reasonably conveys stronger discriminative power. This also explains why the discriminative power of descriptor on the head region is weaker than the ones on upper and lower body regions. As a result, "Upper+Lower body" outperforms "Head+Upper+Lower body", i.e., equally fusing another descriptor with low discriminative power degrades the performance of f ub and f lb . "Head+Upper+Lower body (W)" denotes fusing the three descriptors with di erent weights decided by the size of the three regions, i.e., we set three weights as 0.2, 0.4, 0.4, respectively. It is can be observed that, weighted fusion results in better accuracy than both "Upper+Lower body" and "Head+Upper+Lower body". This means, with proper weight, the descriptor on head region is still helpful in improving the Re-ID performance. By combining both the global and local descriptors, GLAD outperforms all of the global and fused local descriptors. For instance, GLAD signi cantly outperforms our baseline by 13.6% on mAP and 9.2% on Rank-1 accuracy. The above experiments show the validity of our descriptor learning strategy, i.e., embedding local and global cues in a four-stream network with shared convolutional layers.</p><p>Examples of Re-ID results produced by our baseline and GLAD are shown in <ref type="figure" target="#fig_6">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Evaluation on Part Extraction</head><p>Because it is di cult to detect accurate ne-grained parts on person Re-ID data, GLAD should be extracted on three coarse parts rather than the ne-grained parts. To estimate the validity of our part extraction strategy, the most intuitive way is to compare our descriptor with descriptors learned on ne-grained parts using the GLAD structure. Thus, we compare with a recent work that considers ne-grained part cues for descriptor learning <ref type="bibr" target="#b42">[43]</ref>. In <ref type="bibr" target="#b42">[43]</ref>, 10 parts are captured and normalized with a ne transform to achieve pose invariance. The normalized parts then compose a standard pose image, which is expected to be invariant to misalignment and pose changes. For fair comparison, we input the original image and the standard pose image generated by <ref type="bibr" target="#b42">[43]</ref> into our descriptor learning module, then compare the learned GLAD descriptors on these two inputs. In other words, the two GLADs are learned with the same setting but on di erent inputs, i.e., our method embeds coarse part cues, and the other considers ne-grained parts and extra a ne transformation. If the ne-grained part cues are helpful to learn robust descriptor, the GLAD considering ne-grained part cues should outperform the GLAD generated from coarse parts. <ref type="table" target="#tab_0">Table 1</ref> shows the performance of GLAD extracted on coarse parts. The performance of GLAD considering ne-grained part cues is summarized in <ref type="table" target="#tab_3">Table 2</ref>. The comparison between <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref> obviously shows that, descriptors generated on coarse parts gets better performance than those considering ne-grained parts cues. This conclusion thus supports our discussions, i.e., negrained part region detection is unstable and may degrade the performance of person Re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparison with Other Methods</head><p>To test the discriminative power of GLAD, we use the 4096-dim GLAD and squared Euclidean distance for person Re-ID. On mar-ket1501, we compare GLAD with many state-of-the-art works belonging to two categories, i.e., distance metric learning based methods and deep learning based methods, respectively. In the comparison, the metric learning based methods include Bow+Kissme <ref type="bibr" target="#b43">[44]</ref>, WARCA <ref type="bibr" target="#b14">[15]</ref>, LOMO+XQDA <ref type="bibr" target="#b17">[18]</ref>, Null Space <ref type="bibr" target="#b40">[41]</ref>, SCSP <ref type="bibr" target="#b3">[4]</ref>. Deep learning based methods include Gated Siamese <ref type="bibr" target="#b30">[31]</ref>, LSTM Siamese <ref type="bibr" target="#b31">[32]</ref>, PersonNet <ref type="bibr" target="#b35">[36]</ref>, DLCNN <ref type="bibr" target="#b44">[45]</ref> and PIE <ref type="bibr" target="#b42">[43]</ref>. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>. In the table, we observe that our method outperforms these previous works by large margins. For example, our method outperform the best result of those compared works by 10.4% on Rank-1 accuracy and 14.0% on mAP, respectively. On CUHK03, we compare GLAD with recent distance metric learning based methods, including WARCA <ref type="bibr" target="#b14">[15]</ref>, LOMO + XQDA <ref type="bibr" target="#b17">[18]</ref>, Null Space <ref type="bibr" target="#b40">[41]</ref> and MLAPG <ref type="bibr" target="#b18">[19]</ref>. Deep learning based methods including PersonNet <ref type="bibr" target="#b35">[36]</ref>, SI-CI <ref type="bibr" target="#b32">[33]</ref>, Gated Siamese <ref type="bibr" target="#b30">[31]</ref>, LSTM Siamese <ref type="bibr" target="#b31">[32]</ref>, Improved Deep <ref type="bibr" target="#b0">[1]</ref>, DGD <ref type="bibr" target="#b37">[38]</ref> and PIE <ref type="bibr" target="#b42">[43]</ref> are also compared. Experiments are conducted on both the datasets with labeled and detected bounding boxes. The results are show in <ref type="table" target="#tab_5">Table  4</ref> and <ref type="table" target="#tab_6">Table 5</ref>, respectively. From the two tables, it is clear that GLAD achieves promising performance. We achieve Rank-1 accuracy of 85.0% on the labeled dataset and Rank-1 accuracy of 82.2% on the detected dataset, which outperform all the other works.</p><p>The comparisons on VIPeR are summarized in <ref type="table" target="#tab_7">Table 6</ref>. WARCA <ref type="bibr" target="#b14">[15]</ref>, Null Space <ref type="bibr" target="#b40">[41]</ref>, LOMO+XQDA <ref type="bibr" target="#b17">[18]</ref>, Mirror-KMFA <ref type="bibr" target="#b4">[5]</ref>, MLAPG <ref type="bibr" target="#b18">[19]</ref>, SCSP <ref type="bibr" target="#b3">[4]</ref> are compared as distance metric learning based methods. Deep learning based methods include Gated Siamese <ref type="bibr" target="#b30">[31]</ref>, LSTM Siamese <ref type="bibr" target="#b31">[32]</ref>, SI-CI <ref type="bibr" target="#b32">[33]</ref>, PIE <ref type="bibr" target="#b42">[43]</ref> and PIE+Mirror+MFA <ref type="bibr" target="#b42">[43]</ref>. We can observe that traditional distance metric learning based methods show substantial advantages over deep learning based methods. This is mainly because VIPeR is not large enough for deep model training. However, GLAD still achieves the best Rank-1 accuracy among all of those methods, and constantly outperforms all the other deep learning based methods at di erent rank levels.</p><p>It is also necessary to note that, PIE <ref type="bibr" target="#b42">[43]</ref> considers ne-grained parts to learn global descriptors. Our method substantially outperforms PIE on the three datasets. This also shows the advantages of considering coarse part cues and explicitly embedding both local and global cues for descriptor learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Performance of Retrieval Framework</head><p>Market1501 allows to implement person Re-ID as a pedestrian retrieval task. Therefore, we use Market1501 to evaluate our retrieval framework. Our indexing and retrieval method involves one parameter θ in TDC, which is the threshold of dissimilarity degree within each group. We thus rst test the impact of θ on Re-ID performance. Experimental results are summarized in <ref type="table" target="#tab_8">Table 7</ref>. <ref type="table" target="#tab_8">Table 7</ref> shows di erent θ a ect the number of generated groups. Smaller θ requires larger similarity within each group, thus divides   the samples in gallery into more groups. Note that, the group number equals to the sample number when θ = 0. It is obvious that smaller θ improves the Re-ID performance. This is because smaller θ tends to exclude outliers in each group and produces more accurate coarse retrieval results. It also can been seen that, the nal accuracy is mainly decided by the ne retrieval and is not sensitive to θ . θ obviously a ects the retrieval e ciency. Larger θ generates fewer groups, i.e., produces smaller search space for coarse retrieval.</p><p>To further show the bene t of coarse retrieval, we test the impact of feature dimensionality in coarse retrieval in <ref type="table" target="#tab_8">Table 7</ref>. With low dimensionality, the retrieval speed is substantially improved. For example, with θ = 0.0015, reducing the dimensionality from 4096 to 128 substantially reduces the retrieval time from 176ms to 31ms. The mAP and Rank-1 accuracy are almost unchanged, e.g., 73.2% vs. 73% on mAP and 89.9% vs. 89.8% on Rank-1 accuracy, respectively. Such accuracy is similar to the one of GLAD in <ref type="table" target="#tab_4">Table 3</ref>. With 128 or 512-dim feature, increasing θ does not constantly improve the e ciency, because ne retrieval dominates the query time. Higher θ generates larger group size, and enlarges the search space of ne retrieval, i.e., images in top-100 relevant groups. Those comparisons imply our indexing and retrieval framework e ectively accelerates the online Re-ID without degrading the accuracy. We thus conclude that, our proposed person Re-ID procedure is e cient and expected to work well in large-scale person Re-ID tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>This paper presents a GLAD descriptor and an e cient indexing and retrieval framework for pedestrian image retrieval. We rst discuss that person Re-ID can be tackled in a retrieval task. GLAD is hence proposed with the motivation of generating a discriminative feature descriptor robust to misalignment and pose change issues. GLAD is extracted by explicitly learning the global and coarse part cues in human body through a four-stream CNN model. An e cient indexing and retrieval framework is nally proposed to accelerate the online Re-ID procedure. In this framework, the pedestrian images in gallery set are rst divided into groups with TDC for o ine indexing. Online retrieval rst retrieves groups, then conducts ne retrieval to get a precise image ranklist. Extensive experiments show the strong discriminative power of GLAD and high speed of person Re-ID based on our indexing and retrieval framework.</p><p>Our current o ine indexing needs to compute the pair-wise similarity between images, thus requires high o ine complexity. Although this does not a ect the online e ciency, more e cient strategies like hashing and approximate k-NN methods will be explored in our future work. Moreover, better o ine grouping and group feature extraction strategies will be studied, e.g., considering extra features like time stamp and location cues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>MM' 17 ,</head><label>17</label><figDesc>October 23-27, 2017, Mountain View, CA, USA. © 2017 ACM. 978-1-4503-4906-2/17/10. . . $15.00 DOI: https://doi.org/10.1145/3123266.3123279</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Examples of detected pedestrian images from Mar-ket1501 ( rst row) and CUHK03 (second row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Framework of GLAD extraction, which includes two modules, i.e., part extraction and descriptor learning. Three parts are extracted based on four detected key points. A four-stream CNN is designed to generate descriptors from both the global and local regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Examples of detected keypoints and generated three part regions. The four keypoints used in our method are emphasized with large size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Our retrieval framework mainly contains two modules, i.e., o line grouping and coarse-to-ne online retrieval. Two-fold Divisive Clustering (TDC) gathers similar images into groups. Images in returned groups are retrieved with original GLAD to generate an image ranklist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Examples of Re-ID results on Market1501. In each example, the rst row and second row show top-10 retrieved images of baseline and GLAD, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>Two-fold Divisive ClusteringInput: Gallery { 1 , 2 , ..., N }, dissimilarity threshold θ Output: Group set R 1: Initialization:G 1 = { 1 , 2 , ..., N }, R = {G 1 }, Count = 1</figDesc><table><row><cell>2:</cell><cell></cell></row><row><cell cols="2">3: while True do</cell></row><row><cell>4:</cell><cell>∀G  *  ∈ R: compute D dis  *  according to Eq. (6)</cell></row><row><cell>5:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of di erent feature fusion and training strategies on Market1501. Baseline denotes the descriptor generated by our modi ed GoogLeNet<ref type="bibr" target="#b27">[28]</ref> on the original image. ,264 images taken by two cameras. 316 identities are randomly chosen as training data, the rests are hence used as testing data. Because of its small size, the training set in VIPeR is enhanced to make deep model training possible. More details of training procedure on VIPeR are summarized in Sec. 6.2.</figDesc><table><row><cell>Training Strategy</cell><cell>Descriptor</cell><cell cols="2">mAP Rank-1</cell></row><row><cell>-</cell><cell>Baseline</cell><cell>60.3</cell><cell>80.7</cell></row><row><cell></cell><cell>Global</cell><cell>60.3</cell><cell>80.7</cell></row><row><cell></cell><cell>Upper+Lower body</cell><cell>53.8</cell><cell>79.8</cell></row><row><cell>WO/S</cell><cell>Head+Upper+Lower body</cell><cell>49.6</cell><cell>77.3</cell></row><row><cell></cell><cell cols="2">Head+Upper+Lower body (W) 55.7</cell><cell>81.0</cell></row><row><cell></cell><cell>GLAD</cell><cell>71.0</cell><cell>87.9</cell></row><row><cell></cell><cell>Global</cell><cell>66.1</cell><cell>84.6</cell></row><row><cell></cell><cell>Upper+Lower body</cell><cell>60.9</cell><cell>84.2</cell></row><row><cell>W/S</cell><cell>Head+Upper+Lower body</cell><cell>55.6</cell><cell>81.8</cell></row><row><cell></cell><cell cols="2">Head+Upper+Lower body (W) 62.8</cell><cell>85.5</cell></row><row><cell></cell><cell>GLAD</cell><cell>73.9</cell><cell>89.9</cell></row><row><cell cols="4">each person has 4.8 images under each camera. CUHK provides 20</cell></row><row><cell cols="4">split sets, each randomly selects 1,367 identities for training and the</cell></row><row><cell cols="4">rest fort testing. We choose the rst split set and report the average</cell></row><row><cell cols="3">accuracy after repeating the experiments for 1,000 times.</cell><cell></cell></row><row><cell cols="4">VIPeR [10] is smaller than Market1501 and CUHK03. It contains</cell></row><row><cell>632 identities and 1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of descriptors considering negrained part cues on Market1501. "*" denotes descriptors are extracted from the standard pose image<ref type="bibr" target="#b42">[43]</ref> generated based on ne-grained part cues.</figDesc><table><row><cell>Training Strategy</cell><cell>Descriptor</cell><cell cols="2">mAP Rank-1</cell></row><row><cell>-</cell><cell>Baseline*</cell><cell>51.3</cell><cell>72.6</cell></row><row><cell></cell><cell>Global*</cell><cell>55.2</cell><cell>76.8</cell></row><row><cell></cell><cell>Upper+Lower body*</cell><cell>41.2</cell><cell>68.9</cell></row><row><cell>W/S</cell><cell>Head+Upper+Lower body*</cell><cell>37.7</cell><cell>67.4</cell></row><row><cell></cell><cell cols="2">Head+Upper+Lower body (W)* 44.2</cell><cell>72.0</cell></row><row><cell></cell><cell>GLAD*</cell><cell>61.2</cell><cell>81.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison on Market1501 in single query mode.</figDesc><table><row><cell>Methods</cell><cell cols="2">mAP Rank-1</cell></row><row><cell>BoW+Kissme [44]</cell><cell>20.8</cell><cell>44.4</cell></row><row><cell>WARCA [15]</cell><cell>-</cell><cell>45.2</cell></row><row><cell cols="2">LOMO+XQDA [18] 22.2</cell><cell>43.8</cell></row><row><cell>Null Space [41]</cell><cell>35.7</cell><cell>61.0</cell></row><row><cell>SCSP [4]</cell><cell>26.4</cell><cell>51.9</cell></row><row><cell>PersonNet [36]</cell><cell>26.4</cell><cell>37.2</cell></row><row><cell>Gated Siamese [31]</cell><cell>39.6</cell><cell>65.9</cell></row><row><cell cols="2">LSTM Siamese [32] 35.3</cell><cell>61.6</cell></row><row><cell>DLCNN [45]</cell><cell>59.9</cell><cell>79.5</cell></row><row><cell>PIE [43]</cell><cell>56.0</cell><cell>79.3</cell></row><row><cell>Baseline</cell><cell>60.3</cell><cell>80.7</cell></row><row><cell>GLAD</cell><cell>73.9</cell><cell>89.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison on CUHK03 labeled dataset.</figDesc><table><row><cell>Methods</cell><cell cols="4">Rank-1 Rank-5 Rank-10 Rank-20</cell></row><row><cell>LOMO + XQDA [18]</cell><cell>52.2</cell><cell>82.2</cell><cell>94.1</cell><cell>96.3</cell></row><row><cell>WARCA [15]</cell><cell>78.4</cell><cell>94.6</cell><cell>-</cell><cell>-</cell></row><row><cell>MLAPG [19]</cell><cell>58.0</cell><cell>87.1</cell><cell>94.7</cell><cell>96.9</cell></row><row><cell>Null Space [41]</cell><cell>62.6</cell><cell>90.1</cell><cell>94.8</cell><cell>98.1</cell></row><row><cell>PersonNet [36]</cell><cell>64.8</cell><cell>89.4</cell><cell>94.9</cell><cell>98.2</cell></row><row><cell>Improved Deep [1]</cell><cell>54.7</cell><cell>86.5</cell><cell>93.9</cell><cell>98.1</cell></row><row><cell>DGD [38]</cell><cell>72.6</cell><cell>91.6</cell><cell>95.2</cell><cell>97.7</cell></row><row><cell>Baseline</cell><cell>74.4</cell><cell>95.4</cell><cell>97.9</cell><cell>99.1</cell></row><row><cell>GLAD</cell><cell>85.0</cell><cell>97.9</cell><cell>99.1</cell><cell>99.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison on CUHK03 detected dataset.</figDesc><table><row><cell>Methods</cell><cell cols="4">Rank-1 Rank-5 Rank-10 Rank-20</cell></row><row><cell>LOMO + XQDA [18]</cell><cell>46.3</cell><cell>78.9</cell><cell>88.6</cell><cell>94.3</cell></row><row><cell>MLAPG [19]</cell><cell>51.2</cell><cell>83.6</cell><cell>92.1</cell><cell>96.9</cell></row><row><cell>Null Space [41]</cell><cell>54.7</cell><cell>84.8</cell><cell>94.8</cell><cell>95.2</cell></row><row><cell>SI-CI [33]</cell><cell>52.2</cell><cell>84.3</cell><cell>92.3</cell><cell>95.0</cell></row><row><cell>Gated Siamese [31]</cell><cell>61.8</cell><cell>80.9</cell><cell>88.3</cell><cell>-</cell></row><row><cell>LSTM Siamese [32]</cell><cell>57.3</cell><cell>80.1</cell><cell>88.3</cell><cell>-</cell></row><row><cell>PIE [43]</cell><cell>67.1</cell><cell>92.2</cell><cell>96.6</cell><cell>98.1</cell></row><row><cell>Baseline</cell><cell>70.4</cell><cell>93.3</cell><cell>97.0</cell><cell>98.7</cell></row><row><cell>GLAD</cell><cell>82.2</cell><cell>95.8</cell><cell>97.6</cell><cell>98.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison on VIPeR dataset.</figDesc><table><row><cell>Methods</cell><cell cols="4">Rank-1 Rank-5 Rank-10 Rank-20</cell></row><row><cell>LOMO + XQDA [18]</cell><cell>40.0</cell><cell>67.4</cell><cell>80.5</cell><cell>91.1</cell></row><row><cell>WARCA [15]</cell><cell>40.2</cell><cell>68.2</cell><cell>80.7</cell><cell>91.1</cell></row><row><cell>Null Space [41]</cell><cell>51.2</cell><cell>82.1</cell><cell>90.5</cell><cell>95.9</cell></row><row><cell>MLAPG [19]</cell><cell>40.7</cell><cell>-</cell><cell>82.3</cell><cell>92.4</cell></row><row><cell>Mirror-KMFA[5]</cell><cell>43.0</cell><cell>75.8</cell><cell>87.3</cell><cell>94.8</cell></row><row><cell>SCSP [4]</cell><cell>53.5</cell><cell>82.6</cell><cell>91.5</cell><cell>96.7</cell></row><row><cell>SI-CI [33]</cell><cell>35.8</cell><cell>67.4</cell><cell>83.5</cell><cell>-</cell></row><row><cell>Gated Siamese [31]</cell><cell>37.8</cell><cell>66.9</cell><cell>77.4</cell><cell>-</cell></row><row><cell>LSTM Siamese [32]</cell><cell>42.4</cell><cell>68.7</cell><cell>79.4</cell><cell>-</cell></row><row><cell>PIE [43]</cell><cell>27.4</cell><cell>43.0</cell><cell>50.8</cell><cell>60.2</cell></row><row><cell>PIE+Mirror+MFA [43]</cell><cell>43.3</cell><cell>69.4</cell><cell>80.4</cell><cell>89.9</cell></row><row><cell>Baseline</cell><cell>39.2</cell><cell>63.3</cell><cell>75.6</cell><cell>82.9</cell></row><row><cell>GLAD</cell><cell>54.8</cell><cell>74.5</cell><cell>83.5</cell><cell>91.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Re-ID performance with di erent θ and feature dimension in coarse retrieval.</figDesc><table><row><cell>θ</cell><cell cols="5">Group Number Dim mAP Rank-1 Times(ms)</cell></row><row><cell>0.0000</cell><cell>19732</cell><cell cols="2">4096 73.9</cell><cell>89.9</cell><cell>368</cell></row><row><cell>0.0010</cell><cell>13509</cell><cell cols="2">4096 73.7</cell><cell>89.9</cell><cell>267</cell></row><row><cell>0.0015</cell><cell>8509</cell><cell cols="2">4096 73.2</cell><cell>89.9</cell><cell>176</cell></row><row><cell>0.0020</cell><cell>2558</cell><cell cols="2">4096 71.7</cell><cell>89.8</cell><cell>101</cell></row><row><cell>0.0015</cell><cell>8509</cell><cell>512</cell><cell>73.1</cell><cell>89.9</cell><cell>50</cell></row><row><cell>0.0015</cell><cell>8509</cell><cell>128</cell><cell>73.0</cell><cell>89.8</cell><cell>31</cell></row><row><cell>0.0020</cell><cell>2558</cell><cell>512</cell><cell>71.6</cell><cell>89.7</cell><cell>69</cell></row><row><cell>0.0020</cell><cell>2558</cell><cell>128</cell><cell>71.4</cell><cell>89.7</cell><cell>61</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ejaz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Part-based spatio-temporal model for multi-person re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Bedagkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Gala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shishir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1908" to="1915" />
			<date type="published" when="2012-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mirror Representation for Modeling View-Speci c Transform in Person Re-Identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Cong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person re-identi cation by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identi cation by articulated appearance matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Person Re-Identi cation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="139" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identi cation by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PETSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Algorithm AS 136: A k-means clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manchek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DeeperCut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional Architecture for Fast Feature Embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable metric learning via weighted approximate rank component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cijo</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classi cation with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepreid: Deep lter pairing neural network for person re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Person reidenti cation by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">E cient psd constrained asymmetric metric learning for person re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">End-toend comparative attention networks for person re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meibin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04404</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-Task Learning with Low Rank Attribute Embedding for Multi-Camera Person Re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep attributes driven multi-camera person re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-Type Attributes Driven Multi-Camera Person Re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attributes driven tracklet-to-tracklet person reidenti cation using Latent Prototypes Space Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="4" to="15" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In CVPR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint learning of single-image and cross-image representations for person reidenti cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Manifold-manifold distance and its application to face recognition with image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="4466" to="4479" />
			<date type="published" when="2012-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Personnet: person re-identi cation with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07255</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An enhanced deep feature representation for person re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Cong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-Cong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Jie</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person reidenti cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-scale Person Re-Identi cation as Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cross Indexing With Grouplets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1969" to="1979" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pose Invariant Embedding for Deep Person Re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07732</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable person re-identi cation: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A Discriminatively Learned CNN Embedding for Person Re-identi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05666</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
