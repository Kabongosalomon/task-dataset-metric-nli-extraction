<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory In Memory: A Predictive Neural Network for Learning Higher-Order Non-Stationarity from Spatiotemporal Dynamics Beijing Key Laboratory for Industrial Big Data System and Application</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjin</forename><surname>Zhang</surname></persName>
							<email>zhang-jj16@mails.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S Yu</forename><surname>Kliss</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moe</forename><forename type="middle">;</forename><surname>Bnrist</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Research Center for Big Data</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Memory In Memory: A Predictive Neural Network for Learning Higher-Order Non-Stationarity from Spatiotemporal Dynamics Beijing Key Laboratory for Industrial Big Data System and Application</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural spatiotemporal processes can be highly nonstationary in many ways, e.g. the low-level non-stationarity such as spatial correlations or temporal dependencies of local pixel values; and the high-level variations such as the accumulation, deformation or dissipation of radar echoes in precipitation forecasting. From Cram√©r's Decomposition <ref type="bibr" target="#b3">[4]</ref>, any non-stationary process can be decomposed into deterministic, time-variant polynomials, plus a zero-mean stochastic term. By applying differencing operations appropriately, we may turn time-variant polynomials into a constant, making the deterministic component predictable.</p><p>However, most previous recurrent neural networks for spatiotemporal prediction do not use the differential signals effectively, and their relatively simple state transition functions prevent them from learning too complicated variations in spacetime. We propose the Memory In Memory (MIM) networks and corresponding recurrent blocks for this purpose. The MIM blocks exploit the differential signals between adjacent recurrent states to model the non-stationary and approximately stationary properties in spatiotemporal dynamics with two cascaded, self-renewed memory modules. By stacking multiple MIM blocks, we could potentially handle higher-order non-stationarity. The MIM networks achieve the state-of-the-art results on four spatiotemporal prediction tasks across both synthetic and real-world datasets. We believe that the general idea of this work can be potentially applied to other time-series forecasting tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Natural spatiotemporal processes exhibit complex nonstationarity in both space and time, where neighboring pixels exhibit local dependencies, and their joint distributions are * Equal contribution, in alphabetical order Distributions Frames <ref type="figure">Figure 1</ref>: An example of 20 consecutive radar maps to illustrate the complicated non-stationarity in precipitation forecasting. First row: radar maps, in which the whiter pixels show higher precipitation probability. Second, third, last row: pixel values' distributions, means and standard deviations for corresponding local regions that are identified by bounding boxes of different colors. Note that different regions have different variation trends, making the spatiotemporal prediction problem extremely challenging.</p><p>changing over time. Learning higher-order properties underlying the spatiotemporal non-stationarity is particularly significant for many video prediction tasks. Examples include modeling highly complicated real-world systems such as traffic flows <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b34">35]</ref> and weather conditions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>. A well-performed predictive model is expected to learn the intrinsic variations in consecutive spatiotemporal context, which can be seen as a combination of the stationary component and the deterministic non-stationary component. A great challenge in non-stationary spatiotemporal prediction is how to effectively capture higher-order trends regarding each pixel and its local area. For example, when making precipitation forecasting, one should carefully consider the complicated and diverse local trends on the evolving radar maps, shown as <ref type="figure">Figure 1</ref>. But this problem is extremely difficult due to the complicated non-stationarity in both space and time. Most prior work handles trend-like non-stationarity with recursions of CNNs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b34">35]</ref> or relatively simple state transitions in RNNs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>. The lack of non-stationary modeling capability prevents reasoning about uncertainties in spatiotemporal dynamics and partially leads to the blurry effect of the predicted frames.</p><p>We attempt to resolve this problem by proposing a generic RNNs architecture that is more effective in non-stationarity modeling. We find that though the forget gates in the recurrent predictive models could deliver, select, and discard information in the process of memory state transitions, they are too simple to capture higher-order non-stationary trends in high-dimensional time series. In particular, the forget gates in the recent PredRNN model <ref type="bibr" target="#b31">[32]</ref> does not work appropriately on precipitation forecasting: about 80% of them are saturated over all timestamps, implying almost timeinvariant memory state transitions. In other words, future frames are predicted by approximately linear extrapolations.</p><p>In this paper, we focus on improving the memory transition functions of RNNs. Most statistical forecasting methods in classic time series analysis assume that the non-stationary trends can be rendered approximately stationary by performing suitable transformations such as differencing. We introduce this idea to RNNs and propose a new RNNs building block named Memory In Memory (MIM), which leverages the differential information between neighboring hidden states in the recurrent paths. MIM can be viewed as an improved version of LSTM <ref type="bibr" target="#b10">[11]</ref>, whose forget gate is replaced by another two embedded long short-term memories.</p><p>MIM has the following characteristics: (1) It creates unified modeling for the spatiotemporal non-stationarity by differencing neighboring hidden states rather than raw images. (2) By stacking multiple MIM blocks, our model has a chance to gradually stationarize the spatiotemporal process and make it more predictable. (3) Note that over-differencing is no good for time series prediction, as it may inevitably lead to a loss of information. This is another reason that we apply differencing in memory transitions rather than all recurrent signals, e.g. the input gate and the input modulation gate. (4) MIM has one memory cell adopted from LSTMs as well as two additional recurrent modules with their own memories embedded in the transition path of the first memory. We use these modules to respectively model the higher-order non-stationary and approximately stationary components of the spatiotemporal dynamics. The proposed MIM networks achieve the state-of-the-art results on multiple prediction tasks, including a widely used synthetic dataset and three real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">ARIMA Models for Time Series Forecasting</head><p>Our model is inspired by the Autoregressive Integrated Moving Average (ARIMA) models. A time-series random variable whose power spectrum remains constant over time can be viewed as a combination of signal and noise. An ARIMA model aims to separate the signal from the noise. The obtained signal is then extrapolated into the future. In theory, it tackles time series forecasting by transforming the non-stationary process to stationary through differencing <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deterministic Spatiotemporal Prediction</head><p>Spatiotemporal non-stationary processes are more complicated, as the joint distribution of neighboring pixel values is varying in both space and time. Like low-dimensional time series, they can also be decomposed into deterministic and stochastic components. Recent work in neural networks explored spatiotemporal prediction from these two aspects.</p><p>CNNs <ref type="bibr" target="#b16">[17]</ref> and RNNs <ref type="bibr" target="#b26">[27]</ref> have been widely used for learning the deterministic spatial correlations and temporal dependencies from videos. Ranzato et al. <ref type="bibr" target="#b22">[23]</ref> defined a recurrent model predicting frames in a discrete space of patch clusters. Srivastava et al. <ref type="bibr" target="#b25">[26]</ref> introduced the sequence to sequence LSTM network from language modeling to video prediction. But this model can only capture temporal variations. To learn spatial and temporal variations in a unified network structure, Shi et al. <ref type="bibr" target="#b23">[24]</ref> integrated the convolution operator into recurrent state transition functions, and proposed the Convolutional LSTM. Finn et al. <ref type="bibr" target="#b8">[9]</ref> developed an action-conditioned video prediction model that can be further used in robotics planning when combined with the model predictive control methods. Villegas et al. <ref type="bibr" target="#b28">[29]</ref> and Patraucean et al. <ref type="bibr" target="#b20">[21]</ref> presented recurrent models based on the convolutional LSTM that leverage optical flow guided features. Kalchbrenner et al. <ref type="bibr" target="#b13">[14]</ref> proposed the Video Pixel Network (VPN) that encodes the time, space, color structures of videos as a four-dimensional dependency chain. It achieves sharp prediction results but suffers from a high computational complexity. Wang et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref> extended the convolutional LSTM with zigzag memory flows, which provides a great modeling capability for short-term video dynamics. Adversarial learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7]</ref> has been increasingly used in video generation or prediction <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>, as it aims to solve the multi-modal training difficulty of the future prediction and helps generate less blurry frames.</p><p>However, the high-order non-stationarity of video dynamics has not been thoroughly considered by the above work,  <ref type="figure">Figure 2</ref>: The ST-LSTM block <ref type="bibr" target="#b31">[32]</ref> in the left plot and the proposed Memory In Memory (MIM) block in the right plot. MIM is designed to introduce two recurrent modules (yellow squares) to replace the forget gate (dashed box) in ST-LSTM. MIM-N is the non-stationary module and MIM-S is the stationary module. Note that the MIM block cannot be used in the first layer so the input Xt is replaced by H l‚àí1 t . whose temporal transition methods are relatively simple, either controlled by the recurrent gate structures or implemented by the recursion of the feed-forward network. By contrast, our model is characterized by exploiting high-order differencing to mitigate the non-stationary learning difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Stochastic Spatiotemporal Prediction</head><p>Some recent methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref> attempted to model the stochastic component of video dynamics using Variational Autoencoder <ref type="bibr" target="#b15">[16]</ref>. These methods increase the prediction diversity but are difficult to evaluate and require to run a great number of times for a satisfactory result. In this paper, we focus on the deterministic part of spatiotemporal non-stationarity. More specifically, this work attempts to stationarize the complicated spatiotemporal processes and make their deterministic components in the future more predictable by proposing new RNNs architecture for non-stationarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>As mentioned above, the spatiotemporal non-stationarity remains under-explored and its differential features have not been fully exploited by previous methods using neural networks. In this section, we first present the Memory In Memory (MIM) blocks for learning about the higher-order non-stationarity from RNNs memory transitions. We then discuss a new RNN architecture, which interlinks multiple MIM blocks with diagonal state connections, for modeling the differential information in the spatiotemporal prediction. By stacking multiple MIM blocks, we could potentially learn higher-order non-stationarity from spatiotemporal dynamics. The proposed MIM state transition approach can be integrated into all LSTM-like units. We choose the Spatiotemporal LSTM (ST-LSTM) <ref type="bibr" target="#b31">[32]</ref> as our base network for a trade-off between prediction accuracy and computation simplicity. ST-LSTM is characterized by a dual-memory structure, C l t and M l t , as shown in <ref type="figure">Figure 2</ref> (left). The corre-sponding zigzag memory flow of M l t , as illustrated by the black arrows in <ref type="figure">Figure 4</ref>, strengthens its short-term modeling capability by increasing the recurrent transition depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Memory In Memory Blocks</head><p>We observe that the complex dynamics in spatiotemporal sequences can be handled more effectively as a combination of stationary variations and non-stationary variations. Suppose we have a video sequence showing a person walking at a constant speed. The velocity can be seen as a stationary variable and the swing of the legs should be considered as a non-stationary process, which is apparently more difficult to predict. Unfortunately, the forget gate in previous LSTM-like models is a simple gating structure that struggles to capture the non-stationary variations in spacetime. In preliminary experiments, we find that the majority of forget gates in the recent PredRNN model <ref type="bibr" target="#b31">[32]</ref> are saturated, implying that the units always remember stationary variations.</p><p>The Memory In Memory (MIM) block is enlightened by the idea of modeling the non-stationary variations using a series of cascaded memory transitions instead of the simple, saturation-prone forget gate in ST-LSTM. As compared in <ref type="figure">Figure 2</ref> (the smaller dashed boxes), two cascaded temporal memory recurrent modules are designed to replace the temporal forget gate f t in ST-LSTM. The first module additionally taking H l‚àí1 t‚àí1 as input is used to capture the non-stationary variations based on the differencing (H l‚àí1 t ‚àí H l‚àí1 t‚àí1 ) between two consecutive hidden representations. So we name it the non-stationary module (shown as MIM-N in <ref type="figure" target="#fig_2">Figure 3</ref>). It generates differential features D l t based on the difference-stationary assumption <ref type="bibr" target="#b21">[22]</ref>. The other recurrent module takes as inputs the output D l t of the MIM-N module and the outer temporal memory C l t‚àí1 to capture the approximately stationary variations in spatiotemporal sequences. So we call it the stationary module (shown as MIM-S in <ref type="figure" target="#fig_2">Figure 3</ref>). By replacing the forget gate with the final output T l t of the cascaded non-stationary and stationary modules (as shown in <ref type="figure">Figure 2</ref>), the non-stationary dynamics can be captured more effectively. Key calculations inside a MIM block can be shown as follows:</p><formula xml:id="formula_0">gt = tanh(Wxg * H l‚àí1 t + W hg * H l t‚àí1 + bg) it = œÉ(Wxi * H l‚àí1 t + W hi * H l t‚àí1 + bi) D l t = MIM-N(H l‚àí1 t , H l‚àí1 t‚àí1 , N l t‚àí1 ) T l t = MIM-S(D l t , C l t‚àí1 , S l t‚àí1 ) C l t = T l t + it gt g t = tanh(W xg * H l‚àí1 t + Wmg * M l‚àí1 t + b g ) i t = œÉ(W xi * H l‚àí1 t + Wmi * M l‚àí1 t + b i ) f t = œÉ(W xf * H l‚àí1 t + W mf * M l‚àí1 t + b f ) M l t = f t M l‚àí1 t + i t g t ot = œÉ(Wxo * H l‚àí1 t + W ho * H l t‚àí1 + Wco * C l t + Wmo * M l t + bo) H l t = ot tanh(W1√ó1 * [C l t , M l t ]),<label>(1)</label></formula><p>where S and N denote the horizontally-transited memory cells in the non-stationary module (MIM-N) and stationary module (MIM-S) respectively; D is the differential features learned by MIM-N and fed into MIM-S; T is the memory passing the virtual "forget gate"; and * denotes convolution. The cascaded structure enables end-to-end modeling of different orders of non-stationary dynamics. It is based on the difference-stationary assumption that differencing a nonstationary process repeatedly will likely lead to a stationary one <ref type="bibr" target="#b21">[22]</ref>. A schematic of MIM-N and MIM-S is presented in <ref type="figure" target="#fig_2">Figure 3</ref>. We present the detailed calculations of MIM-N as follows:</p><formula xml:id="formula_1">gt = tanh(Wxg * (H l‚àí1 t ‚àí H l‚àí1 t‚àí1 ) + Wng * N l t‚àí1 + bg) it = œÉ(Wxi * (H l‚àí1 t ‚àí H l‚àí1 t‚àí1 ) + Wni * N l t‚àí1 + bi) ft = œÉ(W xf * (H l‚àí1 t ‚àí H l‚àí1 t‚àí1 ) + W nf * N l t‚àí1 + b f ) N l t = ft N l t‚àí1 + it gt ot = œÉ(Wxo * (H l‚àí1 t ‚àí H l‚àí1 t‚àí1 ) + Wno * N l t + bo) D l t = MIM-N(H l‚àí1 t , H l‚àí1 t‚àí1 , N l t‚àí1 ) = ot tanh(N l t ),<label>(2)</label></formula><p>where all gates g t , i t , f t and o t are updated by incorporating the frame difference (H l‚àí1 t ‚àí H l‚àí1 t‚àí1 ), which highlights the non-stationary variations in the spatiotemporal sequence. The detailed calculations of MIM-S are shown as follows:</p><formula xml:id="formula_2">gt = tanh(W dg * D l t + Wcg * C l t‚àí1 + bg) it = œÉ(W di * D l t + Wci * C l t‚àí1 + bi) ft = œÉ(W df * D l t + W cf * C l t‚àí1 + b f ) S l t = ft S l t‚àí1 + it gt ot = œÉ(W do * D l t + Wco * C l t‚àí1 + Wso * S l t + bo) T l t = MIM-S(D l t , C l t‚àí1 , S l t‚àí1 ) = ot tanh(S l t ),<label>(3)</label></formula><p>which takes the memory cells C l t‚àí1 and the differential features D l t generated by MIM-N as input. As can be validated, the stationary module provides a gating mechanism to adaptively decide whether to trust the original memory C l t‚àí1 or the differential features D l t . If the differential features vanish, indicating that the non-stationary dynamics is not prominent, then MIM-S will mainly reuse the original memory. Otherwise, if the differential features are prominent, then MIM-S will overwrite the original memory and focus more on the non-stationary dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory In Memory Networks</head><p>Stacking multiple MIM blocks, our model has a chance to capture higher orders of non-stationarity, gradually stationarizes the spatiotemporal process and makes the future sequence more predictable. The key idea of this architecture is to deliver necessary hidden states for generating differential features and best facilitating non-stationarity modeling.</p><p>A schematic of our proposed diagonal recurrent architecture is shown in <ref type="figure">Figure 4</ref>. We deliver the hidden states H l‚àí1 t‚àí1 and H l‚àí1 t to the Memory In Memory (MIM) block at timestamp t = 1 and layer l = 1 to generate the differenced features for further use. These connections are shown as diagonal arrows in <ref type="figure">Figure 4</ref>. As the first layer does not have any previous layer, we simply use the Spatiotemporal LSTM (ST-LSTM) <ref type="bibr" target="#b31">[32]</ref> to generate its hidden presentations. Note that, the temporal differencing is performed by subtracting hidden state H l‚àí1 t from the hidden state H l‚àí1 t‚àí1 in MIM. Compared to differencing neighboring raw images directly, differencing temporally adjacent hidden states can reveal the non-stationarity more evidently, as the spatiotemporal variations in local areas have been encoded into the hidden representations through the bottom ST-LSTM layer.</p><p>Another distinctive feature of the MIM networks resides in the horizontal state transition paths. As the MIM blocks have two cascaded temporal memory modules to capture the non-stationary and stationary dynamics respectively, we further deliver the two temporal memories (denoted by N for the non-stationary memory and by S for the stationary memory) along the blue arrows in <ref type="figure">Figure 4</ref>.</p><p>The MIM networks generate one frame at one timestamp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Calculations of the entire model with one ST-LSTM and</head><formula xml:id="formula_3">Xt-1/Xt-1 Xt/Xt Xt+1/Xt+1 ST-LSTM MIM MIM MIM MIM ST-LSTM ST-LSTM MIM MIM Xt Xt+1 Xt+2 MIM MIM MIM H t-1^ƒ§ t-1 H t-1^ƒ§ t 1 1 H t 2 2 H t 3 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: A MIM network with three MIMs and one ST-LSTM.</head><p>Red arrows: the diagonal state transition paths of H for differential modeling. Blue arrows: the horizontal transition paths of the memory cells C, N and S. Black arrows: the zigzag state transition paths of M. Input: the input can be either the ground truth frame for input sequence, or the generated frame at previous timestamp. Output: one frame is generated at each timestamp.</p><p>(L ‚àí 1) MIMs can be presented as follows (for 2 ‚â§ l ‚â§ L). Note that there is no MIM block that is marked as MIM 1 .</p><formula xml:id="formula_4">H 1 t , C 1 t , M 1 t = ST-LSTM1(Xt, H 1 t‚àí1 , C 1 t‚àí1 , M L t‚àí1 ) H l t , C l t , M l t , N l t , S l t = MIM l (H l‚àí1 t , H l t‚àí1 , C l t‚àí1 , M l‚àí1 t , N l t‚àí1 , S l t‚àí1 ).<label>(4)</label></formula><p>We formulate high-order non-stationarity as high-order polynomials based on Cram√©r's Decomposition <ref type="bibr" target="#b3">[4]</ref>. In the space-time contexts, it refers to the varying trends of statistics of pixel values. The order of non-stationary polynomials can be reduced by a couple of differencing operations. We blend this idea from time-series analysis with deep learning: stacking MIMs with differential inputs layer by layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the proposed MIM model using four datasets for spatiotemporal prediction: a synthetic dataset with moving digits, a real traffic flow dataset, a real radar echo dataset, and a human action dataset. Here are some common settings all over these datasets. Our model has four layers in all experiments, including one ST-LSTM layer as the first layer and three MIMs. The number of feature channels in each MIM block is 64, as a trade-off of prediction accuracy and memory efficiency. All models are trained with the 2 loss, using the ADAM optimizer <ref type="bibr" target="#b14">[15]</ref> with a learning rate of 0.001. The mini-batch size is set to 8. We apply the layer normalization <ref type="bibr" target="#b0">[1]</ref> to the compared models in order to reduce the covariate shift problem <ref type="bibr" target="#b11">[12]</ref>. Besides, we apply the scheduled sampling <ref type="bibr" target="#b1">[2]</ref> to all models to stitch the discrepancy between training and inference. Code and models are available at https://github.com/Yunbo426/MIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Moving MNIST</head><p>The standard Moving MNIST is a synthetic dataset with grayscale image sequences of flying digits. We follow exactly the experimental settings in PredRNN <ref type="bibr" target="#b31">[32]</ref>.</p><p>We visualize a sequence of predicted frames on the standard Moving MNIST test set in <ref type="figure" target="#fig_3">Figure 5</ref>. This example is challenging, as severe occlusions exist near the junction of the input sequence and the output sequence. The occlusions can be viewed as information bottleneck, in which the mean and variance of the spatiotemporal process meet drastic changes, indicating the presence of a high-order nonstationarity. The generated images of MIM are more satisfactory, less blurry than those of other models. Actually, we cannot even tell the digits in the last frames generated by other models. We may conclude that MIM shows more capability in capturing complicated non-stationary variations.</p><p>We use the per-frame structural similarity index measure (SSIM) <ref type="bibr" target="#b32">[33]</ref>, the mean square error (MSE) and the mean absolute error (MAE) to evaluate our models. A lower MSE or MAE, or a higher SSIM indicates a better prediction. As shown in <ref type="table">Table 1</ref>  <ref type="bibr" target="#b30">[31]</ref> as the first layer, and integrating the cascaded MIM-N and MIM-S modules into the Causal LSTM memory cells. This result shows that MIM is a generic mechanism for improving recurrent memory transitions.</p><p>dataset. In particular, we construct another model named MIM* by using Causal LSTM <ref type="bibr" target="#b30">[31]</ref> as the first layer, and integrating the cascaded MIM-N and MIM-S modules into the Causal LSTM memory cells, using them to replace the temporal forget gates in Causal LSTMs. This result shows that the memory in memory mechanism is not specifically designed for the ST-LSTM; instead, it is a generic mechanism for improving RNNs memory transitions. Though in other parts of this paper, we use ST-LSTM as our base structure for a trade-off between prediction accuracy and computational complexity, we can see that MIM performs better than its ST-LSTM (PredRNN) baseline, while MIM* also performs better than its Causal LSTM baseline. We also adopt the gradient-based sharpness metric from <ref type="bibr" target="#b18">[19]</ref> to measure the sharpness of the generated images. As shown in <ref type="table" target="#tab_1">Table 2</ref>, MIM rises the sharpness score by 16% over PredRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Gradient-Based Sharpness FRNN <ref type="bibr" target="#b19">[20]</ref> 24.99 PredRNN <ref type="bibr" target="#b31">[32]</ref> 23.29 MIM 27.05 We further testify the necessity of cascading inner recurrent modules by respectively removing the stationary modules or non-stationary modules. As illustrated in <ref type="table" target="#tab_2">Table  3</ref>, the MIM network without MIM-N works slightly better than that without MIM-S. Also  We study the sensitivity of our model to the number of MIM blocks. As stacking 2-4 recurrent layers is a common practice, we evaluate models with  The forget gates are easily saturated in PredRNN as well as other LSTM-based spatiotemporal prediction models such as ConvLSTM <ref type="bibr" target="#b23">[24]</ref>. As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, a great portion of forget gates of these models are close to 0, indicating that the long short-term memories do not work properly. As the non-stationary signals within short-term variations are hard to be captured, these models have to refresh the memory states to convey these short-term hidden representations. Our model mitigates this problem and makes better use of long-term variations through the proposed MIM block. As MIM-N mainly reduces the non-stationarity, short-term tendencies become easier to be captured and more neurons in MIM-S can be used for handling long-term variations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">TaxiBJ Traffic Flow</head><p>Traffic flows are collected from the chaotic real-world environment. They will not vary uniformly over time, and there are strong temporal dependencies between the traffic conditions at neighboring timestamps. Each frame in TaxiBJ is a 32 √ó 32 √ó 2 grid of image. Two channels represent the traffic flow entering and leaving the same district. We normalize the data to [0, 1] and follow the experimental settings of ST-ResNet <ref type="bibr" target="#b36">[37]</ref>, which yields the previous state-of-the-art results on this dataset. Each sequence contains 8 consecutive frames, 4 for the inputs and 4 for the predictions. We show the quantitative results in <ref type="table" target="#tab_4">Table 5</ref> and the qualitative results in <ref type="figure">Figure 7</ref>. To make the comparisons conspicuous, we also visualize the difference between the predictions and the ground truth images. Obviously, MIM shows the best performance in all predicted frames among all compared models, with the lowest difference intensities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Frame 1 Frame 2 Frame 3 Frame 4 ST-ResNet <ref type="bibr" target="#b36">[37]</ref> 0.460 0.571 0.670 0.762 VPN <ref type="bibr" target="#b13">[14]</ref> 0.427 0.548 0.645 0.721 FRNN <ref type="bibr" target="#b19">[20]</ref> 0.331 0.416 0.518 0.619 PredRNN <ref type="bibr" target="#b31">[32]</ref> 0.318 0.427 0.516 0.595 Causal LSTM <ref type="bibr" target="#b30">[31]</ref> 0.319 0.399 0.500 0.573 MIM 0.309 0.390 0.475 0.542  <ref type="figure">Figure 7</ref>: Prediction examples on TaxiBJ dataset. For ease of comparison, we also visualize the difference between the ground truth frames (GT) and predicted frames (P).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Radar Echo</head><p>The radar echo dataset contains evolving radar maps that were collected every 6 minutes, from May 1st, 2014 to June 30th, 2014. Each frame is a 64 √ó 64 √ó 1 grid of image, covering 64 √ó 64 square kilometers. We predict 10 timestamps into the future at a time interval of 6 minutes, covering the next hour. We visualize the generated radar maps in <ref type="figure" target="#fig_7">Figure  8</ref>. We can see that the evolution of radar echoes is a highly non-stationary process. The accumulation, deformation, and dissipation of the radar echoes are happening at every moment. In this showcase, the echoes in the bottom left corner aggregate while those in the upper right corner dissipate. Only MIM captures the movement of the echoes correctly.   <ref type="table">Table 6</ref>: A comparison for predicting 10 frames on the subsets of the radar dataset. All of the models are also trained with 10 target frames and made to predict 10 future frames at test time.</p><p>We evaluate the generated radar echoes by MSE in <ref type="table">Table  6</ref>, and then convert pixel values to radar echo intensities in dBZ. We respectively choose 30 dBZ, 40 dBZ and 50 dBZ as thresholds to calculate the Critical Success Index (CSI). CSI is defined as CSI = hits hits+misses+falsealarms , where hits corresponds to true positive, misses corresponds to false positive, and false alarms corresponds to false negative. A higher CSI denotes a better prediction result. MIM consistently outperforms other models in both MSE and CSI. <ref type="figure" target="#fig_8">Figure 9</ref> shows the frame-wise MSE/CSI. CSI-40 and CSI-50 indicate the probabilities of severe weather conditions. MIM performs best though predicting severe weather is non-trivial due to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Human3.6M</head><p>The Human3.6M dataset <ref type="bibr" target="#b12">[13]</ref> contains human actions of 17 scenarios, including 3.6 million poses and corresponding images. We train the models using only the "Walking" scenario. The RGB images in Human3.6M dataset are originally 1000 √ó 1000 √ó 3, and resized to 128 √ó 128 √ó 3 in our experiments. We generate 4 future frames given the previous 4 RGB frames. We use subjects S1, S5, S6, S7, S8 for training, and S9, S11 for testing.</p><p>As shown in <ref type="table" target="#tab_7">Table 7</ref> and <ref type="figure">Figure 10</ref>, the MIM network outperforms the previous state-of-the-art models in both numerical metrics and visual effects. The generated frames by MIM are more accurate in motion positions, as indicated by the orange boxes (versus the green boxes). We notice that some work performs well on human-body datasets by modeling the structures of human joints, while the MIM network is designed for non-structural, general-purpose spacetime data such as RGB videos. The structural and non-structural methods can be integrated and jointly used in a complementary manner, which is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>SSIM MSE MAE FRNN <ref type="bibr" target="#b19">[20]</ref> 0.771 497.   <ref type="figure">Figure 10</ref>: Examples of the generated images on the Human3.6M dataset. We zoom in to show the details of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We investigate the underlying non-stationarity that forms one of the main obstacles in spatiotemporal prediction. Existing LSTM-based models for spatiotemporal prediction are powerful in modeling difference-stationary sequences, whose capability for modeling high-order non-stationary process is limited by their relatively weak forget gates. This paper proposes a new recurrent neural network to enable nonstationary modeling in the spacetime contexts. We formulate high-order non-stationarity as high-order polynomials with respect to the statistically varying trends of pixel intensities. The order of non-stationary polynomials can be reduced by a couple of differencing operations. We leverage this idea from time-series analysis: stacking MIMs with differential inputs layer by layer. The Memory In Memory (MIM) block is derived to model the complicated variations, which uses two cascaded recurrent modules to handle the non-stationary and approximately stationary components in the spatiotemporal dynamics. MIM achieves the state-of-the-art prediction performance on four datasets: a synthetic dataset of flying digits, a traffic flow prediction dataset, a weather forecasting dataset, and a human pose video dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The non-stationary module (MIM-N) and the stationary module (MIM-S), which are interlinked in a cascaded structure in the MIM block. Non-stationarity is modeled by differencing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Prediction examples on the standard Moving MNIST. All models predict 10 frames into the future by observing 10 previous frames. The output frames are shown at two frames intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, either of them has significant improvements over the PredRNN model in MSE/MAE, showing the necessity of cascading them in a unified network. When MIM-N and MIM-S are interlinked, the entire MIM model achieves the best performance. Model SSIM MSE MAE MIM (without MIM-N) 0.858 54.4 124.8 MIM (without MIM-S) 0.853 55.7 125.5 MIM 0.874 52.0 116.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 / 3 / 4</head><label>234</label><figDesc>MIMs on Moving MNIST. The proposed model with 3 MIMs performs best. It is a trade-off: applying too few MIMs leads to inadequate non-stationary modeling capability while the excessively deep recurrent model leads to training difficulty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>The saturated rate of forget gates that corresponds to |Tt/Ct‚àí1| &lt; 0.1 for MIM, and ft &lt; 0.1 for other models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Examples of the next-hour predictions of radar echoes, where higher pixel values indicate higher precipitation probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Frame-wise comparisons of the next 10 generated radar maps. Lower MSE curves or higher CSI curves indicate better forecasting results. The MIM network is the most high-performing method over all timestamps in the forecasting horizon.the long tail distributions of the pixel values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, our proposed MIM model approaches the state-of-the-art results on the standard Moving MNISTTable 1: A comparison for predicting 10 frames on Moving MNIST dataset. All models have comparable numbers of parameters. MIM* is network using Causal LSTM</figDesc><table><row><cell>Model</cell><cell>SSIM MSE MAE</cell></row><row><cell>FC-LSTM [26]</cell><cell>0.690 118.3 209.4</cell></row><row><cell>ConvLSTM [24]</cell><cell>0.707 103.3 182.9</cell></row><row><cell>TrajGRU [25]</cell><cell>0.713 106.9 190.1</cell></row><row><cell>CDNA [9]</cell><cell>0.721 97.4 175.3</cell></row><row><cell>DFN [5]</cell><cell>0.726 89.0 172.8</cell></row><row><cell>FRNN [20]</cell><cell>0.813 69.7 150.3</cell></row><row><cell cols="2">VPN baseline [14] 0.870 64.1 131.0</cell></row><row><cell>PredRNN [32]</cell><cell>0.867 56.8 126.1</cell></row><row><cell cols="2">Causal LSTM [31] 0.898 46.5 106.8</cell></row><row><cell>MIM</cell><cell>0.874 52.0 116.5</cell></row><row><cell>MIM*</cell><cell>0.910 44.2 101.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Sharpness evaluation on Moving MNIST.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study with respect to the MIM block.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The sensitivity of our model to the number of MIM blocks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Per-frame MSE calculated with data in the range of [0, 1] on the TaxiBJ dataset. All compared models take 4 historical traffic flow images as inputs, and predict the next 4 images (traffic flows for the next two hours).</figDesc><table><row><cell cols="3">Input sequence</cell><cell></cell><cell cols="4">Ground truth and predictions</cell></row><row><cell>t=1</cell><cell>t=2</cell><cell>t=3</cell><cell>t=4</cell><cell>t=5</cell><cell>t=6</cell><cell>t=7</cell><cell>t=8</cell></row><row><cell></cell><cell></cell><cell cols="2">Predicted</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Frames</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">|GT-P|</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Predicted</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Frames</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PredRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">|GT-P|</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Predicted</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Frames</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Causal LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">|GT-P|</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Predicted</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MIM</cell><cell></cell><cell cols="2">Frames</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">|GT-P|</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Quantitative results on the Human3.6M dataset.</figDesc><table><row><cell>Ground Truth</cell><cell>t=5</cell><cell>t=6</cell><cell>t=7</cell><cell>t=8</cell></row><row><cell>FRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>t=5</cell><cell>t=6</cell><cell>t=7</cell><cell>t=8</cell></row><row><cell>PredRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>t=5</cell><cell>t=6</cell><cell>t=7</cell><cell>t=8</cell></row><row><cell>MIM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>t=5</cell><cell>t=6</cell><cell>t=7</cell><cell>t=8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by National Natural Science Foundation of China (No. 61772299, 71690231, and 61672313).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Time series analysis: forecasting and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gwilym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greta</forename><forename type="middle">M</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ljung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On some classes of nonstationary stochastic processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Cram√©r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Berkeley Symposium on Mathematical Statistics and Probability</title>
		<imprint>
			<date type="published" when="1961" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="57" to="78" />
		</imprint>
		<respStmt>
			<orgName>University of Los Angeles Press Berkeley and Los Angeles</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1174" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emily L Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pougetabadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wardefarley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Folded recurrent neural networks for future video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Selva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatio-temporal video autoencoder with differentiable memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Viorica Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">T</forename><surname>Percival</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walden</surname></persName>
		</author>
		<title level="m">Spectral Analysis for Physical Applications</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep learning for precipitation nowcasting: A benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PredRNN++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5123" to="5132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">600</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevan</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Predcnn: Predictive learning with cascade convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziru</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moe</forename><surname>Kliss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Katherine Bouman, and Bill Freeman. Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep spatiotemporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
