<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Language Technologies Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Supervised speech enhancement (SE) using deep neural networks (DNNs) has received tremendous attention in recent years. The availability of abundant amounts of training data and advancements in DNN architectures have resulted in systems that provide better performance than the ideal binary mask <ref type="bibr" target="#b0">[1]</ref> -a target that highly correlates with speech intelligibility <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. The core design of a DNN-based SE system involves decisions that perform compensation in one feature domain and calculate the loss function in another. Despite the emerging time-domain compensation methods (e.g., <ref type="bibr" target="#b0">[1]</ref>), predicting a time-varying gain function, or a time-frequency (TF) mask <ref type="bibr" target="#b3">[4]</ref>, has been the most popular and reliable approach.</p><p>Loss functions for supervised SE in the TF domain have historically been calculated in the time or frequency domain. However, most existing loss functions <ref type="bibr" target="#b4">[5]</ref> were motivated by statistically-optimal solutions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> and do not necessarily correlate with perceptual quality or intelligibility of speech <ref type="bibr" target="#b8">[9]</ref>. More recently, perceptually-motivated loss functions have been proposed to optimize modified predictors of speech quality <ref type="bibr" target="#b9">[10]</ref> and intelligibility <ref type="bibr" target="#b10">[11]</ref>. Interestingly, these methods did not show improvement over objective metrics for which the loss functions did not directly optimize, suggesting that there is room for improving their generalization ability.</p><p>Modulation is closely related to speech intelligibility. The speech transmission index (STI) measures the extent to which amplitude modulation of speech is preserved in degraded environments and is highly correlated with speech intelligibility <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref>. The spectro-temporal modulation index (STMI) was subsequently proposed to account for joint spectro-temporal modulation <ref type="bibr" target="#b14">[14]</ref>. SE in the modulation domain has also been explored <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. However, these methods assume that speech and noise are separable in the modulation domain. Moreover, they typically require a complete set of spectrotemporal receptive fields (STRFs) in order to invert the processed modulation spectra back to the TF domain. This may be computationally infeasible for real-time applications.</p><p>In this paper, we propose a simple mean-squared error (MSE)-based loss function in the spectro-temporal modulation domain for supervised SE. We call the loss spectrotemporal modulation error (STME) because of its close relation to template-based STMI <ref type="bibr" target="#b14">[14]</ref>, which correlates well with speech intelligibility. The calculation of the STME is based on a set of pre-selected modulation kernels, which had been shown to be critical for the accuracy of predicted speech intelligibility using speech stimuli <ref type="bibr" target="#b13">[13]</ref>. Following our recent success in discriminating live speech from synthetic or broadcast speech using learnable spectro-temporal receptive field (STRF) kernels <ref type="bibr" target="#b17">[17]</ref>, we develop an automatic way to determine these kernels through an auxiliary speaker identification (SID) task. STME is applicable to any deep neural network (DNN) system as long as the short-time Fourier transform magnitude (STFTM) of the target and degraded speech are accessible when training the DNN. Our proposed system's loss is easy to compute, does not incur additional computation during inference, and avoids lossy inversion, which is a problem with conventional modulation-domain SE approaches <ref type="bibr" target="#b15">[15]</ref>.</p><p>Organization of this paper. In the next section, we introduce related work in supervised SE and the background of STMI. We then describe our selection procedure for the STRFs and the loss function. Finally, we describe the evalua-tion procedure and discuss the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>In this section we briefly review supervised speech enhancement and the spectro-temporal modulation index.</p><p>Supervised DNN-based speech enhancement. We assume that the observed noisy speech contains clean speech corrupted by additive noise. This relationship in the shorttime Fourier transform (STFT) domain is described by</p><formula xml:id="formula_0">X[t, k] = S[t, k] + N [t, k]<label>(1)</label></formula><p>where X[t, k], S[t, k], and N [t, k] represent the STFT at time frame t and frequency bin k of the observed noisy speech, clean speech and noise, respectively. Without loss of generality, we assume that a DNN is trained to predict a magnitude gain, G[t, k], from past and current information of degraded speech. The enhanced STFTM is obtained by element-wise multiplication of the predicted gain by the noisy STFTM,</p><formula xml:id="formula_1">S[t, k] = G[t, k] |X[t, k]| .<label>(2)</label></formula><p>A popular loss function that drives the learning for this DNN is the MSE between the enhanced STFTM and the clean STFTM, or the time-frequency error (TFE),</p><formula xml:id="formula_2">TFE = ||( S − S)|| 2<label>(3)</label></formula><p>where S and S denote the vector representations of the clean STFTM and enhanced STFTM, respectively. Spectro-temporal modulation index. The spectrotemporal modulation index (STMI) is a measure of speech integrity in the modulation domain as viewed by a model of the auditory system <ref type="bibr" target="#b14">[14]</ref>. At the core of this model is a bank of STRFs that are believed to exist in the central auditory system and respond to a range of patterns of temporal and spectral modulation <ref type="bibr" target="#b18">[18]</ref>. Each STRF is a TF template in which two seed functions control the temporal modulation (rate) and spectral modulation (scale) selectivity, respectively. The spectro-temporal modulation response (STMR) of a spectrographic representation of speech to a STRF is defined by the convolution of the two,</p><formula xml:id="formula_3">STMR S [t, k] = STRF[t, k] * f (|S[t, k]| 2 )<label>(4)</label></formula><p>where f is a frequency-integration function followed by a logarithmic compression that mimics the function of the early auditory system. Finally, the template-based STMI <ref type="bibr" target="#b14">[14]</ref> is defined as</p><formula xml:id="formula_4">STMI T = 1 − || − −−− → STMR S − − −−− → STMR X || 2 || − −−− → STMR S || 2 ,<label>(5)</label></formula><p>where STMR[t, k] is typically integrated over time before being converted to the vector form. In previous implementations of modulation-domain SE, compensation was performed directly on STMR X [t, k] <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. In our method, we perform enhancement in the TF domain and use a loss function in the modulation domain that is closely related to STMI T for training the DNN.</p><p>It should be noted that the selection of meaningful modulation frequency becomes an issue when speech signals (instead of modulated noise) are used to calculate an estimate of speech intelligibility <ref type="bibr" target="#b13">[13]</ref>. Previous work using STRFs typically performed dimensionality reduction on features extracted from densely-sampled STRFs <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21]</ref>. We learn the parameters of the STRFs through an auxiliary SID task. We describe our own method next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>In this section, we present the DNN system we used for speech enhancement and the calculation of our spectraltemporal modulation error loss.</p><p>Speech enhancement system. We used the normalized log power spectra (LPS) as the input feature. The STFT is first obtained using a 20-millisecond Hamming window with 50% overlap and a 512-point discrete Fourier transform. Then we take the natural logarithm of the power of the STFT and normalize the LPS with frequency-dependent online normalization following <ref type="bibr" target="#b22">[22]</ref>.</p><p>To estimate the magnitude gain for each frame, we used a similar real-time network architecture to the one described in <ref type="bibr" target="#b4">[5]</ref>. The network consists of a single fully connected (FC) layer followed by two stacked unidirectional Gated Recurrent Units (GRUs) and three more FC layers. Rectified linear unit (ReLU) activation is used after each of the FC layers except the very last one where a sigmoid activation is used to bound the output magnitude gain to be between zero and one. We obtain the enhanced waveform by multiplying the magnitude gain element-wise with the noisy STFTM and using the original noisy phase for reconstruction. In total, the network contained roughly 2.8 million learnable parameters.</p><p>Tuning learnable STRFs on speaker identification. One central problem involving the construction of the loss function is the selection of modulation parameters that are relevant to speech intelligibility <ref type="bibr" target="#b13">[13]</ref>. Previous work has shown that the STMR is redundant <ref type="bibr" target="#b19">[19]</ref> and the possible values for those modulation parameters span a wide range <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. Following the success of our previous work on discriminating live speech from synthetic speech using learnable STRFs <ref type="bibr" target="#b17">[17]</ref>, we trained the STRFNet system on SID using the Librispeech <ref type="bibr" target="#b23">[23]</ref> dataset with artificially-added noise from Sound Bible 1 to learn the parameters of each Gaborbased STRF <ref type="bibr" target="#b20">[20]</ref> automatically. The SID system was able to achieve an average of 95% accuracy with 2484 speakers and signal-to-noise ratios (SNRs) ranging from 0 to 30 dB. We then keep the learned STRFs fixed and utilize them for our loss. We use 60 STRF kernels, each with a time support of 300 milliseconds and a span of 20 channels on the Mel scale. This pipeline is depicted in the upper panel of <ref type="figure" target="#fig_0">Figure 1</ref>. Loss function. Given N STRFs, we define the response of the speech STFTM to the i th STRF to be</p><formula xml:id="formula_5">STMR (i) S [t, k] = STRF (i) [t, k] log m(|S[t, k]| 2 ) , (6)</formula><p>where m is a frequency integration function using the Mel weighting and denotes cross-correlation. The loss function is then</p><formula xml:id="formula_6">STME = N i=1 || − −−− → STMR (i) S − − −−− → STMR (i) S || 2 N i=1 || − −−− → STMR (i) S || 2 ,<label>(7)</label></formula><p>where no time integration is applied to obtain the vector form of STMR. In general, the STME is strongly motivated by STMI and is in fact very similar to the weighted distance term in the definition of STMI T with a few differences. First, the STME uses learned STRF kernels that are discriminatively trained to optimize for a SID task. Second, the STMR is calculated using cross-correlation instead of convolution. Third, the original auditory spectrogram <ref type="bibr" target="#b18">[18]</ref> is approximated by the Mel-spectrogram and logarithmic compression. Finally, the STME calculates a weighted MSE using instantaneous response.</p><p>To train our enhancement system, we use the STME loss in addition to the standard TFE in Eq. (3). This is motivated by the observation that the STMR is smooth and omits spectral details that are available in the TF domain. We believe that the medium-time STME loss complements the short-time TFE for supervised SE.</p><p>A block diagram of the STME calculation is shown in the bottom panel of <ref type="figure" target="#fig_0">Figure 1</ref>. In the next section, we will describe the experimental setup used to evaluate the benefits of including the STME in the training loss. We also assess the value of using automatically-learned Gabor-based STRF kernels compared to randomly-selected kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SETUP</head><p>Datasets. We used a small-scale and a large-scale dataset for evaluating the SE system. The small-scale dataset by Valen-tini et al. <ref type="bibr" target="#b24">[24]</ref> (VBD henceforth) contains 9.4 hours and 35 minutes of noisy speech in the training and test set, respectively. We downsampled the entire dataset to 16 kHz. For the large-scale dataset, we used the Interspeech 2020 Deep Noise Suppression (DNS) dataset <ref type="bibr" target="#b25">[25]</ref> with RIR responses provided by <ref type="bibr" target="#b26">[26]</ref>. The DNS training set contains a total of 500 hours of noisy speech. For evaluation, the DNS dataset has two test sets named no reverb and with reverb, which both contain 25 minutes of noisy speech. In both datasets, we are also provided with the original clean speech that was used to artificially generate the noisy speech. We evaluated our system's capabilities to improve speaker verification performance in noisy conditions by using a modified version of the VoxCeleb1 test set <ref type="bibr" target="#b27">[27]</ref>. The original test set contained 4874 speech pairs spoken by 40 unseen speakers. We modified the VoxCeleb1 test set by randomly adding noise from the DNS test set at SNRs ranging from -6 to 6 dB.</p><p>Training and evaluation procedure. To train our SE systems on the VBD dataset and DNS dataset, we randomly sampled 1-second noisy speech segments from the training data and 5-second noisy speech segments from the training data, respectively. All the SE systems were trained using the Adam optimizer with a learning rate of 5e −4 and a batch size of 64 in PyTorch. For evaluation, we used the perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b28">[28]</ref>, scale-invariant signalto-distortion ratio <ref type="bibr" target="#b29">[29]</ref>, and short-time objective intelligibility (STOI) <ref type="bibr" target="#b30">[30]</ref> metrics.</p><p>To evaluate our SE systems on speaker verification, we used a DNN-based speaker verification system <ref type="bibr" target="#b31">[31]</ref> pretrained on VoxCeleb2 <ref type="bibr" target="#b32">[32]</ref>, a dataset that contains over 1 million utterances and 6112 speakers. The verification system obtained an equal error rate (EER) of 2.2% on the VoxCeleb1 test set, which is one of the lowest reported EER compared to any other method with a similar number of parameters <ref type="bibr" target="#b31">[31]</ref>. Although the system was not trained with artificial degradation, all the audio clips were extracted from YouTube which will naturally contain different acoustical conditions. To test if our SE system improves the performance of this strong speaker verification system in noisy conditions, we added noise from the DNS test set to the original VoxCeleb1 test set at SNRs ranging from -6 to 6 dB. We evaluated the EER of the speaker verification system on clips enhanced by the SE system.</p><p>Baseline systems. We evaluated three different baseline systems to illustrate the benefits of the additional STME loss. Each of the baseline systems have the exact same network architecture, but they were each trained with different loss functions. Our first baseline system, GRU(TFE), was trained only with the TFE loss. To evaluate the benefits of the STME loss by itself, we trained a second baseline system, GRU(STME), using only the STME loss. Our third baseline system, GRU(TFE+STME R ), was trained with both loss terms, although the parameters of the Gabor-based STRF kernels that were used to calculate the STME were randomly selected. Specifically, the temporal and spectral modulation fre- quencies are uniformly sampled over [0, 50) Hz and [0, 0.5) cycles per channel, respectively. This comparison allows us to assess the benefits of using automatically-learned Gaborbased STRF kernels to train the system, GRU(TFE+STME).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS AND DISCUSSION</head><p>In this section, we present and discuss results of our STME loss on speech enhancement and speaker verification. VBD results. In <ref type="table">Table 1</ref>, we show the objective evaluation of each SE system trained with different losses using the VBD dataset. On a small dataset, our GRU(TFE+STME) outperformed the GRU(TFE) baseline in all the objective metrics. Interestingly, different initialization of the STRF kernels in GRU(TFE+STME R ) resulted in similar improvements over the baseline TFE loss, but roughly 20% of the time the random parameter selection resulted in a much worse performance. This highlights the benefits of using automaticallylearned Gabor-based STRF kernels over randomly-selected Gabor-based STRF kernels.</p><p>DNS results. The objective evaluation of our SE systems trained with different losses using the DNS no reverb and with reverb test set is summarized in <ref type="table">Table 2</ref>. Even with a large amount of training data, our GRU(TFE+STME) loss function outperformed both the GRU(TFE) baseline and the provided challenge baseline <ref type="bibr" target="#b22">[22]</ref> in all the objective metrics. Most notably, there is a significant improvement in PESQ which results in our system having a similar PESQ to the top system in the official DNS challenge <ref type="bibr" target="#b34">[34]</ref>. We also evaluated the benefits of the STME loss by itself, GRU(STME). Curiously, training with only our the STME loss provides </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EER (%) VoxCeleb1 Speaker Verification Results</head><p>Noisy GRU(TFE) GRU(TFE+STME) <ref type="figure">Fig. 2</ref>. Equal error rates on the VoxCeleb1 Test Set.</p><p>a higher PESQ but much lower SI-SDR compared to training with only the TFE loss. Nevertheless, optimizing the combination of both losses during training caused both the PESQ and SI-SDR scores to increase compared to training on each loss individually. This confirms our belief that the medium-time STME loss is complemented by the shorttime TFE loss. As in the VBD experiments, the use of automatically-learned Gabor-based STRF kernels provides a greater increase of PESQ scores compared to the use of randomly-selected Gabor-based STRF kernels. Speaker verification results. The performance of the speaker verification system with noise added at low SNRs is shown in <ref type="figure">Figure 2</ref>. At low SNRs, the speaker verification system's performance starts to substantially degrade. Our system GRU(TFE+STME) improved the EER by an average of 15.4% relative and outperformed our baseline GRU(TFE) by an average of 5.5% relative. At higher SNRs, the verification system's performance quickly approached the state-of-the-art performance of 2.2% and our enhancement systems did not provide any additional benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we introduced a novel modulation-domain loss for training neural-network-based speech enhancement systems. We showed that by adding spectro-temporal modulation error to the standard time-frequency error during training, all three common objective speech quality metrics substantially improved on two different datasets. Additionally, we demonstrated the value of utilizing automatically-learned Gabor-based STRF kernels over randomly-selected kernels. We also showed that our speech enhancement system can improve a strong speaker verification system at low SNRs. In the future, we plan on exploring deep-learning-based techniques to perform SE directly in the modulation domain. We will also explore ways of directly optimizing the STRF parameters for speech enhancement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Flow diagrams of the STRF tuning stage (top) and the STME loss calculation stage (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Table summarizing the objective speech quality evaluation on the VBD test set Table summarizing the objective speech quality evaluation on the DNS no reverb (with reverb) test set</figDesc><table><row><cell>Methods</cell><cell></cell><cell>PESQ</cell><cell cols="2">SI-SDR STOI</cell></row><row><cell></cell><cell></cell><cell cols="2">(MOS) (dB)</cell><cell>(%)</cell></row><row><cell>Noisy</cell><cell></cell><cell>1.97</cell><cell>8.5</cell><cell>92.1</cell></row><row><cell>ERNN [33]</cell><cell></cell><cell>2.54</cell><cell>-</cell><cell>-</cell></row><row><cell>GRU(TFE)</cell><cell></cell><cell>2.68</cell><cell>17.0</cell><cell>93.3</cell></row><row><cell>GRU(STME)</cell><cell></cell><cell>2.78</cell><cell>14.4</cell><cell>93.1</cell></row><row><cell cols="3">GRU(TFE+STME R ) 2.76</cell><cell>16.9</cell><cell>93.2</cell></row><row><cell cols="2">GRU(TFE+STME)</cell><cell>2.82</cell><cell>17.0</cell><cell>93.8</cell></row><row><cell>Method</cell><cell cols="2">PESQ</cell><cell>SI-SDR</cell><cell>STOI</cell></row><row><cell></cell><cell cols="2">(MOS)</cell><cell>(dB)</cell><cell>(%)</cell></row><row><cell>Noisy</cell><cell cols="3">1.58 (1.82) 9.1 (9.0)</cell><cell>91.5 (86.6)</cell></row><row><cell>DNS Baseline [22]</cell><cell cols="3">1.83 (1.52) 12.5 (9.2)</cell><cell>90.6 (82.1)</cell></row><row><cell>GRU(TFE)</cell><cell cols="4">2.27 (2.36) 14.9 (13.2) 94.2 (89.4)</cell></row><row><cell>GRU(STME)</cell><cell cols="4">2.59 (2.64) 12.4 (12.0) 94.2 (90.1)</cell></row><row><cell cols="5">GRU(TFE+STME R ) 2.57 (2.63) 15.9 (14.5) 95.2 (90.6)</cell></row><row><cell>GRU(TFE+STME)</cell><cell cols="4">2.71 (2.75) 15.9 (14.5) 95.5 (91.2)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.openslr.org/17/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal timefrequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019-08" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reasons why current speech-enhancement algorithms do not improve speech intelligibility and suggested solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Validation of the articulation index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Kryter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1698" to="1702" />
			<date type="published" when="1962-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A consolidated view of loss functions for supervised deep learning-based speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech enhancement using a minimummean square error short-time spectral amplitude estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ephraim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1109" to="1121" />
			<date type="published" when="1984-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speech enhancement using a minimum mean-square error log-spectral amplitude estimator</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="445" />
			<date type="published" when="1985-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhancement and bandwidth compression of noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1586" to="1604" />
			<date type="published" when="1979-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech Enhancement : Theory and Practice, Second Edition</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deep learning loss function based on the perceptual evaluation of the speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Martin-Donas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Peinado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1680" to="1684" />
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptually guided speech enhancement using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Calgary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-04" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="5074" to="5078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A physical method for measuring speech-transmission quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Steeneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Houtgast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="318" to="326" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A method to determine the speech transmission index from speech waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Payton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Braida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3637" to="3648" />
			<date type="published" when="1999-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A spectro-temporal modulation index (STMI) for assessment of speech intelligibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhilali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="331" to="348" />
			<date type="published" when="2003-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Denoising in the domain of spectrotemporal modulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Audio, Speech, and Music Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlinear filtering of spectrotemporal modulations in speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirbagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-03" />
			<biblScope unit="page" from="5478" to="5481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learnable spectro-temporal receptive fields for robust voice type discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2020. ISCA</title>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="1957" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiresolution spectrotemporal analysis of complex sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="887" to="906" />
			<date type="published" when="2005-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discrimination of speech from nonspeech based on multiscale spectro-temporal modulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="920" to="930" />
			<date type="published" when="2006-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robustness of spectro-temporal features against intrinsic and extrinsic variations in automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kollmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="753" to="767" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Easy does it: Robust spectro-temporal many-stream ASR without fine tuning streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-03" />
			<biblScope unit="page" from="4309" to="4312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weighted speech distortion losses for neural-network-based real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="871" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-04" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Investigating RNN-based speech enhancement methods for noise-robust text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW</publisher>
			<biblScope unit="page" from="146" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective speech quality and testing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08662</idno>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">ICASSP 2021 deep noise suppression challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">VoxCeleb: A large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<biblScope unit="page" from="2616" to="2620" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SDR-halfbaked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A short-time objective intelligibility measure for time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4214" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">In defence of metric learning for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11982</idno>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">VoxCeleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2018. ISCA</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="1086" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Realtime speech enhancement using equilibriated RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yatabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="851" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Poconet: Better speech enhancement with frequencypositional embeddings, semi-supervised conversational data, and biased loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Helwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04470</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
