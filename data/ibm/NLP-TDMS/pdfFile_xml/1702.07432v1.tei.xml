<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Context Attention for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk2macheng13@mails.tsinghua.edu.cn3alan.yuille@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Context Attention for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on the detailed description for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semanticconsistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive field of the network. These units are extensions of residual units with a side branch incorporating filters with larger receptive fields, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation is a challenging task in computer vision due to the articulation of body limbs, self occlusion, various clothing, and foreshortening. Significant improvements have been achieved by Convolutional Neural Networks (ConvNets) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29]</ref>. However, for cluttered background with objects which are similar to body parts or limbs, or body parts with heavy occlusion, ConvNets may have difficulty to locate each body parts cor- * The first two authors contribute equally to this work. rectly, as demonstrated in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>. In the literature, the combination of multiple contextual information has been proved essential for vision tasks such as image classification <ref type="bibr" target="#b25">[26]</ref>, object detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b49">50]</ref> and human pose estimation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>. Intuitively, larger context region captures global spatial configurations of object, while smaller context region focuses on the local part appearance. However, previous works usually use manually designed multicontext representations, e.g., multiple bounding boxes <ref type="bibr" target="#b34">[35]</ref> or multiple image crops <ref type="bibr" target="#b25">[26]</ref>, and hence lack of flexibility and diversity for modeling the multi-context representations.</p><p>Visual attention is an essential mechanism of the human brain for understanding scenes effectively. In this work, we propose to generate contextual representations with an attention scheme. Instead of defining regions of interest manually by a set of rectangle bounding boxes, the attention maps are generated by an attention model, which depends on image features, and provide a principled way to focus on target regions with variable shapes. For example, an attention map focusing on the human body is shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>. It helps recover the missing body parts (e.g., legs), and distinguishes the ambiguous background. This allows the diversity of context to be increased, and so contextual region could be better adapted to each image. Furthermore, instead of adopting the spatial Softmax normalization widely used in conventional attention schemes, we design a novel attention model based on Conditional Random Fields, which is better in modeling the spatial correlations among neighboring regions.</p><p>The combination of multiple contextual information has been proved effective for various vision tasks <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b13">14]</ref>. To use the attention mechanism to guide multicontextual representation learning, we adopt the stacked hourglass network structure <ref type="bibr" target="#b28">[29]</ref>, which provides an ideal architecture to build a multi-context attention model. In each hourglass stack, features are pooled down to a very low resolution, then are upsampled and combined with high-resolution features. This structure is repeated for several times to gradually capture more global representations. Within each hourglass stack, we first generate multiresolution attention maps from features of different resolutions. Secondly, we generate attention maps for multiple hourglass stacks, which results in multi-semantics attention maps with various levels of semantic meaning. Since these attention maps capture the configuration of the full human body, they are referred to as holistic attention models.</p><p>While the holistic attention model is robust to occlusions and cluttered background, it lacks of precise description for different body parts. To overcome this limitation, we design a hierarchical visual attention scheme, which zooms in from holistic attention model to each body part, namely the part attention model. This is helpful for precise localization of the body parts, as shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>.</p><p>Additionally, we introduce a novel "Hourglass Residual Units" as a replacement for the residual unit <ref type="bibr" target="#b19">[20]</ref> in our network. It incorporates the expressive power of multi-scale features while preserving the benefit of residual learning. It also enables deep networks to have a faster growth of receptive field, which is essential for accurately locating body parts. When using these units within the "macro" hourglass network, we obtain a nested hourglass architecture.</p><p>We show the effectiveness of the proposed end-to-end differentiable framework on two broadly used human pose estimation benchmarks, i.e., MPII Human Pose dataset <ref type="bibr" target="#b0">[1]</ref> and the Leeds Sports Dataset <ref type="bibr" target="#b23">[24]</ref>. Our approach outperforms all the previous methods on both benchmarks for all the body parts. The main contributions of this work are three folds:</p><p>• We propose to use visual attention mechanism to automatically learn and infer the contextual representations, driving the model to focus on region of interest. Instead of applying spatial Softmax normalization as in conventional attention models, we tailor the attention scheme for human pose estimation by introducing CRFs to model the spatial correlations among neighborhood joints. To the best of our knowledge, this is the first attempt to utilize attention scheme for human pose estimation. • We use multi-context attention to make the model more robust and more accurate. Specifically, three types of attentions are designed, i.e., multi-resolution attention within each hourglass, multi-semantics attention across several stacks of hourglass, and a hierarchical visual attention scheme to zoom in on local regions to see clearer. • We propose a generic hourglass residual unit (HRU), and build the nested hourglass networks together with the stacked hourglass architecture. The HRUs incorporate features from different scales in the conventional residual unit. They also enable the network to see larger context in an earlier stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human Pose Estimation Articulated human poses were usually modeled by combination of unary term and graph models, e.g., mixture of body parts <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref> or pictorial structures <ref type="bibr" target="#b30">[31]</ref>. Recently, significant progresses have been achieved by introducing ConvNets for learning better feature representation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29]</ref>. For example, Chen and Yuille <ref type="bibr" target="#b7">[8]</ref> introduced the ConvNet to learn both the unary and the pairwise term of a treestructured graphical model. Tompson et al. <ref type="bibr" target="#b36">[37]</ref> used multiple branches of ConvNets to fuse the features from an image pyramid, and used a Markov Random Field (MRF) for post-processing. Convolutional Pose Machine <ref type="bibr" target="#b39">[40]</ref> incorporated the inference of the spatial correlations among body parts within the ConvNets. State-of-the-art performance is achieved by the stacked hourglass network <ref type="bibr" target="#b28">[29]</ref> and its variant <ref type="bibr" target="#b4">[5]</ref>, which use repeated pooling down and upsampling process to learn the spatial distribution. Our approach is complementary to previous approaches by incorporating diverse image dependent multi-context representation to guide the human pose estimation. Multiple Contextual Information The contextual information is generally referred to as regions surrounding the target locations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref>, object-scene relationships <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13]</ref>, and object-object interactions <ref type="bibr" target="#b43">[44]</ref>. It has been proved efficient in vision tasks as object classification <ref type="bibr" target="#b25">[26]</ref> and detection <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Recent works mod-  <ref type="figure">Figure 2</ref>. Framework. The basic structure is an 8-stack hourglass network. In each stack of hourglass, we generate multi-resolution attention maps. We also apply multi-semantic attention map to each hourglass as shown in stack 1 to stack 8. Hierarchical Attention Mechanism for zooming in on local parts is applied in stack 5 to stack 8. eled contextual information by concatenating multi-scale features <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>, or by gated functions to control the mutual influence of different contexts <ref type="bibr" target="#b49">[50]</ref>. The contextual regions, however, are manually defined as rectangles without considering the objects appearance. In this work, we adopt visual attention mechanism to focus on regions which are image dependent and adaptiving for multi-context modeling. Our approach increases the diversity of contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Attention Mechanism</head><p>Since the visual attention model is computationally efficient and is effective in understanding images, it has achieved great success in various tasks such as machine translation <ref type="bibr" target="#b2">[3]</ref>, object recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b40">41]</ref>, image captioning <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b41">42]</ref>, image question answering <ref type="bibr" target="#b46">[47]</ref>, and saliency detection <ref type="bibr" target="#b26">[27]</ref>. Existing approaches usually adopt recurrent neural networks to generate the attention map for an image region at each step, and combine information from different steps overtime to make the final decision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27]</ref>. To the best of our knowledge, our work is the first to investigate the use of attention models for human pose estimation. In addition, our design of the holistic attention map and the part attention map in learning attention in hierarchical order and the modeling of attention from different context and resolution are not investigated in these works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Framework</head><p>An overview of our framework is illustrated in <ref type="figure">Fig. 2</ref>. In this section, we briefly introduce the nested hourglass architecture, and the implementation of the multi-context attention model, including the multi-semantics, multi-resolution, and hierarchical holistic-part attention model. The generated attention maps are then used to reweight the features for automatically infer the regions of interest. Baseline Network We adopt an 8-stack hourglass net-work <ref type="bibr" target="#b28">[29]</ref> as the baseline network. It allows for repeated bottom-up, top-down inference across scales with intermediate supervision at the end of each stack. In experiments, the input images are 256 × 256, and the output heatmaps are P × 64 × 64, where K is the number of body parts. We follow previous work <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29]</ref> to use the Mean Squared Error as the loss function. Nested Hourglass Networks We replace the residual units, which are along the side branches for combining features across multiple resolutions, by the proposed micro hourglass residual units (HRUs), and obtain a nested hourglass network , as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. With this architecture, we enrich the information received by the output of each building block, which makes the whole framework more robust to scale change. Details of HRUs are described in Section 4. Multi-Resolution Attention Within each hourglass, the multi-resolution attention maps Φ r are generated from features of different scales, where r is the size of the features, as shown in <ref type="figure">Fig. 5</ref>. Attention maps are then combined to generate the refined features, which are further used to generate refined attention maps and further refined features, as shown in <ref type="figure">Fig. 4</ref>. Multi-Semantics Attention Different stacks are with different semantics: lower stacks focus on local appearance, while higher stacks encode global representations. Hence attention maps generated from different stacks also encode various semantic meanings. As shown in <ref type="figure">Fig. 2</ref>, compare the left knee in Stack 1 with 8, we can see that deeper stacks with global representations are able to recover occlusions. Hierarchical Attention Mechanism In the lower stacks, i.e., stack 1 to stack 4, we use two holistic attention maps h att 1 and h att 2 to encode configurations of the whole human body. In the higher stacks, i.e., the 5th to the 8th stack, we design a hierarchical coarse-to-fine attention scheme to zoom into local parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Nested Hourglass Networks</head><p>In this section, we provide a detailed description of the proposed hourglass residual units (HRUs). We also provide comprehensive analysis of the receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hourglass Residual Units</head><p>Let us first briefly recall Residual networks <ref type="bibr" target="#b19">[20]</ref>. Deep residual networks achieves compelling accuracy by an extremely deep stacks of "Residual Units", which can be expressed as follows,</p><formula xml:id="formula_0">x n+1 = h(x n ) + F(x n , W F n ),<label>(1)</label></formula><p>where x n and x n+1 are the input and output of the n-th unit, and F is the stacked convolution, batch normalization, and ReLU nonlinearity. In <ref type="bibr" target="#b19">[20]</ref>, h(x n ) = x n is the identity mapping.</p><p>In this paper, we focus on human pose estimation, which larger contextual regions are proved to be important for locating local body parts <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b28">29]</ref>. The contextual region of a neuron is its corresponding receptive field. In this work, we propose to extend the original residual units by a micro hourglass branch. The resulted hourglass residual units (HRUs) have larger receptive field while preserve local details, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. We use this module in the stacked hourglass networks. This architecture is referred to as "nested hourglass networks" because the hourglass structure is used at both the macro and micro levels.</p><p>The mathematical formulation of our proposed HRUs is as follows:</p><formula xml:id="formula_1">x n+1 = x n + F(x n , W F n ) + P(x n , W P n ).<label>(2)</label></formula><p>Each HRU consists of three branches. Branch (A), i.e. x n in (2), is the identity mapping. Hence, the property of ResNet in handling vanishing gradient is preserved in the HRUs. <ref type="formula" target="#formula_1">(2)</ref>, is the residual block like the ResNet in <ref type="bibr" target="#b0">(1)</ref>. Branch (C), i.e. P(x n , W P n ) in <ref type="formula" target="#formula_1">(2)</ref>, is our new design, which is a stack of a 2 × 2 max-pooling, two 3 × 3 convolutions followed by ReLU nonlinearity, and an upsampling operation.</p><formula xml:id="formula_2">Branch (B), i.e. F(x n , W F n ) in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of Receptive Field of HRU</head><p>The identity mapping in branch (A) has receptive size of one. The residual block in branch (B) is a stack of convolutions (Conv 1×1 + Conv 3×3 + Conv 1×1 ). Hence, the neuron in the output feature corresponds to a 3 × 3 region of the input in this HRU. Branch (C) is our added branch. The structure of this branch is Pool 2×2 + Conv 3×3 + Conv 3×3 + Deconv 2×2 . Due to max-pooling, the resolution for convolution in this branch is half of that in branches (A) and (B), and each neuron in the output feature map corresponds to a 10 × 10 region of the input, which is about 3 times the receptive field size of the residual block in branch (B). These three branches, with different receptive fields and resolutions, are added together as the output of the HRU. Therefore, the HRU unit increases the receptive field size by including the branch (C) while preserves the high-resolution information by using branches (A) and (B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Attention Mechanism</head><p>We shall first briefly introduce the conventional soft attention mechanism, and then describe our proposed multicontext framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Conventional Attention</head><p>Denote convolutional features by f . The first step in obtaining soft attention is to generate the summarized feature map as follows:</p><formula xml:id="formula_3">s = g(W a * f + b),<label>(3)</label></formula><p>where * denotes convolution, W a denotes the convolution filters, and g is the nonlinear activation function. s ∈ R H×W summarizes information of all channels in f . Denote s(l) as the feature at location l in the feature map s, where l = (x, y), x is the horizontal location and y is the vertical location. The Softmax operation is applied to s spatially as follows:</p><formula xml:id="formula_4">Φ(l) = e s(l) l ∈L e s(l ) ,<label>(4)</label></formula><p>where L = {(x, y)|x = 1, . . . , W, y = 1, . . . , H}. Φ is the attention map, where l∈L Φ(l) = 1. Then the attention map is applied to the feature f ,</p><formula xml:id="formula_5">h att = Φ f , where h att (c) = f (c) • Φ,<label>(5)</label></formula><p>where c is the index for feature channel. We use to represent the channel-wise Hadamard matrix product operation. h att is the refined feature map, which is the feature reweighted by the attention map, and has the same size as f . att att <ref type="figure">Figure 4</ref>. An illustration of the attention scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Our Multi-Context Attention Model</head><p>Our framework makes the following three modifications to the attention model. First, we replace the global Softmax in 4 with a CRF to taking local pattern correlations into consideration. Global spatial Softmax normalizes the whole image based on a constant factor, which ignores the local neighboring spatial correlations. But we want attention maps to drive the network to concentrate on the complex human body configurations. More details are in Section 5.2.1. Second, we generate attention maps based on features of different resolutions to make the model more robust, as illustrated in Section 5.2.2. Then multi-semantics attention is obtained by generating attention maps for each stack of the hourglass, as described in Section 5.2.3. Finally, a hierarchical coarse to fine(i.e. fully body to parts) attention scheme is used, to zoom into local part regions for more precise localization, which is introduced in Section 5.2.4. The whole framework is differentiable and trained end-to-end with random initialization. An illustration of our attention scheme is shown in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Spatial CRF Model</head><p>In this work, we use Conditional Random Fields (CRFs) to model the spatial correlation. To make them differentiable, we use the mean-field approximation approach to recursively learn the spatial correlation kernel <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>The attention map is modeled as a two-class problem. Denote y l = {0, 1} as the attention label at the i-th location. In the CRF model, the energy of a label assignment y = {y l |l ∈ L} is as follows:</p><formula xml:id="formula_6">E(z) = l y l ψu(l) + l,k y l w l,k y k ,<label>(6)</label></formula><p>where ψ(y l ) = g(h, l) is the unary term that measures the inverse likelihood (and therefore, the cost) of the position l taking the attention label y l = 1. w l,k is the weight for compatibility between y l and y k . Given the image I, the probability of the label assignment y is P (y|I) = 1 Z exp(−E(y|I)), where Z is the partition function. The probability for y l = 1 is obtained iteratively using the mean-field approximation as follows:</p><formula xml:id="formula_7">Φ(y l = 1)t = σ ψu(l) + k w l,k Φ(y k = 1)t−1 ,<label>(7)</label></formula><p>where σ(a) = 1/(1 + exp(−a)) is the sigmoid function. ψ u (l) is obtained by convolution from features h. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hourglass</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Resolution Attention</head><p>Attention map <ref type="figure">Figure 5</ref>. The multi-resolution attention scheme within an hourglass. In each stack of hourglass, we generate multi-resolution attention maps from features with different resolutions (a). These maps are summed into a single attention map, which applies to features f to generate the refined feature h att 1 .</p><p>k w l,k Φ(y j = 1) is implemented by convolving the estimated attention map Φ t−1 at the stage t − 1 with the filters. Initially, Φ(y i = 1) 1 = σ(ψ u (i)).</p><p>In summary, the attention map Φ t at the stage t can be formulated as follows:</p><formula xml:id="formula_8">Φt = M(s, W k ) = σ(W k * s) t = 0, σ(W k * Φt−1) t = 1, 2, 3,<label>(8)</label></formula><p>where M denotes a sequence of weights-sharing convolutions for the mean field approximation, W k denotes the spatial correlation kernel. The W k is shared across different time steps. In our network, we use three steps of recursive convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Multi-Resolution Attention</head><p>As shown in <ref type="figure">Fig. 5</ref>, the up-sampling process generates features of different size r, i.e. f r for r = 8, 16, 32 and 64. s r is used to generate the attention map Φ r using the procedure in <ref type="bibr" target="#b7">(8)</ref>. The attention map Φ r is up-sampled to size 64, the up-sampled map is denoted by Φ {r→64} . These attention maps correspond to different resolutions. As shown in <ref type="figure">Fig. 5 (I)</ref>, Φ {8→64} , which has lower resolution, and highlights the whole configure of human body. Φ 64 , which is generated with higher resolution, focusing on local body parts. All up-sampled attention maps are summed up and then applied to the feature f ,</p><formula xml:id="formula_9">h att 1 = f r=8,16,32,64 Φ {r→64} ,<label>(9)</label></formula><p>where the feature f is the output of the last layer in an hourglass stack as shown in <ref type="figure">Fig. 5</ref>. The operation is illustrated in Eq. <ref type="bibr" target="#b4">(5)</ref>. The conventional way of using an attention map is to directly apply it to the feature which generates it. However,  the features refined by attention map usually have large amount of values close to zero, and so a stack of many refined features makes the back-propagation difficult. To utilize information from multi-resolution features without sacrificing training efficiency, we generate attention maps from features with various resolutions, and apply them to the later features.</p><p>In addition to the multi-resolution attention, a refined attention map Φ and its corresponding refined feature h att 2 are generated from h att 1 ,</p><formula xml:id="formula_10">h att 2 = h att 1 Φ = h att 1 M(h att 1 , w).<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Multi-Semantics Attention</head><p>The above procedure is repeated over stacks of hourglass to generate attention maps with multiple semantic meanings. Samples of Φ are shown in <ref type="figure">Fig. 2</ref> from stack 1 to 8. The attention maps at shallower hourglass stacks capture more local information. For deeper hourglass stacks, the global information about the whole person is captured, which is more robust to occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Hierarchical Holistic-Part Attention</head><p>In the 4th to 8th stacks of hourglass structure, we use the the refined feature h att 1 in Eq. (9) to generate the part attention maps as follows:</p><formula xml:id="formula_11">sp = g(W a p * h att 1 + b), Φp = M(sp, W k p ),<label>(11)</label></formula><p>where p ∈ {1, · · · , P }, W a p denotes the parameters for obtaining the summarization map s p of part p, W k p denotes the spatial correlation modeling for part p. The part attention map Φ p is combined with the refined feature map h att 1 to obtain the refined feature map for part p as follows:</p><formula xml:id="formula_12">h att p = h att 1 Φp.<label>(12)</label></formula><p>The heatmap predication for the pth body joint is based on the refined features h att p , whereŷ p is the heatmap for the pth part, w cls p is the classifier. In this way, we guarantee that the attention map Φ p is specific for the body joint p. Some qualitative results of part attention maps are shown in <ref type="figure" target="#fig_3">Fig. 6</ref>.</p><formula xml:id="formula_13">yp = w cls p * h att p ,<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Training the model</head><p>Each stack in the hourglass produces the estimated heatmaps for the body joints. We adopt the loss function in <ref type="bibr" target="#b28">[29]</ref> for learning the model. For each stack, the Mean Squared Error (MSE) loss is computed by</p><formula xml:id="formula_14">L = P p=1 l∈L ŷ p (l) − y p (l) 2 2<label>(14)</label></formula><p>where p denotes the pth body part, l denotes the lth location.ŷ p denotes the predicted heatmap for part p, and y p the corresponding ground-truth heatmap generated by a 2-D Gaussian centered on the body part location.</p><p>The attention maps help to drive the network to focus on hard negative samples. After several stages of training, the attention maps fire on human body region, where the true positive samples are highlighted by attention maps. The refined features are used for learning classifiers for the regions with human body, with easy background regions removed at the feature level by the learned attention maps. Consequentially, for part attention maps, the classifiers focus on classifying each body joint based on well defined human body regions, without considering the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>Dataset We evaluate the proposed method on two widely used benchmarks, MPII Human Pose <ref type="bibr" target="#b0">[1]</ref> and extended Leeds Sports Poses (LSP) <ref type="bibr" target="#b23">[24]</ref>. The MPII Human Pose dataset includes about 25k images with 40k annotated poses. The images were collected from YouTube videos covering daily human activities with highly articulated human poses. The LSP dataset consists of 11k training images and 1k testing images from sports activities. Data Augmentation During training, we crop the images with the target human centered at the images with roughly the same scale, and warp the image patch to the size  256×256. Then we randomly rotate (±30 • ) and flip the images. We also perform random rescaling (0.75 to 1.25) and color jittering to make the model more robust to scale and illumination change. During testing, we follow the standard routine to crop image patches with the given rough position and the scale of the test human for MPII dataset. For the LSP dataset, we simply use the image size as the rough scale, and the image center as the rough position of the target human to crop the image patches. All the experimental results are produced from the original and flipped image pyramids with 6 scales. Experiment Settings We train our model with Torch7 <ref type="bibr" target="#b10">[11]</ref> using the initial learning rate of 2.5 × 10 −4 . The parameters are optimized by RMSprop <ref type="bibr" target="#b35">[36]</ref> algorithm. We train the model on the MPII dataset for 130 epochs and the LSP dataset for 60 epochs. We adopt the validation split for the MPII dataset used in <ref type="bibr" target="#b36">[37]</ref> to monitor the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Results</head><p>We use the Percentage Correct Keypoints (PCK) <ref type="bibr" target="#b45">[46]</ref> metric for comparisons on the LSP dataset, and the PCKh measure <ref type="bibr" target="#b0">[1]</ref>, where the error tolerance is normalized with respect to head size, for comparisons on the MPII Human Pose dataset. <ref type="table" target="#tab_2">Table 1</ref> reports the comparison of the PCKh performance of our method and previous state-ofthe-art at a normalized distance of 0.5. Our method achieves state of the art 91.5% PCKh scores. In particular, for the most challenging body parts, e.g., wrist and ankle, our method achieves 1.0% and 1.4% improvement compared with the closest competitor respectively, as shown in <ref type="figure" target="#fig_4">Fig. 7</ref>  Leeds Sports Pose We train our model by adding the MPII training set to the extended LSP training set with personcentric annotations, which is a standard routine <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4]</ref>. <ref type="table" target="#tab_3">Table 2</ref> reports the PCK at threshold of 0.2. Our approach outperforms the state-of-the-art across all the body joints, and obtains 1.9% improvement in average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPII Human Pose</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Component Analysis</head><p>To investigate the efficacy of the proposed multi-context attention mechanism and the hourglass residual unit, we conduct ablation experiments on the validation set <ref type="bibr" target="#b36">[37]</ref> of the MPII Human Pose dataset. We use an 8-stack hourglass network <ref type="bibr" target="#b28">[29]</ref> as our baseline model if not specified. The overall result is shown in <ref type="figure" target="#fig_5">Fig. 8</ref>. Based on the baseline network (BL), we analyze each proposed component, i.e., the Multi-Semantics attention model (MS), Hourglass Residual Units (HRUs), Multi-Resolution attention model (MR), and the Hierarchical Part attention model (HP), by comparing the PCKh score. Multi-Semantics Attention We first evaluate the multisemantics attention model. By adding holistic attention model at the end of each stack of hourglass ("BL+MS"), we get an 87.2% PCKh score, which is a 1.2% improvement compared to the baseline model. Hourglass Residual Unit To explore the effect of the residual pooling unit, we further use the HRUs to replace the original residual units when combining features from different resolutions ("BL+MS+HRU"), as illustrated in <ref type="figure">Fig. 2</ref>. The addition of hourglass residual unit result in a further 1% improvement. As discussed in <ref type="bibr" target="#b28">[29]</ref>, improvements cannot be easily obtained by simply stacking more than eight hourglass modules. We provide a way to increase the model capacity effectively. Multi-Resolution Attention By generating attention maps from features with multiple resolution ("BL+MS+HRU+MR"), our method obtains a further 1% improvements. Hierarchical Attention We also show the improvement brought by the hierarchical holistic-local attention model. We replace the refined holistic attention map by a set of part attention maps from stack four to eight, and obtain the high-  est mean PCKh score 89.4%. We observe the improvements are mostly brought by the refined localization of body parts. In some cases, the part attention model could even correct the double counting problem, as demonstrated in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>. Softmax vs. CRF Finally, we compare the proposed CRF spatial attention model with the conventional Softmax attention model based on a 2-stack hourglass network. We compare the accuracy rates, i.e., PCKh at 0.5, on the validation set as training progresses in <ref type="figure" target="#fig_0">Fig. 10</ref>. The CRF attention model converges much faster and achieves higher validation accuracy than the Softmax attention model. We visualize the attention maps generated by these two models, and observe that CRF attention models generates much more cleaner attention maps compared with Softmax attention model due to its better ability to model spatial correlations among body parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Qualitative Results</head><p>To gain insights on how attention works, we compare the baseline model with the proposed model by visualizing the attention maps, the score maps, and the estimated poses, as demonstrated in <ref type="figure" target="#fig_6">Fig. 9 (a-b)</ref>. We observe the baseline model may has difficulty in distinguishing objects with similar appearance with limbs (e.g., the horse leg in <ref type="figure" target="#fig_6">Fig. 9 (a)</ref>), and the heavy shadow with ambiguous shape ( <ref type="figure" target="#fig_6">Fig. 9 (b)</ref>). So the holistic attention maps would be great help for remov- ing cluttered background and reducing ambiguity. For part attention maps, besides providing more precise localization for the body parts, they could even help reduce the double counting problem. For example, the left and right ankle can be distinguished by incorporating the part attention maps. <ref type="figure" target="#fig_6">Fig. 9</ref> (c) demonstrates the poses predicted by our methods on the MPII test set and the LSP test set. Our method is robust to extremely difficult cases, e.g., rare poses, cluttered background, and foreshortening. However, as shown in <ref type="figure" target="#fig_0">Fig. 11</ref>, our method may fail in some cases which are also difficult for human eyes, i.e. (a) heavy occlusion and ambiguity, (b) twisted limbs, (c) significant illumination change, and (d) left/right body confusion caused by clothing/lighting. Please refer to the supplementary materials for more visualized results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>This paper has proposed incorporating multi-context attention and ConvNets into an end-to-end framework. We use visual attention to guide context modeling. Hence our framework has large diversity in contextual regions. Instead of using global Softmax, we introduce CRF for spatial correlation modeling. We build multi-context attention model along three components, i.e., multi-resolution, multi-semantics, and hierarchical holistic-part attention scheme. Additionally, an hourglass residual unit was proposed to enrich the expressive power of conventional residual unit. The proposed multi-context attention and the HRUs are general, and would help other vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Motivation. The 1st row shows the input image, the holistic attention maps, and the part attention maps. The 2nd row shows the predicted heatmaps for part locations, where different colors correspond to different body parts. The 3rd row visualizes the predicted poses. We observe that (a) ConvNets may produce erroneous estimations due to cluttered background and selfocclusion. (b) Visual attention provides an explicit way to model spatial relationships among human body parts, which is more robust. (c) Part attention maps can help further refine the part locations by addressing the double counting problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>An illustration of the hourglass residual unit. It consists of three branches: (A) identity mapping, (B) residual branch, and (C) hourglass residual branch. The receptive field with respect to the input is 3 × 3 and 10 × 10 for the conventional residual branch and the hourglass residual branch, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Coarse-to-fine part attention model and the visualization of examplar part attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Comparisons of PCKh curve on the MPII Human Posetest set on the most challenging body joints, i.e., wrist and ankle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Component analysis. PCKh scores at threshold of 0.5 on the MPII validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Qualitative evaluation. (a-b) 1st row to 3rd row: 2 input images, 4 attention maps, 6 heatmaps, and 6 predicted poses. (c) Examples of estimated poses on the MPII test set and the LSP test set (Best viewed in electronic form with 4× zoom in).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>PCKh@0.5 on the MPII validation set across training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Failure cases caused by (a) overlapping people, (b) twisted limbs, (c) illumination, and (d) left/right confusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>MethodHead Sho. Elb. Wri. Hip Knee Ank. Mean Pishchulin et al.<ref type="bibr" target="#b31">[32]</ref> 74.<ref type="bibr" target="#b2">3</ref> 49.0 40.8 34.1 36.5 34.4 35.2 44.1 Tompson et al. [38] 95.8 90.3 80.5 74.3 77.6 69.7 62.8 79.6 Carreira et al. [7] 95.7 91.7 81.7 72.4 82.8 73.2 66.4 81.3 Tompson et al. [37] 96.1 91.9 83.9 77.8 80.9 72.3 64.8 82.0 Hu&amp;Ramanan [22] 95.0 91.6 83.0 76.6 81.9 74.5 69.5 82.4 Pishchulin et al. [33] 94.1 90.2 83.4 77.3 82.6 75.7 68.6 82.4 Lifshitz et al. Comparisons of PCKh@0.5 score on the MPII test set.</figDesc><table><row><cell>[28]</cell><cell>97.8 93.3 85.7 80.4 85.3 76.6 70.2 85.0</cell></row><row><cell>Gkioxary et al. [17]</cell><cell>96.2 93.1 86.7 82.1 85.2 81.4 74.1 86.1</cell></row><row><cell>Rafi et al. [34]</cell><cell>97.2 93.9 86.4 81.3 86.8 80.6 73.4 86.3</cell></row><row><cell>Insafutdinov et al. [23]</cell><cell>96.8 95.2 89.3 84.4 88.4 83.4 78.0 88.5</cell></row><row><cell>Wei et al. [40]</cell><cell>97.8 95.0 88.7 84.0 88.4 82.8 79.4 88.5</cell></row><row><cell cols="2">Bulat&amp;Tzimiropoulos [5] 97.9 95.1 89.9 85.3 89.4 85.7 81.7 89.7</cell></row><row><cell>Newell et al. [29]</cell><cell>98.2 96.3 91.2 87.1 90.1 87.4 83.6 90.9</cell></row><row><cell>Ours</cell><cell>98.5 96.3 91.9 88.1 90.6 88.0 85.0 91.5</cell></row><row><cell>Method</cell><cell>Head Sho. Elb. Wri. Hip Knee Ank. Mean</cell></row><row><cell cols="2">Belagiannis&amp;Zisserman [4]95.2 89.0 81.5 77.0 83.7 87.0 82.8 85.2</cell></row><row><cell>Lifshitz et al. [28]</cell><cell>96.8 89.0 82.7 79.1 90.9 86.0 82.5 86.7</cell></row><row><cell>Pishchulin et al. [33]</cell><cell>97.0 91.0 83.8 78.1 91.0 86.7 82.0 87.1</cell></row><row><cell>Insafutdinov et al. [23]</cell><cell>97.4 92.7 87.5 84.4 91.5 89.9 87.2 90.1</cell></row><row><cell>Wei et al. [40]</cell><cell>97.8 92.5 87.0 83.9 91.5 90.8 89.9 90.5</cell></row><row><cell cols="2">Bulat&amp;Tzimiropoulos [5] 97.2 92.1 88.1 85.2 92.2 91.4 88.7 90.7</cell></row><row><cell>Ours</cell><cell>98.1 93.7 89.3 86.9 93.4 94.0 92.5 92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparisons of PCK@0.2 score on the LSP dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.</figDesc><table><row><cell>86.7</cell><cell>88.2</cell><cell>89.1</cell><cell>89.8</cell><cell>90.3</cell><cell>81.4</cell><cell>83.6</cell><cell>84.3</cell><cell>85.1</cell><cell>86.1</cell><cell>80.1</cell><cell>82.2</cell><cell>83.9</cell><cell>85.4</cell><cell>85.2</cell><cell>76.2</cell><cell>77.9</cell><cell>80.1</cell><cell>82.1</cell><cell>86.0</cell><cell>87.2</cell><cell>88.2</cell><cell>89.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02914</idno>
		<title level="m">Recurrent human pose estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crf-cnn: Modeling structured information in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="316" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context and observation driven latent variable model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kimber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatial context: Using stuff to find things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent attentional networks for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>2012. 7</idno>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recognizing proxemics in personal photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03925</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-stage contextual deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Crafting gbd-net for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02579</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
