<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Vector Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Montreal Institute for Learning Algorithms</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Montreal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Montreal Institute for Learning Algorithms</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Montreal</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Canadian Institute for Advanced Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Vector Institute for Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Canadian Institute for Advanced Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conditional text-to-image generation is an active area of research, with many possible applications. Existing research has primarily focused on generating a single image from available conditioning information in one step. One practical extension beyond one-step generation is a system that generates an image iteratively, conditioned on ongoing linguistic input or feedback. This is significantly more challenging than one-step generation tasks, as such a system must understand the contents of its generated images with respect to the feedback history, the current feedback, as well as the interactions among concepts present in the feedback history. In this work, we present a recurrent image generation model which takes into account both the generated output up to the current step as well as all past instructions for generation. We show that our model is able to generate the background, add new objects, and apply simple transformations to existing objects. We believe our approach is an important step toward interactive generation. Code and data is available at: https://www.microsoft.com/en-us/research/ project/generative-neural-visual-artist-geneva/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision is one of the most important ways in which humans experience, interact with, understand, and learn about the world around them. Intelligent systems that can generate images and video for human users have a wide range of applications, from education and entertainment to the pursuit of creative arts. Such systems also have the potential to serve as accessibility tools for the physically impaired; many modern and creative works are now generated or edited using digital graphic design tools, and the complexity of these tools can lead to inaccessibility issues, particularly with people with insufficient technical knowledge or resources. A system that can follow speech-or text-based * Work was performed during an internship with Microsoft Research.  <ref type="figure">Figure 1</ref>. We present the Generative Neural Visual Artist (GeNeVA) task. Starting from an empty canvas, a Drawer (GeNeVA-GAN) iteratively constructs a scene based on a series of instructions and feedback from a Teller.</p><p>instructions and then perform a corresponding image editing task could improve accessibility substantially. These benefits can easily extend to other domains of image generation such as gaming, animation, creating visual teaching material, etc. In this paper, we take a step in this exciting research direction by introducing the neural visual artist task. Conditional generative models allow for generation of images from other input sources, such as labels <ref type="bibr" target="#b0">[1]</ref> and dialogue <ref type="bibr" target="#b1">[2]</ref>. Image generation conditioned on natural language is a difficult yet attractive goal <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Though these models are able to produce high quality images for simple datasets, such as birds, flowers, furniture, etc., good caption-conditioned generators of complex datasets, such as Microsoft Common Objects in Context (MS COCO) <ref type="bibr" target="#b6">[7]</ref> are nonexistent. This lack of good generators may be due to the limited information content of captions, which are not rich enough to describe an entire image <ref type="bibr" target="#b1">[2]</ref>. Combining ob-ject annotations with the intermediate steps of generating bounding boxes and object masks before generating the final images can improve results <ref type="bibr" target="#b4">[5]</ref>.</p><p>Instead of constructing images given a caption, we focus on learning to iteratively generate images based on continual linguistic input. We call this task the Generative Neural Visual Artist (GeNeVA), inspired by the process of gradually transforming a blank canvas to a scene. Systems trained to perform this task should be able to leverage advances in text-conditioned single image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">GeNeVA Task and Datasets</head><p>We present an example dialogue for the GeNeVA task in <ref type="figure">Figure 1</ref>, which involves a Teller giving a sequence of linguistic instructions to a Drawer for the ultimate goal of image generation. The Teller is able to gauge progress through visual feedback of the generated image. This is a challenging task because the Drawer needs to learn how to map complex linguistic instructions to realistic objects on a canvas, maintaining not only object properties but relationships between objects (e.g., relative location). The Drawer also needs to modify the existing drawing in a manner consistent with previous images and instructions, so it needs to remember previous instructions. All of these involve understanding a complex relationship between objects in the scene and how those relationships are expressed in the image in a way that is consistent with all instructions given.</p><p>For this task, we use the synthetic Collaborative Drawing (CoDraw) dataset <ref type="bibr" target="#b7">[8]</ref>, which is composed of sequences of images along with associated dialogue of instructions and linguistic feedback ( <ref type="figure">Figure 2</ref>). Also, we introduce the Iterative CLEVR (i-CLEVR) dataset <ref type="figure" target="#fig_1">(Figure 4</ref>), a modified version of the Compositional Language and Elementary Visual Reasoning (CLEVR) <ref type="bibr" target="#b8">[9]</ref> dataset, for incremental construction of CLEVR scenes based on linguistic instructions. Offloading the difficulty of generating natural images by using two well-studied synthetic datasets allowed us to better assess progress on the GeNeVA task and improve the iterative generation process. While photo-realistic images will undoubtedly be more challenging to work with, our models are by no means restricted to synthetic image generation. We expect that insights drawn from this setting will be crucial to success in the natural image setting.</p><p>The most similar task to GeNeVA is the task proposed by the CoDraw <ref type="bibr" target="#b7">[8]</ref> authors. They require a model to build a scene by placing the clip art images of the individual objects in their correct positions. In other words, the model predictions will be in coordinate space for their task, while for a model for the GeNeVA task they will be in pixel space. Natural images are in scope for the GeNeVA task, where Generative Adversarial Networks (GANs) are currently stateof-the-art. Non-pixel-based approaches will be limited to placing pre-segmented specific poses of objects. For such approaches, it will be extremely difficult to obtain a presegmented set of all possible poses of all objects e.g., under different lighting conditions. Additionally, a pixel-based model does not necessarily require object-labels so it can easily scale without such annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>Our primary contributions are summarized as follows:</p><p>• We introduce the GeNeVA task and propose a novel recurrent GAN architecture that specializes in plausible modification of images in the context of an instructional history. • We introduce the i-CLEVR dataset, a sequential version of CLEVR <ref type="bibr" target="#b8">[9]</ref> with associated linguistic descriptions for constructing each CLEVR scene, and establish a baseline for it. • We propose a relationship similarity metric that evaluates the model's ability to place objects in a plausible position compatible with the instructions. • We demonstrate the importance of iterative generation for complex scenes by showing that our approach outperforms the non-iterative baseline. Our experiments on the CoDraw and i-CLEVR datasets show that our model is capable of generating images that incrementally build upon the previously generated images and follow the provided instructions. The model is able to learn complex behaviors such as drawing new objects, moving objects around in the image, and re-sizing these objects. In addition to reporting qualitative results, we train an object localizer and measure precision, recall, F1 score, and our proposed relational similarity metric by comparing detections on ground-truth vs. generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>GANs <ref type="bibr" target="#b9">[10]</ref> represent a powerful family of generative models whose benefits and strengths extend to conditional image generation. Several approaches for conditioning exist, such as conditioning both the generator and discriminator on labels <ref type="bibr" target="#b0">[1]</ref>, as well as training an auxiliary classifier as part of the discriminator <ref type="bibr" target="#b10">[11]</ref>. Closer to GeNeVA text-based conditioning, Reed et al. <ref type="bibr" target="#b2">[3]</ref> generate images conditioned on the provided captions. Zhang et al. <ref type="bibr" target="#b3">[4]</ref> proposed a twostage model called StackGAN, where the first stage generated low resolution images conditioned on the caption, and the second stage generated a higher resolution image conditioned on the previous image and the caption. Hong et al. <ref type="bibr" target="#b4">[5]</ref> proposed a three-step generation process where they use external segmentation and bounding box annotation for MS COCO to first generate bounding boxes, then a mask for the object, and then the final image. Building upon StackGAN, AttnGAN <ref type="bibr" target="#b5">[6]</ref> introduced an attentional generator network that enabled the generator to synthesize different spatial locations in the image, conditioned on an attention mecha-  <ref type="bibr" target="#b11">[12]</ref> dataset) to generate images. The authors showed that the question answering-based dialogue captured richer information about the image than just the caption, which enabled Chat-Painter to generate superior images compared to using captions alone. Since the VisDial dialogues were collected separately from the MS COCO dataset, there are no intermediate incremental images for each turn of the dialogue. The model, thus, only reads the entire dialogue and generates a single final image, so this setup diverges from a real-life sketch artist scenario where the artist has to keep making changes to the current sketch based on feedback.</p><p>There has also been recent work in performing recurrent image generation outside of text-to-image generation tasks. Yang et al. <ref type="bibr" target="#b12">[13]</ref> perform unsupervised image generation in recursive steps, first generating a background, subsequently conditioning on it to generate the foreground and the mask, and finally using an affine transformation to combine the foreground and background. Lin et al. <ref type="bibr" target="#b13">[14]</ref> tackle the image compositing task of placing a foreground object on a background image in a natural location. However, this approach is limited to fixed object templates, and instead of generating images directly, the model recursively generates parameters of transformations to continue applying to an object template until the image is close enough to natural image manifold. Their approach also does not modify existing objects in the image. Both of these approaches aim to generate a single final image without incorporating any external feedback. To the best of our knowledge, the proposed model is the first of its kind that can recursively generate and modify intermediate images based on continual text instructions such that every generated image is consistent with past instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we describe a conditional recurrent GAN model for the GeNeVA task. An overview of the model architecture is shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model</head><p>During an n-step interaction between a Teller and a Drawer, the Teller provides a drawing canvas x 0 and a sequence of instructions Q = (q 1 , . . . , q n ). For every turn in the conversation, a conditioned generator G outputs a new image</p><formula xml:id="formula_0">x t = G(z t , h t , f Gt−1 ),<label>(1)</label></formula><p>where z t is a noise vector sampled from a normal distribution N (0, 1) of dimension N z . G is conditioned on two variables, h t and f Gt−1 , where h t is a context-aware condition and f Gt−1 is context-free.</p><p>The context-free condition f Gt−1 = E G ( x t−1 ) is an encoding of the previously generated image x t−1 using an encoder E G , which is a shallow Convolutional Neural Network (CNN). Assuming square inputs, the encoder produces low resolution feature maps of dimensions (K g ×K g ×N g ).</p><p>The context-aware condition h t needs to have access to the conversation history such that it can learn a better encoding of the instruction in the context of the conversation history up to time t − 1.</p><p>Each instruction q t is encoded using a bi-directional Gated Recurrent Unit (GRU) on top of GloVe word embeddings <ref type="bibr" target="#b14">[15]</ref>. This instruction encoding is denoted by d t .</p><p>We formulate h t as a recursive function R, which takes the instruction encoding d t as well as the previous condition h t−1 as inputs. We implement R with a second GRU, which yields h t with dimension N c : </p><formula xml:id="formula_1">h t = R(d t , h t−1 ).<label>(2)</label></formula><formula xml:id="formula_2">x t-1 f G t-1 E G h t h t+1 D D E D E D E D E D Fusion Fusion x t-1 x t x t x t+1 f G t-1 G G f G t d t d t+1 h t h t+1 GRU GRU Conditioning Augmentation Conditioning Augmentation h t h t+1 h t-1 z t (0,1) z t+1 (0,1) h t h t+1 D(x t ,h t ,x t-1 ) Auxiliary Detector D(x t+1 ,h t+1 ,x t ) Auxiliary</formula><p>Detector <ref type="figure">Figure 3</ref>. Overview of the GeNeVA-GAN architecture. For each time-step t, instruction qt is encoded into dt using a bi-directional GRU.</p><p>The previous time-step generated imagext−1 (teacher-forcing at training time with ground truth xt−1) is encoded into fG t−1 using EG.</p><p>A GRU outputs a context-aware condition ht as a function of dt and the previous condition ht−1. The generator G generates an imagext conditioned on ht and fG t−1 . fG t−1 is concatenated to feature maps from G with the same spatial dimensions while ht is used as the input for conditional batch normalization. The image from the current time-step (ground truth xt or generatedxt) and the previous time-step ground-truth image are encoded using ED. The features from both images are fused and then passed as input to a discriminator D. Finally, D is conditioned using the context-aware condition ht. An auxiliary objective of detecting all the objects in the scene is also added to D.</p><p>The context-free condition f Gt−1 represents the prior given to the model by the most recently generated image (i.e. a representation of the current canvas). On the other hand, the context-aware condition h t represents the modifications the Teller is describing in the new image. In our model, the context-aware condition is concatenated with the noise vector z t after applying conditioning augmentation <ref type="bibr" target="#b3">[4]</ref>, as shown in <ref type="figure">Figure 3</ref>. Similar to Miyato and Koyama <ref type="bibr" target="#b15">[16]</ref>, it is also used in applying conditional batch normalization to all of the generator's convolutional layers. The context-free condition f Gt−1 is concatenated with the feature maps from the generator's intermediate layer L f G which has the same spatial dimensions as f Gt−1 .</p><p>Since we are modeling iterative modifications of images, having a discriminator D that only distinguishes between real and generated images at each step will not be sufficient. The discriminator should also identify cases where the image is modified incorrectly with respect to the instruction or not modified at all. To enforce this, we introduce three modifications to the discriminator. First, an image encoder E D is used to encode the current time step image (real or generated) and the previous time-step ground-truth image as shown in <ref type="figure">Figure 3</ref>. The output feature maps of dimensions (K d ×K d ×N d ) are passed through a fusion layer. We experiment with element-wise subtraction and concatenation of feature maps as different options for fusion. The fused features are passed through a discriminator D. Passing a fused representation of both the current and the previous images to the discriminator encourages it to focus on the quality of the modifications, not only the overall image quality. This provides a better training signal for the generator. Additionally, the context-aware condition h t−1 is used as a condition for D through projection similar to <ref type="bibr" target="#b15">[16]</ref>.</p><p>Second, for the discriminator loss, in addition to labelling real images as positive examples and generated images as negative examples, we add a term for the combination of [real image, wrong instruction], similar to Reed et al. <ref type="bibr" target="#b2">[3]</ref>. Finally, we add an auxiliary objective <ref type="bibr" target="#b10">[11]</ref> of detecting all objects in the scene at the current time step.</p><p>The generator and discriminator are trained alternately to minimize the adversarial hinge loss <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. The discriminator minimizes</p><formula xml:id="formula_3">L D = L Dreal + 1 2 (L Dfake + L Dwrong ) + βL aux ,<label>(3)</label></formula><formula xml:id="formula_4">where L Dreal = −E (xt,ct)∼p data(0:T ) [min(0, −1 + D(x t , c t ))] L Dwrong = −E (xt,ĉt)∼p data(0:T ) [min(0, −1 − D(x t ,ĉ t ))] L Dfake = −E zt∼N ,ct∼p data(0:T ) [min(0, −1 − D(G(z t ,c t ), c t ))], withc t = {h t , f Gt−1 } and c t = {h t , x t−1 }.</formula><p>Finally,ĉ t is the same as c t but with a wrong instruction and T is the length of the instruction sequence Q.</p><p>The loss function for the auxiliary task is a binary cross entropy over all the N possible objects at that time step,</p><formula xml:id="formula_5">L aux = N i=0 − (y i log(p i ) + (1 − y i ) log(1 − p i )) ,</formula><p>where y i is a binary label for each object indicating whether it is present in the scene at the current time step. Note that we do not index the loss with t to simplify notation. A linear layer of dimension N is added to the last discriminator layer before applying projection conditioning with h t . A sigmoid function is applied to each of the N outputs yielding p i , the model detection prediction for object i.</p><p>The generator loss term is</p><formula xml:id="formula_6">L G = −E zt∼pz,ct∼p data (0:T ) D(G(z t ,c t ), c t ) + βL aux (4)</formula><p>Additionally, to help with training stability, we apply zero-centered gradient penalty regularization to the discriminator's parameters Φ on the real data alone with weighting factor γ as suggested by Mescheder et al. <ref type="bibr" target="#b19">[20]</ref>,</p><formula xml:id="formula_7">GPReg(Φ) = γ 2 E p D(x) [ ∇D Φ (x) 2 ].<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>The network architecture for the generator and discriminator follows the ResBlocks architecture as used by Miyato and Koyama <ref type="bibr" target="#b15">[16]</ref>. Following SAGAN <ref type="bibr" target="#b18">[19]</ref>, we add a self-attention layer to the intermediate layers with spatial dimensions of 16×16 for the discriminator and the generator. We use spectral normalization <ref type="bibr" target="#b17">[18]</ref> for all layers in the discriminator.</p><p>For the training dynamics, the generator and discriminator parameters are updated every time step, while the parameters of E G , R and the text encoder are updated every sequence. The text encoder and the network R are trained with respect to the discriminator objective only.</p><p>We add layer normalization <ref type="bibr" target="#b20">[21]</ref> to the text encoding GRU, as well as the the GRU implementing R. We add batch normalization <ref type="bibr" target="#b21">[22]</ref> to the output of the image encoder E G . We found that adding these normalization methods was important for gradient flow to all modalities.</p><p>For training, we used teacher forcing by using the ground truth images x t−1 instead of the generated image x t−1 , but we use x t−1 during test time. We use the Adam <ref type="bibr" target="#b22">[23]</ref> optimizer for the GAN, with learning rates of 0.0004 for the discriminator and the 0.0001 for the generator, trained with an equal number of updates. We use Adam as well for the text encoder with learning rate of 0.003, and for the GRU with learning rate of 3 · 10 −4 .</p><p>In our experiments the following hyper-parameters worked the best, N z = 100, N c = 1024, K g = 16, N g = 128, K d = 16, N d = 256, γ = 10, and β = 20. More details are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>For the GeNeVA task, we require a dataset that contains textual instructions describing drawing actions, along with corresponding ground truth images for each instruction. To the best of our knowledge, the only such dataset publicly available is CoDraw. Additionally, we create a new dataset called i-CLEVR, specifically designed for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CoDraw</head><p>CoDraw <ref type="bibr" target="#b7">[8]</ref> is a recently released clip art-like dataset. It consists of scenes, which are sequences of images of children playing in a park. The children have different poses and expressions and the scenes include other objects such as trees, tables, and animals. There are 58 object types in total. Corresponding to every scene, there is a conversation between a Teller and a Drawer (both Amazon Mechanical Turk workers) in natural language. The Drawer updates the canvas based on the Teller's instructions. The Drawer can ask questions as well for clarification. The dataset consists of 9,993 scenes of varying length. An example of such a scene is shown in <ref type="figure">Figure 2</ref>. The initial drawing canvas x 0 for CoDraw provided to the Drawer consists of the background having just the sky and grass. Pre-processing: In some instances of the original dataset, the Drawer waited for multiple Teller turns before modifying the image. In these cases, we concatenate consecutive turns into a single turn until the Drawer modifies the image. We also concatenate turns until a new object has been added or removed. Thus every turn has an image in which the number of objects has changed since the last turn.</p><p>We treat the concatenated utterances of the Drawer and the Teller at time step t as the instruction, injecting a special delimiting token between the Teller and Drawer. The Teller and Drawer text contains several spelling mistakes and we run the Bing Spell Check API 1 over the entire dataset to make corrections. For words that are not present in the GloVe vocabulary, we use the "unk" word embedding from GloVe. We use the same train-valid-test split proposed in the original CoDraw dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">i-CLEVR</head><p>CLEVR <ref type="bibr" target="#b8">[9]</ref> is a programmatically generated dataset that is popular in the Visual Question Answering (VQA) community. CLEVR consists of images of collections of objects with different shapes, colors, materials and sizes. Each image is assigned complex questions about object counts, attributes or existence. We build on top of the open-source generation code 2 for CLEVR to create Iterative CLEVR (i-CLEVR). Each example in the dataset consists of a sequence of 5 (image, instruction) pairs. Starting from an empty canvas (background), each instruction describes an object to add the canvas in terms of its shape and color. The instruction also describes where the object should be placed relative to existing objects in the scene. To make the task more complex and force the model to make use of context, we refer to the most recently added object by "it" instead of stating its attributes. An example from the i-CLEVR dataset Add a cyan cylinder at the center Add a red cube behind it on the left Add a purple cylinder in front of it on the right and in front of the cyan cylinder Add a purple cube behind it on the right and in front of the red cube on the right Add a yellow cylinder behind the purple cylinder on the left and behind the red cube on the right is presented in <ref type="figure" target="#fig_1">Figure 4</ref>. The initial drawing canvas x 0 for i-CLEVR consists of the empty background. A model is tasked with learning how to add the object with the correct attributes in a plausible position, based on the textual instruction. More details about the dataset generation can be found in the appendix.</p><p>The i-CLEVR dataset consists of 10,000 sequences, totalling 50,000 images and instructions. The training split contains 6,000 sequences, while the validation and testing splits have 2,000 sequences each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first define our evaluation metrics, and then describe the experiments carried out on the CoDraw and i-CLEVR datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation Metrics</head><p>Standard metrics used for evaluating GAN models such as the Inception Score or Fréchet Inception Distance (FID) only capture how realistic the generations look relative to real images. They cannot detect if the model is correctly modifying the images according to the GeNeVA task instructions. A good evaluation metric for this task needs to identify if all the objects that were described by the Teller are present in the generated images. It should also check that the objects' positions and relationships match the instructions. To capture all of these constraints, we train an object localizer on the training dataset. For every example, we compare the detections of this localizer on the real images and the generated ones. We present the precision, recall, and F1-score for this object detection task. We also construct a graph where the nodes are objects present in the images and edges are positional relationships: left, right, behind, front. We compare the graphs constructed from the real and the generated images to test the correct placement of objects, without requiring the model to draw the objects in the same exact locations (which would have defied its generative nature).</p><p>The object detector and localizer is based on the Inception-v3 architecture. We modify the last layer for object detection and replace it with two heads. The first head is a linear layer with a sigmoid activation function to serve as the object detector. It is trained with a binary cross-entropy loss. The second head is a linear layer where we regress all the objects' coordinates. This head is trained with an L 2 -loss with a mask applied to only compute loss over objects that occur in the ground truth image provided in the dataset. We initialize the model using pre-trained weights trained over the ILSVRC12 (ImageNet) dataset and finetune on the CoDraw or i-CLEVR datasets. Its performance is reported in the appendix. Relational Similarity: To compare the arrangement of objects qualitatively, we use the above object detector/localizer to determine the type and position of objects in the ground truth and the generated image. We estimate a scene graph for each image, in which the detected objects and the image center are the vertices. The directed edges are given by the left-right and front-back relations between the vertices. To compute a relational similarity metric on scene graphs, we determine how many of the ground truth relations are present in the generated image:</p><formula xml:id="formula_8">rsim(E Ggt , E Ggen ) = recall × |E Ggen ∩ E Ggt | |E Ggt |<label>(6)</label></formula><p>where "recall" is the recall over objects detected in the generated image w.r.t objects detected in the ground truth image. E Ggt is the set of relational edges for the ground truth image corresponding to vertices common to both ground truth images and generated images, and E Ggen is the set of relational edges for the generated image corresponding to vertices common to both ground truth images and generated images. The graph similarity for the complete dataset is reported by taking the mean of the final time-step value for each example over the entire dataset. This metric is a lower bound on the actual relational accuracy, as it penalizes relations based on how the objects are positioned in the ground truth image. The same instructions may, however, permit different relationship graphs. We present some examples of low-scoring to high-scoring images on this metric as well as additional discussion on rsim in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>We experimented with different variations of our architecture to test the effect of each component. We define the different instantiations of our architecture as follows:</p><p>• Baseline The simplest version of our model. The discriminator loss only includes the adversarial terms </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Fusion</head><p>Model L fake and L real . The generator is only conditioned using the context-aware condition:</p><formula xml:id="formula_9">LD wrong fG t−1 Laux concat subtract Baseline Mismatch G prior Aux D Concat D Subtract</formula><formula xml:id="formula_10">x t = G(z, h t ).</formula><p>As for the discriminator, it has no access to the previous time-step image features. Onlyx t is encoded using E D and then passed to the discriminator D without any fusion operations. • Mismatch The L wrong term is added to the discriminator loss. The rest of the model is similar to the baseline. • G prior We condition the generator on the context-free condition f Gt−1 in addition to h t as in equation <ref type="formula" target="#formula_0">(1)</ref>. • Aux In this model we add the L aux term to both the generator and discriminator losses. The loss functions for this model follow equations <ref type="formula" target="#formula_3">(3)</ref> and <ref type="formula">(4)</ref>. • D Concat In this model, the discriminator's input is the fused features from x t−1 and x t (orx t ) encoded using E D . The fusion is a simple concatenation across the channels dimension. • D Subtract This is the same as "D Concat" except for the fusion operation, which is an element-wise subtraction between the feature maps. • Non-iterative The non-iterative baseline uses the same model as the "Mismatch" baseline. All the input instructions are concatenated into one instruction and the final image is generated in a single-step. A summary of the components that are present for each model we test in the ablation study is provided in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results</head><p>Quantitative Results: We present the results of the ablation study in <ref type="table" target="#tab_2">Table 1</ref>. As expected, among the iterative mod-els, the Baseline model has the weakest performance on all the metrics for both datasets. This is due to the fact that it needs to construct a completely new image from scratch every time-step; there is no consistency enforced between successive generations. As for the Mismatch model, despite suffering from the same problems as the Baseline, training D to differentiate between wrong and right (image, instruction) pairs leads to generated images that better match the instructions. This is clear in <ref type="table" target="#tab_2">Table 1</ref> as the performance improves on all metrics compared to the Baseline.</p><p>The G prior model tries to enforce consistency between generations by using the context-free condition f Gt−1 . Adding this condition leads to a significant improvement to all the metrics for the i-CLEVR dataset. However, for the CoDraw dataset, it shows a less significant improvement to recall and relational similarity, while precision degrades. These results can be explained by the fact that i-CLEVR has much more complex relationships between objects and the instructions have a strong dependence on the existing objects in the scene. Therefore, the model benefits from having access to how the objects were placed in the most recent iteration. As for CoDraw, the relationships among objects are relatively simpler. Nevertheless, adding the contextaware condition helps with placing the objects correctly as shown by the improvement in the relational similarity metric. A possible drawback from using the context-free condition is that it is harder to recover from past mistakes, especially if it has to do with a large objects. This drawback can explain the drop in precision.</p><p>For the Aux model, it had different effects on the two datasets. For CoDraw, it helped improve recall and relational similarity, but caused a significant decrease in precision. For i-CLEVR, it helped improve precision with hurting the recall and relational similarity. This different behavior for each dataset can be explained by the types of objects that are present. While for CoDraw, there are objects that are almost always present like the girl or the boy, for i-CLEVR there is high randomness in objects presence. Adding the auxiliary objective encourages the model to make sure the frequent objects are present, leading to the increase in recall while hurting precision. Finally, we observe that giving D access to the previous image x t−1 shows im-  provement on almost all the metrics for both datasets. We also observe that subtraction fusion consistently performs better than concatenation fusion and outperforms all other models for both datasets. This indicates that encouraging the discriminator to focus on the modifications gives a better training signal for the generator.</p><p>The Non-iterative model performs worse than all of the iterative models. This is likely because the language encoder has difficulty understanding dependencies and object relationships in a lengthy concatenated instruction. The benefit of using an iterative model is more visible in the i-CLEVR dataset since in it, the spatial relationships are always defined in terms of existing objects. This makes it very difficult to comprehend all the relationships across different turns in a single step. By having multiple steps, iterative generation makes this task easier. The results of this experiment make a case for iterative generation in complex text-conditional image generation tasks that have traditionally been performed non-iteratively.</p><p>Qualitative Results: We present some examples of images generated by our model in <ref type="figure" target="#fig_3">Figure 5</ref>. Due to space constraints, more example images are provided in the appendix. On CoDraw, we observe that the model is able to generate scenes consistent with the conversation and generation history and gets most of the coarse details correct, such as large objects and their relative positions. But it has difficulty in capturing fine-grained details, such as tiny objects, facial expressions, and object poses. The model also struggles when a single instruction asks to add several objects at once. For i-CLEVR, the model captures spatial relationships and colors very accurately as demonstrated in <ref type="figure" target="#fig_3">Figure 5</ref>. However, in some instances, the model fails to add the fifth object when the image is already crowded and there is no space left to add it without moving the others. We also experimented with using an intermediate ground truth image as the initial image at test time and the model was able to generalize and place objects correctly in that scenario as well. The results of this experiment are presented in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We presented a recurrent GAN model for the GeNeVA task and show that the model is able to draw reasonable images for the provided instructions iteratively. It also significantly outperforms the non-iterative baseline. We presented an ablation study to highlight the contribution of different components. Since this task can have several plausible solutions and no existing metric can capture all of them, we proposed a relational similarity metric to capture the possible relationships. For future research directions, having a system that can also ask questions from the user when it needs clarifications would potentially be even more useful. Collecting photo-realistic images, transitions between such images, and annotations in the form of instructions for these transitions is prohibitively expensive; hence, no photo-realistic dataset appropriate for this task publicly exists. Such datasets are needed to scale this task to photorealistic images.</p><p>All of the evaluation metrics for the GeNeVA task rely on the object detector and localizer network and hence, it needs to have high detection and localization performance. We report the performance of the trained object detector and localizer network on the test set images of both CoDraw and i-CLEVR datasets in <ref type="table">Table 3</ref>.  <ref type="table">Table 3</ref>. Mean test set Precision, Recall, and F1-Score for the object detector and localizer network. Normalized Root Mean Squared Error (NRMSE) is the root mean square distance between the localizer's predicted and ground truth object centroids normalized by the image dimensions. ↑: higher is better, ↓: lower is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Relational Similarity metric: rsim</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Additional details</head><p>For both CoDraw and i-CLEVR datasets, we determine front-behind and left-right relationships by comparing the coordinates of their centre predicted by the object detector and localizer network. We run the network on both ground truth and generated images to predict the centre coordinates (rather than using perspective coordinates provided by the renderer as these are only available for the ground truth for i-CLEVR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Appropriateness for evaluation</head><p>The CoDraw and i-CLEVR datasets are constructed such that there is at most one object of each object class per image. Hence, we train the object detector to predict only binary presence of each object class and the localizer regresses only one set of centroid coordinates per class. This design breaks if multiple instances of an object class are generated or if the object detector frequently misclassifies objects. However, qualitatively assessing the generated im-ages, over-generation is rare and the object detector accuracy is very high (cf. <ref type="table">Table 3</ref>).</p><p>Since all objects in ground truth scenes occur at most once, generations with multiple instances per class are outof-distribution. The model cannot learn to exploit this flaw, since rsim is not optimized during training. Thus, overgeneration is not a failure mode we have observed. Additionally, rsim is position-sensitive: over-generation would not necessarily produce the correct relative positions of objects since the object localizer only localizes one instance per class. For datasets with multiple instances per class, the rsim metric should be modified such that the denominator is the union of ground-truth and predicted detections, which will penalize over-generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Shortcomings</head><p>Quantitative measures for attributes like "boy kicking" are currently a missing piece. We share this shortcoming with all text-to-image GAN-based methods and most of the conditional GAN literature. At the moment, conditional GANs are evaluated using Inception Score (IS) and FID, both of which do not account for attributes. An evaluation metric that accounts for attributes will be a valuable contribution for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Comparison with existing metrics</head><p>The Scene Similarity Metric (SSM) used by Kim et al. <ref type="bibr" target="#b7">[8]</ref> is well-suited for the setting of predicting object location and attributes. SSM is a weighted score across recall and considers objects that face the wrong direction, incorrect expressions, poses, clip art size, distance between object positions in ground truth and predicted image, and left-right and front-behind relationships. SSM achieves the highest score for exact reconstructions. In our case, we want to not just reward reconstructions but also plausible generations where left-right, front-behind relationships are correct. Our main focus here is to generate complete images instead of predicting object location and attributes. Several attributes, such as boy or girl poses / expressions, or object directions have lower detector accuracy and consequently would reduce metric reliability (cf. Section B.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Qualitative evaluation</head><p>We provide generated image examples with scores spread out between the minimum value (0) and maximum value (1) on the rsim metric in <ref type="figure" target="#fig_6">Figure 6</ref>. This is to provide readers with a more intuitive understanding of how the metric captures which spatial relationships match between the ground truth and the generated image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generation Examples</head><p>We present selected examples generated using our best model (D Subtract) on two datasets. Examples generated for CoDraw are presented in <ref type="figure">Figure 7</ref> and examples generated for i-CLEVR are presented in <ref type="figure">Figure 8</ref>. We also present random examples from all the models present in the ablation study for a qualitative comparison on the Co-Draw dataset. These are shown in <ref type="figure" target="#fig_8">Figure 9</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Generalization to new background images</head><p>GeNeVA-GAN was trained using the empty background image as the initial image. We ran an experiment where we used a different image (intermediate ground truth image from the test set containing objects) as the initial image. We present generated examples from this experiment in <ref type="figure" target="#fig_6">Figure 16</ref>. The model is able to place the desired object at the correct location with the correct color and shape over the provided image. This shows that the model is capable of generalizing to a background it was not trained on and it can understand the existing objects from just the initial image without any instruction history for placing them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. i-CLEVR Dataset Generation</head><p>To generate the image for each step in the sequence, an object with random attributes is rendered to the scene using Blender <ref type="bibr" target="#b23">[24]</ref>. We ensure that all objects have a unique combination of attributes. Each object can have one of 3 shapes (cube, sphere, cylinder) and one of 8 colors. In contrast to CLEVR, we have a fixed material and size for objects. For the first image in the sequence, the object placement is fixed to the image center. For all the following images, the objects are placed in a random position while maintaining visibility (not completely occluded) and at a minimum distance from other objects.</p><p>To generate instructions, we use a simple text template that depends on the instruction number. For example, the second instruction in the sequence will have the following template: From the third instruction onward, the object position is described relative to two objects. These two objects are chosen randomly from the existing objects in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional implementation details</head><p>We use 300-dimensional GloVe 3 word embeddings for representing the words in each instruction q t . These word embeddings are encoded using a bi-directional-GRU to obtain a 1024-dimensional instruction encoding d t . All state dimensions for the higher level GRU R are set to 1024. The output of the conditioning augmentation module is also 1024-dimensional.</p><p>The code for this project was implemented in Py-Torch <ref type="bibr" target="#b24">[25]</ref>. For the generator and discriminator optimizers, "betas" was set to (0.0, 0.9) and weight decay was set to 0. The learning rates for the image encoding modules were set to 0.006. Gradient norm was clipped at 50. For each training experiment, we used a batch size of 32 over 2 NVIDIA P100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional language encoder experiments</head><p>We experimented with using skip-thought encoding for sentences instead of training the bi-directional-GRU encoder over GloVe embeddings. For the paper, we chose to use the latter since it performed better.</p><p>We also experimented with passing the previous image through the language encoder, but observed that it was easier for the model to generate an accurate image when the previous image features are passed to the Generator directly. Drawer: ready to draw ? Teller: Medium sun is on the left corner fully visible.</p><p>Teller: Below sun sits a mad girl with legs on front she faces right and hand touches the left border a 1 2 head is above horizon. Drawer: ok.</p><p>Teller: The girl is big. A fire is on front feet of girl like 1 2 ".</p><p>Drawer: ok.</p><p>Teller: A grill is just next to fire the grill is a little lower than top flame. Drawer: ok.</p><p>Teller: A small pine is on right side 1 4 " left side is cut also the tip is cut. Drawer: ok</p><p>Teller: In top left hand corner is medium sun cut off on top and on side. Drawer: I am a patient worker ready to start.</p><p>Teller: In middle of screen is a medium pine tree trunk starts dead middle of screen. Drawer: Got it.</p><p>Teller: A large boy is sitting cross legged almost in left corner slightly higher and to right he is facing right . Drawer: ok.    Teller: on his mouth is an o shape Teller: to the right of the boy is a dog with a blue collar Teller: above the boys left hand in the blue sky is a yellow ball</p><p>Drawer: may you please tell the first thing to draw Teller: there is a small tree on the right side of the scene sort of in the background Drawer:</p><p>Teller: there is a bear to the left but in the foreground of the tree Drawer: what next Teller: the bear is small Drawer: Teller: the girl is to the left facing left looking at the bear with her leg out scared facing right Drawer:</p><p>Teller: there is a small dog below the girl and a angry boy to left facing left with a racket in the left hand Drawer: what is next</p><p>Teller: there is a small helicopter at the top in the sky in between the boy and girl Drawer: right above the bear Teller: there is a cloud in the top left corner it is cut off on the top and left sides two puffs on the right and 3 puffs on the bottom Teller: an inch from the right of the cloud are small balloons the orange balloon is on the right   Drawer: ready Teller: there is a medium in the center of the sky just below the top edge Drawer: medium cloud Teller: oh sorry medium sun the medium cloud is down and to the right in the sky Teller: there is a small oak tree on the left an inch away from the left edge hole facing right 2 3s of the leaves are above the horizon Teller: on the right side the kids are both medium sized and facing left jenny is happy jumping half inch from the right edge  <ref type="figure" target="#fig_6">Figure 16</ref>. When GeNeVA-GAN is provided with an initial image different from the background image used during training, it still adds the desired object with the right properties at the correct location. The model was not trained in this setting and the success of this experiment demonstrates that it has learnt to preserve the existing canvas, understand the existing objects, and add new objects with the correct relationships to existing objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Add a purple cube behind it on the rightAdd a blue sphere in front of it on the left and in front of the yellow sphere on the left Add a yellow sphere at the center</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Example sequence of image-instruction pairs from the i-CLEVR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Drawer: ready Teller: large apple tree left side trunk start 2 3 way up green and about 1 and 1 4 inches from left side Teller: big cloud right side almost touching apple tree 1 2 inch up into blue Teller: below the cloud full size girl her head touches top of green hands over head centered under cloud Teller: boy 1 1 2 to left of girl under right side of tree same height as girl facing right hands out to right holding a football in both Teller: sandbox medium size lower left corner facing right left side off screen lower right corner is equal to end of tree trunk Drawer: yes Add a blue cube at the center Add a red cylinder behind it on the left Add a purple sphere in front of it on the right and in front of the blue cube Add a cyan cylinder behind it on the right and in front of the red cylinder on the right Add a yellow sphere in front of the red cylinder on the left and on the left of the blue cube</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Example images generated by our model (D Subtract) on CoDraw (top row) and i-CLEVR (bottom row); shown with the provided instructions. We scale images from both datasets to 128x128 in a pre-processing step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(Baseline), Figure 10 (Mismatch), Figure 11 (G prior), Figure 12 (Aux), Figure 13 (D Concat), Figure 14 (D Subtract), and Figure 15 (Non-iterative).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>"</head><label></label><figDesc>Add a [object color] [object shape] [relative position: depth] it on the [relative position: horizontal]"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Column 1: Generated final image; Column 2: Ground truth final image; Column 3: List of objects detected in the generated and ground truth image, the recall on object detection, the value of the relational similarity (rsim) metric. The examples have been selected to qualitatively show examples with diverse score values between the minimum (0) and the maximum (1) values of the rsim metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Generation examples from our best model (D Subtract) for the CoDraw dataset. Add a purple cylinder at the center Add a green sphere on the left of it Add a gray sphere in front of it on the right and in front of the purple cylinder on the left Add a red cylinder behind it on the right and behind the green sphere on the right Add a gray cylinder behind the green sphere on the right and behind the purple cylinder on the left Add a brown cylinder at the center Add a green sphere behind it on the left Add a cyan cylinder in front of it on the left and in front of the brown cylinder on the left Add a yellow cube in front of the green sphere on the right and in front of the brown cylinder on the right Add a brown sphere behind it and behind the cyan cylinder on the right Add a brown cube at the center Add a red cube in front of it on the left Add a purple sphere in front of it on the right and in front of the brown cube on the right Add a cyan cylinder behind it on the right and on the right of the brown cube Add a green cube behind the red cube on the right and behind the brown cube on the right Add a cyan cube at the center Add a brown sphere on the right of it Add a blue sphere behind it on the left and behind the cyan cube on the right Add a red sphere in front of the brown sphere on the left and in front of the cyan cube on the right Add a purple cylinder behind it on the left and in front of the cyan cube on the left Add a yellow sphere at the center Add a red cube behind it Add a blue sphere in front of it on the left and behind the yellow sphere on the left Add a gray sphere in front of it on the right and in front of the red cube on the right Add a green cylinder in front of the blue sphere on the right and behind the yellow sphere on the right Add a cyan cube at the center Add a red cylinder in front of it on the right Add a purple cylinder behind it on the right and behind the cyan cube on the right Add a brown sphere in front of it on the left and in front of the red cylinder on the left Add a yellow sphere in front of the purple cylinder on the left and behind the red cylinder on the left Generation examples from our best model (D Subtract) for the i-CLEVR dataset. Instructions where the model made a mistake are marked in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Random selection of examples generated by our Baseline model for the CoDraw dataset. Drawer: ready Teller: 1 girl happy running facing right 0 2 inch from bottom to top and 0 2 inches from left to right with a chef Teller: hat on her a table 1 and a half inches from left to right 1 2 inches from bottom to top with a pizza in the middle Teller: and and a hot dog facing left to the right of the pizza a fire 0 1 inches from bottom to top 0 4 inches from right to left and above a Teller: tent facing left and top of the tent is above the horizon line 0 1 inches and right is cover a little bit and above Teller: a air balloon small like 1 2 inches from right to left and 0 2 inches from top to bottom and that 's it Drawer: what 's in the picture Teller: in the top right is a sun covered partially by clouds Drawer: is it a large sun Teller: in the middle left is a helicopter Drawer: which way is it facing Teller: yes a large sun Teller: heli is facing to the right Teller: tail is to the left Teller: on bottom right is a girl in pink with left arm raised</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 .</head><label>14</label><figDesc>Random selection of examples generated by our D Subtract model for the CoDraw dataset.Drawer: what is there Teller: big thunderbolt on left bolt facing right touching on left edge close to top Drawer: and Teller: big raindrop cloud to the right of thunderbolt cloud cutting it off on right side a little big shocked jenny with arms up Drawer: where is she Teller: head right below horizon sad big sitting mike with legs facing right is beside her Drawer: and Teller: he 's wearing a star hat soccer ball is covering his left foot and shin Drawer: and Teller: ready Drawer: and ready Teller: upper right corner large sun with right edges a bit cut off and top cut off Teller: under sun happy boy standing facing left with right arm up his shoulders just above horizon line Teller: he is wearing a pirate hat it touches on of the sun tips on the left side Teller: happy girl kicking on left side she is about 1 5 inches in from left side her mouth is at the horizon line Drawer: got it Teller: just a tiny bit off of girls kicking foot is a beach ball a cloud is over the girl towards the right center Drawer: is the cloud on the right or the sun you said sun upper right cornerTeller: boy left side kicking leg facing right his half torso aligns with horizon he is shocked Drawer: go Teller: finger away from his leg soccer ball its bottom part touches horizon Teller: right side medium tree 1 4 cut off right side and trunk half way in grass with slight cut off as well right side hole facing left Drawer: go Teller: plain cloud top middle top part cut off big size in front of tree dog its legs behind completely cut off and it 's facing left Teller: near dog is a big cat its tail cover 's dog 's front leg slightly and facing right and then girl sitting smiling facing right Drawer: ready Teller: top left facing left one 1 4 inch from side blade touch top small helicopter facing left Drawer: what 's in the left the helicopter Teller: nothing it is a 1 4 inch from side flying left Drawer: got it it 's tiny right Teller: yes Teller: below copter is large boy facing right arms out mouth open neck at horizon Teller: right of boy his top hand is on first plank is a large picnic table left top corner is highest point pie is there in corner Drawer: which side is the pie Teller: right of pie is large girl facing left standing with smile no teeth one arm up and one down pie top left corner Drawer: where is she to the horizon and she is in front of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 .</head><label>15</label><figDesc>Random selection of examples generated by our Non-iterative model for the CoDraw dataset.(a) Left: Initial Image Right: Final Image Instruction: Add a yellow cylinder behind the gray cylinder on the right and behind the yellow cube on the right (b) Left: Initial Image Right: Final Image Instruction: Add a cyan cube behind the brown cylinder on the left and behind the purple cylinder on the left (c) Left: Initial Image Right: Final Image Instruction: Add a gray sphere behind the blue cylinder on the right and behind the purple sphere on the right</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Teller: top left corner big sun, orange part cut. right side far right medium apple tree. i see 4 applesTeller: left side girl big size, running, facing right. head above horizon.Teller: covering the tree, on the right side of the scene is a boy, kicking, facing left. head on green part. big size, black glasses. kicking ball.Teller: make tree a size bigger, move it up and left a bit. boys hand covers trunk. An example from the CoDraw<ref type="bibr" target="#b7">[8]</ref> dataset. Each example from the dataset involves a conversation between a Teller and a Drawer. The Teller has access to a final image and has to iteratively provide text instructions and feedback to the Drawer to guide them to draw the same image. The Drawer updates the image on receiving instructions or feedback. In the original CoDraw setup, the Drawer predicted the position and attributes of objects to compose a scene. In GeNeVA, we task systems with generating the images directly in pixel space.</figDesc><table><row><cell>Turn 1</cell><cell>Turn 2</cell><cell>Turn 3</cell><cell>Turn 4</cell></row><row><cell>Drawer: ok ready</cell><cell>Drawer: ok</cell><cell>Drawer: ok</cell><cell>Drawer: ok</cell></row><row><cell>Figure 2.</cell><cell></cell><cell></cell><cell></cell></row></table><note>nism over words in the caption. It also introduced an image- text similarity module which encouraged generating images more relevant to the provided caption. Departing from purely caption data, Sharma et al. [2] proposed a non-iterative model called ChatPainter that gen- erates images using dialogue data. ChatPainter condi- tions on captions from MS COCO and a Recurrent Neural Network (RNN)-encoded dialogue relevant to the caption (obtained from the Visual Dialog (VisDial)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Score rsim(EG gt , EG gen ) Precision Recall F1-Score rsim(EG gt , EG gen ) Results of the GeNeVA-GAN ablation study on the CoDraw and i-CLEVR datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CoDraw</cell><cell></cell><cell></cell><cell></cell><cell>i-CLEVR</cell><cell></cell></row><row><cell cols="4">Model F1-Non-iterative Precision Recall 50.60 43.42 44.96</cell><cell>22.33</cell><cell>25.49</cell><cell>20.95</cell><cell>22.63</cell><cell>11.52</cell></row><row><cell>Baseline</cell><cell>55.61</cell><cell>42.31</cell><cell>48.05</cell><cell>25.31</cell><cell>69.09</cell><cell>56.38</cell><cell>62.08</cell><cell>45.19</cell></row><row><cell>Mismatch</cell><cell>62.47</cell><cell>48.95</cell><cell>54.89</cell><cell>32.74</cell><cell>71.15</cell><cell>60.57</cell><cell>65.44</cell><cell>50.21</cell></row><row><cell>G prior</cell><cell>60.78</cell><cell>49.37</cell><cell>54.48</cell><cell>33.60</cell><cell>82.80</cell><cell>77.22</cell><cell>79.91</cell><cell>63.93</cell></row><row><cell>Aux</cell><cell>54.78</cell><cell>51.51</cell><cell>53.10</cell><cell>33.83</cell><cell>83.63</cell><cell>75.63</cell><cell>79.43</cell><cell>55.36</cell></row><row><cell>D Concat</cell><cell>66.38</cell><cell>51.27</cell><cell>57.85</cell><cell>33.57</cell><cell>88.47</cell><cell>83.35</cell><cell>85.83</cell><cell>70.22</cell></row><row><cell>D Subtract</cell><cell>66.64</cell><cell>52.66</cell><cell>58.83</cell><cell>35.41</cell><cell>92.39</cell><cell>84.72</cell><cell>88.39</cell><cell>74.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Description of the components present in each model we test in the ablation study.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Random selection of examples generated by our Mismatch model for the CoDraw dataset. Random selection of examples generated by our G prior model for the CoDraw dataset. Random selection of examples generated by our Aux model for the CoDraw dataset. Teller: on the left an inch from the edge is a boy Drawer: what is he doing Teller: he is facing right standing one hand up teeth showing and holding a racket with one hand that is in the air Teller: next to him a medium tree hole facing left head touches the tip of last branch truck aligns with his waist Teller: on the right 1 inch from edge is a girl sad looking left one hand in the air head aligns with the boy 's Drawer: her left hand cut off Teller: above her is a small cloud right above her can i check Teller: no about 2 cm from the edge the hand Teller: move her 1 more cm from the edge she is holding a yellow small ball in the hand in air</figDesc><table><row><cell>Drawer: ready Teller: a small bear close to the right side one finger off picture small sliding on his left side bear left foot touching the sliding Figure 10. Teller: medium sun on your right hand with a small half apple tree under it Teller: big cloud top left side Drawer: got it Drawer: i 'm ready Teller: boy flying a kite Drawer: ready when you are Teller: right side medium pine tree cut in half by right edge and cut top Drawer: got it Drawer: what is in the scenery of the image Teller: medium pine tree on right trunk is about 1 inch from bottom and trunk is 1 2 from right top is cut off on top and right Drawer: what 's next Teller: ready Drawer: what what is the first object and location Teller: small table middle of green Drawer: next Figure 11. Drawer: go Teller: large rain Teller: left from pine tree big Teller: table medium in the center front Teller: small snake left of apple tree snake facing left Teller: on the right side is a swing big size Drawer: any Teller: wearing blue t shirt Teller: blue shoes Teller: lighter blue shorts Drawer: got it Teller: has black hair Drawer: i have all of him Teller: sunny outside size bear facing left head above horizon Teller: on left is medium apple tree cut off on left trunk is halfway down grass big sun in center of two trees Drawer: what 's next Teller: there is a pine to the left med size upper peck can not see Drawer: next cloud left corner touches side cut off on top drops almost touch grass Drawer: next Teller: large rocket on right tip of cloud flying left with very small girl sad legs out sitting on its upper wing Drawer: sitting on rocket the rocket is middle scene Drawer: where is jenny and mike Teller: on left hand side of the screen 2 inches above bottom is a pine tree cut off at top Teller: straight down from tree is jenny legs crossed facing right right arm in the air Teller: large cloud in left corner top and left are off screen Teller: large apple tree right side top of trunk lines up with horizon right side and top of screen Drawer: what do we have Teller: med sun right corner Drawer: and Teller: middle of green with trees half in blue half in green is a apple tree med size Drawer: and Teller: large slide to the left facing right with owl sitting on top of platform Drawer: top of handles or where we stand Teller: small cloud in top center a bit to the right owl is on the platform Drawer: go trunk 1 4 inch from bottom cut off top and right Teller: large sand box on left mound on left half bottom edge cut off and left corner cut off mound fully visible Teller: there is a large sun in the left hand corner Teller: there is a small girl facing left she has one leg in the air 3 inches from the right 1 2 an inch from the bottom Teller: top left corner is large sun half of it hidden Drawer: what else Teller: top right corner is large cloud half of it hidden Drawer: Teller: on left side of screen is a large girl standing with her arms in front of her facing right her eyes are even with the horizon line Drawer: cut a little bit the yellow part apple tree cut top Teller: sun top right almost touching top 1 " from left medium size Teller: big bear at left facing right arm pits at horizon parts cut off Drawer: ready Teller: big oak on right hole facing right hole almost touching horizon Drawer: if its large it is huge how much is cut off on the right Teller: smiling big hands out mike on right left trunk point touching his back hair above horizon Drawer: go Teller: small bushy tree facing left owl on right middle of tree Teller: to the left of tree is a medium sandbox mound on right close to bottom Drawer: next Teller: tree hole facing left cut off from right side a little bit Teller: bumblebee with ear touching the bottom left of tree Drawer: got it Figure 12. Teller: large pine tree on right Teller: big sun on left only top Teller: on right sun small top hiding a bit of the sun trunk facing to the right side</cell><cell>Teller: airplane 1 4 inch to the right of the balloon the nose of the plane is in line with the yellow balloon it faces left Teller: medium pine tree behind the table one inch Teller: small bushy tree is on your left hand side hole facing left 1 2 inch from side 1 inch from top Teller: girl on the left side neg horizon one arm up facing Teller: a girl next to a grill Teller: left side running angry big size girl facing right horn hat holding football in right hand our right Teller: on the right side of apple tree trunk is sad boy sitting legs out facing right hes 1 inch from bottom very close to trunk Drawer: what 's next Teller: med sun in upper right corner Drawer: any of it cut off Teller: rocket overcloud large regular cloud on right side cut off on top and side a bit surprised boy legs out facing left under cloud Drawer: next Teller: next to jenny is mike same level standing facing right with arms out mouth open Teller: in front of trunk little over to the right of trunk by right side is a soccer ball Teller: med boy standing on right of tree Drawer: face expression and where are his hands Teller: he has a tennis ball in right hand he is smiling showing teeth Drawer: and Teller: there is a medium to small apple tree on right half in blue half in green right side of tree cut off a bit Drawer: go Teller: middle of sandbox large girl running right big toothy smile Teller: small duck facing girl bill at her foot like she is going to kick the duck but move the duck 1 4 inch away about an inch from bottom Teller: girl is wearing a chefs hat sunglasses and is holding a pink shovel in her right hand Drawer: mike hand front stand facing left head touches the tree Teller: medium to small size girl about 3 " away from bear facing left angry arms out holding a bat in right hand head in the blue area right happy face Drawer: got it Teller: small hot balloon on left 1 4 from top 1 in from left big tent on left facing right cut off slightly on back top above horizon Teller: girl sitting in left corner indian style smiling one arm up Drawer: next Teller: girl sitting smiling facing right hand behind her crown Teller: on right of tree happy one inch from side wearing</cell><cell>Teller: angry boy sitting below the plane facing left about 1 2 inch grass above and below him Teller: sad girl standing far left part hand missing sad face medium Teller: large table in left hand corner slanted right north with medium owl on right end facing right Teller: one part cut from swing just a bit from the right Teller: i am sorry only one girl in the scene Teller: next to a girl Teller: on the grill Drawer: is a burger Teller: there are hamburgers Teller: now behind girl they overlap running angry boy facing right purple glasses witch hat on facing right half body above horizon Drawer: got it Teller: to right of boy align with his hand is a fire to right of fire is girl kneeling smiling one arm up girl and boy small Drawer: what 's next Teller: no sun is whole straight down from sun is a boy standing with a laugh on face Drawer: facing left Teller: cat facing boy 1 2 inch to left of his feet Drawer: next Teller: on the right hand side inch from the bottom is a duck facing jenny and mike Teller: left side large girl angry face sitting cross legged facing right Teller: right arm sticking out across from him is a girl almost to the left edge left arm down right hand with a ball glove Drawer: smiling please give more elaborations Teller: a dog directly under tree facing left Drawer: size Teller: bottom left corner of sandbox large cat looking right top right corner sandbox is baseball Teller: soccer ball to the left of the duck to the left 1 4 inch and up 2 inches Teller: on right side of screen is large boy one hand on hip facing the girl with angry expression his neck is even with horizon line Drawer: got it what else mike is medium box mound on left left corner is hidden a small duck inside the sandbox Teller: boy behind her about 1 5 " away arms up startled glove on left hand rainbow hat on his eyes slightly below horizon Drawer: Teller: next to the right of the cloud is a basketball a bit over the cloud Drawer: Teller: smiling big hands out jenny is in front of left opening tent Teller: boy in right corner sitting indian style smiling with arms open both facing right Drawer: next Teller: crown almost touches the horizon Drawer: got it smiling his feet r half inch from hers and raised up a little he wears a beanie with top of it just at horizon Teller: below sun small sand Teller: boy faces girl sitting</cell><cell>Teller: a girl sits to the left of the boy facing him feet almost touching surprised wearing viking hat top brown part of hat touches horizon Teller: boy sitting on her right side look mad Teller: a boy is on the swing Teller: bushy tree looks like it the right sit legs cross 's sitting on table Teller: 3 of them Teller: she is wearing pink overall holding ketchup in her hand and she is wearing glasses on Teller: movie girl down a bit move boy right a bit lines with girl next to girls what is a frisbee Teller: girls right hand overlaps pine tree a little left side of sun is overlapped by apple tree below fire is ketchup left mustard right Drawer: next Teller: yes boy is almost at bottom of screen Drawer: what else Teller: there is a girl standing at corner of table looks likes she is running smiling Drawer: next Teller: i will check and send adjustments Drawer: yes i do n't have the girl tell me where is the girl Teller: she is sitting on the rockets upper wing her back arm is under the window Drawer: check Teller: straight above duck is soccer ball Teller: neck on horizon line she 's holding a baseball in her up hand and wearing rainbow hat Teller: yes smiling looking right she is standing her middle is on the line of green Drawer: and Teller: a girl standing in front of slide arms up smiling dog looks small Teller: girl is wearing pirate cap will check when ready Teller: boy like he is running facing the duck and girl they playing keep away from duck his arms up and looks like he is running mouth open Teller: he has glasses on Teller: between them is a large grill Drawer: right side of sandbox crossed legs and one hand up facing right head cover right corner of sandbox Teller: move boy down about an inch face him to left and his striped hat and we are good surprised face facing left color hat baseball glove Drawer: got it Teller: she has an owl sitting on her left wrist her hand is in the dark opening Drawer: she is facing him and large owl right Teller: under boys right hand is cup in sand straw to left to left of cup is medium beach ball Teller: duck between the two with ducks feet level to boys Teller: medium jenny sits on top foot</cell></row></table><note>Figure 13. Random selection of examples generated by our D Concat model for the CoDraw dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>table</head><label></label><figDesc>Teller: girl in front of table nose at horizon top right corner of table is ketchup Drawer: got it Teller: 1 2 inch from right side and 1 2 inch from horizon is large grill</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://azure.microsoft.com/en-us/services/ cognitive-services/spell-check/ 2 https://github.com/facebookresearch/ clevr-dataset-gen</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://nlp.stanford.edu/data/glove.840B.300d.zip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Philip Bachman for valuable discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cs.AI</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Chatpainter: Improving text to image generation using dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dendi</forename><surname>Suhubdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CoDraw: Collaborative drawing as a testbed for grounded goal-driven communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Visual Dialog</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LR-GAN: Layered recursive generative adversarial networks for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ST-GAN: Spatial transformer generative adversarial networks for image compositing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Hsuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
	</analytic>
	<monogr>
		<title level="j">Geometric GAN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>stat.ML]</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Blender -a 3D modelling and rendering package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blender Online Community</surname></persName>
		</author>
		<ptr target="http://www.blender.org" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Explanation: None of the correct objects are drawn. Objects detected in generated image: cube gray, cylinder gray, cylinder purple, cylinder cyan Objects detected in ground truth image: cube purple, sphere yellow, cylinder gray, cylinder brown</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<idno>cylinder cyan Recall: 0.4 rsim: 0.25</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Objects detected in generated image: cube gray, cube brown, sphere gray, sphere blue Objects detected in ground truth image: cube yellow, sphere yellow, cylinder green, cylinder brown</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Explanation: The cyan and gray cylinders are the only two objects detected in the generated image from the five ground truth objects detected</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Objects detected in generated image: cube red, cube green, sphere brown, cylinder gray Objects detected in ground truth image: cube red, sphere red, sphere brown, sphere yellow</title>
		<idno>brown Recall: 0.4 rsim: 0.35</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Explanation: The red cube and brown sphere are detected common to both images</title>
		<imprint/>
	</monogr>
	<note>Most of the relationships of these two and the center are correct</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Objects detected in generated image: cube gray, cube red, cube yellow, sphere purple Objects detected in ground truth image: cube gray, cube red, cube blue, cube yellow</title>
		<idno>sphere purple Recall: 0.8 rsim: 0.45</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Explanation: Only the blue cube is not detected in the generated image</title>
		<imprint/>
	</monogr>
	<note>Several spatial relationships of the common objects and the center are incorrect</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Objects detected in generated image: sphere brown, sphere cyan, cylinder blue, cylinder purple, cylinder cyan Objects detected in ground truth image: cube cyan, sphere brown, cylinder blue, cylinder purple</title>
		<idno>cylinder cyan Recall: 0.8 rsim: 0.675</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Explanation: Cyan cube detected in ground truth image is missing from the generated image</title>
		<imprint/>
	</monogr>
	<note>Most spatial relationships of the common objects and center are correct</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Objects detected in generated image: sphere green, sphere purple, cylinder green, cylinder purple, cylinder cyan Objects detected in ground truth image: sphere green, sphere purple, cylinder green, cylinder purple</title>
		<idno>cylinder cyan Recall: 1.0 rsim: 0.76</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Explanation: All the objects are detected correctly but some of the spatial relationships are incorrect. Objects detected in generated image: cube red, cube blue, cube yellow, sphere brown, cylinder blue Objects detected in ground truth image: cube red, cube blue, cube yellow, sphere brown</title>
		<idno>blue Recall: 1.00 rsim: 1.00</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Explanation: All the objects are detected correctly and are in the correct relative positions</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
