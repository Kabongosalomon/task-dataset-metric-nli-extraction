<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PHASE CONDUCTOR ON MULTI-LAYERED ATTEN- TIONS FOR MACHINE COMPREHENSION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liu</surname></persName>
							<email>ruil@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
							<email>weiwei@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguang</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computational &amp; Systems Biology</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Chikina</surname></persName>
							<email>mchikina@pitt.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computational &amp; Systems Biology</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PHASE CONDUCTOR ON MULTI-LAYERED ATTEN- TIONS FOR MACHINE COMPREHENSION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question-aware passage attention model and selfmatching attention model. Our research proposes phase conductor (PhaseCond) for attention models in two meaningful ways. First, PhaseCond, an architecture of multi-layered attention models, consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow. Second, we extend and improve the dot-product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives. We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both stateof-the-art single-layered and multiple-layered attention models. We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models. * Authors' contributions are equally important to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Attention-based neural networks have demonstrated success in a wide range of NLP tasks ranging from neural machine translation , image captioning <ref type="bibr" target="#b17">(Xu et al., 2015)</ref>, and speech recognition <ref type="bibr" target="#b2">(Chorowski et al., 2015)</ref>. Benefiting from the availability of large-scale benchmark datasets such as SQuAD <ref type="bibr" target="#b8">(Rajpurkar et al., 2016)</ref>, the attention-based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors <ref type="bibr" target="#b14">(Wang &amp; Jiang, 2017;</ref><ref type="bibr" target="#b9">Seo et al., 2017;</ref><ref type="bibr" target="#b16">Xiong et al., 2017;</ref><ref type="bibr" target="#b5">Hu et al., 2017;</ref><ref type="bibr" target="#b6">Pan et al., 2017)</ref>. <ref type="bibr" target="#b14">Wang &amp; Jiang (2017)</ref> uses attention mechanism in Pointer Network to detect an answer boundary by predicting the start and the end indices in the passage. <ref type="bibr" target="#b9">Seo et al. (2017)</ref> introduces a bi-directional attention flow network that attention models are decoupled from the recurrent neural networks. <ref type="bibr" target="#b16">Xiong et al. (2017)</ref> employs a coattention mechanism that attends to the question and document together.  uses a gated attention network that includes both question and passage match and self-matching attentions. Both <ref type="bibr" target="#b6">Pan et al. (2017)</ref> and <ref type="bibr" target="#b5">Hu et al. (2017)</ref> employs the structure of multi-hops or iterative aligner to repeatedly fuse the passage representation with the question representation as well as the passage representation itself. Inspired by the above-mentioned works, we are proposing to introduce a general framework PhaseCond for the use of multiple attention layers. There are two motivations. First, previous research on the self-attention model is to purely capture long-distance dependencies <ref type="bibr" target="#b12">(Vaswani et al., 2017)</ref>, and therefore a multi-hops architecture <ref type="bibr" target="#b5">(Hu et al., 2017;</ref><ref type="bibr" target="#b6">Pan et al., 2017)</ref> is used to alternatively captures question-aware passage representations and refines the results by using a self-attention model. In contrast to the multi-hops and interactive architecture, our motivation of using the self-attention model for machine comprehension is to propagate answer evidence which is derived from the preceding question-passage representation layers. This perspective leads to a different attention-based architecture containing two sequential phases, question-aware passage representation phase and evidence propagation phase.  <ref type="bibr" target="#b9">(Seo et al., 2017)</ref>, RNET , MReader <ref type="bibr" target="#b5">(Hu et al., 2017)</ref>, and PhaseCond (our proposed model).</p><formula xml:id="formula_0">Model Q-P Attention Self-Attention Structure Fusion BIDAF W T [H; U ; H • U ] N/A Single Bi-LSTM RNET V T tanh(W T [H t−1,t ; U ]) V T tanh(W T [H; U ]) Single Gate MReader softmax(HU T )U softmax(HH T )H Alternative Gate PhaseCond softmax(HU T )V softmax(HH T )H Phased, Stacking Inner/Outer</formula><p>Second, unlike the domains such as machine translation  which jointly align and translate words, question-passage attention models for machine comprehension and question answering calculate the alignment matrix corresponding to all question and passage word pairs <ref type="bibr" target="#b14">(Wang &amp; Jiang, 2017;</ref><ref type="bibr" target="#b9">Seo et al., 2017)</ref>. Despite the attention models' success on the machine comprehension task, there has not been any other work exploring learning to encode multiple representations of question or passage from different perspectives for different parts of attention functions. More specifically, most approaches use two same question representations U for the question-passage attention model α(H, U )U , where H is the passage representation. Our hypothesis is that attention models can be more effective by learning different encoders for a question representation U and a question representation V from different aspects. The key differences between our proposed model and competing approaches are summarized at <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Our contributions are threefold: 1) we proposed a phase conductor for attention models containing multiple phases, each with a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow, 2) we present an improved attention function for question-passage attention based on two kinds of encoders: an independent question encoder and a weight-sharing encoder jointly considering the question and the passage, as opposed to most previous works which only using the same encoder for one attention model, and 3) we provide both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models. Experimental results show that our proposed PhaseCond lead to significant performance improvements over the state-of-the-art single-layered and multilayered attention models. Moreover, we observe several meaningful trends: a) during the questionpassage attention phase, repeatedly attending the passage with the same question representation "forces" each passage word to become increasingly closer to the original question representation, and therefore increasing the number of layers has a risk of degrading the network performance, b) during the self-attention phase, the self-attention's alignment weights of the second layer become noticeably "sharper" than the first layer, suggesting the importance of fully propagating evidence through the passage itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL ARCHITECTURE</head><p>We proposed phased conductor model (or PhaseCond), which consisting of multiple phases and each phase has two parts, a stack of attention layers L and a stack of fusion layers F controlling information flow. In our model, a fusion layer F can be an inner fusion layer F inner inside of a stack of attention layers, or an outer fusion layer F outer immediately following a stack of attention layers. Without loss of generality, PhaseCond's configurable computational path for two-phase, a question-passage attention phase containing N question-passage attention layers L Q , and a selfattention phase containing K self-attention layers L S , can be defined as question-attended passage representation is directly matching against itself, for the purpose of propagating information through the whole passage detailed in Section 2.3. For each self-attention layer, we configure an inner fusion layer to obtain a gated representation that is learned to decide how much of the current output is fused by the input from the previous layer detailed in Section 2.3.1. Finally, the fused vectors are sent to the output layer to predict the boundary of the answer span described in Section 2.4.</p><formula xml:id="formula_1">{L Q → F inner } ×N → F outer → {L S → F inner } ×L → F outer .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ENCODER LAYERS</head><p>The concatenation of raw features as inputs are processed in fusion layers followed by encoder layers to form more abstract representations. Here we choose a bi-directional Long Short-Term Memory (LSTM) <ref type="bibr" target="#b4">(Hochreiter &amp; Schmidhuber, 1997)</ref> to obtain more abstract representations for words in passages and questions.</p><p>Different from the commonly used approaches that every single model has exactly one question and passage encoder <ref type="bibr" target="#b9">(Seo et al., 2017;</ref><ref type="bibr" target="#b5">Hu et al., 2017)</ref>, our encoder layers simultaneously calculate multiple question and passage representations, for the purpose of serving different parts of attention functions of different phases. We use two types of encoders, independent encoder and shared encoder. In terms of independent encoder, a bi-directional LSTM is used to</p><formula xml:id="formula_2">produce new representation v Q 1 , . . . , v Q m of all words in the question, v Q j = BiLSTM Q (v Q j−1 , v Q j )<label>(1)</label></formula><p>where v Q j ∈ R 2d are concatenated hidden states of two independent BiLSTM for the j-th question word and d is the hidden size.</p><p>In terms of shared encoder, we jointly produce new representation h P 1 , . . . , h P n and u Q 1 , . . . , u Q m for the passage and question via a shared bi-directional LSTM,</p><formula xml:id="formula_3">h P i = BiLSTM S (h P i−1 , h P i ) (2) u Q j = BiLSTM S (u Q j−1 , u Q j )<label>(3)</label></formula><p>where h P i ∈ R 2d and u Q j ∈ R 2d are concatenated hidden states of BiLSTM for the i-th passage word and j-th question word, sharing the same trainable BiLSTM parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">QUESTION-PASSAGE ATTENTION LAYERS</head><p>The process of representing a passage with a question essentially includes two sub-tasks: 1) calculating the similarity between the question and different parts of the passage, and 2) representing the passage part with the given question depending on how similar they are.</p><p>A single question-passage attention layer is illustrated in <ref type="figure">Figure 2</ref>. In this model, at the t-th layer an alignment matrix A t ∈ R m , whose shape equals the number of words n in a passage multiplied by the number of words m in a question, is derived by aligning the passage representation at the t − 1 layer with the shared weight question representation,</p><formula xml:id="formula_4">A t (i, j) = exp(h t−1 i · u Q j ) k exp(h t−1 i · u Q k )<label>(4)</label></formula><p>where h t−1 i is the the i-th passage word representation at the t − 1 layer, h 0 i equals to h P i calculated from Eq 2, u Q j calculated from Eq 3 is the same for all the layers, the alignment matrix element A t (i, j) is a scalar, denoting the similarity between the i-th passage word and the j-th question word by using dot product of the passage word vector and the question word vector.</p><p>Given the alignment matrix element as weights, we compute the new passage representation h t i for the t-th layer by using weighted average over all the independent question representation v Q calculated from Eq 1, as shown in the following.</p><formula xml:id="formula_5">h t i = m k A t ik · v Q k (5) where h t−1 i ∈ R 2d . Note the independent representation v Q k for k-th question word is different with the shared weight question representation u Q k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">OUTER FUSION LAYERS</head><p>For each question-passage attention layer, its output of h t i , where t ∈ N , is concatenated to form the final output vector to represent the i-th passage word</p><formula xml:id="formula_6">C 0 i = [h 1 i ; . . . ; h N i ].</formula><p>Increasing the number of layers N allows an increasingly more complex representation for a passage word.</p><p>In order to regulate the flow of N question-passage attention layers and to prevent the over-fitting problem, we use fusion layers, which is highway networks <ref type="bibr" target="#b11">(Srivastava et al., 2015)</ref> using of GRUlike gating units and taking C 0 i as its input:</p><formula xml:id="formula_7">C t i = ReLU(W t C · C t−1 i + b t C ) (6) z t = σ(W t z · C t−1 i + b t z )<label>(7)</label></formula><formula xml:id="formula_8">C t i = (1 − z t ) • C t−1 + z t •C t i (8) where t ∈ K, K is the number of fusion layers, W t C , W t z are the weights, b t C , b t z</formula><p>are the bias of t-th fusion layer, and the transform gate z t is a non-linear activation function. The final result of fusion layers C N i ∈ R 2N d is sent to self-attention models as input for processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SELF-ATTENTION LAYERS</head><p>Following the question-passage attention layers, self-attention layers propagate evidence through the passage context. This process is similar in spirit to the steps of exploring similarity or redundancy between answer candidates (e.g., "J.F.K" and "Kennedy" can, in fact, be equivalent despite their different surface forms) that have been shown to be very effective during answer merging stage <ref type="bibr" target="#b3">(Ferrucci et al., 2010)</ref>. More generally, propagating evidence among the passage words allows correct answers to have better evidence for the question than the rest part of the passage.</p><p>For a single self-attention layer, we first compute a self alignment matrix S t ∈ R n×n by comparing the passage representation itself,</p><formula xml:id="formula_9">S t (i, j) = exp(h t−1 i · h t−1 j ) k exp(h t−1 i · h t−1 k )<label>(9)</label></formula><p>Figure 2: Improved question-passage attention model. We use blue color to denote question representations and use green color for passage representations.</p><p>where h t−1 i is the i-th passage word as input for the t-th self-attention layer, initial value h 0 i is defined as the final fused result C N i from question-passage attention model in section 2.2.1. Given the alignment matrix element as weights, evidences are propagate from the previous layer to the next to produce the new passage representation h t i by using the weighted average over all the t − 1 layer passage representation:</p><formula xml:id="formula_10">B t i = n k S t ik · h t−1 k<label>(10)</label></formula><p>where h t−1 k is the passage representation for the k-th word at the t − 1 self-attention layer, B t i ∈ R 2N d is the output the self-attention layer and it will be sent to a fusion layer, described in section 2.3.1, to obtain the t-th layer passage representation h t i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">INNER FUSION LAYERS</head><p>To efficiently propagate evidence through the passage, we refine the self-attended representations by using multiple layers. At the end of each self-attention layer, a GRU-like gating mechanism <ref type="bibr" target="#b5">(Hu et al., 2017)</ref> is used to decide what information to store and send to the next self-attention layer, by merging the newly produced representation of the current layer and the input representation from the previous layer,B</p><formula xml:id="formula_11">t i = tanh(W t B · [B t i ; B t−1 i ; B t i • B t−1 i ] + b t B ) (11) f t = σ(W t f · [B t i ; B t−1 i ; B t i • B t−1 i ] + b t f )<label>(12)</label></formula><formula xml:id="formula_12">h t i = (1 − f t ) • h t−1 + f t •B t i<label>(13)</label></formula><p>where W t B , W t f are the weights, b t B , b t f are the bias of t-th fusion layer, and f t is a non-linear activation function. The output h t i , whose dimensions are the same as its input vector B t i , is then sent to the next layer of self-attention model as input to calculate Eq 9 and Eq 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">OUTPUT LAYERS</head><p>We directly follow <ref type="bibr" target="#b5">Hu et al. (2017)</ref> and use a memory-based answer pointer networks to predict boundary of the answer. The memory-based answer pointer network contains multiple hops. For the t-th hop, the pointer network produces the probability distribution of the start index p t s and the end index p t e using a pointer network <ref type="bibr" target="#b13">(Vinyals et al., 2015)</ref> respectively. If the t-th hop is not the last hop, then the hidden states for the start and end indices are transformed and fed into the next-hop prediction. The training loss is defined as the sum of the negative log probabilities of the last hop start and end indices averaged over all examples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS AND ANALYSIS</head><p>This paper focuses on the Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b8">(Rajpurkar et al., 2016)</ref> to train and evaluate our model. SQuAD, which has gained a significant attention recently, is a largescale dataset consisting of more than 100,000 questions manually created through crowdsourcing on 536 Wikipedia articles. The dataset is randomly partitioned into a training set (80%), a development set (10%), and a blinded test set (10%). Two metrics are used to perform evaluation: Exact Match (EM) score which calculates the ratio of questions that are answered correctly by exact string match, and F1 score which calculates the harmonic mean of the precision and recall between predicted answers and ground true answers at the character level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TRAINING DETAILS</head><p>Our input for the encoding layer in Section 2.1 includes a list of commonly used features. We use pre-trained GloVe 100-dimensional word vectors <ref type="bibr" target="#b7">(Pennington et al., 2014)</ref>, parts-of-speech tag features, named-entity tag feature, and binary features of exact matching  which indicate if a passage word can be exactly matched to any question word and vice versa. Following <ref type="bibr" target="#b5">Hu et al. (2017)</ref>, we also use question type (what, how, who, when, which, where, why, be, and other) features <ref type="bibr" target="#b18">(Zhang et al., 2017)</ref> where each type is represented by a trainable embedding. We use CNN with 100 one-dimensional filters with width 5 to encode character level embedding. The hidden size is set as 128 for all the LSTM layers. Dropout <ref type="bibr" target="#b10">(Srivastava et al., 2014)</ref> are used for all the learnable parameters with a ratio as 0.2. We use the Adam optimizer (Kingma &amp; Ba, 2014) with an initial learning rate of 0.0006, which is halved when a bad checkpoint is met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MAIN RESULTS OF MODEL COMPARISON</head><p>We compare our proposed model PhaseCond with a multi-layered attention model, the Iterative Aligner, as well as various other recently published systems, which include a single-layered model, BIDAF <ref type="bibr" target="#b9">(Seo et al., 2017)</ref>, and a single-layered model containing both the question-passage attention and self-attention, RNET . We first compare our proposed model PhaseCond with Iterative Aligner, which is employed by two top ranked systems <ref type="bibr">MEMEN (Pan et al., 2017)</ref> and MReader <ref type="bibr" target="#b5">(Hu et al., 2017)</ref> on the SQuAD leaderboard 1 . Since our goal is to show the effectiveness of our proposed model PhaseCond, we use a baseline system implementing MReader for the direct comparison. All the experiment settings are the same for PhaseCond and Iterative Aligner including the number of attention layers, input features, optimizer and learning rate, number of training steps and etc. As shown in <ref type="table" target="#tab_1">Table 2</ref> which summarizes the performance of single models, we achieve steady improvements when 1) additional question encoders are used to extend the passage-question attention function, denoted as QPAtt+, as detailed in Section 2.1 and Section 2.2, and 2) on top of that, using PhaseCond making our model better than using Iterative Aligner. Specifically, PhaseCond's computational path for two question-aware passage attention layers L Q and two self-attention layers L S goes from L Q 1 → L Q 2 → F outer → L S 1 → F inner → L S 2 → F inner . On the other hand, Iterative Aligner builds path in turn through different kinds of attention layers: To perform a fair comparison as much as possible, we collect the results of BiDAF <ref type="bibr" target="#b9">(Seo et al., 2017)</ref> and RNET  from their recently published papers instead of using the up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline is one implementation of MReader, re-named as Iterative Aligner which has very similar results as those of MReader <ref type="bibr" target="#b5">(Hu et al., 2017)</ref> posted on the SQuAD Leaderboard on Jul 14, 2017.</p><formula xml:id="formula_13">L Q 1 → F inner → L S 1 → F inner → L Q 1 → F inner → L S 2 → F inner .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Model Ensemble Models Dev Set</head><p>Test Set Dev Set Test Set Attention-based Systems EM / F1 EM / F1 EM / F1 EM / F1 BiDAF <ref type="bibr" target="#b9">(Seo et al., 2017)</ref> 67.7 / 77.3 68.0 / 77.3 73.3 / 81.1 73.3 / 81.1 RNET  71.1 / 79.5 71.3 / 79.7 75.6 / 82.8 75.9 / 82.9 MReader <ref type="bibr" target="#b5">(Hu et al., 2017)</ref> N/A 71.0 / 80.1 N/A 74.3 / 82.4 Iterative Aligner <ref type="bibr" target="#b5">(Hu et al., 2017)</ref>   As shown in <ref type="table" target="#tab_2">Table 3</ref>, in the single model setting, our model PhaseCond is clearly more effective than all the single-layered models (BiDAF and RNET) and multi-layered models (MReader and Iterative Aligner). We draw the same conclusion for the ensemble model setting, despite that the RNET works better on the Dev EM measure. The EM result of our baseline Iterative Aligner is lower than RNET, confirming that the problem is not caused by our proposed model. Our explanations is that 1) RNET uses a different feature set (e.g., GloVe 300 dimensional word vectors are employed) and different encoding steps (e.g., three GRU layers are used for encoding question and passage representations), and 2) RNET uses a different ensemble method from our implementation. <ref type="table" target="#tab_4">Table 4</ref> shows the performance with different number of layers for both question-passage attention phase and self-attention phase. We change the layer number separately to compare the performance. For the question-passage attention phase, using single layer doesn't degrade the performance significantly from the default setting of two layers, resulting in a different conclusion from <ref type="bibr" target="#b5">Hu et al. (2017)</ref>; <ref type="bibr" target="#b16">Xiong et al. (2017)</ref>. Intuitively, this is largely expected because representing the passage repeatedly with the same question doesn't constantly add more information. In contrast, multiple stacking layers are needed to allow the evidence fully propagated through the passage. This is exactly what we observed in two stacking layered self-attention phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ANALYSIS ON ATTENTION LAYERS</head><p>In <ref type="figure" target="#fig_4">Figure 3</ref>, we visualize the attention matrices for each layer to show dynamic attention changes. The model is based on the main setting which has two question-passage layers and two self-attention layers. We observed several critical trends. First, the first layer of the question-passage attention phase can successfully align question keywords with the corresponding passage keywords, as shown in <ref type="figure" target="#fig_4">Figure 3a</ref>. For example, the question keyword "represented" have been successfully aligned with related passage keywords "champion", "defeated", and "earned". Second, patterns of striped color in <ref type="figure" target="#fig_4">Figure 3a</ref> indicate similar weights among all the passage words, meaning that it becomes indistinguishable among passage words, and therefore adding another layer of question-passage attention model degrades the alignment quality dramatically. This observation is meaningful which    Generally, the darker the color is the higher the weight is (the only exception is <ref type="figure" target="#fig_4">Figure 3b</ref> which contains negative values). Given the question "Which NFL team represented the AFC at Super Bowl 50?", the system correctly detects the answer "Denver Broncos" from the passage part "The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 2410 to earn their third Super Bowl title."</p><p>shows that repeatedly representing a passage word regarding the same question representation can make the passage embedding become closer to the original question representation. Third, when comparing <ref type="figure" target="#fig_4">Figure 3c</ref> and <ref type="figure" target="#fig_3">Figure 3d</ref>, we observed that the color is diluted for most of the weights in the second layer of self-attention phase, meanwhile a small portion of weights is strengthened, suggesting that information propagation is converging. For example, in <ref type="figure" target="#fig_3">Figure 3d</ref> as the last attention layer, the phrase "Denver Broncos" becomes more concentrated on the phrase "Carolina Panthers" than that of <ref type="figure" target="#fig_4">Figure 3c</ref>. In contrast, "Denver Broncos" becomes less focused on the other keywords (e.g., "champion" and "title") of the same passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In this paper, we introduce a general framework PhaseCond, on multi-layered attention models with two phases including a question-aware passage representation phase and an evidence propagation phase. The question-aware passage representation phase has a stack of question-aware passage attention models, followed by outer fusion layers that regularize concatenated passage representations. The evidence propagation phase has a stack of self-attention layers, each of which is followed by inner fusion layers that control the information to propagate and output. Also, an improved attention mechanism for PhaseCond is proposed based on a popular dot-product attention function by simultaneously encoding both the independent question embedding layers, the weight-sharing question embedding layer and weight-sharing passage embedding layer. The experimental results show that our model significantly outperforms single-layered or multiple-layered attention networks on blinded test data of SQuAD. Moreover, our in-depth quantitative analysis and visualizations provide meaningful findings for both question-aware passage attention mechanism and self-matching attention mechanism.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>gives an concrete example of building PhaseCond based network for the machine comprehension task. The network contains encoding layers, question-passage attention layers, self-attention layers and output layers. The encoding layer maps various groups of features, such as character features and word features, to their corresponding embeddings. Those raw embeddings are then fed into an outer fusion layer to encode these embeddings as passage or question representations in Section 2.1. Next, the representations are sent to question-passage attention layers to align and represent passage representation with the whole question representation in Section 2.3. The output of each layer is concatenated and regularized by a stack of fusion layers in Section 2.2.1. After that, theFigure 1: PhaseCond: our proposed attention model structure overview. We use the colored rectangle to highlight the focus of this paper. The question and passage encoder layers and attention layers are colored in blue, the fusion layers are colored in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( d )</head><label>d</label><figDesc>The second layer of self-attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Dynamic attention changes of multiple layers on a visualized example. The matrices are the attention weights computed by the dot-product attention function before any normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of attention architectures of competing approaches: BIDAF</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of single models on the development set. Each setting contains five runs trained consecutively. Standard deviations across five runs are shown in the parenthesis for single models. Daggers indicate the level of significance.</figDesc><table><row><cell></cell><cell></cell><cell>EM</cell><cell></cell><cell>F1</cell></row><row><cell>Attention Models</cell><cell>Max</cell><cell>Mean (±SD)</cell><cell>Max</cell><cell>Mean (±SD)</cell></row><row><cell>Iterative Aligner</cell><cell cols="2">70.95 70.64 (±0.34)</cell><cell cols="2">80.46 80.23 (±0.16)</cell></row><row><cell cols="5">Iterative Aligner, QPAtt+ 71.21 71.11 (±0.31) † 80.73 80.52 (±0.16)  †</cell></row><row><cell>PhaseCond</cell><cell cols="4">71.36 71.07 (±0.28)  † 80.76 80.53 (±0.22)  †</cell></row><row><cell>PhaseCond, QPAtt+</cell><cell cols="4">71.85 71.60 (±0.22)  ‡ 81.13 81.04 (±0.17)  ‡</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The performance of our models and published results of competing attention-based architectures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Varying number of question-passage attention layers and self-attention layers. We set layer number in PhaseCond for question-passage attention model (denoted as QPAtt) and self-attention model (denoted as SelfAtt) respectively. L1 means a single layer and L2 means two stacking layers.</figDesc><table><row><cell></cell><cell>EM</cell><cell>F1</cell></row><row><cell>Attention Layers</cell><cell cols="2">Max Mean (±SD) Max Mean (±SD)</cell></row><row><cell cols="3">QPAtt-L1, SelfAtt-L1 71.26 71.29 (±0.19) 80.83 80.68 (±0.17)</cell></row><row><cell cols="3">QPAtt-L1, SelfAtt-L2 72.05 71.56 (±0.30) 81.11 80.98 (±0.15)</cell></row><row><cell cols="3">QPAtt-L2, SelfAtt-L1 71.26 70.88 (±0.30) 80.79 80.41 (±0.31)</cell></row><row><cell cols="3">QPAtt-L2, SelfAtt-L2 71.85 71.60 (±0.22) 81.13 81.04 (±0.17)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://rajpurkar.github.io/SQuAD-explorer/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Zeyu Zheng for discussions on GPUs and tensorflow.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Building watson: An overview of the deepqa project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">AI magazine</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Reinforced mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bin Cao, Deng Cai, and Xiaofei He. MEMEN: multi-layer embedding with memory networks for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ba ; Boyuan Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09098</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring question understanding and adaptation in neural-network-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
