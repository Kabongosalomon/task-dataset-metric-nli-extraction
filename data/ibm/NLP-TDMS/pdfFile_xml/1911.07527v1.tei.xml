<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SOGNet: Scene Overlap Graph Network for Panoptic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution" key="instit1">Academy for Advanced Interdisciplinary Studies</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
							<email>zhaoqijie@pku.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
							<email>zlin@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SOGNet: Scene Overlap Graph Network for Panoptic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The panoptic segmentation task requires a unified result from semantic and instance segmentation outputs that may contain overlaps. However, current studies widely ignore modeling overlaps. In this study, we aim to model overlap relations among instances and resolve them for panoptic segmentation. Inspired by scene graph representation, we formulate the overlapping problem as a simplified case, named scene overlap graph. We leverage each objects category, geometry and appearance features to perform relational embedding, and output a relation matrix that encodes overlap relations. In order to overcome the lack of supervision, we introduce a differentiable module to resolve the overlap between any pair of instances. The mask logits after removing overlaps are fed into per-pixel instance id classification, which leverages the panoptic supervision to assist in the modeling of overlap relations. Besides, we generate an approximate ground truth of overlap relations as the weak supervision, to quantify the accuracy of overlap relations predicted by our method. Experiments on COCO and Cityscapes demonstrate that our method is able to accurately predict overlap relations, and outperform the state-of-the-art performance for panoptic segmentation. Our method also won the Innovation Award in COCO 2019 challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Convolutional Neural Networks (CNNs) have achieved huge success in computer vision tasks such as image recognition <ref type="bibr" target="#b4">(He et al. 2016;</ref>, semantic segmentation <ref type="bibr" target="#b15">(Long, Shelhamer, and Darrell 2015;</ref>, object detection <ref type="bibr" target="#b4">(Girshick 2015;</ref>, and instance segmentation ). The semantic segmentation task answers which background scene a pixel belongs to, while the instance segmentation task predicts foreground object masks. Recently, the panoptic segmentation task introduced in <ref type="bibr" target="#b7">(Kirillov et al. 2019b</ref>) aims to unify the results of semantic and instance segmentation into a single pipeline. The system performs semantic segmentation for pixels that belong to amorphous background scenes, named stuff. For countable foreground objects, named things, the goal is to assign each <ref type="figure">Figure 1</ref>: Instance segmentation has overlapping regions for objects, while panoptic segmentation requires a unified result for each pixel. Our study aims to explicitly predict overlap relations and resolve overlaps for the panoptic output. object region with the right thing class, as well as an instance id, identifying which object it belongs to. As a result, panoptic segmentation cannot have overlapping segments. However, most cutting-edge high-performance instance segmentation methods ) adopt the region-based strategy <ref type="bibr" target="#b4">(Girshick et al. 2014)</ref>, and output overlapping segments. As shown in <ref type="figure">Figure 1</ref>, the object pairs, such as cup-dinning table, bottle-dinning table, and bowl-dinning table, share overlapping regions from instance segmentation output. Therefore, resolving overlaps and producing coherent segmentation results are the main challenge for the panoptic segmentation task <ref type="bibr" target="#b7">(Kirillov et al. 2019b)</ref>.</p><p>In <ref type="bibr" target="#b7">(Kirillov et al. 2019b)</ref>, the semantic and instance segmentation are trained separately, and their panoptic results are merged by heuristic post-processing steps. Later studies aim to unify the semantic and instance segmentation into an end-to-end training framework <ref type="bibr" target="#b7">(Kirillov et al. 2019a;</ref><ref type="bibr" target="#b20">Xiong et al. 2019;</ref><ref type="bibr" target="#b18">Porzi et al. 2019;</ref><ref type="bibr" target="#b22">Yang et al. 2019;</ref><ref type="bibr" target="#b9">Li et al. 2018)</ref>. The panoptic results are usually produced by fusion strategies <ref type="bibr" target="#b7">(Kirillov et al. 2019a;</ref><ref type="bibr" target="#b7">Li et al. 2019b)</ref>, or predicted by a panoptic head <ref type="bibr" target="#b20">Xiong et al. 2019)</ref>. These studies do not explicitly model overlap relations among objects, which is especially important for datasets with rich categories and complex scenes. However, modeling overlap is challenging without the supervision of object relations or depth information.</p><p>In this study, we introduce the scene overlap graph network (SOGNet) for panoptic segmentation. The SOGNet consists of four components: the joint segmentation, the relational embedding module, the overlap resolving module, and the panoptic head. The SOGNet trains semantic and instance segmentation in an end-to-end fashion, explicitly encodes overlap relations, resolves the overlap between any pair of objects in a differentiable way, and outputs a unified panoptic result in the panoptic head.</p><p>Similar to <ref type="bibr" target="#b7">(Kirillov et al. 2019a;</ref><ref type="bibr" target="#b20">Xiong et al. 2019;</ref><ref type="bibr" target="#b18">Porzi et al. 2019;</ref><ref type="bibr" target="#b9">Li et al. 2018)</ref>, we also use ResNets <ref type="bibr" target="#b4">(He et al. 2016)</ref> with feature pyramid network (FPN) <ref type="bibr" target="#b14">(Lin et al. 2017)</ref> as the shared backbone for our semantic and instance segmentation branches. Inspired by the relation classification in scene graph parsing tasks <ref type="bibr" target="#b22">(Zellers et al. 2018;</ref><ref type="bibr" target="#b19">Woo et al. 2018</ref>), we formulate the overlapping problem in panoptic segmentation as a simplified scene graph with directed edges, in which there are only three relation types for instance i with respect to j: no overlap, covering as a subject, and being covered as an object. We name this representation as scene overlap graph in this study. We leverage the category, geometry, and appearance information of objects to perform edge feature embedding for the scene overlap graph, and output a matrix that explicitly encodes overlap relations. However, different from scene graph parsing tasks with the commonly used Visual Genome dataset <ref type="bibr" target="#b7">(Krishna et al. 2017</ref>) that has relation annotations, the panoptic segmentation task does not offer annotations of object relations or depth information, so the overlap relations cannot be modeled with direct supervision.</p><p>In order to overcome this problem, we develop the overlap resolving module, which resolves the overlaps between any pair of instances in a differentiable way. The mask logits after removing overlaps are then used for per-pixel instance id classification in the panoptic head with the panoptic annotation. In doing so, the supervision from pixel-level classification helps the instance-level modeling of overlap relations.</p><p>We list the contributions in this study as follows: • We formulate the overlapping problem in panoptic segmentation as a structured representation, named scene overlap graph. Using category, geometry and appearance features, we perform relational embedding and output a matrix that explicitly encode overlap relations. • In order to deal with the lack of supervision on overlap relations, we develop an overlap resolving module that resolves overlaps between any pair of instances in a differentiable way. The supervision from per-pixel instance id classification in the panoptic head helps to encode overlap relations. We also generate an approximate ground truth as weak supervision to quantify the accuracy of overlap relations predicted by our network. • Experiments on the COCO and Cityscapes datasets show that, our proposed method is able to accurately predict overlap relations, and outperform the state-of-the-art performance for panoptic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Image Segmentation The semantic segmentation task focuses on background scenes and is based on fully convolutional networks (FCNs) <ref type="bibr" target="#b15">(Long, Shelhamer, and Darrell 2015)</ref>. Because detail information is important for dense prediction problems, later studies learn finer representation by deconvolution <ref type="bibr" target="#b17">(Noh, Hong, and Han 2015)</ref>, encoder-decoder structures (Badrinarayanan, Kendall, and Cipolla 2017), or introducing skip connections between down-sampling and up-sampling paths <ref type="bibr" target="#b19">(Ronneberger, Fischer, and Brox 2015)</ref>.</p><p>Other methods aim to aggregate multi-scale context <ref type="bibr">(Farabet et al. 2013;</ref>, and better capture long-range dependencies <ref type="bibr" target="#b23">(Zheng et al. 2015;</ref><ref type="bibr" target="#b7">Li et al. 2019a</ref>). The instance segmentation task deals with foreground objects. Similar to object detection <ref type="bibr" target="#b4">(Girshick 2015;</ref>, many instance segmentation studies <ref type="bibr" target="#b7">(Li et al. 2017;</ref>) also adopt the region-based strategy <ref type="bibr" target="#b4">(Girshick et al. 2014)</ref>, and are able to achieve strong performance due to accurate localization for instances. As another stream, segmentation-based methods <ref type="bibr" target="#b13">(Liang et al. 2018;</ref><ref type="bibr" target="#b0">Arnab and Torr 2017)</ref> perform pixel-wise classification and then construct object instances by grouping.</p><p>The recently proposed task, panoptic segmentation <ref type="bibr" target="#b7">(Kirillov et al. 2019b)</ref>, requires a unified result for background scenes and foreground objects. A naive implementation is to train the two sub-tasks separately, and then fuse the results by heuristic rules <ref type="bibr" target="#b7">(Kirillov et al. 2019b)</ref>. Follow-up studies train semantic and instance segmentation in an endto-end network by sharing backbone <ref type="bibr" target="#b3">(de Geus, Meletis, and Dubbelman 2018;</ref><ref type="bibr" target="#b7">Kirillov et al. 2019a;</ref><ref type="bibr" target="#b20">Xiong et al. 2019;</ref><ref type="bibr" target="#b18">Porzi et al. 2019;</ref><ref type="bibr" target="#b22">Yang et al. 2019;</ref><ref type="bibr" target="#b9">Li et al. 2018</ref>). Most of them use fusion heuristics to produce the final output. In <ref type="bibr" target="#b20">Xiong et al. 2019</ref>), a panoptic head is constructed to predict instance id. Li et al. ) introduce a binary mask to differentiate between thing or stuff for each pixel. A semiand weakly-supervised method is proposed in <ref type="bibr" target="#b9">(Li, Arnab, and Torr 2018)</ref> to relieve the cost of pixel-level annotation.</p><p>An important aspect ignored by current panoptic segmentation studies is modeling and resolving overlaps. The study <ref type="bibr" target="#b8">(Lazarow, Lee, and Tu 2019)</ref> tries to learn instance occlusions but cannot resolve them in the end-to-end training. As a comparison, our study is able to explicitly model overlap relations, telling us whether an instance lies upon or beneath another, and resolve their overlaps in a differentiable way to generate the panoptic output.</p><p>Relational Modeling Parsing relationships of objects has been one of the core components of visual understanding. In (Hu et al. 2018), appearance and geometry features are used to build interactions for object detection. The visual relationship datasets, such as Visual Genome, inspire a series of studies on scene graph generation. In <ref type="bibr" target="#b22">(Zellers et al. 2018;</ref><ref type="bibr" target="#b19">Woo et al. 2018)</ref>, the low-rank outer product ) is adopted to perform relational embedding from object features. Other relation reasoning methods are proposed by graph-based propagation (Xu et al. 2017), associative embedding <ref type="bibr" target="#b16">(Newell and Deng 2017)</ref>, and introducing an efficient module <ref type="bibr" target="#b19">(Santoro et al. 2017)</ref>.</p><p>In our study, we formulate the overlapping problem as a simplified scene graph, and also perform relational embedding to encode overlap relations. Our method differs from these studies in that our problem does not offer relation annotation to supervise. We use the supervision from panoptic head to help the modeling of overlap relations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Overlap Graph Network</head><p>In the scene graph generation task <ref type="bibr" target="#b22">(Zellers et al. 2018;</ref><ref type="bibr" target="#b19">Woo et al. 2018;</ref><ref type="bibr" target="#b20">Xu et al. 2017)</ref>, objects in an image are constructed as a graph and their relations are directed edges. We formulate the overlapping problem in panoptic segmentation as a similar structure, named scene overlap graph (SOG). There are three relation types for instance i with respect to j: no overlap, covering as a subject, and being covered as an object. Our proposed SOGNet consists of four components. The joint segmentation connects semantic and instance segmentation in a unified network. The relational embedding module explicitly encodes overlap relations of objects. After the overlap resolving module, overlaps among instances are removed in a differentiable way. Finally, the panoptic head performs per-pixel instance id classification. An illustration of our SOGNet architecture is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Segmentation</head><p>Following current popular methods, we use ResNet with FPN as the shared backbone of semantic and instance segmentation branches. The Mask R-CNN structure is adopted for our instance segmentation head, which outputs the box regression, class prediction, and mask segmentation for foreground objects. As for semantic head, the FPN feature maps first go through three 3 × 3 deformable convolution layers , and then are up-sampled to the 1/4 scale. Finally, they are concatenated to generate the per-pixel category prediction. This branch is supervised with both stuff and thing classes, and then the semantic logits of stuff classes are extracted into the panoptic head. We train our model using instance and panoptic annotation. The panoptic annotation that gives perpixel category and instance id supervises the semantic and panoptic head, respectively. The instance annotation contains overlaps and is used for instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Embedding Module</head><p>For any training image, we are given the ground truth</p><formula xml:id="formula_0">{b i , c i , M i } Ninst i=1 , where b i , c i ,</formula><p>and M i refer to the bounding box, one-hot category, and corresponding mask for instance i, respectively, and N inst is the number of instances in this image. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, we perform relational embedding using the ground truth in the training phase. During inference, we replace them with the prediction from Mask R-CNN branch. The b i ∈ R 4 and c i ∈ R 80 (there are 80 thing classes for COCO) encode geometry and category information, respectively. In order to include appearance feature, we resize the values inside box b i from M i as 28 × 28, which is consistent with the size of Mask R-CNN's output. The resized mask is flattened to be a vector, denoted as m i ∈ R 784 .</p><p>The bilinear pooling method learns joint representation for pair of features and is widely applied to visual question answering <ref type="bibr" target="#b6">Kim, Jun, and Zhang 2018)</ref>, and image recognition (Yu et al. 2018) tasks. We construct our category and appearance relation features using the low-rank outer product in . For a pair of instances i and j, their category relation feature is calculated as:</p><formula xml:id="formula_1">E (c) i|j = P T σ(V T c i ) • σ(U T c j ) ,<label>(1)</label></formula><p>where • denotes the Hadamard product (element-wise multiplication), σ is the ReLU non-linear activation, V and U are two linear embeddings that project the input into subject and object features, respectively, and P maps the relation feature into output dimension d c . We then have the category relation features as:</p><formula xml:id="formula_2">E (c) = E (c) 1|1 , E (c) 1|2 , · · · , E (c) Ninst|Ninst T ∈ R N 2 inst ×dc ,<label>(2)</label></formula><p>where "[ ]" is the concatenation operation. In a similar way, using m i as the input of Eq.</p><p>(1), we can also construct the appearance relation features E (m) ∈ R N 2 inst ×dm . The relative geometry provides strong information to infer whether two objects have overlap or not. Following <ref type="bibr" target="#b4">(Hu et al. 2018;</ref><ref type="bibr" target="#b19">Woo et al. 2018)</ref>, we have the translation-and scaleinvariant relative geometry feature encoded as:</p><formula xml:id="formula_3">E (b) i|j = K T x i − x j w j , y i − y j h j , log w i w j , log h i h j T ,<label>(3)</label></formula><p>where x i , y i , w i , h i are coordinates and scales extracted from b i , and K ∈ R 4×d b is a linear matrix that maps the 4dimensional relative geometry feature into high-dimensional d b . We can further have the geometry relation features</p><formula xml:id="formula_4">E (b) ∈ R N 2 inst ×d b .</formula><p>We concatenate these edge representations about appearance, category, and geometry as:</p><formula xml:id="formula_5">E = [E (m) , E (c) , E (b) ] ∈ R N 2 inst ×d ,<label>(4)</label></formula><formula xml:id="formula_6">where d = d m + d c + d b .</formula><p>The relational embedding is further used to encode overlap relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overlap Resolving Module</head><p>Based on relational embedding, we introduce the overlap resolving module to explicitly model overlap relations and resolve overlaps among instances in a differentiable way. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, the relation features, E ∈ R N 2 inst ×d , go through a f c(d, 1) layer to have a singlechannel output with the sigmoid activation to restrict the values within (0, 1). We reshape the output as a square matrix, denoted as M ∈ R Ninst×Ninst . The element M ij has a physical meaning to represent the potential of instance i being covered by instance j. Because there can be only one overlap relation between instances i and j, we then introduce the overlap relation matrix defined as:</p><formula xml:id="formula_7">O = σ(M − M T ) ∈ R Ninst×Ninst ,<label>(5)</label></formula><p>where σ denotes the ReLU activation that is used to filter out the negative differences between potentials on symmetric positions. In doing so, if O ij &gt; 0, it encodes that instance i is being covered by instance j, while on its symmetric position, O ji = 0. When O ij = O ji = 0, the instances i and j do not have overlaps. Besides, all diagonal elements O ii equals to 0. As explained later, the positive elements in O will be optimized towards 1 in implementations. We now show how to leverage the overlap relation matrix O to resolve overlaps. For each bounding box, b i , of the ground truth instances, we have its mask logits (the activations before sigmoid) of 28 × 28 from the Mask R-CNN output. We then interpolate these logits back to the image scale H × W by bilinear interpolation and padding outside the box. These logits, denoted as {A i } Ninst i=1 , may have overlaps because Mask R-CNN is region-based and operates on each region independently. Using the matrix O, we can deal with the overlaps between instances i and j as:</p><formula xml:id="formula_8">A i = A i − A i • [s(A i ) • s(A j )] O ij ,<label>(6)</label></formula><p>where A i is the output logit of instance i, and s represents the sigmoid activation that turns the logit A i into a binary-like mask s(A i ). The element-wise multiplication, s(A i ) • s(A j ), calculates the intersecting region between instances i and j. The value O ij decides whether the elements in intersecting region should be removed from the logit A i . When O ij approaches 1, O ji equals to 0, thus the logit A j will not be affected, and vice versa.</p><p>Considering the overlap relations of all the other instances on i, we have:</p><formula xml:id="formula_9">A i = A i − A i • s(A i ) • Ninst j=1 s(A j )O ij ,<label>(7)</label></formula><p>and then the computational steps of the overlap resolving module can be formulated as:</p><formula xml:id="formula_10">A = A − A • s(A) • s(A) × 3 O T ,<label>(8)</label></formula><p>where A = [A 1 , A 2 , · · · , A Ninst ] ∈ R H×W ×Ninst , and × 3 denotes the Tucker product along the 3-rd dimension (reshape s(A) as R HW ×Ninst for inner product with O T , and then return to R H×W ×Ninst ). We see that our module is friendly to tensor operations in current deep learning frameworks, and is differentiable for resolving overlaps, so that the SOGNet can be trained in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Panoptic Head</head><p>The overlap relation matrix, O, explicitly encodes whether there is intersection between any pair of instances, and if there is, the overlapping region should be removed from which instance. However, we are not provided with the supervision of overlap relations by the panoptic segmentation task. Because accurately resolving overlaps has a strong correlation with the quality of final panoptic output, we can exploit the pixel-level panoptic annotation to assist in the process of modeling overlap relations encoded by O. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, the instance logits A after the SOG module are then fed into the panoptic head. Following UPSNet <ref type="bibr" target="#b20">(Xiong et al. 2019)</ref>, we incorporate the logits from semantic head into the mask logits A . We get the logits of i-th object from semantic output X i by taking the values inside its ground truth box B i from the channel corresponding to its ground truth category C i , and padding zeros outside the box. In UPSNet, they are combined by addition, which is denoted as "Panoptic Head 1". Here we develop an improved combination denoted as "Panoptic Head 2". They are compared as:</p><formula xml:id="formula_11">Panoptic Head 1 : Z i = X i + A i , (9) Panoptic Head 2 : Z i = k · X i • s(A i ) + A i , (10)</formula><p>where Z i is the combined logit, s denotes the sigmoid function and k is a factor to balance the numerical difference between semantic output values and mask logits. We set k to be 2 in our experiments. Finally, we concatenate the combined instance logits Z inst = [Z 1 , ..., Z Ninst ] and the stuff logits Z stuf f from the semantic head to perform per-pixel instance id classification with the standard cross entropy loss function, L panoptic .</p><p>Despite we do not have the supervision to know which instance lies on the other one, we can leverage the ground truth binary masks, {M i } Ninst i=1 , to infer whether two instances have overlaps or not. We produce a symmetric relation matrix R ∈ R Ninst×Ninst defined as:</p><formula xml:id="formula_12">R ij = 1 |M i • M j | min{|M i |, |M j |} ≥ 0.1 , i = j,<label>(11)</label></formula><p>where | · | calculates the area of a binary mask through sum operation, • calculates the intersection mask through elementwise multiplication, and 1 denotes the indicator function that equals to 1 when the condition holds. All diagonal elements R ii are filled with 0. When R ij = R ji = 1, it indicates that the overlapped intersection over the smaller object is larger than 0.1, which means there is a significant overlap between instances i and j. With the symmetric relation matrix R, we can introduce the relation loss function as:</p><formula xml:id="formula_13">L R = 1 N 2 inst O + O T − R 2 F ,<label>(12)</label></formula><p>which calculates the mean squared error between (O + O T ) and R. In doing so, when there is overlap between instances i and j, i.e., R ij = R ji = 1, the overlap relation O ij or O ji is forced to approach 1, so that it will not contribute trivially when removing overlaps by Eq. (6).</p><p>In total, our SOGNet has the loss functions for semantic and instance segmentation, the panoptic loss L panoptic for instance id classification, and the relation loss L R to help optimizing the overlap relation matrix O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>Panoptic Quality We adopt the evaluation metric introduced in <ref type="bibr" target="#b7">(Kirillov et al. 2019b)</ref>, called Panoptic Quality (PQ). It can be viewed as the multiplication of a segmentation term (SQ) and a recognition term (RQ):</p><formula xml:id="formula_14">PQ = (p,q)∈T P IoU(p, g) |T P | SQ × |T P | |T P | + 1 2 |F P | + 1 2 |F N | RQ ,<label>(13)</label></formula><p>where p and g are predicted and ground truth segments, and T P , F P and F N denote the true positive, false positive and false negative sets, respectively.</p><p>Overlap Accuracy For dataset, such as COCO, the instance annotation permits overlapping instances, while the panoptic annotation contains no overlaps. We can leverage the difference between the two annotations to generate an approximate ground truth of overlap relations, in order to test the quality of overlap relations predicted by our model. The method is also used in <ref type="bibr" target="#b8">(Lazarow, Lee, and Tu 2019)</ref> to generate their occlusion ground truth.</p><p>Concretely, we are provided with the instance annotation {M i } Ninst i=1 , and the panoptic annotation {M i } Ninst i=1 . For any pair of instances i and j, we calculate the intersecting region by M i • M j , and inspect which one ofM i andM j mainly covers the intersecting region, to know if i lies upon j or the other way round. Note that the instance and panoptic annotation are not seamlessly matched. Thus this method can only produce approximately true overlap relations</p><p>Using the synthetic ground truth as weak supervision, we construct a new asymmetric relation matrix R . When R ij = 1, we have R ji = 0, and it means instance i is covered by j. We can have a new relation loss function in this weaklysupervised setting to replace Eq. (12) with:</p><formula xml:id="formula_15">L R = 1 N 2 inst O − R 2 F ,<label>(14)</label></formula><p>which directly supervises the overlap relation matrix O. In experiments, the weakly-supervised manner by Eq. <ref type="figure" target="#fig_1">(14)</ref> and our method by Eq. (12) have similar performances. Note that the supervision is only valid for datasets such as COCO that has difference between instance and panoptic annotations. It will be ineffective for datasets such as Cityscapes. But our method by Eq.(12) works in both cases. Thus the weakly-supervised manner by Eq. <ref type="formula" target="#formula_1">(14)</ref> is served to test the efficacy of our method. Using the weak supervision R , we develop a metric, named overlap accuracy (OA), to quantify the quality of overlap predictions encoded by O. The OA of image I is calculated as:</p><formula xml:id="formula_16">OA(I) = |T P | + |T N | N inst × (N inst − 1) ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_17">T P = {(i, j)|R ij = 1, O ij ≥ 0.5, i = j}, and T N = {(i, j)|R ij = 0, O ij &lt; 0.5, i = j}.</formula><p>Our reported OA is an average over all images in the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We conduct experiments on the COCO and Cityscapes datasets for panoptic segmentation, and show that our proposed SOGNet is able to accurately predict overlap relations and outperform state-of-the-art performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Training We set the weights of loss functions following <ref type="bibr" target="#b20">(Xiong et al. 2019)</ref>. The weight of panoptic head is 0.1 for COCO and 0.5 for Cityscapes. The weight of relation loss is set to 1.0. We train the models with a batchsize of 8 images distributed on 8 GPUs. The region proposal network (RPN) is also trained end-to-end. The SGD optimizer with 0.9 Nesterov momentum and a weight decay of 10 −4 is used. We use an equivalent setting to UPSNet for fair comparison. Images are resized with the shorter edge as 800, and the longer edge less than 1333. We freeze all batch normalization (BN) (Ioffe and Szegedy 2015) layers within the ResNet backbone. For COCO, we train the SOGNet for 180K iterations. The initial learning rate is set to 0.01 and is divided by 10 at the 120Kth and 160K-th iterations. For Cityscapes, we train for 24K iterations and drop the learning rate at the 18K-th iteration. Besides, in order to test the quality of our overlap predictions, we perform an ablation study on COCO using a shorter training schedule because our relation loss converges soon. We only train for 45K iterations and drop the learning rate at iteration 30K and 40K. We do not adopt the void channel prediction proposed in UPSNet. In implementations, we filter out the instances that have no overlap with any other instance to reduce negative samples and computation overhead.  <ref type="table">Table 1</ref>: Different input for the relational embedding module. "Cat", "Box" and "Mask" denote the category, geometry and appearance features, respectively.</p><p>Inference During inference phase, the ground truths</p><formula xml:id="formula_18">{b i , c i , M i } Ninst i=1</formula><p>as the input of our relational embedding are replaced with the predictions from Mask R-CNN branch. In order to remove invalid instances, we filter out instances whose probability is lower than a threshold, and perform an NMS-like procedure, following <ref type="bibr" target="#b7">(Kirillov et al. 2019b;</ref><ref type="bibr" target="#b20">Xiong et al. 2019)</ref>. For highly overlapped predictions of the same class, we keep the mask with the higher confidence score and discard the other one if the intersection is larger than a threshold. Otherwise, we keep the non-interacting part and deal with the next instance. The final output is predicted by our panoptic head. For stuff segment whose area is lower than 4096, we set the corresponding region as void.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We use ResNet-50 as backbone with a short training schedule, and conduct experiments to analyze feature combinations for our relational embedding, and test the quality of overlap relations predicted by our method. As shown in <ref type="table">Table 1</ref>, we use different features as the input of our relational embedding. When only category or geometry feature is adopted, the performance improvement on PQ is not so significant, and the overlap prediction does not show high accuracy. When  <ref type="table">Table 2</ref>: PlainNet denotes the joint segmentation component of SOGNet. They are trained in the same condition. "PH 1 / 2" denotes the "Panoptic Head 1 / 2", respectively.</p><p>category and geometry features are used together, the embedding becomes much more powerful. Mask feature also slightly improves the overlap accuracy. We expect that a more sophisticated feature design will further boost the performance. It is observed that the weakly-supervised method by Eq. (14) achieves a similar result to our method by Eq. (12). As shown in <ref type="figure">Figure 3</ref>, we visualize the overlap relations predicted by O, as well as the approximate ground truth, R , on images from the validation set. More examples can be found in the supplementary material. It is shown that the matrix O accurately predicts some overlap relations, including baseball glove→person, tie→person→bus, and spoon→cup→dinning table. The results demonstrate that the overlap relations are modeled well with the help of supervision from per-pixel instance id classification in the panoptic head. Our method is able to encode overlap relations without direct supervision on them.</p><p>Using the standard training schedule and ResNet-50 as the backbone, we also perform comparisons between SOGNet and heuristic inference. The heuristics in <ref type="bibr" target="#b7">(Kirillov et al. 2019b</ref>) sort instances according to their objectness scores to deal with overlaps. In ), some hand-crafted label priors are made to rule overlap orders. For example, tie should always cover person. As a comparison, SOGNet  explicitly predict overlap relations and resolve overlaps in a differentiable way. We train the joint segmentation component of SOGNet as a PlainNet, and perform inference with different methods. As shown in <ref type="table">Table 2</ref>, label prior helps to improve the performance. When PlainNet adds the panoptic head for inference to produce the panoptic results, the performance becomes better. The SOGNet with relational embedding and overlap resolving has a further improvement. And our proposed Panoptic Head 2 (PH2) performs better than PH1. In <ref type="figure" target="#fig_1">Figure 4</ref>, we visualize the panoptic segmentation results of heuristic inference and SOGNet. It is shown that SOGNet better deals with the overlapping problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Other Methods</head><p>We run SOGNet on the COCO and Cityscapes datasets, and compare the results with state-of-the-art methods including the method in <ref type="bibr" target="#b9">(Li, Arnab, and Torr 2018)</ref>, JSIS ( <ref type="bibr" target="#b3">de Geus, Meletis, and Dubbelman 2018)</ref>, TASCNet , Panoptic FPN <ref type="bibr" target="#b7">(Kirillov et al. 2019a</ref>), OANet , AUNet ), UPSNet <ref type="bibr" target="#b20">(Xiong et al. 2019)</ref>, and OCFusion <ref type="bibr" target="#b8">(Lazarow, Lee, and Tu 2019)</ref>. As shown in <ref type="table" target="#tab_4">Table 3</ref>, with ResNet-101-FPN as the backbone, our proposed SOGNet achieves the highest singlemodel performance on the COCO test-dev set. It has a 1.3% PQ improvement than AUNet that uses a larger backbone. SOGNet also performs better than UPSNet using the same backbone and training schedule.</p><p>The results of SOGNet on the COCO and Cityscapes validation set are shown in <ref type="table" target="#tab_6">Table 4</ref>. It is observed that SOGNet generalizes well to Cityscapes. It has a 0.7% improvement than TASCNet and UPSNet. On the COCO validation set, SOGNet has a 1.2% improvement than UPSNet using the same backbone. The mIoU and AP of SOGNet are 54.56 and 34.2 on COCO, which are similar to the results of UPSNet (54.3 and 34.3 as reported). It indicates that our better panoptic performance is not derived from a stronger semantic or instance segmentation model. More importantly, SOGNet is the only method that can explicitly encode overlap relations and tell us which instance lies upon or beneath another.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this study, we aim to model overlap relations and resolve overlaps in a differentiable way for panoptic segmentation. We develop the SOGNet composed of the joint segmentation, the relational embedding module, the overlap resolving module, and the panoptic head. It is able to explicitly encode overlap relations without direct supervision on them.</p><p>Ablation studies detach SOGNet and analyze the efficacy of each component. Experiments demonstrate that SOGNet accurately predicts overlap relations, and outperforms the state-of-the-art methods on both COCO and Cityscapes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of the SOGNet for panoptic segmentation. The instance ground truths are input of our relational embedding module. During inference, they are replaced with the predictions from the instance segmentation head. The architecture is trained in an end-to-end manner. σ denotes the ReLU non-linear function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The Visualization of panoptic segmentation results of heuristic inference and SOGNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Visualization of the overlap relations encoded by O (down left) and the approximate ground truth, R (down right). Note that the activation on location (i, j) represents that the instance i is covered by (lies below) j. The indices of instances are marked in the images. Zoom in to have a better view. More visualization results can be found in the supplementary material.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0:dinning table</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4:laptop</cell></row><row><cell cols="3">Figure 3: Box Cat Mask PQ</cell><cell>SQ</cell><cell>RQ</cell><cell>OA</cell></row><row><cell>-</cell><cell>-</cell><cell cols="3">37.5 76.3 47.0 69.62</cell></row><row><cell>-</cell><cell>-</cell><cell cols="3">37.9 76.6 47.3 75.48</cell></row><row><cell></cell><cell>-</cell><cell cols="3">38.3 76.8 47.7 88.19</cell></row><row><cell></cell><cell></cell><cell cols="3">38.4 76.9 47.8 89.22</cell></row><row><cell cols="5">Weakly supervised 38.4 77.0 47.7 89.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparisons with SOTA performances on COCO test-dev set. The first block shows the top-3 entries in public leaderboard of COCO 2018 competition. The second block shows results in recent literatures. * denotes that the backbone has extra deformable convolution layers and longer training schedule is adopted.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Panoptic segmentation results of SOGNet and other state-of-the-art methods on Cityscapes and COCO. Multiscale testing and flipping are not used.</figDesc><table><row><cell>Image</cell><cell>GT</cell><cell>Heuristic</cell><cell>SOGNet</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We offer more visualization examples of the overlap relations predicted by our method, as shown in <ref type="figure">Figure 5</ref>. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
	<note>Pixelwise instance segmentation with a dynamically instantiated network</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Deformable convolutional networks. In ICCV</title>
		<imprint>
			<biblScope unit="page" from="764" to="773" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Panoptic segmentation with a joint semantic and instance segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meletis</forename><surname>Geus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dubbelman ; De Geus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">; C</forename><surname>Dubbelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02110</idno>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning hierarchical features for scene labeling</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>On, K.-W</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1564" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
		</imprint>
	</monogr>
	<note>Panoptic feature pyramid networks</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning instance occlusion for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Lazarow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tu ; Lazarow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05896</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename><forename type="middle">;</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2359" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<idno type="arXiv">arXiv:1812.01192</idno>
		<title level="m">Learning to fuse things and stuff</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Expectation-maximization attention networks for semantic segmentation</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9167" to="9176" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Proposal-free network for instance-level object segmentation</title>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2978" to="2991" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pixels to graphs by associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2171" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Han ; Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<idno type="arXiv">arXiv:1905.01220</idno>
		<title level="m">Seamless scene segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="560" to="570" />
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks with alternately updated clique</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2413" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<idno type="arXiv">arXiv:1902.05093</idno>
	</analytic>
	<monogr>
		<title level="m">Deeperlab: Single-shot image parser</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
