<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-to-Text Generation with Content Selection and Planning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
							<email>r.puduppully@sms.ed.ac.ukli.dong@ed.ac.ukmlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data-to-Text Generation with Content Selection and Planning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in data-to-text generation have led to the use of large-scale datasets and neural network models which are trained end-to-end, without explicitly modeling what to say and in what order. In this work, we present a neural network architecture which incorporates content selection and planning without sacrificing end-to-end training. We decompose the generation task into two stages. Given a corpus of data records (paired with descriptive documents), we first generate a content plan highlighting which information should be mentioned and in which order and then generate the document while taking the content plan into account. Automatic and human-based evaluation experiments show that our model 1 outperforms strong baselines improving the state-of-the-art on the recently released ROTOWIRE dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data-to-text generation broadly refers to the task of automatically producing text from non-linguistic input <ref type="bibr" target="#b36">(Reiter and Dale 2000;</ref><ref type="bibr" target="#b11">Gatt and Krahmer 2018)</ref>. The input may be in various forms including databases of records, spreadsheets, expert system knowledge bases, simulations of physical systems, and so on. <ref type="table" target="#tab_0">Table 1</ref> shows an example in the form of a database containing statistics on NBA basketball games, and a corresponding game summary.</p><p>Traditional methods for data-to-text generation <ref type="bibr" target="#b21">(Kukich 1983;</ref><ref type="bibr" target="#b28">McKeown 1992</ref>) implement a pipeline of modules including content planning (selecting specific content from some input and determining the structure of the output text), sentence planning (determining the structure and lexical content of each sentence) and surface realization (converting the sentence plan to a surface string). Recent neural generation systems <ref type="bibr" target="#b22">(Lebret et al. 2016;</ref><ref type="bibr" target="#b29">Mei et al. 2016;</ref><ref type="bibr" target="#b42">Wiseman et al. 2017</ref>) do not explicitly model any of these stages, rather they are trained in an end-to-end fashion using the very successful encoder-decoder architecture <ref type="bibr" target="#b1">(Bahdanau et al. 2015)</ref> as their backbone.</p><p>Despite producing overall fluent text, neural systems have difficulty capturing long-term structure and generating documents more than a few sentences long. <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref> show that neural text generation techniques perform poorly at content selection, they struggle to maintain inter-sentential coherence, and more generally a reasonable ordering of the selected facts in the output text. Additional challenges include avoiding redundancy and being faithful to the input. Interestingly, comparisons against templatebased methods show that neural techniques do not fare well on metrics of content selection recall and factual output generation (i.e., they often hallucinate statements which are not supported by the facts in the database).</p><p>In this paper, we address these shortcomings by explicitly modeling content selection and planning within a neural data-to-text architecture. Our model learns a content plan from the input and conditions on the content plan in order to generate the output document (see <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration). An explicit content planning mechanism has at least three advantages for multi-sentence document generation: it represents a high-level organization of the document structure allowing the decoder to concentrate on the easier tasks of sentence planning and surface realization; it makes the process of data-to-document generation more interpretable by generating an intermediate representation; and reduces redundancy in the output, since it is less likely for the content plan to contain the same information in multiple places. We train our model end-to-end using neural networks and evaluate its performance on ROTOWIRE <ref type="bibr" target="#b42">(Wiseman et al. 2017)</ref>, a recently released dataset which contains statistics of NBA basketball games paired with human-written summaries (see <ref type="table" target="#tab_0">Table 1</ref>). Automatic and human evaluation shows that modeling content selection and planning improves generation considerably over competitive baselines.</p><p>The Boston Celtics defeated the host Indiana Pacers 105-99 at Bankers Life Fieldhouse on Saturday. In a battle between two injury-riddled teams, the Celtics were able to prevail with a much needed road victory. The key was shooting and defense, as the Celtics outshot the Pacers from the field, from three-point range and from the free-throw line. Boston also held Indiana to 42 percent from the field and 22 percent from long distance. The Celtics also won the rebounding and assisting differentials, while tying the Pacers in turnovers. There were 10 ties and 10 lead changes, as this game went down to the final seconds. Boston (5-4) has had to deal with a gluttony of injuries, but they had the fortunate task of playing a team just as injured here. Isaiah Thomas led the team in scoring, totaling 23 points and five assists on 4-of-13 shooting. He got most of those points by going 14-of-15 from the free-throw line. Kelly Olynyk got a rare start and finished second on the team with his 16 points, six rebounds and four assists. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The generation literature provides multiple examples of content selection components developed for various domains which are either hand-built <ref type="bibr" target="#b21">(Kukich 1983;</ref><ref type="bibr" target="#b28">McKeown 1992;</ref><ref type="bibr" target="#b35">Reiter and Dale 1997;</ref><ref type="bibr" target="#b9">Duboue and McKeown 2003)</ref> or learned from data <ref type="bibr" target="#b2">(Barzilay and Lapata 2005;</ref><ref type="bibr" target="#b7">Duboue and McKeown 2001;</ref><ref type="bibr" target="#b9">2003;</ref><ref type="bibr" target="#b23">Liang et al. 2009;</ref><ref type="bibr" target="#b0">Angeli et al. 2010;</ref><ref type="bibr" target="#b16">Kim and Mooney 2010;</ref><ref type="bibr" target="#b20">Konstas and Lapata 2013)</ref>. Likewise, creating summaries of sports games has been a topic of interest since the early beginnings of generation systems <ref type="bibr" target="#b37">(Robin 1994;</ref><ref type="bibr" target="#b40">Tanaka-Ishii et al. 1998)</ref>.</p><p>Earlier work on content planning has relied on generic planners <ref type="bibr" target="#b5">(Dale 1988)</ref>, based on Rhetorical Structure Theory <ref type="bibr" target="#b14">(Hovy 1993) and</ref><ref type="bibr">schemas (McKeown et al. 1997)</ref>. They defined content planners by analysing the target texts and devising hand-crafted rules. <ref type="bibr" target="#b7">Duboue and McKeown (2001)</ref> studied ordering constraints for content plans and <ref type="bibr" target="#b8">Duboue and McKeown (2002)</ref> learn a content planner from an aligned corpus of inputs and human outputs. A few researchers <ref type="bibr" target="#b30">(Mellish et al. 1998;</ref><ref type="bibr" target="#b15">Karamanis 2004</ref>) rank content plans according to a ranking function.</p><p>More recent work focuses on end-to-end systems instead of individual components. However, most models make simplistic assumptions such as generation without any content selection or planning <ref type="bibr" target="#b3">(Belz 2008;</ref><ref type="bibr" target="#b43">Wong and Mooney 2007)</ref> or content selection without planning <ref type="bibr" target="#b19">(Konstas and Lapata 2012;</ref><ref type="bibr" target="#b0">Angeli et al. 2010;</ref><ref type="bibr" target="#b16">Kim and Mooney 2010</ref>). An exception are <ref type="bibr" target="#b20">Konstas and Lapata (2013)</ref> who incorporate content plans represented as grammar rules operating on the document level. Their approach works reasonably well with weather forecasts, but does not scale easily to larger databases, with richer vocabularies, and longer text descriptions. The model relies on the EM algorithm <ref type="bibr" target="#b6">(Dempster, Laird, and Rubin 1977)</ref> to learn the weights of the grammar rules which can be very many even when tokens are aligned to database records as a preprocessing step.</p><p>Our work is closest to recent neural network models which learn generators from data and accompanying text resources. Most previous approaches generate from Wikipedia infoboxes focusing either on single sentences <ref type="bibr" target="#b22">(Lebret et al. 2016;</ref><ref type="bibr" target="#b24">Sha et al. 2017;</ref><ref type="bibr" target="#b24">Liu et al. 2017)</ref> or short texts (Perez-Beltrachini and Lapata 2018). <ref type="bibr" target="#b29">Mei et al. (2016)</ref> use a neural encoder-decoder model to generate weather forecasts and soccer commentaries, while <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref> generate NBA game summaries (see <ref type="table" target="#tab_0">Table 1</ref>). They introduce a new dataset for data-to-document generation which is sufficiently large for neural network training and adequately challenging for testing the capabilities of document-scale text generation (e.g., the average summary length is 330 words and the average number of input records is 628). Moreover, they propose various automatic evaluation measures for assessing the quality of system output. Our model follows on from <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref> addressing the challenges for data-to-text generation identified in their work. We are not aware of any previous neural network-based approaches which incorporate content selection and planning mechanisms and generate multi-sentence documents. Perez-Beltrachini and Lapata (2018) introduce a content selection component (based on multi-instance learning) without content planning, while <ref type="bibr" target="#b24">Liu et al. (2017)</ref> propose a sentence planning mechanism which orders the contents of a Wikipedia infobox in order to generate a single sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>The input to our model is a table of records (see <ref type="table" target="#tab_0">Table 1</ref> left hand-side). Each record r j has four features including its type (r j,1 ; e.g., LOSS, CITY), entity (r j,2 ; e.g., Pacers, Miles Turner), value (r j,3 ; e.g., 11, Indiana), and whether a player is on the home-or away-team (r j,4 ; see column H/V in <ref type="table" target="#tab_0">Table 1</ref>), represented as {r j,k } 4 k=1 . The output y is a document containing words y = y 1 · · · y |y| where |y| is the document length. <ref type="figure" target="#fig_1">Figure 2</ref> shows the overall architecture of our model which consists of two stages: (a) content selection and planning operates on the input records of a database and produces a content plan specifying which records are to be verbalized in the document and in which order (see <ref type="table" target="#tab_4">Table 2</ref>) and (b) text generation produces the output text given the content plan as input; at each decoding step, the generation model attends over vector representations of the records in the content plan.</p><p>Let r = {r j } |r| j=1 denote a table of input records and y the output text. We model p(y|r) as the joint probability of text y and content plan z, given input r. We further decompose p(y, z|r) into p(z|r), a content selection and planning  In the following we explain how the components p(z|r) and p(y|r, z) are estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Record Encoder</head><p>The input to our model is a table of unordered records, each represented as features {r j,k } 4 k=1 . Following previous work <ref type="bibr" target="#b44">(Yang et al. 2017;</ref><ref type="bibr" target="#b42">Wiseman et al. 2017)</ref>, we embed features into vectors, and then use a multilayer perceptron to obtain a vector representation r j for each record:</p><formula xml:id="formula_0">r j = ReLU(W r [r j,1 ; r j,2 ; r j,3 ; r j,4 ] + b r )</formula><p>where [; ] indicates vector concatenation, W r ∈ R n×4n , b r ∈ R n are parameters, and ReLU is the rectifier activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content Selection Gate</head><p>The context of a record can be useful in determining its importance vis-a-vis other records in the table. For example, if a player scores many points, it is likely that other meaningfully related records such as field goals, three-pointers, or rebounds will be mentioned in the output summary. To better capture such dependencies among records, we make use of the content selection gate mechanism as shown in <ref type="figure">Figure 3</ref>.</p><p>We first compute the attention scores α j,k over the input table and use them to obtain an attentional vector r att j for each record r j :</p><formula xml:id="formula_1">α j,k ∝ exp(r j W a r k ) c j = k =j α j,k r k r att j = W g [r j ; c j ]</formula><p>where W a ∈ R n×n , W g ∈ R n×2n are parameter matrices, and k =j α j,k = 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content Selection</head><p>Figure 3: Content selection mechanism.</p><p>We next apply the content selection gating mechanism to r j , and obtain the new record representation r cs j via: g j = sigmoid r att j r cs j = g j r j where denotes element-wise multiplication, and gate g j ∈ [0, 1] n controls the amount of information flowing from r j . In other words, each element in r j is weighed by the corresponding element of the content selection gate g j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content Planning</head><p>In our generation task, the output text is long but follows a canonical structure. Game summaries typically begin by discussing which team won/lost, following with various statistics involving individual players and their teams (e.g., who performed exceptionally well or under-performed), and finishing with any upcoming games. We hypothesize that generation would benefit from an explicit plan specifying both what to say and in which order. Our model learns such content plans from training data. However, notice that RO-TOWIRE (see <ref type="table" target="#tab_0">Table 1</ref>) and most similar data-to-text datasets do not naturally contain content plans. Fortunately, we can obtain these relatively straightforwardly following an information extraction approach (which we explain in Section 4).</p><p>Suffice it to say that plans are extracted by mapping the text in the summaries onto entities in the input table, their values, and types (i.e., relations). A plan is a sequence of pointers with each entry pointing to an input record {r j } |r| j=1 . An excerpt of a plan is shown in <ref type="table" target="#tab_4">Table 2</ref>. The order in the plan corresponds to the sequence in which entities appear in the game summary. Let z = z 1 . . . z |z| denote the content planning sequence. Each z k points to an input record, i.e., z k ∈ {r j } |r| j=1 . Given the input records, the probability p(z|r) is decomposed as:</p><formula xml:id="formula_2">p(z|r) = |z| k=1 p(z k |z &lt;k , r) where z &lt;k = z 1 . . . z k−1 .</formula><p>Since the output tokens of the content planning stage correspond to positions in the input sequence, we make use of Pointer Networks <ref type="bibr" target="#b41">(Vinyals et al. 2015)</ref>. The latter use attention to point to the tokens of the input sequence rather than creating a weighted representation of source encodings. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, given {r j } |r| j=1 , we use an LSTM decoder to generate tokens corresponding to positions in the <ref type="table" target="#tab_0">Value   Entity  Type  H/V  Boston  Celtics  TEAM-CITY  V  Celtics  Celtics  TEAM-NAME  V  105  Celtics  TEAM-PTS  V  Indiana  Pacers  TEAM-CITY  H  Pacers  Pacers  TEAM-NAME  H  99  Pacers  TEAM-PTS  H  42  Pacers  TEAM-FG PCT  H  22  Pacers  TEAM-FG3 PCT  H  5  Celtics  TEAM-WIN  V  4</ref> Celtics  input. The first hidden state of the decoder is initialized by avg({r cs j } |r| j=1 ), i.e., the average of record vectors. At decoding step k, let h k be the hidden state of the LSTM. We model p(z k = r j |z &lt;k , r) as the attention over input records:</p><formula xml:id="formula_3">p(z k = r j |z &lt;k , r) ∝ exp(h k W c r cs j )</formula><p>where the probability is normalized to 1, and W c are parameters. Once z k points to record r j , we use the corresponding vector r cs j as the input of the next LSTM unit in the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Generation</head><p>The probability of output text y conditioned on content plan z and input table r is modeled as:</p><formula xml:id="formula_4">p(y|r, z) = |y| t=1 p(y t |y &lt;t , z, r)</formula><p>where y &lt;t = y 1 . . . y t−1 . We use the encoder-decoder architecture with an attention mechanism to compute p(y|r, z).</p><p>We first encode the content plan z into {e k } |z| k=1 using a bidirectional LSTM. Because the content plan is a sequence of input records, we directly feed the corresponding record vectors {r cs j } |r| j=1 as input to the LSTM units, which share the record encoder with the first stage.</p><p>The text decoder is also based on a recurrent neural network with LSTM units. The decoder is initialized with the hidden states of the final step in the encoder. At decoding step t, the input of the LSTM unit is the embedding of the previously predicted word y t−1 . Let d t be the hidden state of the t-th LSTM unit. The probability of predicting y t from the output vocabulary is computed via:</p><formula xml:id="formula_5">β t,k ∝ exp(d t W b e k )</formula><p>(1)</p><formula xml:id="formula_6">q t = k β t,k e k d att t = tanh(W d [d t ; q t ]) p gen (y t |y &lt;t , z, r)=softmax yt (W y d att t + b y ) (2) where k β t,k = 1, W b ∈ R n×n , W d ∈ R n×2n , W y ∈ R n×|Vy| , b y ∈ R |Vy| are parameters, and |V y | is the output vocabulary size.</formula><p>We further augment the decoder with a copy mechanism, i.e., the ability to copy words directly from the value portions of records in the content plan (i.e., {z k } |z| k=1 ). We experimented with joint <ref type="bibr" target="#b12">(Gu et al. 2016</ref>) and conditional copy methods <ref type="bibr" target="#b13">(Gulcehre et al. 2016)</ref>. Specifically, we introduce a variable u t ∈ {0, 1} for each time step to indicate whether the predicted token y t is copied (u t = 1) or not (u t = 0). The probability of generating y t is computed by:</p><formula xml:id="formula_7">p(y t |y &lt;t , z, r) = ut∈{0,1} p(y t , u t |y &lt;t , z, r)</formula><p>where u t is marginalized out.</p><p>Joint Copy The probability of copying from record values and generating from the vocabulary is globally normalized:</p><formula xml:id="formula_8">p(y t , u t |y &lt;t , z, r) ∝ yt←z k exp(d t W b e k ) u t = 1 exp (W y d att t + b y ) u t = 0</formula><p>where y t ← z k indicates that y t can be copied from z k , W b is shared as in Equation <ref type="formula">(1)</ref>, and W y , b y are shared as in Equation <ref type="formula">(2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional Copy</head><p>The variable u t is first computed as a switch gate, and then is used to obtain the output probability:</p><formula xml:id="formula_9">p(u t = 1|y &lt;t , z, r) = sigmoid(w u · d t + b u ) p(y t , u t |y &lt;t , z, r) = p(u t |y &lt;t , z, r) yt←z k β t,k u t = 1 p(u t |y &lt;t , z, r)p gen (y t |y &lt;t , z, r) u t = 0</formula><p>where β t,k and p gen (y t |y &lt;t , z, r) are computed as in Equations (1)-(2), and w u ∈ R n , b u ∈ R are parameters. Following <ref type="bibr" target="#b13">Gulcehre et al. (2016)</ref> and <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref>, if y t appears in the content plan during training, we assume that y t is copied (i.e., u t = 1). 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Inference</head><p>Our model is trained to maximize the log-likelihood of the gold 3 content plan given table records r and the gold output text given the content plan and where z and y represent content plan and output text candidates, respectively. For each stage, we utilize beam search to approximately obtain the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Data We trained and evaluated our model on <ref type="bibr">ROTOWIRE Wiseman et al. (2017)</ref>, a dataset of basketball game summaries, paired with corresponding box-and line-score tables. The summaries are professionally written, relatively well structured and long (337 words on average). The number of record types is 39, the average number of records is 628, the vocabulary size is 11.3K words and token count is 1.6M. The dataset is ideally suited for document-scale generation. We followed the data partitions introduced in Wiseman et al. <ref type="bibr" target="#b39">(2017)</ref>: we trained on 3,398 summaries, tested on 728, and used 727 for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content Plan Extraction</head><p>We extracted content plans from the ROTOWIRE game summaries following an information extraction (IE) approach. Specifically, we used the IE system introduced in Wiseman et al. <ref type="bibr" target="#b39">(2017)</ref> which identifies candidate entity (i.e., player, team, and city) and value (i.e., number or string) pairs that appear in the text, and then predicts the type (aka relation) of each candidate pair. For instance, in the document in <ref type="table" target="#tab_0">Table 1</ref>, the IE system might identify the pair "Jeff Teague, 20" and then predict that that their relation is "PTS", extracting the record (Jeff Teague, 20, PTS). <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref> train an IE system on RO-TOWIRE by determining word spans which could represent entities (i.e., by matching them against players, teams or cities in the database) and numbers. They then consider each entity-number pair in the same sentence, and if there is a record in the database with matching entities and values, the pair is assigned the corresponding record type or otherwise given the label "none" to indicate unrelated pairs. We adopted their IE system architecture which predicts relations by ensembling 3 convolutional models and 3 bidirectional LSTM models. We trained this system on the train-3 Strictly speaking, the content plan is not gold since it was not created by an expert but is the output of a fairly accurate IE system. ing portion of the ROTOWIRE corpus. 4 On held-out data it achieved 94% accuracy, and recalled approximately 80% of the relations licensed by the records. Given the output of the IE system, a content plan simply consists of (entity, value, record type, h/v) tuples in their order of appearance in a game summary (the content plan for the summary in <ref type="table" target="#tab_0">Table 1</ref> is shown in <ref type="table" target="#tab_4">Table 2</ref>). Player names are pre-processed to indicate the individual's first name and surname (see Isaiah and Thomas in <ref type="table" target="#tab_4">Table 2)</ref>; team records are also pre-processed to indicate the name of team's city and the team itself (see Boston and Celtics in <ref type="table" target="#tab_4">Table 2</ref>).</p><p>Training Configuration We validated model hyperparameters on the development set. We did not tune the dimensions of word embeddings and LSTM hidden layers; we used the same value of 600 reported in <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref>. We used one-layer pointer networks during content planning, and two-layer LSTMs during text generation. Input feeding <ref type="bibr" target="#b26">(Luong et al. 2015)</ref> was employed for the text decoder. We applied dropout <ref type="bibr" target="#b45">(Zaremba et al. 2014</ref>) at a rate of 0.3. Models were trained for 25 epochs with the Adagrad optimizer <ref type="bibr" target="#b10">(Duchi et al. 2011)</ref>; the initial learning rate was 0.15, learning rate decay was selected from {0.5, 0.97}, and batch size was 5. For text decoding, we made use of BPTT <ref type="bibr" target="#b31">(Mikolov et al. 2010</ref>) and set the truncation size to 100. We set the beam size to 5 during inference. All models are implemented in OpenNMT-py <ref type="bibr" target="#b18">(Klein et al. 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Automatic Evaluation We evaluated model output using the metrics defined in <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref>. The idea is to employ a fairly accurate IE system (see the description in Section 4) on the gold and automatic summaries and compare whether the identified relations align or diverge.</p><p>Letŷ be the gold output, and y the system output. Content selection (CS) measures how well (in terms of precision and recall) the records extracted from y match those found inŷ. Relation generation (RG) measures the factuality of the generation system as the proportion of records extracted from y which are also found in r (in terms of precision and number of unique relations). Content ordering (CO) measures how well the system orders the records it has chosen and is computed as the normalized Damerau-Levenshtein Distance between the sequence of records extracted from y andŷ. In addition to these metrics, we report BLEU <ref type="bibr" target="#b33">(Papineni et al. 2002)</ref>, with human-written game summaries as reference.</p><p>Our results on the development set are summarized in <ref type="table" target="#tab_7">Table 3</ref>. We compare our Neural Content Planning model (NCP for short) against the two encoder-decoder (ED) models presented in <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref> with joint copy (JC) and conditional copy (CC), respectively. In addition to our own re-implementation of these models, we include the best scores reported in <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref> which were obtained with an encoder-decoder model enhanced with con-    <ref type="table" target="#tab_7">Table 3</ref> also shows results when NCP uses oracle content plans (OR) as input. In addition, we report the performance of a template-based generator <ref type="bibr" target="#b42">(Wiseman et al. 2017)</ref> which creates a document consisting of eight template sentences: an introductory sentence (who won/lost), six player-specific sentences (based on the six highest-scoring players in the game), and a conclusion sentence. As can be seen, NCP improves upon vanilla encoderdecoder models (ED+JC, ED+CC), irrespective of the copy mechanism being employed. In fact, NCP achieves comparable scores with either joint or conditional copy mechanism which indicates that it is the content planner which brings performance improvements. Overall, NCP+CC achieves best content selection and content ordering scores in terms of BLEU. Compared to the best reported system in Wiseman et al. <ref type="bibr" target="#b39">(2017)</ref>, we achieve an absolute improvement of approximately 12% in terms of relation generation; content selection precision also improves by 5% and recall by 15%, content ordering increases by 3%, and BLEU by 1.5 points. The results of the oracle system (NCP+OR) show that content selection and ordering do indeed correlate with the quality of the content plan and that any improvements in our planning component would result in better output. As far as the template-based system is concerned, we observe that it obtains low BLEU and CS precision but scores high on CS recall and RG metrics. This is not surprising as the template system is provided with domain knowledge which our model does not have, and thus represents an upper-bound on content selection and relation generation. We also measured  <ref type="bibr">-2017 23.72 74.80 29.49 36.18 15.42 14.19 NCP+JC 34.09 87.19 32.02 47.29 17.15 14.89 NCP+CC 34.28 87.47 34.18 51.22 18.58 16.50</ref>  We further conducted an ablation study with the conditional copy variant of our model (NCP+CC) to establish whether improvements are due to better content selection (CS) and/or content planning (CP). We see in <ref type="table" target="#tab_8">Table 4</ref> that content selection and planning individually contribute to performance improvements over the baseline (ED+CC), and accuracy further increases when both components are taken into account. In addition we evaluated these components on their own (independently of text generation) by comparing the output of the planner (see p(z|r) block in <ref type="figure">Figure</ref> 2) against gold content plans obtained using the IE system (see row NCP in <ref type="table" target="#tab_8">Table 4</ref>. Compared to the full system (NCP+CC), content selection precision and recall are higher (by 4.5% and 2%, respectively) as well as content ordering (by 1.8%). In another study, we used the CS and CO metrics to measure how well the generated text follows the content plan produced by the planner (instead of arbitrarily adding or removing information). We found out that NCP+CC generates game summaries which follow the content plan closely: CS precision is higher than 85%, CS recall is higher than 93%, and CO higher than 84%. This reinforces our claim that higher accuracies in the content selection and planning phases will result in further improvements in text generation.</p><p>The test set results in <ref type="table" target="#tab_10">Table 5</ref> follow a pattern similar to the development set. NCP achieves higher accuracy in all metrics including relation generation, content selection, content ordering, and BLEU compared to <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref>. We provide examples of system output in <ref type="table" target="#tab_11">Table 6</ref> and the supplementary material.</p><p>Human-Based Evaluation We conducted two human evaluation experiments using the Amazon Mechanical Turk (AMT) crowdsourcing platform. The first study assessed relation generation by examining whether improvements in relation generation attested by automatic evaluation metrics are indeed corroborated by human judgments. We compared our best performing model (NCP+CC), with gold reference summaries, a template system and the best model of Wiseman et al. <ref type="bibr" target="#b39">(2017)</ref>. AMT workers were presented with a spe-  <ref type="bibr">(bottom)</ref>. Text that accurately reflects a record in the associated box or line score is in blue, erroneous text is in red.</p><p>cific NBA game's box score and line score, and four (randomly selected) sentences from the summary. They were asked to identify supporting and contradicting facts mentioned in each sentence. We randomly selected 30 games from the test set. Each sentence was rated by three workers.</p><p>The left two columns in <ref type="table">Table 7</ref> contain the average number of supporting and contradicting facts per sentence as determined by the crowdworkers, for each model. The template-based system has the highest number of supporting facts, even compared to the human gold standard. TEMPL does not perform any content selection, it includes a large number of facts from the database and since it does not perform any generation either, it exhibits a few contradictions. Compared to WS-2017 and the Gold summaries, NCP+CC displays a larger number of supporting facts. All models are significantly 5 different in the number of supporting facts (#Supp) from TEMPL (using a one-way ANOVA with posthoc Tukey HSD tests). NCP+CC is significantly different from WS-2017 and Gold. With respect to contradicting facts (#Cont), Gold and TEMPL are not significantly different from each other but are significantly different from the neural systems (WS-2017, NCP+CC).</p><p>In the second experiment, we assessed the generation quality of our model. We elicited judgments for the same 30 games used in the first study. For each game, participants were asked to compare a human-written summary, NCP with conditional copy (NCP+CC), <ref type="bibr" target="#b42">Wiseman et al.'s (2017)</ref> best model, and the template system. Our study used Best-Worst Scaling (BWS; <ref type="bibr" target="#b25">Louviere, Flynn, and Marley 2015)</ref>, a technique shown to be less labor-intensive and providing more reliable results as compared to rating scales <ref type="bibr" target="#b17">(Kiritchenko and Mohammad 2017)</ref>. We arranged every 4-tuple of com-5 All significance differences reported throughout this paper are with a level less than 0.05.  <ref type="table">Table 7</ref>: Average number of supporting (#Support) and contradicting (#Contra) facts in game summaries and bestworst scaling evaluation (higher is better) for grammaticality (Gram), Coherence (Cohere), and Conciseness (Concise).</p><p>peting summaries into 6 pairs. Every pair was shown to three crowdworkers, who were asked to choose which summary was best and which was worst according to three criteria: Grammaticality (is the summary fluent and grammatical?), Coherence (is the summary easy to read? does it follow a natural ordering of facts?), and Conciseness (does the summary avoid redundant information and repetitions?). The score of a system for each criterion is computed as the difference between the percentage of times the system was selected as the best and the percentage of times it was selected as the worst <ref type="bibr" target="#b32">(Orme 2009</ref>). The scores range from −100 (absolutely worst) to +100 (absolutely best).</p><p>The results of the second study are summarized in <ref type="table">Table 7</ref>. Gold summaries were perceived as significantly better compared to the automatic systems across all criteria (again using a one-way ANOVA with post-hoc Tukey HSD tests). NCP+CC was perceived as significantly more grammatical than WS-2017 but not compared to TEMPL which does not suffer from fluency errors since it does not perform any generation. NCP+CC was perceived as significantly more coherent than TEMPL and WS-2017. The template fairs poorly on coherence, its output is stilted and exhibits no variability (see top block in <ref type="table" target="#tab_11">Table 6</ref>). With regard to conciseness, the neural systems are significantly worse than TEMPL, while NCP+CC is significantly better than WS-2017. By design the template cannot repeat information since there is no redundancy in the sentences chosen to verbalize the summary.</p><p>Taken together, our results show that content planning improves data-to-text generation across metrics and systems. We find that NCP+CC overall performs best, however there is a significant gap between automatically generated summaries and human-authored ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented a data-to-text generation model which is enhanced with content selection and planning modules. Experimental results (based on automatic metrics and judgment elicitation studies) demonstrate that generation quality improves both in terms of the number of relevant facts contained in the output text, and the order according to which these are presented. Positive side-effects of content planning are additional improvements in the grammaticality, and conciseness of the generated text. In the future, we would like to learn more detail-oriented plans involving inference over multiple facts and entities. We would also like to verify our approach across domains and languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the Results in Wiseman et al.'s (2017) Webpage</head><p>There was a bug in the dataset creation of <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref> which they identified and corrected. They also posted updated scores on their webpage. <ref type="bibr">6</ref> We have used this corrected dataset in our experiments. We then discovered a bug in their code which computes the automatic metrics. The scores reported in this paper are using the corrected automatic metrics. To make the scores on our paper comparable to the numbers published on the webpage of <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref>, we recompute here our scores with older IE metrics (without the bug fix) in  <ref type="table" target="#tab_0">Table 10</ref> shows two sample documents generated using the template system, <ref type="bibr" target="#b42">Wiseman et al. (2017</ref><ref type="bibr">) (WS-2017</ref> and our neural content planning model with conditional copy (NCP+CC). The text is highlighted in blue if it agrees with respective box/line scores and red if the text contradicts box/line scores. We also use the orange color to highlight repetitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Examples</head><p>The template documents are gold standard in relation generation accuracy and they appear all in blue. The documents of <ref type="bibr" target="#b42">Wiseman et al. (2017)</ref> show instances of contradictions and tend to be verbose containing duplicate text too. In contrast, our neural content planning model generates more factual text with fewer contradictions to box/line scores and less duplicate information.  <ref type="bibr">-2017 16.93 75.74 31.2 38.94 14.98 14.57 ED+JC 16.65 77.82 31.79 37.82 14.50 13.22 ED+CC 16.5 74.7 29.87 37.00 14.10 13.31 NCP+JC 24.74 88.66 34.80 53.67 16.34 14.92 NCP+CC 24.95 88.08 35.50 55.36 17.23 16.19 NCP+OR 16.11 89.35 86.39 87.12 52.25 24.11</ref>    <ref type="bibr">(8)</ref><ref type="bibr">(9)</ref><ref type="bibr">(10)</ref><ref type="bibr">(11)</ref><ref type="bibr">(12)</ref><ref type="bibr">(13)</ref><ref type="bibr">(14)</ref><ref type="bibr">(15)</ref><ref type="bibr">(4)</ref><ref type="bibr">(5)</ref><ref type="bibr">(6)</ref><ref type="bibr">(7)</ref> to go with 3 rebounds. Nikola Jokic scored 17 points (6-10 FG, 0-0 3PT, 5-7 FT) to go with 11 rebounds. Markieff Morris scored 15 points (5-12 FG, 0-0 3PT, 5-5 FT) to go with 3 rebounds. John Wall scored 15 points (5-14 FG, 0-4 3PT, 5-6 FT) to go with 7 rebounds. Danilo Gallinari scored 14 points (3-11 FG, 1-8 3PT, 7-9 FT) to go with 4 rebounds. Jusuf Nurkic scored 13 points (6-6 FG, 0-0 3PT, 1-2 FT) to go with 7 rebounds. The Washington Wizards' next game will be at home against the Dallas Mavericks, while the Denver Nuggets will travel to play the Bulls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WS-2017</head><p>The Washington Wizards (8-13) defeated the Denver Nuggets (8-15) 92-85 on Wednesday at the Verizon Center in Washington. The Wizards got off to a hot start in this one, out-scoring the Nuggets 29-18 in the first quarter alone. The Nuggets were able to out-score the Nuggets 28-12 in the third quarter, while the Wizards were able to coast to a victory in front of their home crowd. The Nuggets were the superior shooters in this game, going 45 percent from the field and 32 percent from the three-point line, while the Wizards went just 41 percent from the floor and 32 percent from beyond the arc. The Nuggets were also able to shoot 41 percent from the field and 32 percent from the three-point line , while the Nuggets went just 41 percent from the floor and 32 percent from beyond the arc. The Nuggets were led by the duo of John Wall and Bradley Beal. Wall went 5-for-14 from the field and 0-for-4 from the three-point line to score a game-high of 15 points, while also adding seven rebounds and five assists. The Golden State Warriors (10-2) defeated the Boston Celtics (6-6) 104-88. Klay Thompson scored 28 points (12-21 FG, 3-6 3PT, 1-1 FT) to go with 4 rebounds. Kevin Durant scored 23 points (10-13 FG, 1-2 3PT, 2-4 FT) to go with 10 rebounds. Isaiah Thomas scored 18 points (4-12 FG, 1-6 3PT, 9-9 FT) to go with 2 rebounds. Avery Bradley scored 17 points (7-15 FG, 2-4 3PT, 1-2 FT) to go with 10 rebounds. Stephen Curry scored 16 points (7-20 FG, 2-10 3PT, 0-0 FT) to go with 3 rebounds. Terry Rozier scored 11 points (3-5 FG, 2-3 3PT, 3-4 FT) to go with 7 rebounds. The Golden State Warriors' next game will be at home against the Dallas Mavericks, while the Boston Celtics will travel to play the Bulls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WS-2017</head><p>The Golden State Warriors defeated the Boston Celtics, 104-88, at TD Garden on Wednesday. The Warriors (10-2) checked in to Saturday's contest with only two road wins in their last 11 games, but they were able to come away with a win against the Celtics (6-6) on Friday. The Warriors (10-2) were able to pull away in the second half, outscoring the Celtics (6-6) by a 31-9 margin over the final 12 minutes. However, Golden State was able to pull away in the second half, outscoring the Celtics by a 31-9 margin over the final 12 minutes. The Warriors were led by Kevin Durant's 23 points, which he supplemented with seven rebounds, seven assists, two steals and a block. Stephen Curry was next with 16 points, eight assists, three rebounds and four steals. Klay Thompson was next with a 28-point, 10-rebound double-double that also included three assists, two steals and a block. Draymond Green was next with 11 points, eight rebounds, eight assists and two blocks. Draymond Green was next with 11 points, eight rebounds, eight assists and two blocks. Draymond Green supplied 11 points, eight rebounds, eight assists, two blocks and a steal. David West paced the reserves with 4 points, two rebounds, a block and a block. The Celtics were paced by Thomas' 18 points, which he supplemented with four assists, two rebounds and four steals. Avery Bradley posted a 17-point, 10-rebound double-double that also included two assists, two steals and a block. Avery Bradley posted a 17-point, 10-rebound double-double that also included two assists, two steals and a block. Kelly Olynyk led the second unit with 11 points, three rebounds, two assists and a pair of steals. The Warriors head back home to face off with the Detroit Pistons on Friday night, while the Celtics remain home and await the Toronto Raptors for a Wednesday night showdown. NCP+CC</p><p>The Golden State Warriors defeated the Boston Celtics 104-88 at TD Garden on Friday. The Warriors (10-2) came into this game winners of five of their last six games, but the Warriors (6-6) were able to pull away in the second half. Klay Thompson led the way for the Warriors with 28 points on 12-of-21 shooting, while Kevin Durant added 23 points, 10 rebounds, seven assists and two steals. Stephen Curry added 16 points and eight assists, while Draymond Green rounded out the box score with 11 points, eight rebounds and eight assists. For the Celtics, it was Isaiah Thomas who shot just 4-of-12 from the field and finished with 18 points. Avery Bradley added 17 points and 10 rebounds, while the rest of the Celtics combined to score just seven points. Boston will look to get back on track as they play host to the 76ers on Friday. <ref type="table" target="#tab_0">Table 10</ref>: Example documents from the template-based system, WS-2017, the best system of Wiseman et al. <ref type="bibr" target="#b39">(2017)</ref>, and our Neural Content Planning model with conditional copy (NCP+CC). Text that accurately reflects a record in the associated box or line score is recorded in blue, erroneous text is marked in red, duplicate text is marked in orange.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Block diagram of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Generation model with content selection and planning; the content selection gate is illustrated in Figure 3. phase, and p(y|r, z), a text generation phase: p(y|r) = z p(y, z|r) = z p(z|r)p(y|r, z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example of data-records and document summary. Entities and values corresponding to the plan inTable 2 are boldfaced.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Content plan for the example in Table 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table records :</head><label>records</label><figDesc>max (r,z,y)∈D log p (z|r) + log p (y|r, z)where D represents training examples (input records, plans, and game summaries). During inference, the output for input r is predicted by:ẑ</figDesc><table><row><cell>= arg max</cell><cell>p(z |r)</cell></row><row><cell>z</cell><cell></cell></row><row><cell>y = arg max</cell><cell>p(y |r,ẑ)</cell></row><row><cell>y</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>23.95 75.10 28.11 35.86 15.33 14.57 ED+JC 22.98 76.07 27.70 33.29 14.36 13.22 ED+CC 21.94 75.08 27.96 32.71 15.03 13.31 NCP+JC 33.37 87.40 32.20 48.56 17.98 14.92 NCP+CC 33.88 87.51 33.52 51.21 18.57 16.19 NCP+OR 21.59 89.21 88.52 85.84 78.51 24.11</figDesc><table><row><cell>Model</cell><cell>#</cell><cell>RG P%</cell><cell>P%</cell><cell>CS</cell><cell>R%</cell><cell>CO DLD%</cell><cell>BLEU</cell></row><row><cell cols="6">TEMPL 54.29 99.92 26.61 59.16</cell><cell>14.42</cell><cell>8.51</cell></row><row><cell>WS-2017</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="6">: Automatic evaluation on ROTOWIRE development</cell></row><row><cell cols="6">set using relation generation (RG) count (#) and preci-</cell></row><row><cell cols="6">sion (P%), content selection (CS) precision (P%) and re-</cell></row><row><cell cols="6">call (R%), content ordering (CO) in normalized Damerau-</cell></row><row><cell cols="5">Levenshtein distance (DLD%), and BLEU.</cell><cell></cell></row><row><cell>Model</cell><cell>#</cell><cell>RG P%</cell><cell>CS P% R%</cell><cell>CO DLD%</cell><cell>BLEU</cell></row><row><cell cols="4">ED+CC 21.94 75.08 27.96 32.71</cell><cell>15.03</cell><cell>13.31</cell></row><row><cell cols="4">CS+CC 24.93 80.55 28.63 35.23</cell><cell>15.12</cell><cell>13.52</cell></row><row><cell cols="4">CP+CC 33.73 84.85 29.57 44.72</cell><cell>15.84</cell><cell>14.45</cell></row><row><cell cols="4">NCP+CC 33.88 87.51 33.52 51.21</cell><cell>18.57</cell><cell>16.19</cell></row><row><cell>NCP</cell><cell cols="2">34.46 -</cell><cell>38.00 53.72</cell><cell>20.27</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Ablation results on ROTOWIRE development set us- ing relation generation (RG) count (#) and precision (P%), content selection (CS) precision (P%) and recall (R%), con- tent ordering (CO) in normalized Damerau-Levenshtein dis- tance (DLD%), and BLEU.ditional copy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Automatic evaluation on ROTOWIRE test set using</cell></row><row><cell>relation generation (RG) count (#) and precision (P%), con-</cell></row><row><cell>tent selection (CS) precision (R%) and recall (R%), content</cell></row><row><cell>ordering (CO) in normalized Damerau-Levenshtein distance</cell></row><row><cell>(DLD%), and BLEU.</cell></row><row><cell>the degree to which the game summaries generated by our</cell></row><row><cell>model contain redundant information as the proportion of</cell></row><row><cell>non-duplicate records extracted from the summary by the IE</cell></row><row><cell>system. 84.5% of the records in NCP+CC are non-duplicates</cell></row><row><cell>compared to Wiseman et al. (2017) who obtain 72.9% show-</cell></row><row><cell>ing that our model is less repetitive.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Example output from TEMPL (top) and NPC+CC</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>(development set) and</cell></row><row><cell>Table 9 (test set).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>Model</cell><cell>#</cell><cell>RG P%</cell><cell>P%</cell><cell>CS</cell><cell>R%</cell><cell>CO DLD%</cell><cell>BLEU</cell></row><row><cell cols="6">TEMPL 54.17 99.95 23.74 72.36</cell><cell>11.68</cell><cell>8.46</cell></row><row><cell cols="6">WS-2017 16.83 75.62 32.80 39.93</cell><cell cols="2">15.62 14.19</cell></row><row><cell cols="6">NCP+JC 26.14 90.74 34.15 53.61</cell><cell cols="2">15.94 14.89</cell></row><row><cell cols="6">NCP+CC 25.41 88.31 36.07 56.17</cell><cell cols="2">16.94 16.50</cell></row><row><cell>: Automatic system evaluation on the ROTOWIRE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>development set using automatic evaluation metrics; rela-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tion generation (RG) count and precision, content selection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(CS) precision and recall, content ordering (CO) in normal-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ized Damerau-Levenshtein distance, and BLEU. TEMPL is</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>template system, WS-2017 is the best system of Wiseman et</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>al. (2017), ED+JC, ED+CC are encoder-decoder with joint</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>copy and conditional copy, respectively, NCP+JC, NCP+CC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>are our Neural Content Planning models with joint copy and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conditional copy, respectively. NCP+OR is the system with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>oracle content plan as input.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6 https://github.com/harvardnlp/data2text</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9</head><label>9</label><figDesc>-13) defeated the Denver Nuggets (8-15) 92-85. Bradley Beal scored 26 points</figDesc><table><row><cell>: Automatic system evaluation on the ROTOWIRE</cell></row><row><cell>test set using automatic evaluation metrics; relation gen-</cell></row><row><cell>eration (RG) count and precision, content selection (CS)</cell></row><row><cell>precision and recall, content ordering (CO) in normalized</cell></row><row><cell>Damerau-Levenshtein distance, and BLEU. TEMPL is tem-</cell></row><row><cell>plate system, WS-2017 is the best system of Wiseman et</cell></row><row><cell>al. (2017), NCP+JC, NCP+CC are our Neural Content Plan-</cell></row><row><cell>ning models with joint copy and conditional copy, respec-</cell></row><row><cell>tively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>It was his second double-double in a row, as he's combined for 47 points and 12 rebounds over his last two games. The only other Nugget to reach double figures in points was Wilson Chandler, who finished with 11 points and eight rebounds. It was his second double-double in a row, as he's combined for 47 points and 14 rebounds over that span. The only other Nugget to reach double figures in points was Wilson Chandler, who finished with 11 points and eight rebounds. The Nuggets' next game will be on the road against the Detroit Pistons on Friday, while the Nuggets will be at home against the New York Knicks on Friday.NCP+CCThe Washington Wizards defeated the visiting Denver Nuggets 92-85 at Verizon Center on Monday. The Wizards (8-13) came into this game winners of five of their last eight games, but the Wizards (8-15) jumped out to a 10-point lead at the end of the first quarter. Bradley Beal led the way for the Wizards with a game-high 26 points on 8-of-15 shooting from the field. John Wall shot 5-of-14 from the field on his way to 15 points, to go along with seven rebounds, five assists, three steals and one block. Jusuf Nurkic chipped in 13 points, seven rebounds and one assist. Jameer Nelson filled out the stat sheet with 10 points, eight assists, four rebounds and two steals in 36 minutes. As a team, it was a forgettable shooting night for the Nuggets, as the team shot just 46 percent from the field. Next up, the Nuggets play the second game of a back-to-back when they host the Denver Nuggets on Wednesday, while the Wizards host the Portland Trail Blazers on Friday. Template</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Following previous work<ref type="bibr" target="#b13">(Gulcehre et al. 2016;</ref><ref type="bibr" target="#b42">Wiseman et al. 2017)</ref> we learn whether yt can be copied from candidate z k by applying supervision during training. Specifically, we retain z k when the record entity and its value occur in same sentence in y.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">A bug in the code of<ref type="bibr" target="#b42">Wiseman et al. (2017)</ref> excluded number words from the output summary. We corrected the bug and this resulted in greater recall for the relations extracted from the summaries. See the supplementary material for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">points and 10 rebounds, while the rest of the Celtics combined to score just seven points. Boston will look to get back on track as they play host to the 76ers on Friday.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Golden State Warriors (10-2) defeated the Boston Celtics (6-6) 104-88. Klay Thompson scored 28 points (12-21 FG, 3-6 3PT, 1-1 FT) to go with 4 rebounds. Kevin Durant scored 23 points (10-13 FG, 1-2 3PT, 2-4 FT) to go with 10 rebounds. Isaiah Thomas scored 18 points (4-12 FG, 1-6 3PT, 9-9 FT) to go with 2 rebounds. Avery Bradley scored 17 points (7-15 FG, 2-4 3PT, 1-2 FT) to go with 10 rebounds. Stephen Curry scored 16 points (7-20 FG, 2-10 3PT, 0-0 FT) to go with 3 rebounds. Terry Rozier scored 11 points (3-5 FG, 2-3 3PT, 3-4 FT) to go with 7 rebounds. The Golden State Warriors' next game will be at home against the Dallas Mavericks, while the Boston Celtics will travel to play the Bulls.</p><p>The Golden State Warriors defeated the Boston Celtics 104-88 at TD Garden on Friday. The Warriors (10-2) came into this game winners of five of their last six games, but the Warriors (6-6) were able to pull away in the second half. Klay Thompson led the way for the Warriors with 28 points on 12-of-21 shooting, while Kevin Durant added 23 points, 10 rebounds, seven assists and two steals. Stephen Curry added 16 points and eight assists, while Draymond Green rounded out the box score with 11 points, eight rebounds and eight assists. For the Celtics, it was Isaiah Thomas who shot just 4-of-12 from the field and finished with 18 points. Avery Bradley added</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple domainindependent probabilistic approach to generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="502" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collective content selection for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="431" to="455" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to generate one-sentence biographies from wikidata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating referring expressions in a domain of objects and processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Ph.D. Dissertation</publisher>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. Series B (methodological)</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Empirically estimating order constraints for content planning in generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Content planner construction via evolutionary algorithms and a corpus-based fitness function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Natural Language Generation Conference</title>
		<meeting>the International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Statistical acquisition of content selection rules for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="65" to="170" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated discourse generation using discourse structure relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="341" to="385" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Entity coherence for descriptive text structuring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karamanis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>School of Informatics, University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative alignment and semantic parsing for learning from ambiguous supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010: Posters</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="543" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Best-worst scaling more reliable than rating scales: A case study on sentiment intensity annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02810</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised concept-to-text generation with hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="752" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A global model for concept-totext generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="305" to="346" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Design of a knowledge-based report generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09724</idno>
		<title level="m">Table-totext generation by structure-aware seq2seq learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A J</forename><surname>Marley</surname></persName>
		</author>
		<title level="m">Best-worst scaling: Theory, methods and applications</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language generation for multimedia healthcare briefings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ANLP</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="277" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-tofine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Experiments using stochastic search for text planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mellish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oberlander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Generation</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Maxdiff analysis: Simple counting, individuallevel logit, and hb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Orme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Sawtooth Software</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bootstrapping generators from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06385</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Building applied natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="87" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Building natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Revision-based generation of Natural Language Summaries providing historical Background</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation. Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<idno type="arXiv">arXiv:1709.00155</idno>
		<title level="m">Order-planning neural text generation from structured data</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reactive content selection in the generation of real-time soccer commentary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hasida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Noda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<editor>Cortes, C.</editor>
		<editor>Lawrence, N. D.</editor>
		<editor>Lee, D. D.</editor>
		<editor>Sugiyama, M.</editor>
		<editor>and Garnett, R.</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Challenges in datato-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generation by inverting a semantic parser that uses statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Referenceaware language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
