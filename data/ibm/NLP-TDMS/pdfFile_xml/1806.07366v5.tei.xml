<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Ordinary Differential Equations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
							<email>rtqichen@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
							<email>rubanova@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
							<email>duvenaud@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Ordinary Differential Equations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Models such as residual networks, recurrent neural network decoders, and normalizing flows build complicated transformations by composing a sequence of transformations to a hidden state:</p><formula xml:id="formula_0">h t+1 = h t + f (h t , θ t )<label>(1)</label></formula><p>where t ∈ {0 . . . T } and h t ∈ R D . These iterative updates can be seen as an Euler discretization of a continuous transformation <ref type="bibr" target="#b49">Ruthotto and Haber, 2018)</ref>.</p><p>What happens as we add more layers and take smaller steps? In the limit, we parameterize the continuous dynamics of hidden units using an ordinary differential equation (ODE) specified by a neural network:</p><formula xml:id="formula_1">dh(t) dt = f (h(t), t, θ)<label>(2)</label></formula><p>Starting from the input layer h(0), we can define the output layer h(T ) to be the solution to this ODE initial value problem at some time T . This value can be computed by a black-box differential equation solver, which evaluates the hidden unit dynamics f wherever necessary to determine the solution with the desired accuracy. <ref type="figure" target="#fig_0">Figure 1</ref> contrasts these two approaches.</p><p>Defining and evaluating models using ODE solvers has several benefits:</p><p>Memory efficiency In Section 2, we show how to compute gradients of a scalar-valued loss with respect to all inputs of any ODE solver, without backpropagating through the operations of the solver. Not storing any intermediate quantities of the forward pass allows us to train our models with constant memory cost as a function of depth, a major bottleneck of training deep models.</p><p>Adaptive computation Euler's method is perhaps the simplest method for solving ODEs. There have since been more than 120 years of development of efficient and accurate ODE solvers <ref type="bibr" target="#b48">(Runge, 1895;</ref><ref type="bibr" target="#b29">Kutta, 1901;</ref><ref type="bibr" target="#b21">Hairer et al., 1987)</ref>. Modern ODE solvers provide guarantees about the growth of approximation error, monitor the level of error, and adapt their evaluation strategy on the fly to achieve the requested level of accuracy. This allows the cost of evaluating a model to scale with problem complexity. After training, accuracy can be reduced for real-time or low-power applications.</p><p>Scalable and invertible normalizing flows An unexpected side-benefit of continuous transformations is that the change of variables formula becomes easier to compute. In Section 4, we derive this result and use it to construct a new class of invertible density models that avoids the single-unit bottleneck of normalizing flows, and can be trained directly by maximum likelihood.</p><p>Continuous time-series models Unlike recurrent neural networks, which require discretizing observation and emission intervals, continuously-defined dynamics can naturally incorporate data which arrives at arbitrary times. In Section 5, we construct and demonstrate such a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Reverse-mode automatic differentiation of ODE solutions</head><p>The main technical difficulty in training continuous-depth networks is performing reverse-mode differentiation (also known as backpropagation) through the ODE solver. Differentiating through the operations of the forward pass is straightforward, but incurs a high memory cost and introduces additional numerical error.</p><p>We treat the ODE solver as a black box, and compute gradients using the adjoint sensitivity method <ref type="bibr" target="#b42">(Pontryagin et al., 1962)</ref>. This approach computes gradients by solving a second, augmented ODE backwards in time, and is applicable to all ODE solvers. This approach scales linearly with problem size, has low memory cost, and explicitly controls numerical error.</p><p>Consider optimizing a scalar-valued loss function L(), whose input is the result of an ODE solver:</p><formula xml:id="formula_2">L(z(t 1 )) = L z(t 0 ) + t1 t0 f (z(t), t, θ)dt = L (ODESolve(z(t 0 ), f, t 0 , t 1 , θ))<label>(3)</label></formula><p>Adjoint State State <ref type="figure">Figure 2</ref>: Reverse-mode differentiation of an ODE solution. The adjoint sensitivity method solves an augmented ODE backwards in time. The augmented system contains both the original state and the sensitivity of the loss with respect to the state. If the loss depends directly on the state at multiple observation times, the adjoint state must be updated in the direction of the partial derivative of the loss with respect to each observation.</p><p>To optimize L, we require gradients with respect to θ. The first step is to determining how the gradient of the loss depends on the hidden state z(t) at each instant. This quantity is called the adjoint a(t) = ∂L /∂z(t). Its dynamics are given by another ODE, which can be thought of as the instantaneous analog of the chain rule:</p><formula xml:id="formula_3">da(t) dt = −a(t) T ∂f (z(t), t, θ) ∂z<label>(4)</label></formula><p>We can compute ∂L /∂z(t0) by another call to an ODE solver. This solver must run backwards, starting from the initial value of ∂L /∂z(t1). One complication is that solving this ODE requires the knowing value of z(t) along its entire trajectory. However, we can simply recompute z(t) backwards in time together with the adjoint, starting from its final value z(t 1 ).</p><p>Computing the gradients with respect to the parameters θ requires evaluating a third integral, which depends on both z(t) and a(t):</p><formula xml:id="formula_4">dL dθ = − t0 t1 a(t) T ∂f (z(t), t, θ) ∂θ dt (5)</formula><p>The vector-Jacobian products a(t) T ∂f ∂z and a(t) T ∂f ∂θ in (4) and (5) can be efficiently evaluated by automatic differentiation, at a time cost similar to that of evaluating f . All integrals for solving z, a and ∂L ∂θ can be computed in a single call to an ODE solver, which concatenates the original state, the adjoint, and the other partial derivatives into a single vector. Algorithm 1 shows how to construct the necessary dynamics, and call an ODE solver to compute all gradients at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Reverse-mode derivative of an ODE initial value problem</head><p>Input: dynamics parameters θ, start time t 0 , stop time t 1 , final state z(t 1 ), loss gradient ∂L /∂z(t1)</p><formula xml:id="formula_5">s 0 = [z(t 1 ), ∂L ∂z(t1) , 0 |θ| ] Define initial augmented state def aug_dynamics([z(t), a(t), ·], t, θ): Define dynamics on augmented state return [f (z(t), t, θ), −a(t) T ∂f ∂z , −a(t) T ∂f ∂θ ] Compute vector-Jacobian products [z(t 0 ), ∂L ∂z(t0) , ∂L ∂θ ] = ODESolve(s 0 , aug_dynamics, t 1 , t 0 , θ) Solve reverse-time ODE return ∂L ∂z(t0) , ∂L ∂θ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Return gradients</head><p>Most ODE solvers have the option to output the state z(t) at multiple times. When the loss depends on these intermediate states, the reverse-mode derivative must be broken into a sequence of separate solves, one between each consecutive pair of output times <ref type="figure">(Figure 2</ref>). At each observation, the adjoint must be adjusted in the direction of the corresponding partial derivative ∂L /∂z(ti). Software To solve ODE initial value problems numerically, we use the implicit Adams method implemented in LSODE and VODE and interfaced through the scipy.integrate package. Being an implicit method, it has better guarantees than explicit methods such as Runge-Kutta but requires solving a nonlinear optimization problem at every step. This setup makes direct backpropagation through the integrator difficult. We implement the adjoint sensitivity method in Python's autograd framework <ref type="bibr" target="#b36">(Maclaurin et al., 2015)</ref>. For the experiments in this section, we evaluated the hidden state dynamics and their derivatives on the GPU using Tensorflow, which were then called from the Fortran ODE solvers, which were called from Python autograd code. </p><formula xml:id="formula_6">1-Layer MLP † 1.60% 0.24 M - - ResNet 0.41% 0.60 M O(L) O(L) RK-Net 0.47% 0.22 M O(L) O(L) ODE-Net 0.42% 0.22 M O(1) O(L)</formula><p>Model Architectures We experiment with a small residual network which downsamples the input twice then applies 6 standard residual blocks <ref type="bibr" target="#b23">He et al. (2016b)</ref>, which are replaced by an ODESolve module in the ODE-Net variant. We also test a network with the same architecture but where gradients are backpropagated directly through a Runge-Kutta integrator, referred to as RK-Net. <ref type="table" target="#tab_1">Table 1</ref> shows test error, number of parameters, and memory cost. L denotes the number of layers in the ResNet, andL is the number of function evaluations that the ODE solver requests in a single forward pass, which can be interpreted as an implicit number of layers. We find that ODE-Nets and RK-Nets can achieve around the same performance as the ResNet.</p><p>Error Control in ODE-Nets ODE solvers can approximately ensure that the output is within a given tolerance of the true solution. Changing this tolerance changes the behavior of the network. We first verify that error can indeed be controlled in <ref type="figure" target="#fig_1">Figure 3a</ref>. The time spent by the forward call is proportional to the number of function evaluations <ref type="figure" target="#fig_1">(Figure 3b</ref>), so tuning the tolerance gives us a trade-off between accuracy and computational cost. One could train with high accuracy, but switch to a lower accuracy at test time.  <ref type="figure" target="#fig_1">Figure 3c</ref>) shows a surprising result: the number of evaluations in the backward pass is roughly half of the forward pass. This suggests that the adjoint sensitivity method is not only more memory efficient, but also more computationally efficient than directly backpropagating through the integrator, because the latter approach will need to backprop through each function evaluation in the forward pass.</p><p>Network Depth It's not clear how to define the 'depth' of an ODE solution. A related quantity is the number of evaluations of the hidden state dynamics required, a detail delegated to the ODE solver and dependent on the initial state or input. <ref type="figure" target="#fig_1">Figure 3d</ref> shows that he number of function evaluations increases throughout training, presumably adapting to increasing complexity of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Continuous Normalizing Flows</head><p>The discretized equation (1) also appears in normalizing flows (Rezende and Mohamed, 2015) and the NICE framework <ref type="bibr" target="#b12">(Dinh et al., 2014)</ref>. These methods use the change of variables theorem to compute exact changes in probability if samples are transformed through a bijective function f :</p><formula xml:id="formula_7">z 1 = f (z 0 ) =⇒ log p(z 1 ) = log p(z 0 ) − log det ∂f ∂z 0 (6)</formula><p>An example is the planar normalizing flow (Rezende and Mohamed, 2015):</p><formula xml:id="formula_8">z(t + 1) = z(t) + uh(w T z(t) + b), log p(z(t + 1)) = log p(z(t)) − log 1 + u T ∂h ∂z<label>(7)</label></formula><p>Generally, the main bottleneck to using the change of variables formula is computing of the determinant of the Jacobian ∂f /∂z, which has a cubic cost in either the dimension of z, or the number of hidden units. Recent work explores the tradeoff between the expressiveness of normalizing flow layers and computational cost <ref type="bibr" target="#b28">(Kingma et al., 2016;</ref><ref type="bibr" target="#b57">Tomczak and Welling, 2016;</ref><ref type="bibr" target="#b5">Berg et al., 2018)</ref>.</p><p>Surprisingly, moving from a discrete set of layers to a continuous transformation simplifies the computation of the change in normalizing constant: Theorem 1 (Instantaneous Change of Variables). Let z(t) be a finite continuous random variable with probability p(z(t)) dependent on time. Let dz dt = f (z(t), t) be a differential equation describing a continuous-in-time transformation of z(t). Assuming that f is uniformly Lipschitz continuous in z and continuous in t, then the change in log probability also follows a differential equation,</p><formula xml:id="formula_9">∂ log p(z(t)) ∂t = −tr df dz(t)<label>(8)</label></formula><p>Proof in Appendix A. Instead of the log determinant in (6), we now only require a trace operation. Also unlike standard finite flows, the differential equation f does not need to be bijective, since if uniqueness is satisfied, then the entire transformation is automatically bijective.</p><p>As an example application of the instantaneous change of variables, we can examine the continuous analog of the planar flow, and its change in normalization constant:</p><formula xml:id="formula_10">dz(t) dt = uh(w T z(t) + b), ∂ log p(z(t)) ∂t = −u T ∂h ∂z(t)<label>(9)</label></formula><p>Given an initial distribution p(z(0)), we can sample from p(z(t)) and evaluate its density by solving this combined ODE.</p><p>Using multiple hidden units with linear cost While det is not a linear function, the trace function is, which implies tr( n J n ) = n tr(J n ). Thus if our dynamics is given by a sum of functions then the differential equation for the log density is also a sum:</p><formula xml:id="formula_11">dz(t) dt = M n=1 f n (z(t)), d log p(z(t)) dt = M n=1 tr ∂f n ∂z<label>(10)</label></formula><p>This means we can cheaply evaluate flow models having many hidden units, with a cost only linear in the number of hidden units M . Evaluating such 'wide' flow layers using standard normalizing flows costs O(M 3 ), meaning that standard NF architectures use many layers of only a single hidden unit.</p><p>Time-dependent dynamics We can specify the parameters of a flow as a function of t, making the differential equation f (z(t), t) change with t. This is parameterization is a kind of hypernetwork <ref type="bibr" target="#b19">(Ha et al., 2016)</ref>. We also introduce a gating mechanism for each hidden unit, dz dt = n σ n (t)f n (z) where σ n (t) ∈ (0, 1) is a neural network that learns when the dynamic f n (z) should be applied. We call these models continuous normalizing flows (CNF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments with Continuous Normalizing Flows</head><p>We first compare continuous and discrete planar flows at learning to sample from a known distribution. We show that a planar CNF with M hidden units can be at least as expressive as a planar NF with K = M layers, and sometimes much more expressive.</p><p>Density matching We configure the CNF as described above, and train for 10,000 iterations using Adam <ref type="bibr" target="#b26">(Kingma and Ba, 2014)</ref>. In contrast, the NF is trained for 500,000 iterations using RMSprop <ref type="bibr" target="#b24">(Hinton et al., 2012)</ref>, as suggested by <ref type="bibr" target="#b47">Rezende and Mohamed (2015)</ref>. For this task, we minimize KL (q(x) p(x)) as the loss function where q is the flow model and the target density p(·) can be evaluated. <ref type="figure">Figure 4</ref> shows that CNF generally achieves lower loss.</p><p>Maximum Likelihood Training A useful property of continuous-time normalizing flows is that we can compute the reverse transformation for about the same cost as the forward pass, which cannot be said for normalizing flows. This lets us train the flow on a density estimation task by performing maximum likelihood estimation, which maximizes E p(x) [log q(x)] where q(·) is computed using the appropriate change of variables theorem, then afterwards reverse the CNF to generate random samples from q(x).</p><p>For this task, we use 64 hidden units for CNF, and 64 stacked one-hidden-unit layers for NF. <ref type="figure">Figure 5</ref> shows the learned dynamics. Instead of showing the initial Gaussian distribution, we display the  <ref type="figure">Figure 5</ref>: Visualizing the transformation from noise to data. Continuous-time normalizing flows are reversible, so we can train on a density estimation task and still be able to sample from the learned density efficiently.</p><p>transformed distribution after a small amount of time which shows the locations of the initial planar flows. Interestingly, to fit the Two Circles distribution, the CNF rotates the planar flows so that the particles can be evenly spread into circles. While the CNF transformations are smooth and interpretable, we find that NF transformations are very unintuitive and this model has difficulty fitting the two moons dataset in <ref type="figure">Figure 5b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A generative latent function time-series model</head><p>Applying neural networks to irregularly-sampled data such as medical records, network traffic, or neural spiking data is difficult. Typically, observations are put into bins of fixed duration, and the latent dynamics are discretized in the same way. This leads to difficulties with missing data and illdefined latent variables. Missing data can be addressed using generative time-series models <ref type="bibr" target="#b0">(Álvarez and Lawrence, 2011;</ref><ref type="bibr" target="#b16">Futoma et al., 2017;</ref><ref type="bibr" target="#b37">Mei and Eisner, 2017;</ref><ref type="bibr" target="#b53">Soleimani et al., 2017a)</ref> or data imputation <ref type="bibr" target="#b9">(Che et al., 2018)</ref>. Another approach concatenates time-stamp information to the input of an RNN <ref type="bibr" target="#b10">(Choi et al., 2016;</ref><ref type="bibr" target="#b33">Lipton et al., 2016;</ref><ref type="bibr" target="#b13">Du et al., 2016;</ref><ref type="bibr" target="#b32">Li, 2017)</ref>.</p><p>We present a continuous-time, generative approach to modeling time series. Our model represents each time series by a latent trajectory. Each trajectory is determined from a local initial state, z t0 , and a global set of latent dynamics shared across all time series. Given observation times t 0 , t 1 , . . . , t N and an initial state z t0 , an ODE solver produces z t1 , . . . , z t N , which describe the latent state at each observation.We define this generative model formally through a sampling procedure:</p><formula xml:id="formula_12">z t0 ∼ p(z t0 ) (11) z t1 , z t2 , . . . , z t N = ODESolve(z t0 , f, θ f , t 0 , . . . , t N ) (12) each x ti ∼ p(x|z ti , θ x )<label>(13)</label></formula><p>Function f is a time-invariant function that takes the value z at the current time step and outputs the gradient: ∂z(t) /∂t = f (z(t), θ f ). We parametrize this function using a neural net. Because f is time-</p><formula xml:id="formula_13">µ z t 0 z t 1 RNN encoder Latent space Data spaceq (z t0 |x t0 ...x tN ) h t 0 h t 1 h t N ODE Solve(z t0 , f, ✓ f , t 0 , ..., t M ) z t M … z t N z t N +1</formula><p>Observed Unobserved</p><p>x(t) invariant, given any latent state z(t), the entire latent trajectory is uniquely defined. Extrapolating this latent trajectory lets us make predictions arbitrarily far forwards or backwards in time.</p><p>Training and Prediction We can train this latent-variable model as a variational autoencoder <ref type="bibr" target="#b27">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b46">Rezende et al., 2014)</ref>, with sequence-valued observations. Our recognition net is an RNN, which consumes the data sequentially backwards in time, and outputs q φ (z 0 |x 1 , x 2 , . . . , x N ). A detailed algorithm can be found in Appendix E. Using ODEs as a generative model allows us to make predictions for arbitrary time points Poisson Process likelihoods The fact that an observation occurred often tells us something about the latent state. For example, a patient may be more likely to take a medical test if they are sick. The rate of events can be parameterized by a function of the latent state: p(event at time t| z(t)) = λ(z(t)). Given this rate function, the likelihood of a set of independent observation times in the interval [t start , t end ] is given by an inhomogeneous Poisson process <ref type="bibr" target="#b39">(Palm, 1943)</ref>:</p><formula xml:id="formula_14">log p(t 1 . . . t N | t start , t end ) = N i=1 log λ(z(t i )) − tend tstart λ(z(t))dt</formula><p>We can parameterize λ(·) using another neural network. Conveniently, we can evaluate both the latent trajectory and the Poisson process likelihood together in a single call to an ODE solver. <ref type="figure" target="#fig_2">Figure 7</ref> shows the event rate learned by such a model on a toy dataset. A Poisson process likelihood on observation times can be combined with a data likelihood to jointly model all observations and the times at which they were made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Time-series Latent ODE Experiments</head><p>We investigate the ability of the latent ODE model to fit and extrapolate time series. The recognition network is an RNN with 25 hidden units. We use a 4-dimensional latent space. We parameterize the dynamics function f with a one-hidden-layer network with 20 hidden units. The decoder computing p(x ti |z ti ) is another neural network with one hidden layer with 20 hidden units. Our baseline was a recurrent neural net with 25 hidden units trained to minimize negative Gaussian log-likelihood. We trained a second version of this RNN whose inputs were concatenated with the time difference to the next observation to aid RNN with irregular observations.</p><p>Bi-directional spiral dataset We generated a dataset of 1000 2-dimensional spirals, each starting at a different point, sampled at 100 equally-spaced timesteps. The dataset contains two types of spirals: half are clockwise while the other half counter-clockwise. To make the task more realistic, we add gaussian noise to the observations. Time series with irregular time points To generate irregular timestamps, we randomly sample points from each trajectory without replacement (n = {30, 50, 100}). We report predictive rootmean-squared error (RMSE) on 100 time points extending beyond those that were used for training. <ref type="table" target="#tab_3">Table 2</ref> shows that the latent ODE has substantially lower predictive RMSE.  Latent space interpolation <ref type="figure" target="#fig_5">Figure 8c</ref> shows latent trajectories projected onto the first two dimensions of the latent space. The trajectories form two separate clusters of trajectories, one decoding to clockwise spirals, the other to counter-clockwise. <ref type="figure" target="#fig_4">Figure 9</ref> shows that the latent trajectories change smoothly as a function of the initial point z(t 0 ), switching from a clockwise to a counter-clockwise spiral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Scope and Limitations</head><p>Minibatching The use of mini-batches is less straightforward than for standard neural networks. One can still batch together evaluations through the ODE solver by concatenating the states of each batch element together, creating a combined ODE with dimension D × K. In some cases, controlling error on all batch elements together might require evaluating the combined system K times more often than if each system was solved individually. However, in practice the number of evaluations did not increase substantially when using minibatches.</p><p>Uniqueness When do continuous dynamics have a unique solution? Picard's existence theorem <ref type="bibr" target="#b11">(Coddington and Levinson, 1955)</ref> states that the solution to an initial value problem exists and is unique if the differential equation is uniformly Lipschitz continuous in z and continuous in t. This theorem holds for our model if the neural network has finite weights and uses Lipshitz nonlinearities, such as tanh or relu.</p><p>Setting tolerances Our framework allows the user to trade off speed for precision, but requires the user to choose an error tolerance on both the forward and reverse passes during training. For sequence modeling, the default value of 1.5e-8 was used. In the classification and density estimation experiments, we were able to reduce the tolerance to 1e-3 and 1e-5, respectively, without degrading performance.</p><p>Reconstructing forward trajectories Reconstructing the state trajectory by running the dynamics backwards can introduce extra numerical error if the reconstructed trajectory diverges from the original. This problem can be addressed by checkpointing: storing intermediate values of z on the forward pass, and reconstructing the exact forward trajectory by re-integrating from those points. We did not find this to be a practical problem, and we informally checked that reversing many layers of continuous normalizing flows with default tolerances recovered the initial states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The use of the adjoint method for training continuous-time neural networks was previously proposed <ref type="bibr" target="#b30">(LeCun et al., 1988;</ref><ref type="bibr" target="#b41">Pearlmutter, 1995)</ref>, though was not demonstrated practically. The interpretation of residual networks <ref type="bibr" target="#b22">He et al. (2016a)</ref> as approximate ODE solvers spurred research into exploiting reversibility and approximate computation in ResNets <ref type="bibr" target="#b7">(Chang et al., 2017;</ref>. We demonstrate these same properties in more generality by directly using an ODE solver.</p><p>Adaptive computation One can adapt computation time by training secondary neural networks to choose the number of evaluations of recurrent or residual networks <ref type="bibr" target="#b18">(Graves, 2016;</ref><ref type="bibr" target="#b25">Jernite et al., 2016;</ref><ref type="bibr" target="#b15">Figurnov et al., 2017;</ref><ref type="bibr" target="#b8">Chang et al., 2018)</ref>. However, this introduces overhead both at training and test time, and extra parameters that need to be fit. In contrast, ODE solvers offer well-studied, computationally cheap, and generalizable rules for adapting the amount of computation.</p><p>Constant memory backprop through reversibility Recent work developed reversible versions of residual networks <ref type="bibr" target="#b17">(Gomez et al., 2017;</ref><ref type="bibr" target="#b7">Chang et al., 2017)</ref>, which gives the same constant memory advantage as our approach. However, these methods require restricted architectures, which partition the hidden units. Our approach does not have these restrictions.</p><p>Learning differential equations Much recent work has proposed learning differential equations from data. One can train feed-forward or recurrent neural networks to approximate a differential equation <ref type="bibr" target="#b43">(Raissi and Karniadakis, 2018;</ref><ref type="bibr" target="#b44">Raissi et al., 2018a;</ref><ref type="bibr" target="#b34">Long et al., 2017)</ref>, with applications such as fluid simulation <ref type="bibr" target="#b58">(Wiewel et al., 2018)</ref>. There is also significant work on connecting Gaussian Processes (GPs) and ODE solvers <ref type="bibr" target="#b51">(Schober et al., 2014)</ref>. GPs have been adapted to fit differential equations <ref type="bibr" target="#b45">(Raissi et al., 2018b</ref>) and can naturally model continuous-time effects and interventions <ref type="bibr" target="#b54">(Soleimani et al., 2017b;</ref><ref type="bibr" target="#b52">Schulam and Saria, 2017)</ref>. <ref type="bibr" target="#b50">Ryder et al. (2018)</ref> use stochastic variational inference to recover the solution of a given stochastic differential equation.</p><p>Differentiating through ODE solvers The dolfin library <ref type="bibr" target="#b14">(Farrell et al., 2013)</ref> implements adjoint computation for general ODE and PDE solutions, but only by backpropagating through the individual operations of the forward solver. The Stan library <ref type="bibr" target="#b6">(Carpenter et al., 2015)</ref> implements gradient estimation through ODE solutions using forward sensitivity analysis. However, forward sensitivity analysis is quadratic-time in the number of variables, whereas the adjoint sensitivity analysis is linear <ref type="bibr" target="#b6">(Carpenter et al., 2015;</ref><ref type="bibr" target="#b59">Zhang and Sandu, 2014)</ref>. <ref type="bibr" target="#b38">Melicher et al. (2017)</ref> used the adjoint method to train bespoke latent dynamic models.</p><p>In contrast, by providing a generic vector-Jacobian product, we allow an ODE solver to be trained end-to-end with any other differentiable model components. While use of vector-Jacobian products for solving the adjoint method has been explored in optimal control <ref type="bibr" target="#b2">(Andersson, 2013;</ref><ref type="bibr">Andersson et al., In Press, 2018)</ref>, we highlight the potential of a general integration of black-box ODE solvers into automatic differentiation <ref type="bibr" target="#b4">(Baydin et al., 2018)</ref> for deep learning and generative modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We investigated the use of black-box ODE solvers as a model component, developing new models for time-series modeling, supervised learning, and density estimation. These models are evaluated adaptively, and allow explicit control of the tradeoff between computation speed and accuracy. Finally, we derived an instantaneous version of the change of variables formula, and developed continuous-time normalizing flows, which can scale to large layer sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Proof of the Instantaneous Change of Variables Theorem</head><p>Theorem (Instantaneous Change of Variables). Let z(t) be a finite continuous random variable with probability p(z(t)) dependent on time. Let dz dt = f (z(t), t) be a differential equation describing a continuous-in-time transformation of z <ref type="bibr">(t)</ref>. Assuming that f is uniformly Lipschitz continuous in z and continuous in t, then the change in log probability also follows a differential equation:</p><formula xml:id="formula_15">∂ log p(z(t)) ∂t = −tr df dz (t)</formula><p>Proof. To prove this theorem, we take the infinitesimal limit of finite changes of log p(z(t)) through time. First we denote the transformation of z over an ε change in time as</p><formula xml:id="formula_16">z(t + ε) = Tε(z(t))<label>(14)</label></formula><p>We assume that f is Lipschitz continuous in z(t) and continuous in t, so every initial value problem has a unique solution by Picard's existence theorem. We also assume z(t) is bounded. These conditions imply that f , Tε, and ∂ ∂z Tε are all bounded. In the following, we use these conditions to exchange limits and products.</p><p>We can write the differential equation ∂ log p(z(t)) ∂t using the discrete change of variables formula, and the definition of the derivative:</p><formula xml:id="formula_17">∂ log p(z(t)) ∂t = lim ε→0 + log p(z(t)) − log det ∂ ∂z Tε(z(t)) − log p(z(t)) ε (15) = − lim ε→0 + log det ∂ ∂z Tε(z(t)) ε (16) = − lim ε→0 + ∂ ∂ε log det ∂ ∂z Tε(z(t)) ∂ ∂ε ε (by L'Hôpital's rule) (17) = − lim ε→0 + ∂ ∂ε det ∂ ∂z Tε(z(t)) det ∂ ∂z Tε(z(t)) ∂ log(z) ∂z z=1 = 1 (18) = − lim ε→0 + ∂ ∂ε det ∂ ∂z Tε(z(t)) bounded lim ε→0 + 1 det ∂ ∂z Tε(z(t)) =1 (19) = − lim ε→0 + ∂ ∂ε det ∂ ∂z Tε(z(t))<label>(20)</label></formula><p>The derivative of the determinant can be expressed using Jacobi's formula, which gives</p><formula xml:id="formula_18">∂ log p(z(t)) ∂t = − lim ε→0 + tr adj ∂ ∂z Tε(z(t)) ∂ ∂ε ∂ ∂z Tε(z(t)) (21) = −tr      lim ε→0 + adj ∂ ∂z Tε(z(t)) =I lim ε→0 + ∂ ∂ε ∂ ∂z Tε(z(t))      (22) = −tr lim ε→0 + ∂ ∂ε ∂ ∂z Tε(z(t))<label>(23)</label></formula><p>Substituting Tε with its Taylor series expansion and taking the limit, we complete the proof.</p><formula xml:id="formula_19">∂ log p(z(t)) ∂t = −tr lim ε→0 + ∂ ∂ε ∂ ∂z z + εf (z(t), t) + O(ε 2 ) + O(ε 3 ) + . . .<label>(24)</label></formula><p>= −tr lim Hamiltonian CNF. The continuous analog of NICE <ref type="bibr" target="#b12">(Dinh et al., 2014</ref>) is a Hamiltonian flow, which splits the data into two equal partitions and is a volume-preserving transformation, implying that ∂ log p(z) ∂t = 0. We can verify this. Let</p><formula xml:id="formula_20">ε→0 + ∂ ∂ε I + ∂ ∂z εf (z(t), t) + O(ε 2 ) + O(ε 3 ) + . . .<label>(25)</label></formula><formula xml:id="formula_21">= −tr lim ε→0 + ∂ ∂z f (z(t), t) + O(ε) + O(ε 2 ) + . . .<label>(26)</label></formula><formula xml:id="formula_22">= −tr ∂ ∂z f (z(t), t)<label>(27</label></formula><formula xml:id="formula_23">dz 1:d dt dz d+1:D dt = f (z d+1:D ) g(z 1:d )<label>(29)</label></formula><p>Then because the Jacobian is all zeros on its diagonal, the trace is zero. This is a volume-preserving flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Connection to Fokker-Planck and Liouville PDEs</head><p>The Fokker-Planck equation is a well-known partial differential equation (PDE) that describes the probability density function of a stochastic differential equation as it changes with time. We relate the instantaneous change of variables to the special case of Fokker-Planck with zero diffusion, the Liouville equation.</p><p>As with the instantaneous change of variables, let z(t) ∈ R D evolve through time following dz(t) dt = f (z(t), t). Then Liouville equation describes the change in density of z-a fixed point in space-as a PDE,</p><formula xml:id="formula_24">∂p(z, t) ∂t = − D i=1 ∂ ∂zi [fi(z, t)p(z, t)]<label>(30)</label></formula><p>However, (30) cannot be easily used as it requires the partial derivatives of p(z,t) ∂z , which is typically approximated using finite difference. This type of PDE has its own literature on efficient and accurate simulation <ref type="bibr" target="#b55">(Stam, 1999)</ref>.</p><p>Instead of evaluating p(·, t) at a fixed point, if we follow the trajectory of a particle z(t), we obtain ∂p(z(t), t) ∂t = ∂p(z(t), t) ∂z(t) ∂z(t) ∂t partial derivative from first argument, z(t) + ∂p(z(t), t) ∂t partial derivative from second argument, t</p><formula xml:id="formula_25">= D i=1 ∂p(z(t), t) ∂zi(t) ∂zi(t) ∂t − D i=1 ∂fi(z(t), t) ∂zi p(z(t), t) − D i=1 fi(z(t), t) ∂p(z(t), t) ∂zi(t) = − D i=1 ∂fi(z(t), t) ∂zi p(z(t), t)<label>(31)</label></formula><p>We arrive at the instantaneous change of variables by taking the log,</p><formula xml:id="formula_26">∂ log p(z(t), t) ∂t = 1 p(z(t), t) ∂p(z(t), t) ∂t = − D i=1 ∂fi(z(t), t) ∂zi<label>(32)</label></formula><p>While still a PDE, (32) can be combined with z(t) to form an ODE of size D + 1,</p><formula xml:id="formula_27">d dt z(t) log p(z(t), t) = f (z(t), t) − D i=1 ∂f i (z(t),t) ∂t<label>(33)</label></formula><p>Compared to the Fokker-Planck and Liouville equations, the instantaneous change of variables is of more practical impact as it can be numerically solved much more easily, requiring an extra state of D for following the trajectory of z(t). Whereas an approach based on finite difference approximation of the Liouville equation would require a grid size that is exponential in D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Continuous Backpropagation</head><p>Let z(t) follow the differential equation dz(t) dt = f (z(t), t, θ), where θ are the parameters. We will prove that if we define an adjoint state</p><formula xml:id="formula_28">a(t) = dL dz(t)<label>(34)</label></formula><p>then it follows the differential equation</p><formula xml:id="formula_29">da(t) dt = −a(t) ∂f (z(t), t, θ) ∂z(t)<label>(35)</label></formula><p>For ease of notation, we denote vectors as row vectors, whereas the main text uses column vectors.</p><p>The adjoint state is the gradient with respect to the hidden state at a specified time t. In standard neural networks, the gradient of a hidden layer ht depends on the gradient from the next layer ht+1 by chain rule dL dht = dL dht+1 dht+1 dht .</p><p>With a continuous hidden state, we can write the transformation after an ε change in time as</p><formula xml:id="formula_31">z(t + ε) = t+ε t f (z(t), t, θ)dt + z(t) = Tε(z(t), t)<label>(37)</label></formula><p>and chain rule can also be applied dL ∂z(t) = dL dz(t + ε) dz(t + ε) dz(t) or a(t) = a(t + ε) ∂Tε(z(t), t) ∂z(t)</p><p>The proof of (35) follows from the definition of derivative: </p><formula xml:id="formula_33">da(t) dt = lim ε→0 + a(t + ε) − a(t) ε<label>(</label></formula><p>= −a(t) ∂f (z(t), t, θ) ∂z(t)</p><p>We pointed out the similarity between adjoint method and backpropagation (eq. 38). Similarly to backpropagation, ODE for the adjoint state needs to be solved backwards in time. We specify the constraint on the last time point, which is simply the gradient of the loss wrt the last time point, and can obtain the gradients with respect to the hidden state at any time, including the initial value. B.2 Gradients wrt. θ and t</p><p>We can generalize (35) to obtain gradients with respect to θ-a constant wrt. t-and and the initial and end times, t0 and tN . We view θ and t as states with constant differential equations and write ∂θ(t) ∂t = 0 dt(t) dt = 1 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: A Residual network defines a discrete sequence of finite transformations. Right: A ODE network defines a vector field, which continuously transforms the state. Both: Circles represent evaluation locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Statistics of a trained ODE-Net. (NFE = number of function evaluations.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Fitting a latent ODE dynamics model with a Poisson process likelihood. Dots show event times. The line is the learned intensity λ(t) of the Poisson process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and extrapolation of spirals with irregular time points by a recurrent neural network. (b): Reconstructions and extrapolations by a latent neural ODE. Blue curve shows model prediction. Red shows extrapolation. (c) A projection of inferred 4-dimensional latent ODE trajectories onto their first two dimensions. Color indicates the direction of the corresponding trajectory. The model has learned latent dynamics which distinguishes the two directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Data-space trajectories decoded from varying one dimension of z t0 . Color indicates progression through time, starting at purple and ending at red. Note that the trajectories on the left are counter-clockwise, while the trajectories on the right are clockwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8</head><label>8</label><figDesc>shows examples of spiral reconstructions with 30 sub-sampled points. Reconstructions from the latent ODE were obtained by sampling from the posterior over latent trajectories and decoding it to data-space. Examples with varying number of time points are shown in Appendix F. We observed that reconstructions and extrapolations are consistent with the ground truth regardless of number of observed points and despite the noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>+ ε) − a(t + ε) ∂ ∂z(t) z(t) + εf (z(t), t, θ) + O(ε 2 )ε (Taylor series around z(t)) + ε) − a(t + ε) I + ε ∂f (z(t),t,θ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>that loss function L depends only on the last time point tN . If function L depends also on intermediate time points t1, t2, . . . , tN−1, etc., we can repeat the adjoint step for each of the intervals [tN−1, tN ], [tN−2, tN−1] in the backward order and sum up the obtained gradients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>d e f g r a d _ a r g n u m s _ w r a p p e r ( a l l _ v j p _ b u i l d e r ) : # A g e n e r i c a u t o g r a d h e l p e r f u n c t i o n . T a k e s a f u n c t i o n t h a t # b u i l d s v j p s f o r a l l a r g u m e n t s , and w r a p s i t t o r e t u r n o n l y r e q u i r e d v j p s . d e f b u i l d _ s e l e c t e d _ v j p s ( argnums , ans , c o m b i n e d _ a r g s , k w a r g s ) : v j p _ f u n c = a l l _ v j p _ b u i l d e r ( ans , * c o m b i n e d _ a r g s , * * k w a r g s ) d e f c h o s e n _ v j p s ( g ) : # R e t u r n w h i c h e v e r v j p s were a s k e d f o r . a l l _ v j p s = v j p _ f u n c ( g ) r e t u r n [ a l l _ v j p s [ argnum ] f o r argnum i n argnums ] r e t u r n c h o s e n _ v j p s r e t u r n b u i l d _ s e l e c t e d _ v j p s d e f v j p _ a r g n u m s ( o d e i n t , g r a d _ a r g n u m s _ w r a p p e r ( g r a d _ o d e i n t _ a l l ) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance on MNIST.</figDesc><table><row><cell>† From LeCun</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">: Predictive RMSE on test set</cell></row><row><cell cols="2"># Observations 30/100 50/100 100/100</cell></row><row><cell>RNN</cell><cell>0.3937 0.3202 0.1813</cell></row><row><cell>Latent ODE</cell><cell>0.1642 0.1502 0.1346</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Let f (z) = uh(w z + b), then ∂f ∂z = u ∂h ∂z T . Since the trace of an outer product is the inner product, we have</figDesc><table><row><cell>A.1 Special Cases</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Planar CNF. ∂ log p(z) ∂t</cell><cell>= −tr u</cell><cell>∂h ∂z</cell><cell>T</cell><cell>= −u T ∂h ∂z</cell><cell>(28)</cell></row><row><cell cols="4">This is the parameterization we use in all of our experiments.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t 0 t 1 t N Time t N +1 t M Prediction Extrapolation t 0 t 1 t N t N +1 t M x(t)Figure 6: Computation graph of the latent ODE model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that we've overloaded t to be both a part of the state and the (dummy) independent variable. The distinction is clear given context, so we keep t as the independent variable for consistency with the rest of the text.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>We thank Wenyi Wang and Geoff Roeder for help with proofs, and Daniel Duckworth, Ethan Fetaya, Hossein Soleimani, Eldad Haber, Ken Caluwaerts, Daniel Flam-Shepherd, and Harry Braviner for feedback. We thank Chris Rackauckas, Dougal Maclaurin, and Matthew James Johnson for helpful discussions. We also thank Yuval Frommer for pointing out an unsupported claim about parameter efficiency.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B A Modern Proof of the Adjoint Method</head><p>We present an alternative proof to the adjoint method <ref type="bibr" target="#b42">(Pontryagin et al., 1962)</ref> that is short and easy to follow.</p><p>We can then combine these with z to form an augmented state 1 with corresponding differential equation and adjoint state, , at(t) := dL dt(t) <ref type="bibr">(48)</ref> Note this formulates the augmented ODE as an autonomous (time-invariant) ODE, but the derivations in the previous section still hold as this is a special case of a time-variant ODE. The Jacobian of f has the form</p><p>where each 0 is a matrix of zeros with the appropriate dimensions. We plug this into (35) to obtain</p><p>The first element is the adjoint differential equation <ref type="formula">(35)</ref>, as expected. The second element can be used to obtain the total gradient with respect to the parameters, by integrating over the full interval and setting a θ (tN ) = 0.</p><p>Finally, we also get gradients with respect to t0 and tN , the start and end of the integration interval.</p><p>Between <ref type="formula">(35)</ref>, (46), (51), and (52) we have gradients for all possible inputs to an initial value problem solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Full Adjoint sensitivities algorithm</head><p>This more detailed version of Algorithm 1 includes gradients with respect to the start and end times of integration.</p><p>Algorithm 2 Complete reverse-mode derivative of an ODE initial value problem Input: dynamics parameters θ, start time t 0 , stop time t 1 , final state z(t 1 ), loss gradient ∂L /∂z(t1)</p><p>Return all gradients Appendix D Autograd Implementation f o r i i n r a n g e ( T − 1 , 0 , −1):</p><p># </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E Algorithm for training the latent ODE model</head><p>To obtain the latent representation zt 0 , we traverse the sequence using RNN and obtain parameters of distribution q(zt 0 |{xt i , ti}i, θenc). The algorithm follows a standard VAE algorithm with an RNN variational posterior and an ODESolve model:</p><p>1. Run an RNN encoder through the time series and infer the parameters for a posterior over zt 0 :</p><p>where µz 0 , σz 0 comes from hidden state of RNN({xt i , ti}i, φ)</p><p>2. Sample zt 0 ∼ q(zt 0 |{xt i , ti}i)</p><p>3. Obtain zt 1 , zt 2 , . . . , zt M by solving ODE ODESolve(zt 0 , f, θ f , t0, . . . , tM ), where f is the function defining the gradient dz/dt as a function of z 4. Maximize ELBO = M i=1 log p(xt i |zt i , θx) + log p(zt 0 ) − log q(zt 0 |{xt i , ti}i, φ), where p(zt 0 ) = N (0, 1) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix F Extra Figures</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computationally efficient convolved multiple output Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mauricio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1459" to="1500" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">OptNet: Differentiable optimization as a layer in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A general-purpose software framework for dynamic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Andersson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CasADi -A software framework for nonlinear optimization and optimal control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Joel A E Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Rawlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming Computation</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>In Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic differentiation in machine learning: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Atilim Gunes Baydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Mark</forename><surname>Andreyevich Radul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">153</biblScope>
			<biblScope unit="page" from="1" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05649</idno>
		<title level="m">Sylvester normalizing flows for variational inference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Stan math library: Reverse-mode automatic differentiation in c++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Betancourt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.07164</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Begert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Holtham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03698</idno>
		<title level="m">Reversible architectures for arbitrarily deep residual neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-level residual networks from dynamical systems view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Begert</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SyJS-OgR-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-018-24271-9</idno>
		<ptr target="https://doi.org/10.1038/s41598-018-24271-9" />
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Doctor AI: Predicting clinical events via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taha</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v56/Choi16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Machine Learning for Healthcare Conference</title>
		<meeting>the 1st Machine Learning for Healthcare Conference</meeting>
		<imprint>
			<date type="published" when="2016-08-19" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="301" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Theory of ordinary differential equations. Tata McGraw-Hill Education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Coddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">NICE: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent marked temporal point processes: Embedding event history to vector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1555" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated derivation of the adjoint of high-level transient finite element programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Rognes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<title level="m">Spatially adaptive computation time for residual networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to Detect Sepsis with a Multitask Gaussian Process RNN Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Futoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger B</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2211" to="2221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Solving Ordinary Differential Equations I -Nonstiff Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hairer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Nørsett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wanner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06188</idno>
		<title level="m">Variable computation in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beitrag zur näherungsweisen Integration totaler Differentialgleichungen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kutta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift für Mathematik und Physik</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="435" to="453" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A theoretical framework for back-propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Touresky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1988 connectionist models summer school</title>
		<meeting>the 1988 connectionist models summer school<address><addrLine>Pittsburgh, Pa</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1988" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Time-dependent representation for neural event sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00065</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Directly modeling missing data in sequences with RNNs: Improved classification of clinical time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzel</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v56/Lipton16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Machine Learning for Healthcare Conference</title>
		<meeting>the 1st Machine Learning for Healthcare Conference</meeting>
		<imprint>
			<date type="published" when="2016-08-19" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="253" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<title level="m">PDE-Net: Learning PDEs from Data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxiao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10121</idno>
		<title level="m">Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Autograd: Reverse-mode differentiation of native Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop on Automatic Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The neural Hawkes process: A neurally self-modulating multivariate point process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6757" to="6767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast derivatives of likelihood functionals for ODE based models using adjoint-state method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valdemar</forename><surname>Melicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><surname>Vanroose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1621" to="1643" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Intensitätsschwankungen im fernsprechverker. Ericsson Technics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conny</forename><surname>Palm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gradient calculations for dynamic recurrent neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1212" to="1228" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The mathematical theory of optimal processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Semenovich Pontryagin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mishchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Vg Boltyanskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gamkrelidze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hidden physics models: Machine learning of nonlinear partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multistep neural networks for datadriven discovery of nonlinear dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01236</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Numerical Gaussian processes for time-dependent and nonlinear partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="198" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Danilo J Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Runge</surname></persName>
		</author>
		<title level="m">Über die numerische Auflösung von Differentialgleichungen. Mathematische Annalen</title>
		<imprint>
			<date type="published" when="1895" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="167" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04272</idno>
		<title level="m">Deep neural networks motivated by partial differential equations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Black-box Variational Inference for Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golightly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Mcgough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prangle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Probabilistic ODE solvers with Runge-Kutta means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schulam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchi</forename><surname>Saria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10651</idno>
		<title level="m">What-if reasoning with counterfactual Gaussian processes</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scalable joint models for reliable uncertaintyaware event prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Soleimani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchi</forename><surname>Saria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Treatment-response models for counterfactual reasoning with continuous-time, continuous-valued interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Soleimani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Subbaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchi</forename><surname>Saria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02038</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Stable fluids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos</forename><surname>Stam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Optimization and uncertainty analysis of ODE models using second order adjoint sensitivity analysis. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Stapor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Froehlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hasenauer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">272005</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Improving variational auto-encoders using Householder flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09630</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Wiewel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10123</idno>
		<title level="m">Moritz Becher, and Nils Thuerey. Latent-space physics: Towards learning the temporal evolution of fluid flow</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fatode: a library for forward, adjoint, and tangent linear integration of ODEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Sandu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="504" to="523" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
