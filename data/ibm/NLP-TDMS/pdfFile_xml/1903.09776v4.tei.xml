<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Auto-ReID: Searching for a Part-Aware ConvNet for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Quan</surname></persName>
							<email>ruijie.quan@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
							<email>xuanyi.dong@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
							<email>linchao.zhu@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Auto-ReID: Searching for a Part-Aware ConvNet for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prevailing deep convolutional neural networks (CNNs) for person re-IDentification (reID) are usually built upon ResNet or VGG backbones, which were originally designed for classification. Because reID is different from classification, the architecture should be modified accordingly. We propose to automatically search for a CNN architecture that is specifically suitable for the reID task. There are three aspects to be tackled. First, body structural information plays an important role in reID but it is not encoded in backbones. Second, Neural Architecture Search (NAS) automates the process of architecture design without human effort, but no existing NAS methods incorporate the structure information of input images. Third, reID is essentially a retrieval task but current NAS algorithms are merely designed for classification. To solve these problems, we propose a retrievalbased search algorithm over a specifically designed reID search space, named Auto-ReID. Our Auto-ReID enables the automated approach to find an efficient and effective CNN architecture for reID. Extensive experiments demonstrate that the searched architecture achieves state-of-theart performance while reducing 50% parameters and 53% FLOPs compared to others.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-IDentification (reID) aims to retrieve the images of a person recorded by different surveillance cameras <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref>. With the success of deep convolutional neural networks (CNNs) in recent years, researchers in this area have mainly focused on improving the representation capability of the features extracted from CNN models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30]</ref>. Hundreds of different CNN models have been designed for reID, and the rank-1 accuracy has been improved from 44.4% <ref type="bibr" target="#b37">[38]</ref> to 93.8% <ref type="bibr" target="#b29">[30]</ref> on the Market-1501 benchmark <ref type="bibr" target="#b37">[38]</ref>. <ref type="bibr">Figure 1</ref>. Our Auto-ReID learns to search for a suitable architecture on a specific reID dataset, and it is supervised by the retrieval objective during searching. Auto-ReID finds architecture from a reID search space, which consists of a large number of candidate architectures. These candidates are generated by combining basic operations, such as a 3-by-3 convolutional layer, a 3-by-3 max pooling operation, and the proposed part-aware module.</p><p>Most recent reID models are based on deep CNNs. They are usually built upon convolutional neural network backbones for image classification <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>, such as VGG <ref type="bibr" target="#b25">[26]</ref>, Inception <ref type="bibr" target="#b30">[31]</ref>, and ResNet <ref type="bibr" target="#b12">[13]</ref>. These backbones can be readily used for retrieval as the inputs of both tasks are images. However, there are still a few differences between the reID task and the classification task. For example, in image classification, the appearance of two objects could be different, e.g., a cat looks different from a tree. In contrast, all input examples of the reID task are person images with different attributes, e.g., apparel or hair styles. A CNN focusing on recognizing over 1,000 objects <ref type="bibr" target="#b6">[7]</ref> should be modified when it is applied to the reID task.</p><p>A straightforward method is to manually design a reID oriented CNN architecture which is specifically suit-able for the reID problem. However, manually designing an exquisite architecture for the reID task may take months <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref> even for human experts. This is inefficient and labor intensive. In this paper, we propose an automated approach to search for an optimal CNN architecture that is explicitly suited to the reID task. Our premise is that the CNN backbones designed for classification may have redundant and missing components for retrieval (the reID task), e.g., <ref type="bibr" target="#b0">(1)</ref> less pooling layers benefit to reID accuracy and <ref type="bibr" target="#b1">(2)</ref> no component for classification explicitly captures body structure information. There remain three challenges to automate Neural Architecture Search (NAS) for reID. First, no existing NAS approaches search for a CNN architecture that preserves body structural information. The body structure information plays an important role in reID, which is a major difference between reID and classification <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b16">17]</ref>. Second, reID methods usually encode structural information in a backbonedependent way. They require extensive manual tuning of the hyper-parameters when a different backbone network is adopted <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b23">24]</ref>. Third, reID is essentially a retrieval task, but most NAS algorithms are designed for classification. Since retrieval and classification have different objectives, existing NAS algorithms are not directly applicable to the reID problem <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>In this paper, we propose an approach called Auto-ReID to solve these three challenges. The key contribution of Auto-ReID lies in the design of a new reID search space. This design enables us to construct more optimal architectures which make best use of human body structure information. Specifically, we design a part-aware module to enhance the body structure information of a given input feature tensor. Unlike existing part-based reID models, the proposed part-aware module is flexible and able to handle features with various input shapes. We use this module as a basic operation for constructing a number of reID candidate architectures. In addition to a typical softmax loss, the proposed Auto-ReID equips the differentiable NAS method <ref type="bibr" target="#b19">[20]</ref> with a retrieval loss, making the search results particularly suitable for the reID task. The combination of the proposed reID search space and reID search algorithm enables us to find an efficient and effective architecture for reID in an automated way ( <ref type="figure">Fig. 1</ref>). In summary, our contributions are as follows:</p><p>• This is the first approach that searches neural architectures for the reID task, eliminating human experts' effort in the manual design of CNN models for reID. • We propose a novel reID search space in which body structure is formulated as a trainable and operational CNN component. The proposed reID search space combines (1) modules that explicitly capture pedestrian body part information and (2) typical modules that have been used in the standard NAS search space.</p><p>• We integrate a retrieval loss into the differentiable NAS algorithm so as to better fit the reID task. We adopt the modified searching strategy and batch data sampling method in accordance with the new retrieval objective.</p><p>• Extensive experiments show that the searched CNN achieves competitive accuracy compared to reID baselines, while this CNN has less than 40% parameters of the reID baselines. By pre-training this CNN on ImageNet for initialization, we achieve state-of-the-art performance on three reID benchmarks with only half the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Person reID. Prevailing algorithms have achieved great success with the deep learning technique <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref>. Xiao et al. <ref type="bibr" target="#b35">[36]</ref> propose a pipeline to deep feature representations from multiple datasets. Chen et al. <ref type="bibr" target="#b4">[5]</ref> design a quadruplet loss to make deep CNN capture both interclass and intra-class variations. Saquib et al. <ref type="bibr" target="#b23">[24]</ref> take the body joint maps as additional inputs to enable deep CNN to learn pose sensitive representations. Sun et al. <ref type="bibr" target="#b29">[30]</ref> leverage a part-based CNN model and a refined part pooling method to learn discriminative part-informed features.</p><p>On the one hand, these deep-based reID algorithms <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28]</ref> heavily rely on the classification CNN backbones, such as VGG <ref type="bibr" target="#b25">[26]</ref>, Inception <ref type="bibr" target="#b30">[31]</ref>, and ResNet <ref type="bibr" target="#b12">[13]</ref>. These CNN backbones are specifically designed for the classification problem and experimented on classification datasets, which may not align with reID and limit the performance of reID algorithms. On the other hand, they incorporate reID specific domain knowledge to boost the classic CNN models, such as part cues <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28]</ref>, pose <ref type="bibr" target="#b23">[24]</ref>, and reID specific loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>. In this work, we not only inherit the merit of previous reID methods but also overcome their disadvantages. We automatically find a reID specific CNN architecture over a reID search space.</p><p>Neural Architecture Search. Our work is motivated by recent researches on NAS <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b5">6]</ref>, while we focus on searching for a reID model with high performance instead of a classification model. Most of NAS approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22]</ref> search CNN on a small proxy task and transfer the found CNN structure to another large target task. Zoph et al. <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> apply reinforcement learning to search CNN, while the search cost is more than hundreds of GPU days. Real et al. <ref type="bibr" target="#b22">[23]</ref> modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger CNN candidates. Brock et al. <ref type="bibr" target="#b1">[2]</ref> and Bender et al. <ref type="bibr" target="#b0">[1]</ref> explore the one-shot NAS approaches. Liu et al. <ref type="bibr" target="#b19">[20]</ref> relax the discrete search space so as to search CNN in a differentiable way. Dong et al. <ref type="bibr" target="#b8">[9]</ref> propose a differentiable sampling approach to improve <ref type="bibr" target="#b19">[20]</ref>. Benefited from parameter sharing tech-nique <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref>, we discard the proxy paradigm and directly search a robust CNN on the target reID dataset. Besides, previous NAS algorithms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b0">1]</ref> focus on the classification problem. They are generic and can be readily applied to the reID problem. However, without considering reID specific information, such as semantics <ref type="bibr" target="#b15">[16]</ref>, occlusion <ref type="bibr" target="#b14">[15]</ref>, pose <ref type="bibr" target="#b23">[24]</ref>, and part <ref type="bibr" target="#b29">[30]</ref>, generic NAS approaches can not guarantee that the searched CNN is suitable for reID tasks. In this work, based on an efficient NAS algorithm <ref type="bibr" target="#b19">[20]</ref>, we adopt two techniques to modify it for the reID problem. We modified the objective function and training strategy to adapt to the reID problem. In addition, we design a part-aware module and integrate it into the standard NAS search space, which could allow us to find a better CNN and advance the study of the NAS search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we will show how to search for a reID CNN with high performance. We will first introduce the preliminary background of NAS in Sec. 3.1. Then we propose a new search algorithm for reID, introduced in Sec. 3.2. Furthermore, we design a new reID search space in Sec. 3.3, which integrates our proposed part-aware module and the standard NAS search space. Lastly, we discuss some future directions for reID in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Most NAS approaches stack multiple copies of a neural cell to construct a CNN model <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>. A neural cell consists of several different kinds of layers, taking output tensors from previous cells and generating a new output tensor. We follow previous NAS approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20]</ref> to search for the topology structure of neural cells.</p><p>Specifically, a neural cell can be viewed as a directed acyclic graph (DAG) with B blocks. Each block has three steps: (1) take two tensors as inputs, (2) apply two operations on these two tensors, respectively, (3) sum these two tensors. The applied operation is selected from an operation candidate set O. Following some previous works <ref type="bibr" target="#b21">[22]</ref>, we use the following operations in our O: (1) 3×3 max pooling, (2) 3×3 average pooling, (3) 3×3 depth-wise separable convolution, (4) 3×3 dilated convolution, (5) zero operation (none), (6) identity mapping. The i-th block in the c-th neural cell can be represented as a 4-tuple, i.e., (</p><formula xml:id="formula_0">I c i1 , I c i2 , O c i1 , O c i2 )</formula><p>. Besides, the output tensor of the i-th block in the c-th neural cell is:</p><formula xml:id="formula_1">I c i = O c i1 (I c i1 ) + O c i2 (I c i2 ),<label>(1)</label></formula><p>where O c i1 and O c i2 are selected operations from O for the i-th block. I c i1 and I c i2 are selected from the candidate input tensors I c i , which consists of output tensors from the last two neural cells (I c-1 and I c-2 ) and output tensors from the previous block in the current cell.  <ref type="formula" target="#formula_1">1)</ref>, we relax the categorical choice of a particular operation as a softmax over all possible operations following <ref type="bibr" target="#b19">[20]</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The Auto-ReID Algorithm</head><formula xml:id="formula_2">O c i1 (I c i1 ) = H∈I c i o∈O exp(α (H,i) o ) o ∈O exp(α (H,i) o ) o(H),<label>(2)</label></formula><formula xml:id="formula_3">where α = {α (H,i) o</formula><p>} represents the topology structure for a neural cell, named as architecture parameters. Denote the parameters of all operations in O as ω, named as operation parameters, a typical differentiable NAS approach <ref type="bibr" target="#b19">[20]</ref> jointly trains ω on the training set and α on the validation set. After training, the strength of</p><formula xml:id="formula_4">H to I c i is defined as max o∈O,o =none exp(α (H,i) o ) o ∈O exp(α (H,i) o )</formula><p>. The H ∈ I c i with the maximum strength is selected as I c i1 , and the operation with the maximum weight for I c i1 is selected as O c i1 . This paradigm is designed for the classification problem. Inspired from them, we apply several improvements to adapt this paradigm into the person reID problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ReID Search Algorithm</head><p>Prevailing NAS approaches focus on searching for a well-performed architecture in the classification task, in which the softmax with cross-entropy loss is applied to optimizing both α and ω <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43]</ref>. In contrast, reID tasks aim to learn a discriminative feature extractor during training, so that the extracted feature can retrieve images of the same identity during evaluation. Simply inheriting the crossentropy loss can not guarantee a good retrieve performance. We need to incorporate reID specific knowledge into the searching algorithm.</p><p>Network Structure. We use the macro structure of ResNet <ref type="bibr" target="#b12">[13]</ref> for our reID backbone, where each residual layer is replaced by a neural cell. We search the topology structure of neural cells. Denote the feature extracted from <ref type="figure">Figure 2</ref>. The proposed part-aware module for the reID search space. Given a pedestrian feature tensor, this module can integrate human body structural cues into the input tensor. It first vertically splits the input feature tensor into M = 4 body part features, and then averages each part tensor into a vector and uses a linear layer to transform each of them into a new part feature vector, denoted as "body vectors". These M part vectors are interacted via a self-attention mechanism, and each part vectors could include more body part specific information. Later, these M vectors are repeated and concatenated to recover them into the same spatial shape as the input tensor, named as "enhanced tensor". Finally, we fuse this global feature tensor and the original input tensor by a one-by-one convolutional layer.</p><p>the backbone as f , we use one embedding layer to transfer the feature f into g following <ref type="bibr" target="#b29">[30]</ref>, and we use another linear transformation layer to map the feature g into the logits h with the output dimension of C, where C denotes the number of training identities. Two dropout layers are added between f &amp;g and g&amp;h, respectively.</p><p>Objective. The classification model usually applies the softmax with cross-entropy loss on h as follows:</p><formula xml:id="formula_5">L s = N i=1 − log exp(h i [c]) C c =1 exp(h i [c ]) ,<label>(3)</label></formula><p>where h i indicates the feature h of the i-th sample, and h i [c] indicates the c-th element in h i . N is the number of samples during training. The reID model usually applies the triplet loss as:</p><formula xml:id="formula_6">L t = N i=1 max(margin, ||f i − f p i || − ||f i − f n i ||),<label>(4)</label></formula><p>where f i indicates the feature f of the i-th sample. f p i indicates the hardest positive feature of f i . The margin term indicates the margin of triplet loss. In another word, f p i is another feature with the maximum Euclidean distance of f i and the same identity of f i in one batch. f n i is the hardest negative feature of f i . In another word, f n i is another feature with the minimum Euclidean distance of f i and the different identity of f i in one batch. Since the triplet loss is sensitive to the batch data, we should carefully sample training data in each batch. We adopt a class-balance data sampler to sample batch data for triplet loss. This sampler first samples uniformly sample some identities, and then, for each identity, it randomly sample the same number of images. To align with the reID problem and leverage the mutual benefit from the cross-entropy and triplet losses, we consider a mixture retrieval loss of L s and L t as follows:</p><formula xml:id="formula_7">L ret = λL s + (1 − λ)L t ,<label>(5)</label></formula><p>where λ ∈ [0, 1] is a weight balancing of L s and L t . We show our overall algorithm (Auto-ReID) in Alg. 1, which solves a bi-level optimization problem <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42]</ref>. We first search for a robust reID model by alternatively optimizing α with L t and ω with L ret . The searched CNN is derived from α based on the same strategy as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9]</ref>. After we find a robust CNN for the reID task, we train and evaluate this CNN in the standard way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ReID Search Space with Part-Aware Module</head><p>The search space covers all possible candidate CNNs to be found, which is important for NAS. A standard search space in NAS is "NASNet search space" <ref type="bibr" target="#b42">[43]</ref>, which contains different kinds of convolutional layers, different kinds of pooling layers, etc. None of these layers can explicitly handle pedestrian information, which requires a delicate design and some unique operations. In this paper, we take the first step to explore a search space that fits the reID problem.</p><p>Motivated by the fact that body part information can improve the performance of a reID model <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27]</ref>, we design a part-aware module and combine it with a common search space (O) to construct our reID search space O reid : (1) part-aware module, (2) 3 × 3 max pooling, (3) 3 × 3 average pooling, (4) 3 × 3 depth-wise separable convolution, (5) 3 × 3 dilated convolution, (6) zero operation, and (7) identity mapping.</p><p>The part-aware module is shown in <ref type="figure">Fig. 2</ref>. Given an input feature tensor F , we first split it into M parts vertically, where we show an example of M = 4 in <ref type="figure">Fig. 2</ref> part feature over the spatial dimension and apply a linear transformation to the pooled features, and can thus obtain M local body part feature vectors. Then, we apply a selfattention mechanism <ref type="bibr" target="#b31">[32]</ref> on these M part feature vectors.</p><p>In this way, we can incorporate global information into each part vectors to enhance its body structure cues. Later, we repeat each part vector into its original spatial shape and and concatenate the repeated part features vertically into a body structure enhanced feature tensor. Finally, we fuse this part-aware tensor and the original input feature tensor via channel-wise concatenate way, and apply a one-by-one convolutional layer on this fusion tensor to generate the output tensor. Our designed part-aware module can capture useful body part cues and integrate this structural information into the input features. Besides, the parameter size and number of calculation of the proposed part-aware module are similar to the 3x3 depth-wise separable convolution, and thus will not affect the efficiency of the found CNN compared with using a standard NAS search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>Researchers have trend to move their focus from manually architecture design to automated architecture design in many areas, e.g., classification <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9]</ref> and segmentation <ref type="bibr" target="#b3">[4]</ref>. In the reID community, the breakthrough of the reID performance is usually benefited from improvements on the CNN architecture. We present the first effort towards applying automated machine learning to reID. After so many different architectures proposed for reID <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28]</ref>, it becomes more difficult to manually find a better architecture. It is time to automatically design a good reID architecture, and to our knowledge, this is the first time that an automated algorithm has matched state-ofthe-art performance using architecture search techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We empirically evaluate the proposed method in this section. We will first introduce the used datasets in Sec. 4.1 and implementation details in Sec. 4.2. Then, we will ablatively study different aspects of our Auto-ReID algorithm in Sec. 4.3, and also compare the CNN found by our approach with other state-of-the-art algorithms in Sec. 4.4. Lastly, we make some qualitative analysis in Sec. 4.5.  <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>MSMT17 <ref type="bibr" target="#b33">[34]</ref> is currently the largest person reID dataset, which contains 126,441 images of 4,101 identities in 15 cameras. This dataset is composed of the training set, which contains 32,621 bounding boxes of 1,041 identities and the test set including 93,820 bounding boxes of 3,060 identities. From the test set, 11,659 images are used as query images and the other 82,161 bounding boxes are used as gallery images. This challenging dataset has more complex scenes and backgrounds, e.g., indoor and outdoor scenes, than others.</p><p>Evaluation Metrics. To evaluate the performance of our Auto-ReID and compare with other reID methods, we report two common evaluation metrics: the cumulative matching characteristics (CMC) at rank-1, rank-5 and rank-10 and mean average precision (mAP) on the above three benchmarks following the common settings <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Search Configurations. During the searching period, we randomly select 50% images from official training set as the search training set D train and other images as the search validation set D val . We choose the ResNet macro structure to construct the overall network. This network has a 3x3 convolutional head and four blocks sequentially, where each block has several neural cells. We denote the number of cells in each block is l 1 , l 2 , l 3 , and l 4 . We denote l = [l 1 , l 2 , l 3 , l 4 ]. We denote the channel of the first convolutional layer as C, and each block will double the number of channels. The first cell in the 2-th, 3-th, and 4th block is a reduction cell, and other cells are the normal cell <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. By default, we use C=32 and l = [2, 2, 2, 2] to search a suitable CNN architecture.</p><p>During searching, we use a input size of 384 × 128, a batch size of 16, the total epoch of 200. We use momentum SGD to optimize ω with the initial learning rate of 0.1 and decrease it to 0.001 in a cosine scheduler. The momentum for SGD is set as 0.9. We use Adam to optimize α with the initial learning rate of 0.02, which is decayed by 10 at 60th and 150-th epoch. The weight decay for both SGD and Adam is set as 0.0005. The margin is set as 0.3 when using the retrieval objective. The λ is set as 0.5.</p><p>In experiments, "DARTS" and "GDAS" in <ref type="table">Table 1</ref> denotes that we train the network provided in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9]</ref> on Market-1501. "Baseline" indicates we use DARTS (first order <ref type="bibr" target="#b19">[20]</ref>) to search an architecture on Market-1501, and then train the searched model. We use "Baseline + ReID Search Space" to denote the baseline searching algorithm on the proposed reID search space. And we use "Retrieval + ReID Search Space" to denote the proposed retrieval-based searching algorithm on the proposed reID search space. "Retrieval + ReID Search Space" costs about 1 day to finish one searching procedure on a single NVIDIA Tesla V100.</p><p>Training Configurations. In the training phrase, we use an input size of 384×128, C=64, and l = [2, 2, 2, 2]. Following previous works, we use random horizontal flipping and cropping for data augmentation. We set λ as 0.5 for the retrieval loss. For training from scratch, in one batch, our class-balance data sampler will first random select 8 identities and then random sample 4 images for each identity. When using ImageNet pre-trained models, it randomly samples 16 identities and then samples 4 images for each identity. We train the model for 240 epochs, using Adam as the optimizer with a momentum of 0.9 and a weight decay of 0.0005. We start the learning rate from 0.0035 and decay it by 10 at the 80-th and 150-th epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To investigate the effect of each component in our Auto-ReID, we perform extensive ablation studies on the Market-1501 dataset. We show the results in <ref type="table">Table 1 and Table 2</ref>.</p><p>We compare four searching options in <ref type="table">Table 1</ref> without using ImageNet pre-training. We make several observations: (1) DARTS <ref type="bibr" target="#b19">[20]</ref> and GDAS <ref type="bibr" target="#b8">[9]</ref> are searched on CIFAR-10, in which the found CNN is worse than a simple reID model based on ResNet-18. (2) By directly searching on the reID dataset ("Baseline"), we can find better CNNs, which on average have higher mAP and rank-1 accuracy with similar numbers of parameters and FLOPs. (3) By searching on the proposed reID search space, the performance of the searched CNN can be significantly improved. (4) Replacing the classification searching loss with the retrieval searching loss, we can obtain slightly better CNNs in most cases. Though "Retrieval + ReID Search Space" finds better CNNs on average compared to "Baseline + ReID  <ref type="table">Table 3</ref>. Comparisons with state-of-art reID models on Market-1501. "R-1" indicates the rank-1 accuracy.</p><p>Search Space", "Baseline + ReID Search Space (run 3)" is the architecture with the highest accuracy among all 12 runs. Therefore, we use this model as the "Auto-ReID" searched model for our following experiments by default.</p><p>The effect of different configurations. We try different configurations in the searching and training procedure. We use the "Baseline + ReID Search Space" and keep other hyper-parameters the same. Results are shown in <ref type="table">Table 2</ref>. First, a higher number of channels during training will yield better accuracy and mAP. Second, more layers (a larger l) can result in a better performance only when the value of l during searching is the same as the value during training. In another word, a neural cell searched by l = [2, 2, 2, 2] is more suitable for an architecture with l = <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b1">2]</ref>. Third, if we use C=64 or l= <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3]</ref> for experiments in <ref type="table">Table 1</ref>, we might find a better CNN. Consider the efficiency, we use a small C and l during searching.</p><p>The effect of formulating the part-aware module as a trainable and operational CNN component. We equip NAS-searched models with PCB and show their results in <ref type="table">Table 4</ref>. Compared to leveraging the part module at the tail, our method searches the most appropriate number and location the for part-aware module. Therefore, we can find a better architecture that is particularly suitable for the reID problem. As shown in <ref type="table">Table 4</ref>, our method outperforms both "DARTS+PCB", "GDAS+PCB" and "Baseline+PCB" by a large margin of more than 2% in mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architectures mAP Rank-1 Params(M) FLOPs(G)</head><p>DARTS <ref type="bibr" target="#b19">[20]</ref> 65.2 85.6 9.1 1.75 GDAS <ref type="bibr" target="#b8">[9]</ref> 66.8 86. <ref type="bibr" target="#b4">5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-art ReID Models</head><p>Since all state-of-the-art reID algorithms pre-train their models on ImageNet, we also pre-train our searched CNN on ImageNet for a fair comparison in this section. We train "Auto-ReID" with ImageNet initialization on Market-1501 then evaluate the trained model on three reID benchmarks.</p><p>Results on Market-1501. <ref type="table">Table 3</ref> compares our method with other state-of-the-art reID models. Our baseline searching algorithm finds a CNN, which achieves a rank-1 accuracy of 93.8% and a mAP of 83.4%, it outperforms other state-of-the-art reID algorithms. Our Auto-ReID further boosts the performance of "Baseline". The CNN found by our Auto-ReID achieves a rank-1 accuracy of 94.5% and a mAP of 85.1%. Note that this CNN reduces the parameters of ResNet-50 based reID models by more than 45%, whereas it obtains a much higher accuracy and mAP than them. This experiment demonstrates that our automated architecture search approach can find an efficient and effective model, which successfully removes the noises, redundancies, and missing components in other typical backbones.</p><p>Note that our Auto-ReID is orthogonal to other reID techniques, such as re-ranking (RK) <ref type="bibr" target="#b40">[41]</ref>, as shown in Table 3. Using the same augmentation technique <ref type="bibr" target="#b40">[41]</ref>, our Auto-ReID also outperforms other reID models. For example, PCB+RK achieves the mAP of 91.9%, whereas Auto-ReID+RK achieves the mAP of 94.2%, which is higher than it by 2.3%. Although other techniques can further improve the performance of Auto-ReID, we do not discuss more since it is not the focus of this paper.</p><p>Results on CUHK03 in <ref type="table">Table 5</ref>. There are two types of person bounding boxes: manually labeled and automatically detected. On both settings, our Auto-ReID obtains significantly higher accuracy and mAP than other models.</p><p>Results on MSMT17 in <ref type="table">Table 6</ref>: The previous state-ofthe-art method on MSMT17 is PCB, and our Auto-ReID significantly outperforms it by the mAP of 12% and the rank-1 accuracy of 10%.</p><p>We made the following observations on three benchmarks: (1) the CNN model automatically designed by our Auto-ReID outperforms most state-of-the-art reID mod-  <ref type="table">Table 5</ref>. Comparison of accuracy and mAP with the state-of-theart reID models on CUHK03. Note that we use the new evaluation protocol reported in <ref type="bibr" target="#b40">[41]</ref>. els on all three datasets. (2) our Auto-ReID significantly outperforms the baseline searching algorithm. Although "Auto-ReID" is searched on Market-1501, it achieves high accuracy on other two benchmarks. This suggests that our searched model has a strong transferable ability, which allows us to search over a small proxy dataset when the computational resources are not sufficient to directly search on the target dataset. Since MSMT17 is much larger than Market-1501, it could be possible to find a better CNN architecture by searching on MSMT17. When using the default hyper-parameters to search on MSMT17, it yields a slightly worse model compared to the "Auto-ReID" model. It is caused by that different datasets might require different hyper-parameters. With carefully tuned parameters, we believe better CNN could be found using MSMT17. Future Work. Our Auto-ReID takes the first step to automate the reID model design. The proposed reID search space only considers one possible reID specific module. More carefully designed basic reID modules can benefit to find a better reID architecture. We suggest that a reID specific module for NAS is supposed to meet the following requirements: (1) enhancing the human body structure information; (2) eliminating the reID irrelevant (e.g., background) information; (3) being able to take tensors with any shape as input and output tensors with a flexible shape. We would explore this in our future work. In addition, the proposed searching algorithm is a simple extension to the existing NAS algorithm <ref type="bibr" target="#b19">[20]</ref>. We would consider more reID specific knowledge to design more efficient and effective searching algorithms in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>To better understand what we found during searching, we display one of our searched architectures in <ref type="figure">Fig. 3</ref>. We show both the normal cell and the reduction cell. These automatically discovered cells are quite complex and are difficult to be found by a human expert with manual tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Rank-1 Rank-5 Rank-10 mAP GoogleNet <ref type="bibr" target="#b30">[31]</ref> 47.6 65.0 71.8 23.0 PDC <ref type="bibr" target="#b26">[27]</ref> 58  <ref type="table">Table 6</ref>. Comparison of accuracy and mAP with the state-of-theart reID models on MSMT17. <ref type="figure">Figure 3</ref>. The normal cell and the reduction cell used in <ref type="table">Table 3</ref>, <ref type="table">Table 5</ref>, and <ref type="table">Table 6</ref>. This topology structure is complex and hard to designed by human expert.</p><p>As shown in the reduction cell, two part-aware modules are incorporated in the architecture. Manually designing a similar architecture as in <ref type="figure">Fig. 3</ref> will cost months, which is inefficient and labor intensive. This further shows that it is necessary to automate the reID architecture design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an automated neural architecture search for the reID tasks, and we name our method as Auto-ReID. The proposed Auto-ReID involves a new reID search space and a new retrieval-based searching algorithm. The proposed reID search space incorporates body structure information into the candidate CNN in the search space. Specifically, it combines a typical classification search space and a novel part-aware module. Since reID is essentially a retrieval task but current NAS algorithms are merely designed for classification. We equip the NAS algorithm with a retrieval loss, making it particularly suitable for reID. In experiments, the CNN architecture found by our Auto-ReID significantly outperforms all state-of-the-art reID models on three benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Input: the architecture parameter α and the operation parameter ω; the training set DT and the evaluation set DE; a class-balance data sampler; 1: Split DT into the search training set Dtrain and the search validation set D val while not converged do 2: Use the sampler to get batch data from Dtrain 3: Update ω via the retrieval loss in Eq. (5) 4: Use the sampler to get batch data from D val 5: Update α via the retrieval loss in Eq. (5) end while Obtain the final CNN from α following the strategy in [20] Optimize this CNN on the training set DT by the standard reID training strategy Evaluate the trained CNN on the evaluation set DE</figDesc><table><row><cell>To</cell></row></table><note>search for the choices of O c i1 and I c i1 in Eq. (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Market-1501 [38] is a large-scale person reID dataset which contains 19,372 gallery images, 3,368 query images and 12,396 training images collected from six cameras. There are 751 identities in training set and 750 identities in the test set and they have no overlap. Every identity in the training set has 17.2 images on average. CUHK03 [17] consists of 1,467 identities and 28,192 bounding boxes. There are 26,264 images of 1,367 identities are used for training and 1,928 images of 100 identities are used for testing. We use the new protocol to split the training and test data as proposed by</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SMASH: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RENAS: Reinforced evolutionary neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisen</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adaptive exploration for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04194</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised person re-identification: Clustering and finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TOMCCAP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Densely connected search space for more flexible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09607</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarially occluded samples for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emrah</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prototype propagation networks (PPN) for weakly-supervised few-shot learning on category graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Saquib Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlou</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Person trasfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glad: Global-local-alignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
