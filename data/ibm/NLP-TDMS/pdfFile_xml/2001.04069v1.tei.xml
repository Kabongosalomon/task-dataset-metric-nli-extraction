<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Natural Image Matting via Guided Contextual Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
							<email>htlu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Natural Image Matting via Guided Contextual Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the last few years, deep learning based approaches have achieved outstanding improvements in natural image matting. Many of these methods can generate visually plausible alpha estimations, but typically yield blurry structures or textures in the semitransparent area. This is due to the local ambiguity of transparent objects. One possible solution is to leverage the far-surrounding information to estimate the local opacity. Traditional affinity-based methods often suffer from the high computational complexity, which are not suitable for high resolution alpha estimation. Inspired by affinity-based method and the successes of contextual attention in inpainting, we develop a novel end-to-end approach for natural image matting with a guided contextual attention module, which is specifically designed for image matting. Guided contextual attention module directly propagates high-level opacity information globally based on the learned low-level affinity. The proposed method can mimic information flow of affinity-based methods and utilize rich features learned by deep neural networks simultaneously. Experiment results on Composition-1k testing set and alphamatting.com benchmark dataset demonstrate that our method outperforms state-ofthe-art approaches in natural image matting. Code and models are available at https://github.com/Yaoyi-Li/GCA-Matting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The natural image matting is one of the important tasks in computer vision. It has a variety of applications in image or video editing, compositing and film post-production <ref type="bibr">(Wang, Cohen, and others 2008;</ref><ref type="bibr" target="#b0">Aksoy, Ozan Aydin, and Pollefeys 2017;</ref><ref type="bibr" target="#b24">Lutz, Amplianitis, and Smolic 2018;</ref><ref type="bibr" target="#b32">Xu et al. 2017;</ref><ref type="bibr">Tang et al. 2019</ref>). Matting has received significant interest from the research community and been extensively studied in the past decade. Alpha matting refers to the problem that separating a foreground object from the background and estimating transitions between them. The result of image matting is a prediction of alpha matte which represents the opacity of a foreground at each pixel.</p><p>Mathematically, the natural image I is defined as a convex combination of foreground image F and background image <ref type="figure">Figure 1</ref>: The visualization of our guided contextual attention map. Top row from left to right, the image, trimap and ground-truth. Second row, the alpha matte prediction, attention offset map from first GCA block in the encoder, offset from GCA block in the decoder.</p><p>B at each pixel i as:</p><formula xml:id="formula_0">I i = α i F i + (1 − α i )B i , α i ∈ [0, 1],<label>(1)</label></formula><p>where α i is the alpha value at pixel i that denotes the opacity of the foreground object. If α i is not 0 or 1, then the image at pixel i is mixed. Since the foreground color F i , background color B i and the alpha value α i are left unknown, the expression of alpha matting is ill-defined. Thus, most of the previous hand-crafted algorithms impose a strong inductive bias to the matting problem. One of the basic idea widely adopted in both affinitybased and sampling-based algorithms is to borrow information from the image patches with similar appearance. Affinity-based methods <ref type="bibr" target="#b18">(Levin, Lischinski, and Weiss 2008;</ref><ref type="bibr" target="#b2">Chen, Li, and Tang 2013;</ref><ref type="bibr" target="#b0">Aksoy, Ozan Aydin, and Pollefeys 2017)</ref> borrow the opacity information from known patches with the similar appearance to unknown ones. Sampling-based approaches <ref type="bibr" target="#b29">(Wang and Cohen 2007;</ref><ref type="bibr" target="#b7">Gastal and Oliveira 2010;</ref><ref type="bibr" target="#b9">He et al. 2011;</ref><ref type="bibr" target="#b6">Feng, Liang, and Zhang 2016)</ref> borrow a pair of samples from the foreground and background to estimate the alpha value at each pixel in the unknown region based on some specific assumption. One obstacle of the previous affinity-based and sampling-based methods is that they cannot handle the situation that there are only background and unknown areas in the trimap. It is because that these methods have to make use of both foreground and background information to estimate the alpha matte.</p><p>Benefiting from the Adobe Image Matting dataset <ref type="bibr" target="#b32">(Xu et al. 2017)</ref>, more learning-based image matting methods <ref type="bibr" target="#b32">(Xu et al. 2017;</ref><ref type="bibr" target="#b24">Lutz, Amplianitis, and Smolic 2018;</ref><ref type="bibr">Tang et al. 2019)</ref> has emerged in recent years. Most of learning-based approaches use network prior as the inductive bias and predict alpha mattes directly. <ref type="bibr">Moreover, SampleNet (Tang et al. 2019)</ref> proposed to leverage deep inpainting methods to generate foreground and background pixels in the unknown region rather than select from the image. It provides a combination of the learning-based and sampling-based approach.</p><p>In this paper, we propose a novel image matting method based on the opacity propagation in a neural network. The information propagation has been widely adopted within the neural network framework in recent years, from natural language processing <ref type="bibr" target="#b27">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b33">Yang et al. 2019)</ref>, data mining <ref type="bibr" target="#b16">(Kipf and Welling 2016;</ref><ref type="bibr" target="#b28">Veličković et al. 2017)</ref> to computer vision <ref type="bibr" target="#b34">(Yu et al. 2018;</ref><ref type="bibr" target="#b30">Wang et al. 2018</ref><ref type="bibr">). SampleNet Matting (Tang et al. 2019</ref>) indirectly leveraged the contextual information for foreground and background inpainting. In contrast, our proposed method conducts information flow from the image context to unknown pixels directly. We devise a guided contextual attention module, which mimic the affinity-based propagation in a fully convolutional network. In this module, the low-level image features are used as a guidance and we perform the alpha feature transmission based on the guidance. We show an example of our guided contextual attention map in <ref type="figure">Figure  1</ref> and more details in the section of results. In the guided contextual attention module, features from two distinct network branches are leveraged together. The information of both known and unknown patches are transmitted to feature patches in the unknown region with similar appearance.</p><p>Our proposed method can be viewed from two different perspectives. On one hand, the guided contextual attention can be elucidated as an affinity-based method for alpha matte value transmission with a network prior. Unknown patches share high-level alpha features with each other under the guidance of similarity between low-level image features. On the other hand, the proposed approach can also be seen as a guided inpainting task. In this aspect, image matting task is treated as an inpainting task on the alpha image under the guidance of input image. The unknown region is analogous to the holes to be filled in image inpainting. Unlike inpainting methods which borrows pixels from background of the same image, image matting borrows pixel value 0 or 1 from the known area in the alpha matte image under the guidance of original RGB image to fill in the unknown region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In general, natural image matting methods can be classified into three categories: sampling-based methods, propagation methods and learning-based methods.</p><p>Sampling-based methods <ref type="bibr" target="#b29">(Wang and Cohen 2007;</ref><ref type="bibr" target="#b7">Gastal and Oliveira 2010;</ref><ref type="bibr" target="#b9">He et al. 2011;</ref><ref type="bibr" target="#b6">Feng, Liang, and Zhang 2016)</ref> solve combination equation (1) by sampling colors from foreground and background regions for each pixel in the unknown region. The pair of foreground and background samples are selected under different metrics and assumptions. Then the initial alpha matte value is calculated by the combination equation. Robust Matting <ref type="bibr" target="#b29">(Wang and Cohen 2007)</ref> selected samples along the boundaries with confidence. The matting function was optimized by a Random Walk. Shared Matting <ref type="bibr" target="#b7">(Gastal and Oliveira 2010)</ref> selected the best pairs of samples for a set of neighbor pixels and reduced much of redundant computation cost. In Global Matting <ref type="bibr" target="#b9">(He et al. 2011)</ref>, all samples available in image were utilized to estimate the alpha matte. The sampling was achieve by a randomized patch match algorithm. More recently, CSC Matting <ref type="bibr" target="#b6">(Feng, Liang, and Zhang 2016)</ref> collected a set of more representative samples by sparse coding to avoid missing out true sample pairs.</p><p>Propagation methods <ref type="bibr" target="#b18">(Levin, Lischinski, and Weiss 2008;</ref><ref type="bibr" target="#b2">Chen, Li, and Tang 2013;</ref><ref type="bibr" target="#b0">Aksoy, Ozan Aydin, and Pollefeys 2017)</ref>, which are also known as affinity-based methods, estimate alpha mattes by propagating the alpha value from foreground and background to each pixel in the unknown area. The Closed-form Matting <ref type="bibr" target="#b18">(Levin, Lischinski, and Weiss 2008)</ref> is one of the most prevailing algorithm in propagation-based methods. It solved the cost function under the constraint of local smoothness. KNN Matting <ref type="bibr" target="#b2">(Chen, Li, and Tang 2013)</ref> collected matching nonlocal neighborhoods globally by K nearest neighbors. Moreover, the Information-flow Matting (Aksoy, Ozan Aydin, and Pollefeys 2017) proposed a color-mixture flow which combined the local and nonlocal affinities of colors and spatial smoothness. Due to the tremendous success of deep convolutional neural networks, learning-based methods achieve a dominate position in recent natural image matting <ref type="bibr" target="#b3">(Cho, Tai, and Kweon 2016;</ref><ref type="bibr" target="#b32">Xu et al. 2017;</ref><ref type="bibr" target="#b24">Lutz, Amplianitis, and Smolic 2018;</ref><ref type="bibr">Tang et al. 2019)</ref>. DCNN Matting <ref type="bibr" target="#b3">(Cho, Tai, and Kweon 2016)</ref> is the first method that introduced a deep neural network into image matting task. It made use of the network to learn a combination of results from different previous methods. Deep Matting <ref type="bibr" target="#b32">(Xu et al. 2017)</ref> proposed a fully neural network model with a largescale dataset for learning-based matting methods, which was one of the most significant work in deep image matting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Network for Deep Image Matting</head><p>Our proposed model uses the guided contextual attention module and a customized U-Net <ref type="bibr" target="#b26">(Ronneberger, Fischer, and Brox 2015)</ref> architecture to perform deep natural image matting. We first construct our customized U-Net baseline for matting, then introduce the proposed guided contextual attention (GCA) module.  </p><formula xml:id="formula_1">E-Res 2-2 E-Conv 2 E-Conv 1 E-Conv 3 E-Res 1-2 x2 E-Res 2-1 E-Res 1-1 x3 GCA E-Res 3-1 E-Res 3-2 x3 E-Res 4-1 E-Res 4-2 D-Res 4-1 D-Res 4-2 D-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Structure</head><p>The U-Net <ref type="bibr" target="#b26">(Ronneberger, Fischer, and Brox 2015)</ref> like architecture are prevailing in recent matting tasks <ref type="bibr" target="#b24">(Lutz, Amplianitis, and Smolic 2018;</ref><ref type="bibr">Tang et al. 2019;</ref> as well as image segmentation <ref type="bibr" target="#b22">(Long, Shelhamer, and Darrell 2015)</ref>, image-to-image translation <ref type="bibr" target="#b14">(Isola et al. 2017)</ref> and image inpainting ). Our baseline model shares almost the same network architecture with guided contextual attention framework in <ref type="figure" target="#fig_1">Figure 2</ref>. The only difference is that the baseline model replaces GCA blocks with identity layers and has no image feature block. The input to this baseline network is a cropped image patch and a 3-channel one-hot trimap which are concatenated as a 6channel input. The output is corresponding estimated alpha matte. The baseline structure is built as an encoder-decoder network with stacked residual blocks <ref type="bibr" target="#b10">(He et al. 2016</ref>).</p><p>Since the low-level features play a crucial role in retaining the detailed texture information in alpha mattes, in our customized baseline model, the decoder combines encoder features just before upsampling blocks instead of after each upsampling block. Such a design can avoid more convolutions on the encoder features, which are supposed to provide lower-level feature. We also use a two layer short cut block to align channels of encoder features for feature fusion. Moreover, in contrast to the typical U-Net structure which only combines different middle-level features, we directly forward the original input to the last convolutional layer through a short cut block instead. These features do not share any computation with the stem. Hence, this short cut branch only focuses on detailed textures and gradients.</p><p>In addition to the widely used batch normalization <ref type="bibr" target="#b13">(Ioffe and Szegedy 2015)</ref>, we introduce the spectral normalization <ref type="bibr" target="#b24">(Miyato et al. 2018)</ref> to each convolutional layer to add a constraint on Lipschitz constant of the network and stable the training, which is prevalent in image generation tasks <ref type="bibr" target="#b1">(Brock, Donahue, and Simonyan 2019;</ref><ref type="bibr" target="#b35">Zhang et al. 2019</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>Our network only leverages one alpha prediction loss. The alpha prediction loss is defined as an absolute difference between predicted and ground-truth alpha matte averaged over the unknown area:</p><formula xml:id="formula_2">L = 1 |U| i∈U |α i − α i |,<label>(2)</label></formula><p>where U indicates the region labeled as unknown in the trimap,α i and α i denote the predicted and ground-truth value of alpha matte as position i.</p><p>There are some losses proposed in prior work for the deep image matting tasks, like compositional loss <ref type="bibr" target="#b32">(Xu et al. 2017</ref><ref type="bibr">), gradient loss (Tang et al. 2019</ref> and Gabor loss . Compositional loss used in Deep Matting <ref type="bibr" target="#b32">(Xu et al. 2017</ref>) is the absolute difference between the original input image and predicted image composited by the ground-truth foreground, background and the predicted alpha mattes. The gradient loss calculates the averaged absolute difference between the gradient magnitude of predicted and ground-truth alpha mattes in the unknown region. Gabor loss proposed in ) substitutes the gradient operator with a bundle of Gabor filters and aims to have a more comprehensive supervision on textures and gradients than gradient loss. We delve into these losses to reveal whether involving different losses can benefit the alpha matte estimation in our baseline model. We provide an ablation study on Composition-1k testing set <ref type="bibr" target="#b32">(Xu et al. 2017)</ref> in <ref type="table" target="#tab_1">Table 1. As  Table 1</ref> shows, the use of compositional loss does not bring any notable difference under MSE and Gradient error, and both errors increase when we incorporate the gradient loss and alpha prediction loss. Although the adoption of Gabor loss can reduce the Gradient error to some degree, it also slightly increases the MSE. Consequently, we only opt for the alpha prediction loss in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>Since the most dominant image matting dataset proposed by Xu et al. only contains 431 foreground objects for training. We treat the data augmentation as a necessity of our baseline model. We introduce a sequence of data augmentation.</p><p>Firstly, following the data augmentation in (Tang et al. 2019), we randomly select two foreground object images with a probability of 0.5 and combine them to obtain a new foreground object as well as a new alpha image. Subsequently, the foreground object and alpha image will be resized to 640 × 640 images with a probability of 0.25. In this way, the network can nearly see the whole foreground image instead of a cropped snippet. Then, a random affine transformation are applied to the foreground image and the corresponding alpha image. We define a random rotation, scaling, shearing as well as the vertical and horizontal flipping in this affine transformation. Afterwards, trimaps are generated by a dilation and an erosion on alpha images with random number of pixels ranging from 5 to 29. With the trimap obtained, we randomly crop one 512×512 patch from each foreground image, corresponding alpha and trimap respectively. All of the cropped patches are centered on an unknown region. The foreground images are then converted to HSV space, and different jitters are imposed to the hue, saturation and value. Finally, we randomly select one background image from MS COCO dataset <ref type="bibr" target="#b20">(Lin et al. 2014)</ref> for each foreground patch and composite them to get the input image.</p><p>To demonstrate the effectiveness of data augmentation, we conduct an experiment with minimal data augmentation. In this case, only two necessary operations, image cropping and trimap dilation are retained. More augmentations like  <ref type="figure">Figure 3</ref>: The illustration of the guided contextual attention block. Computation is implemented as a convolution or a deconvolution. Two additional 1 × 1 convolutional layers for adaptation are not shown in this figure to keep neat. One is applied to the input image feature before extracting patches, and the other one is applied to the result of propagation before the element-wise summation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guided Contextual Attention Module</head><p>The guided contextual attention module contains two kinds of components, an image feature extractor block for lowlevel image feature and one or more guided contextual attention blocks for information propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-level Image Feature</head><p>Most of the affinity-based approaches have a basic inductive bias that local regions with almost identical appearance should have similar opacity. This inductive bias allows the alpha value propagates from the known region of a trimap to the unknown region based on affinity graph, which can often yields impressive alpha matte prediction. Motivated by this, we define two different feature flows in our framework ( <ref type="figure" target="#fig_1">Figure 2)</ref>: alpha feature flow (blue arrows) and image feature flow (yellow arrows). Alpha features are generated from the 6-channel input which is a concatenation of original image and trimap. The final alpha matte can be predicted directly from alpha features. Low-level image features contrast with the high-level alpha features. These features are generated only from the input image by a sequence of three convolutional layer with stride 2, which are analogous to the local color statistics in conventional affinitybased methods.</p><p>In other words, the alpha feature contains opacity information and low-level image feature contains appearance information. Given both opacity and appearance information, we can build an affinity graph and carry out opacity propagation as affinity-based methods. Specifically, we utilize the low-level image feature to guide the information flow on alpha features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guided Contextual Attention</head><p>Inspired by the contextual attention for image inpainting proposed in <ref type="bibr" target="#b34">(Yu et al. 2018)</ref>, we introduce our guided contextual attention block.</p><p>As shown in <ref type="figure">Figure 3</ref>, the guided contextual attention leverages both the image feature and alpha feature. Firstly, the image feature are divided into known part and unknown part and 3 × 3 patches are extracted from the whole image feature. Each feature patch represents the appearance information at a specific position. We reshape the patches as convolutional kernels. In order to measure the correlation between an unknown region patch U x,y centered on (x, y) and an image feature patch I x ,y centered on (x , y ) , the similarity is defined as the normalized inner product:</p><formula xml:id="formula_3">s (x,y),(x ,y ) = λ (x, y) = (x , y ); Ux,y Ux,y , I x ,y I x ,y otherwise,<label>(3)</label></formula><p>where U x,y ∈ U is also an element of the image feature patch set I, i.e. U ⊆ I. The constant λ is a punishment hyperparameter that we use −10 4 in our model, which can avoid a large correlation between each unknown patch and itself. In implementation, this similarity is computed by a convolution between unknown region features and kernels reshaped from image feature patches. Given the correlation, we carry out a scaled softmax along (x , y ) dimension to attain the guided attention score for each patch as following, a (x,y),(x ,y ) = softmax(w(U, K, x , y )s (x,y),(x ,y ) ), (4)</p><formula xml:id="formula_4">w(U, K, x , y ) =    clamp( |U | |K| ) I x ,y ∈ U; clamp( |K| |U | ) I x ,y ∈ K,<label>(5)</label></formula><p>clamp(φ) = min(max(φ, 0.1), 10), (6) in which w(·) is a weight function and K = I − U is the set of image feature patches from known region. As distinct from image inpainting task, the area of unknown region in a trimap is not under control. In many input trimaps, there are overwhelming unknown region and scarcely any known pixel. Thus, typically it is not feasible that only propagate the opacity information from the known region to unknown part. In our guided contextual attention, we let the unknown part borrow features from both known patches and unknown ones. Different weights are assigned to known and unknown patches based on the area of each region as the weight function defined in Eq. (5). If the area of known region is larger, the known patches can convey more accurate appearance information which exposes the difference between foreground and background, hence we weigh known patches with a larger weight. Whereas, if the unknown region has an overwhelming area, the known patches only provide some local appearance information, which may harm the opacity propagation. Then a small weight is assigned to known patches. When we get guided attention scores from image features, we do the propagation on alpha features based on the affinity graph defined by guided attention. Analogous to image features, patches are extracted and reshaped as filter kernels from alpha features. The information propagation is implemented as a deconvolution between guided attention scores and reshaped alpha feature patches. This deconvolution yields a reconstruction of alpha features in the unknown area and the values of overlapped pixels in the deconvolution are averaged. Finally, we combine the input alpha features and the propagation result by an element-wise summation. This element-wise summation works as a residual connection which can stable the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network with Guided Contextual Attention</head><p>Most of the affinity-based matting methods result in a closed-form solution based on the graph Laplacian <ref type="bibr" target="#b18">(Levin, Lischinski, and Weiss 2008;</ref><ref type="bibr" target="#b17">Lee and Wu 2011;</ref><ref type="bibr" target="#b2">Chen, Li, and Tang 2013)</ref>. The closed-form solution can be seen as a fixed point of the propagation or a limitation of infinite propagation iterations <ref type="bibr" target="#b37">(Zhou et al. 2004)</ref>. Motivated by this, we stick two guided contextual attention blocks to the encoder and decoder symmetrically in our stem. It aims to propagate more times in our model and take full advantage of the opacity information flow.</p><p>When we compute the guided contextual attention on higher-resolution features, more detailed appearance information will be attended. However, on the other hand, the computational complexity of the attention block is O(c(hw) 2 ), where c, h, w are the channels, height and width of the feature map respectively. Therefore, we append two guided contextual attention blocks to the stage with 64 × 64 feature maps.</p><p>The network of our GCA Matting is trained for 200, 000 iterations with a batch size of 40 in total on the Adobe Image Matting dataset <ref type="bibr" target="#b32">(Xu et al. 2017)</ref>. We perform optimization using Adam optimizer <ref type="bibr" target="#b15">(Kingma and Ba 2014)</ref> with β 1 = 0.5 and β 2 = 0.999. The learning rate is initialized to 4 × 10 −4 . Warmup and cosine decay <ref type="bibr" target="#b23">(Loshchilov and Hutter 2016;</ref><ref type="bibr" target="#b8">Goyal et al. 2017;</ref><ref type="bibr" target="#b11">He et al. 2019</ref>) are applied to the learning rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In this section we report the evaluation results of our proposed model on two datasets, the Composition-1k testing set and alphamatting.com dataset. Both quantitative and qualitative results are shown in this section. We evaluate the quantitative results under the Sum of Absolute Differences (SAD), Mean Squared Error (MSE), Gradient error (Grad) and Connectivity error (Conn) proposed by <ref type="bibr" target="#b25">(Rhemann et al. 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Composition-1k Testing Dataset</head><p>The Composition-1k testing dataset proposed in <ref type="bibr" target="#b32">(Xu et al. 2017)</ref> contains 1000 testing images which are composed from 50 foreground objects and 1000 different background images from Pascal VOC dataset <ref type="bibr" target="#b5">(Everingham et al. 2015)</ref>.</p><p>We compare our approach and the baseline model with three state-of-the-art deep image matting methods: Deep Matting <ref type="bibr" target="#b32">(Xu et al. 2017</ref><ref type="bibr">), IndexNet Matting (Lu et al. 2019</ref><ref type="bibr">) and SampleNet Matting (Tang et al. 2019</ref>, as well as three conventional hand-crafted algorithms: Learning Based Matting <ref type="bibr" target="#b36">(Zheng and Kambhamettu 2009)</ref>, Closed-Form Matting <ref type="bibr" target="#b18">(Levin, Lischinski, and Weiss 2008)</ref> and KNN Matting <ref type="bibr" target="#b2">(Chen, Li, and Tang 2013)</ref>. The quantitative results are shown in <ref type="table" target="#tab_4">Table 2</ref>. Our method outperforms all of the stateof-the-art approaches. In addition, our baseline model also get better results than some of the top performing methods. The effectiveness of the proposed guided contextual attention can be validated by the results displayed in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Some qualitative results are given in <ref type="figure" target="#fig_2">Figure 4</ref>. The results of Deep Matting and IndexNet Matting are generated by source codes and pretrained models provided in . As displayed in <ref type="figure" target="#fig_2">Figure 4</ref>, our approach achieves better performance on different foreground objects, especially in the semitransparent regions. Advantages are more obvious with a larger unknown region. This good performance profits from the information flow between feature patches with similar appearance features.</p><p>Additionally, our proposed method can evaluate each image in Composition-1k testing dataset as a whole on a single Nvidia GTX 1080 with 8GB memory. Since we take each image as a whole in our network without scaling, the guided contextual attention blocks are applied to feature maps with a much higher resolution than 64×64 in training phase. This results in a better performance in the detailed texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alphamatting.com Benchmark dataset</head><p>The alphamatting.com benchmark dataset <ref type="bibr" target="#b25">(Rhemann et al. 2009</ref>) has eight different images. For each testing image, there are three corresponding trimaps, namely, "small", "large" and "user". The methods on the benchmark are ranked by the averaged rank over 24 alpha matte estimations in terms of four different metrics. We evaluate our method on the the alphamatting.com benchmark, and show the scores in <ref type="table" target="#tab_5">Table 3</ref>. Some top approaches in the benchmark are also displayed for comparison.</p><p>As displayed in <ref type="table" target="#tab_5">Table 3</ref>, GCA Matting ranks the first place under the Gradient Error metric in the benchmark. The evaluation results of our method under the "large" and "user" trimaps are much better than the other top approaches. The image matting becomes more difficult as the trimap has a larger unknown region. Therefore, we can say that our approach is more robust to changes in the area of unknown region. Additionally, our approach has almost the same overall ranks with the SampleNet under the MSE metric. Generally, the proposed GCA Matting is one of the top performing  <ref type="figure" target="#fig_1">5 1 1.1 1.3 1.1 1.2 1.2</ref>  method on this benchmark dataset. We provide some of the visual examples in <ref type="figure" target="#fig_3">Figure 5</ref>. The results of our method and some top algorithms on "Elephant" and "Plastic bag" are displayed to demonstrate the good performance of our approach. For example, in the test image "Plastic bag", most of the previous methods make a mistake at the iron wire. However, our method learns from the contextual information in the surrounding background patches and predicts these pixels correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Attention Map</head><p>We visualize the attention map learned in the guided contextual attention block by demonstrating the pixel position with the largest attention score. Unlike the offset map widely used in optical flow estimation <ref type="bibr" target="#b4">(Dosovitskiy et al. 2015;</ref><ref type="bibr" target="#b12">Hui, Tang, and Loy 2018;</ref><ref type="bibr" target="#b26">Sun et al. 2018</ref>) and image inpainting <ref type="bibr" target="#b34">(Yu et al. 2018</ref>) which indicates the relative displacement of each pixel, our attention map demonstrates the absolute position of the corresponding pixel with highest attention activation. From this attention map, we can easily identify where the opacity information is propagated from for each feature pixel. As we can see in <ref type="figure">Figure 1</ref>, there is no information flow in the known region and feature patches in the unknown region tend to borrow information from the patches with similar appearance. <ref type="figure">Figure 1</ref> reveals where our GCA blocks attend to physically in the input image. Since there is an adaption convolutional layer in the guided contextual attention block before patch extraction on image features, attention maps from two attention blocks are not identical. The weights of known and unknown part are shown in the top-left corner of the attention map.</p><p>From the attention offset map in <ref type="figure">Figure 1</ref>, we can easily recognize the car in the sieve. The light pink patches at the center of the sieve indicate that these features are propagated from the left part of the car. While blue patches show the features which are borrowed from the right-hand side road. These propagated features will assist in the identification of foreground and background in ensuing convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we propose to solve the image matting problem by opacity information propagation in an end-to-end neural network. Consequently, a guided contextual attention module is introduced to imitate the affinity-based propagation method by a fully convolutional manner. In the proposed attention module, the opacity information is transmitted between alpha features under the guidance of appearance information. The evaluation results on both Composition-1k testing dataset and alphamatting.com dataset show the superiority of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Following Deep Matting, AlphaGan (Lutz, Amplianitis, and Smolic 2018) explored the deep image matting within a generative adversarial framework. More subsequent work like SampleNet Matting (Tang et al. 2019) and IndexNet (Lu et al. 2019) with different architectures also yielded appealing alpha matte estimations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our proposed guided contextual attention matting framework. The baseline model shares the same architecture without GCA blocks and image feature block. Original image and trimap are the inputs of alpha feature. Image feature block and GCA blocks only takes the original merged image as input. The blue arrows denote alpha feature flow and yellow arrows denote low-level image feature flow. GCA: guided contextual attention; SN: spectral normalization; BN: batch normalization; ×N : replicate N times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The visual comparison results on Adobe Composition-1k. From left to right, the original image, trimap, ground-truth, Deep Matting (Xu et al. 2017), IndexNet Matting (Lu et al. 2019), baseline and ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The alpha matte predictions of the test images from alphamatting.com benchmark. From left to right, the original image, trimap, Information-flow Matting (Aksoy, Ozan Aydin, and Pollefeys 2017) , Deep Matting (Xu et al. 2017), AlphaGAN (Lutz, Amplianitis, and Smolic 2018), IndexNet Matting (Lu et al. 2019), SampleNet Matting (Tang et al. 2019) and ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on data augmentation and different loss functions with baseline structure. The quantitative results are tested on Composition-1k testing set. Aug: data augmentation; Rec: alpha prediction loss; Comp: compositional loss; GradL: gradient loss; Gabor: Gabor loss.</figDesc><table><row><cell>Aug Rec Comp GradL Gabor</cell><cell>MSE</cell><cell>Grad</cell></row><row><cell></cell><cell cols="2">0.0106 21.53</cell></row><row><cell></cell><cell cols="2">0.0107 21.85</cell></row><row><cell></cell><cell cols="2">0.0108 22.51</cell></row><row><cell></cell><cell cols="2">0.0109 20.66</cell></row><row><cell></cell><cell cols="2">0.0146 32.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The quantitative results on Composition-1k testing set. Best results are emphasized in bold. (-indicates not given in the original paper.)</figDesc><table><row><cell>Methods</cell><cell>MSE</cell><cell>SAD</cell><cell>Grad</cell><cell>Conn</cell></row><row><cell>Learning Based Matting</cell><cell>0.048</cell><cell>113.9</cell><cell>91.6</cell><cell>122.2</cell></row><row><cell>Closed-Form Matting</cell><cell>0.091</cell><cell cols="3">168.1 126.9 167.9</cell></row><row><cell>KNN Matting</cell><cell>0.103</cell><cell cols="3">175.4 124.1 176.4</cell></row><row><cell>Deep Matting</cell><cell>0.014</cell><cell>50.4</cell><cell>31.0</cell><cell>50.8</cell></row><row><cell>IndexNet Matting</cell><cell>0.013</cell><cell>45.8</cell><cell>25.9</cell><cell>43.7</cell></row><row><cell>SampleNet Matting</cell><cell cols="2">0.0099 40.35</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline</cell><cell cols="4">0.0106 40.62 21.53 38.43</cell></row><row><cell>Ours</cell><cell cols="4">0.0091 35.28 16.92 32.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Our scores in the alpha matting benchmark, S, L and U denote the three trimap types, small, large and user, included in the benchmark. (Bold numbers indicate scores which rank the 1st place in the benchmark at the time of submission)</figDesc><table><row><cell>Gradient Error</cell><cell cols="3">Average Rank Overall S L</cell><cell>U</cell><cell>Troll S L U S L U S L U S L U S L U S L U S L U S L U Doll Donkey Elephant Plant Pineapple Plastic bag Net</cell></row><row><cell>Ours</cell><cell>5.2</cell><cell>5</cell><cell>4</cell><cell cols="2">6.5 0.1 0.1 0.2 0.1 0.1 0.3 0.2 0.2 0.2 0.2 0.2 0.3 1.3 1.6 1.9 0.7 0.8 1.4 0.6 0.7 0.6 0.4 0.4 0.4</cell></row><row><cell>SampleNet Matting</cell><cell>7.2</cell><cell cols="4">3.6 4.4 13.6 0.1 0.1 0.2 0.1 0.1 0.2 0.2 0.3 0.3 0.1 0.2 0.5 1.1 1.5 2.7 0.6 0.9 1 0.8 0.9 0.9 0.4 0.4 0.4</cell></row><row><cell>IndexNet Matting</cell><cell>10.3</cell><cell cols="4">8.6 8.8 13.6 0.2 0.2 0.2 0.1 0.1 0.3 0.2 0.2 0.2 0.2 0.2 0.4 1.7 1.9 2.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This paper is supported by NSFC (No.61772330,  61533012, 61876109), the advanced research project (No.61403120201), Shanghai authentication key Lab. (2017XCWZK01), Technology Committee the interdisciplinary Program of Shanghai Jiao Tong University (YG2015MS43). We also would like to thank the help and support from Versa.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing effective inter-pixel information flow for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ozan Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knn matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A cluster sampling method for image matting via sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="204" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shared sampling for real-time alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="575" to="584" />
			<date type="published" when="2010" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8981" to="8989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonlocal matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2193" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>IEEE TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06747</idno>
		<title level="m">Inductive guided filter: Real-time deep image matting with weakly annotated masks on mobile devices</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Indices matter: Learning to index for deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Sgdr: Stochastic gradient descent with warm restarts</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Alphagan: Generative adversarial networks for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amplianitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bmvc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
	</analytic>
	<monogr>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miccai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Aydın</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Learning-based sampling for natural image matting</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimized color sampling for robust matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image and video matting: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="175" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selfattention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning based digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
