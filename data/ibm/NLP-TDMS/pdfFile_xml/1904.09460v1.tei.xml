<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-Driven Neuron Allocation for Scale Aggregation Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
							<email>liyi@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
							<email>kuangzhanghui@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
							<email>chenyimin@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Zhang</forename><surname>Sensetime</surname></persName>
						</author>
						<title level="a" type="main">Data-Driven Neuron Allocation for Scale Aggregation Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Successful visual recognition networks benefit from aggregating information spanning from a wide range of scales. Previous research has investigated information fusion of connected layers or multiple branches in a block, seeking to strengthen the power of multi-scale representations. Despite their great successes, existing practices often allocate the neurons for each scale manually, and keep the same ratio in all aggregation blocks of an entire network, rendering suboptimal performance. In this paper, we propose to learn the neuron allocation for aggregating multiscale information in different building blocks of a deep network. The most informative output neurons in each block are preserved while others are discarded, and thus neurons for multiple scales are competitively and adaptively allocated. Our scale aggregation network (ScaleNet) is constructed by repeating a scale aggregation (SA) block that concatenates feature maps at a wide range of scales. Feature maps for each scale are generated by a stack of downsampling, convolution and upsampling operations. The data-driven neuron allocation and SA block achieve strong representational power at the cost of considerably low computational complexity. The proposed ScaleNet, by replacing all 3×3 convolutions in ResNet with our SA blocks, achieves better performance than ResNet and its outstanding variants like ResNeXt and SE-ResNet, in the same computational complexity. On ImageNet classification, ScaleNets absolutely reduce the top-1 error rate of ResNets by 1.12 (101 layers) and 1.82 (50 layers). On COCO object detection, ScaleNets absolutely improve the mmAP with backbone of ResNets by 3.6 (101 layers) and 4.6 (50 layers) on Faster RCNN, respectively. Code and models are released at https://github.com/Eli-YiLi/ScaleNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks (CNNs) have been successfully applied to a wide range of computer vision tasks, such as image classification <ref type="bibr" target="#b16">[18]</ref>, object detection <ref type="bibr" target="#b23">[25]</ref>, and semantic segmentation <ref type="bibr" target="#b20">[22]</ref>, due to their powerful end-to-end learnable representations. From bottom to top, the layers of CNNs have larger receptive fields with coarser scales, and their corresponding representations become more semantic. Aggregating context information from multiple scales has been proved to be effective for improving accuracy <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b2">4]</ref>. Small scale representations encode local structures such as textures, corners and edges, and are useful for localization, while coarse scale representations encode global contexts such as object categories, object interaction and scene, and thus clarify local confusion. There exist many previous attempts to fuse multi-scale representations by designing network architecture. They aggregate multi-scale representations of connected layers with different depths <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b24">26]</ref> or multiple branches in a block with different convolutional kernel sizes <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b2">4]</ref>. The proportion of multi-scale representations for in each aggregation block is manually set in a trial-and-error process and kept the same in the entire network. Ideally, the most efficient architecture design of multi-scale information aggregation is adaptive. The pro-   portion of neurons for each scale is determinate according to the importance of the scale in gathering context. Such proportion should also be adaptive to the stage in the network. Bottom layers may prefer fine scales and top layers may prefer coarse scales.</p><p>In this paper, we propose a novel data-driven neuron allocation method for multi-scale aggregation, which automatically learns the neuron proportion for each scale in all aggregation blocks of one network. We model the neuron allocation as one network optimization problem under FLOPs constraints which is solved by SGD and back projection. Concretely, we train one seed network with abundant output neurons for all scales using SGD, and then project the trained network into one feasible network that meets the constraints by selecting the top most informative output neurons amongst all scales. In this way, the neuron allocation for multi-scale representations is learnable and tailored for the network architecture.</p><p>To effectively extract and utilize multi-scale information, we present a simple yet effective Scale Aggregation (SA) block to strengthen the multi-scale representational power of CNNs. Instead of generating multi-scale representations with connected layers of different depths or multi-branch different kernel sizes as done in <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b14">16]</ref>, an SA block explicitly downsamples the input feature maps with a group of factors to small sizes, and then independently conducts convolution, resulting in representations in different scales. Finally, the SA block upsamples the multi-scale representations back to the same resolution as that of the input feature maps and concatenate them in channel dimension together. We use SA blocks to replace all 3×3 convolutions in ResNets to form ScaleNets. Thanks to downsampling in each SA block, ScaleNets are very ef-ficient by decreasing the sampling density in the spatial domain, which is independent yet complementary to network acceleration approaches in the channel domain. he proposed SA block is more computationally efficient and can capture a larger scale (or receptive field) range as shown in <ref type="figure" target="#fig_8">Figure 6</ref>, compared with previous multi-scale architecture.</p><p>We apply the proposed technique of data-driven neuron allocation to the SA block to form a learnable SA block. To demonstrate the effectiveness of the learnable SA block, we use learnable SA blocks to replace all 3 × 3 convolutions in ResNet to form a novel architecture called ScaleNet. The proposed ScaleNet outperforms ResNet and its outstanding variants such as ResNeXt <ref type="bibr" target="#b29">[31]</ref> and SE-ResNet <ref type="bibr" target="#b9">[11]</ref>, as well as recent popular architectures such as DenseNet <ref type="bibr" target="#b11">[13]</ref>, with impressive margins on image classification and object detection while keeping the same computational complexity as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Specifically, ScaleNet-50 and ScaleNet-101 absolutely reduces the top-1 error rate of ResNet-101 and ResNet-50 by 1.12% and 1.82% on Ima-geNet respectively. Benefiting from the strong multi-scale representation power of learnable SA blocks, ScaleNets are considerably effective on object detection. The Faster-RCNN <ref type="bibr" target="#b23">[25]</ref> with backbone ScaleNet-101 and ScaleNet-50 absolutely improve the mmAP of those with ResNet-101 and ResNet-50 by 3.6 and 4.6 on MS COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-scale representation aggregation has been studied for a long time. It can be categorized into shortcut connection approaches and multi-branch approaches.</p><p>Shortcut connection approaches. Connected layers with different depths usually have different receptive fields, and thus multi-scale representations. Shortcut connections between layers not only maximize information flow to avoid vanishing gradient, but also strengthen multi-scale representation power of CNNs. ResNet <ref type="bibr" target="#b7">[9]</ref>, DenseNet <ref type="bibr" target="#b11">[13]</ref>, and Highway Network <ref type="bibr" target="#b25">[27]</ref> fuse multi-scale information by identity shortcut connections or gating function based ones. Deep layer aggregation <ref type="bibr" target="#b30">[32]</ref> further extends shortcut connection with trees that cross stages. In object detection, FPN <ref type="bibr" target="#b18">[20]</ref> fuses coarse scale representations to fine scale ones from top to down in one detector's header <ref type="bibr" target="#b18">[20]</ref>. ASIF <ref type="bibr" target="#b4">[6]</ref> merges multi-scale representations from 4 layers both from top to down and from down to top. HyperNet <ref type="bibr" target="#b14">[16]</ref> and ION <ref type="bibr" target="#b1">[3]</ref> concatenate multi-scale features from different layers to make prediction. All the shortcut connection approaches focus on reusing fine scale representations from preceding layers or coarse scale ones from subsequent layers. Due to limited connection patterns between layers, the scale (or receptive filed) range is limited. Instead, the proposed approach generates a wide range scale of representations with a group of downsampling factors itself in each SA block. Moreover, it is a general and standard module which can replace any convolutional layer of existing networks, and be effectively used in various tasks such as image classification and object detection as validated in our experiments.</p><p>Multi-branch approaches. The most influential multibranch network is GoogleNet <ref type="bibr" target="#b26">[28]</ref>, where each branch is designed with different depths and convolutional kernel sizes. Its branches have varied receptive fields and multi-scale representations. Similar multi-branch network is designed for crowd counting in <ref type="bibr" target="#b2">[4]</ref>. Different from previous multibranch approaches, the proposed SA block generates multiscale representations by downsampling the input feature maps by different factors to expand the scale of representations. Again, it can generate representations with wider scale range than <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b2">4]</ref>. Downsampling is also used in the context module of PSPNet <ref type="bibr" target="#b32">[34]</ref> and ParseNet <ref type="bibr" target="#b22">[24]</ref>. However, the context module is only used in the network header while the proposed SA block is used in the whole backbone and thus more general. Moreover, the neuron proportion for each scale is manually set and fixed in the context module while automatically learned and different from one SA block to another in one network.</p><p>Our data-driven neuron allocation method is also related to network pruning methods <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b17">19]</ref> or network architecture search methods <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b27">29]</ref>. However, our data-driven neuron allocation method targets at multi-scale representation aggregation but not the whole architecture design. It learns the neuron proportion for scales in each SA block separately. In this way, the neuron allocation problem is greatly simplified, and easily optimized. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ScaleNets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scale Aggregation Block</head><p>The proposed scale aggregation block is a standard computational module which readily replaces any given transformation Y = T(X), where X ∈ R H×W ×C , Y ∈ R H×W ×Co with C and C o being the input and output channel number respectively. T is any operator such as a convolution layer or a series of convolution layers. Assume we have L scales. Each scale l is generated by sequentially conducting a downsampling D l , a transformation T l and an unsampling operator U l :</p><formula xml:id="formula_0">X l = D l (X),<label>(1)</label></formula><formula xml:id="formula_1">Y l = T l (X l ),<label>(2)</label></formula><formula xml:id="formula_2">Y l = U l (Y l ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">X l ∈ R H l ×W l ×C , Y l ∈ R H l ×W l ×C l , and Y l ∈ R H×W ×C l .</formula><p>Notably, T l has the similar structure as T. Substitute Equation <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref> into Equation <ref type="formula" target="#formula_2">(3)</ref>, and concatenate all L scales together, getting</p><formula xml:id="formula_4">Y = L 1 U l (T l (D l (X))),<label>(4)</label></formula><p>where indicates concatenating feature maps along the channel dimension, and Y ∈ R H×W × L 1 C l is the final output feature maps of the scale aggregation block.</p><p>In our implementation, the downsampling D l with factor s is implemented by a max pool layer with s × s kernel size and s stride. The upsampling U l is implemented by resizing with the nearest neighbor interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data-Driven Neuron Allocation</head><p>There exist L scales in each SA block. Different scales should play different roles in blocks with different depths. Therefore, simply allocating the output neuron proportion of scales equally would lead to suboptimal performance. Our core idea is to identify the importance of each output neuron, and then prune the unimportant neurons while preserving the important ones. For each output neuron, we employ its corresponding scale weight (γ) of its subsequent BatchNorm <ref type="bibr" target="#b13">[15]</ref> layer to evaluate its importance. The underlying reason is that γ is positively correlated with the output confidence of its corresponding neuron.</p><p>Let K, O k (1 ≤ k ≤ K), and O kl (1 ≤ k ≤ K and 1 ≤ l ≤ L) denote the total SA block index of the target network, the computational complexity budget of the k th SA block, and the computational complexity of one output neuron at scale l in the k th block respectively. We target at optimally allocating neurons for each scale in the k th SA block with the budget O k . Formally, we have</p><formula xml:id="formula_5">min θ F (θ), s.t. ∀k, 1≤n≤N k O kl(θ kn ) ≤ O k ,<label>(5)</label></formula><p>where F (θ) is the loss function of the whole network with θ being the learnable weights of the network, and θ kn being the weight of n th output neuron in the k th SA block. l(·) indicates the scale index, and N k is the total number of output neurons in the k th SA block. We optimize the objective function <ref type="formula" target="#formula_5">(5)</ref> by SGD with projection. We first optimize F (θ), getting θ t , we then project θ t back to the feasible domain defined by constraints for each SA block k by optimizing</p><formula xml:id="formula_6">min θ n V(θ kn ) − V(θ t kn ) s.t. 1≤n≤N k O kl(θ kn ) ≤ O k ,<label>(6)</label></formula><p>where V(θ kn ) indicates the importance of the n th neuron in the k th SA block. It is defined to be the scale weight which corresponds to the target channel (k, n) in its subsequent BatchNorm layer as done in <ref type="bibr" target="#b6">[8]</ref>. The more important and of less computational complexity the neuron is, the more likely it should be preserved. Equation <ref type="formula" target="#formula_6">(6)</ref> is greedily solved by selecting neurons with top biggest V(θ kn )/O b kl(θ kn ) in the k th SA block. Note that b is an exponential balance factor of computational complexity. We found b = 0 achieves good results in our experiments.</p><p>Algorithm 1 lists the procedure of neuron allocation. First, we set a seed network by setting C neurons for each scale (i.e., N k = CL). Second, we train the seed network till convergence. Third, we select the top most important neurons in SA blocks by solving Equation <ref type="formula" target="#formula_6">( 6)</ref>, and get a new network. Finally, we retrain the new network from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Instantiations</head><p>The proposed SA block can be integrated into standard architectures by replacing its existing convolutional Algorithm 1 Data-driven neuron allocation Initialize a seed network by setting N k = LC Train the seed network till convergence for k = 1 : K do for n = 1 :</p><formula xml:id="formula_7">N k do Compute p kn = V(θ kn )/O b kl(θ kn ) end</formula><p>Select neurons with top biggest p kn under the constraint of Equation <ref type="formula" target="#formula_6">(6)</ref> end Retrain the new network till convergence. layers or modules. To illustrate this point, we develop ScaleNets by incorporating SA blocks into the recent popular ResNets <ref type="bibr" target="#b7">[9]</ref>.</p><p>In ResNets <ref type="bibr" target="#b7">[9]</ref>, 3 × 3 convolutions account for most of the whole network computational complexity. Therefore, we replace all 3 × 3 layers with SA blocks as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We replace the stride in 3 × 3 convolution by extra max pool layer as done in DenseNets <ref type="bibr" target="#b11">[13]</ref>. In this way, all 3 × 3 layers can be replaced by SA blocks consistently. As shown in <ref type="table" target="#tab_1">Table 1</ref>, using ResNet-50, ResNet-101, and ResNet-152 as the start points, we obtain the corresponding ScaleNets 1 by setting the computational complexity budget of each SA block to that of its corresponding 3 × 3 conv in the residual block during the neuron allocation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Computational Complexity</head><p>The proposed SA block is of practical use. It makes ScaleNets efficient, because the feature maps are smaller. Theoretically, if we set the output channel number of one SA block to C (i.e.,</p><formula xml:id="formula_8">L 1 C l = C), the saved FLOPs is 9C( L 1 H l W l C l )−9C(HW C).</formula><p>Taking ScaleNet-50-light as an example, it reduces FLOPs of its start point ResNet-50 by 29% while absolutely improving the single-crop top-1 accuracy by 0.98 on ImageNet as shown in <ref type="table" target="#tab_2">Table 2</ref>. Also we compare the ScaleNet-50-light with the state-of-art pruning methods in Appendix B. Besides the FLOPs, we evaluate the real GPU time in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation</head><p>Our implementation for ImageNet follows the practice in <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b26">28]</ref>. We perform standard data augmentation with random cropping, random horizontal flipping and photometric distortions <ref type="bibr" target="#b26">[28]</ref> during training. All input images are resized to 224×224 before feeding them into networks. Optimization is performed using synchronous SGD with momentum 0.9, weight decay 0.0001 and batch size 256 on servers with 8 GPUs. The initial learning rate is set to 0.1 output size</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ScaleNet-50</head><p>ScaleNet-101 ScaleNet-152 112×112 7 × 7 conv, stride 2 56×56</p><p>3 × 3 max pool, stride 2  and decreased by a factor of 10 every 30 epoches. All models are trained for 100 epoches from scratch.</p><formula xml:id="formula_9">56×56       1 × 1 conv, 64 D [1,2,4,7] 3 × 3 conv [C l ,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 256       ×3       1 × 1 conv, 64 D [1,2,4,7] 3 × 3 conv [C1,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 256       ×3       1 × 1 conv, 64 D [1,2,4,7] 3 × 3 conv [C1,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 256       ×3 28×28 2 × 2 max pool, stride 2 28×28       1 × 1 conv, 128 D [1,2,4,7] 3 × 3 conv [C1,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 512       ×4       1 × 1 conv, 128 D [1,2,4,7] 3 × 3 conv [C1,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 512       ×4       1 × 1 conv, 128 D [1,2,4,7] 3 × 3 conv [C1,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 512       ×8 14×14 2 × 2 max pool, stride 2 14×14       1 × 1 conv, 256 D [1,2,4,7] 3 × 3 conv [C1,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 1024       ×6       1 × 1 conv, 256 D [1,2,4,7] 3 × 3 conv [C1,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 1024       ×23       1 × 1 conv, 256 D [1,2,4,7] 3 × 3 conv [C1,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 1024       ×36 7 × 7 2 × 2 max pool, stride 2 7 × 7       1 × 1 conv, 512 D [1,2,4,7] 3 × 3 conv [C1,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 2048       ×3       1 × 1 conv, 512 D [1,2,4,7] 3 × 3 conv [C1,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 2048       ×3       1 × 1 conv, 512 D [1,2,4,7] 3 × 3 conv [C1,C2,C3,C4] U [1,2,4,7] 1 × 1 conv, 2048       ×3 1 × 1 avg pool, 1000-d fc, softmax</formula><p>On CIFAR-100, we train models with a batch size of 64 for 300 epoches. The initial learning rate is set to 0.1, and is reduced by 10 times in 150 and 225. The data augmentation only includes random horizontal flipping and random cropping with 4 pixels padding.</p><p>On MS COCO, we train all detection models using the publicly available implementation 2 of Faster RCNN. Mod-2 https://github.com/jwyang/faster-rcnn.pytorch els are trained on servers with 8 GPUs. The batch size and epoch number are set to 16 and 10 respectively. The initial learning rate is set to 0.01 and reduced by a factor of 10 at epoch 4 and epoch 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet Classification</head><p>We evaluate our method on the ImageNet 2012 classification dataset <ref type="bibr" target="#b16">[18]</ref> that consists of 1000 classes. The models are trained on the 1.28 million training images, and evaluated on the 50k validation images with both top-1 and top-5 error rate. When evaluating the models we apply centrecropping so that 224×224 pixels are cropped from each image after its shorter edge is first resized to 256.</p><p>Comparisons with baselines. We begin evaluations by comparing the proposed ScaleNets with their corresponding baseline networks in <ref type="table" target="#tab_4">Table 3</ref>    <ref type="table" target="#tab_5">Table 4</ref>. It has been shown that ScaleNets consistently outperform them. Remarkably, ScaleNet-50, 101 and 152 absolutely reduce the top-1 error rate by 1.09 , 1.41 and 0.95 compared with their counterparts SE-ResNet-50, 101 and 152 respectively. Surprisingly, our ScaleNets-101 performs better than ResNeXt-101 by 0.23 without group convolutions which are not GPU friendly. We also evaluate the running time in Appendix C, which suggests ScaleNet achieves the best accuracy on ImageNet while with the least GPU running time</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CIFAR Classification</head><p>We also conduct experiments on CIFAR-100 dataset <ref type="bibr" target="#b15">[17]</ref>. To make full use of the same SA block  architecture, our baseline ResNets on CIFAR-100 also employ residual bottleneck blocks (i.e., a subsequent layers of 1 × 1 conv, 3 × 3 conv and 1 × 1 conv) instead of basic residual blocks (two 3 × 3 conv layers) in <ref type="bibr" target="#b7">[9]</ref>. The network inputs are 32 × 32 images. The first layer is 3 convolutions with 16 channels. Then we use a stack of n residual bottleneck blocks on each of these three stages with the feature maps of sizes 32×32, 16×16 and 8×8 respectively. The numbers of channels for 1 × 1 conv , 3 × 3 conv, and 1 × 1 conv in each residual block are set to <ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b14">16</ref> and 64 on the first stage, 32, 32 and 128 on the second stage, 64, 64 and 256 on the third stage. The subsampling is performed by convolutions with a stride of 2 at beginning of each stage. The network ends with a global average pool layer, a 100-way fully-connected layer, and softmax layer. There are totally 9n+2 stacked weighted layers. When n = 4, 6, and 10, we get baselines ResNet-38, ResNet-56 and ResNet-101 respectively for tiny images. Their corresponding ScaleNets with comparable computational complexity are denoted by ScaleNet-38, ScaleNet-56, and ScaleNet-101.</p><p>We compare the performances between ScaleNets and their baselines on CIFAR-100 in <ref type="table" target="#tab_7">Table 5</ref>. Again, the proposed ScaleNets outperforms ResNets with big margins. It has been validated that ScaleNets can effectively enhance and improve its strong baseline ResNets across multiple datasets from ImageNet to CIFAR-100, and multi-scale aggregation is also important for tiny image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data-Driven Neuron Allocation</head><p>The proposed ScaleNets can automatically learn the neuron proportion for each scale in each SA block. The neuron allocation depends on the training data distribution and network architectures.  Even allocation vs. data-driven allocation. <ref type="figure" target="#fig_5">Figure 4</ref> compares even neuron allocation for scales in each SA block and data-driven neuron allocation. We conduct experiments on both CIFAR-100 and ImageNet with scale number L from 2 to 5. Data-driven neuron allocation outperforms even allocation with impressive margins in all setting except that on CIFAR-100 with L = 2. We also observe that data-driven allocation performs best on CIFAR-100 with L = 3 and ImageNet with L = 4. This is reasonable since ImageNet has bigger resolution and needs representation with wider scale range than CIFAR-100. We set L to 3 on CIFAR-100 and L to 4 on ImageNet in all our experiments except otherwise noted. Based on even allocation (gains from SA block), ScaleNet-50 achieves top-1 error rate of 22.76%. With data-driven allocation, the top-1 error rate can be further reduced to 22.20%.</p><p>Visualization of neuron allocation. <ref type="figure" target="#fig_7">Figure 5</ref> shows learned neuron proportion in each SA block of ScaleNets. We observe that neuron proportions for scales are different from one SA block to another in one network. Specifically, scale 2 accounts for more and more proportion from bottom to top on both CIFAR-100 and ImageNet. Scale 4 mainly exists in the first two stages of ScaleNet-50 on ImageNet.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Object Detection on MS COCO</head><p>To further evaluate the generalization on other recognition tasks, we conduct object detection experiments on MS COCO <ref type="bibr" target="#b19">[21]</ref> consisting of 80k training images and 40k validation images, which are further split into 35k miniusmini and 5k mini-validation set. Following the common setting <ref type="bibr" target="#b7">[9]</ref>, we combine the training images and miniusmini images and thus obtain 115k images for training, and the 5k mini-validation set for evaluation. We employ the Faster RCNN framework <ref type="bibr" target="#b23">[25]</ref>. We test models by resizing the shorter edge of image to 800 (or 600) pixels, and restrict the max size of the longer edge to 1200 (or 1000).</p><p>Comparisons with baselines. <ref type="table" target="#tab_9">Table 6</ref> compares the detection results of ScaleNets and their baseline ResNets on MS COCO. With multi-scale aggregation, Faster RCNN achieves impressive gains with range from 3.2 to 4.9. Especially, ScaleNet-101 reaches an mmAP of 39.5.</p><p>ScaleNets are effective for object detection. <ref type="table" target="#tab_10">Table 7</ref> compares the effectiveness of backbones for object detection. It has been shown that ScaleNet-101 achieves the best detection performance with the minimal computational complexity amongst ResNets, ResNeXts <ref type="bibr" target="#b29">[31]</ref>, SE-ResNets <ref type="bibr" target="#b9">[11]</ref>, and Xception <ref type="bibr" target="#b3">[5]</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis</head><p>The role of max pool. Downsampling can be implemented in several ways: (i) a 3 × 3 conv with stride s; (ii) a dilated 3 × 3 conv with stride s <ref type="bibr" target="#b21">[23]</ref>; (iii) a s × s avg pool with stride s; (iv) a s × s max pool with stride s. We evaluate all the above settings with ScaleNet-56 on CIFAR-100 by setting scale number L to 2 and s to 2 for simplicity. As shown in <ref type="table" target="#tab_11">Table 8</ref>, (iv) performs best. It suggests that max pool is the key factor of performance boosting. It is reasonable since max pool preserves and enhances the maximum activation from previous layers so that the high response of small foreground regions would not be drowned by background features as information flows from bottom to top.</p><p>Wide range of receptive field. <ref type="figure" target="#fig_8">Figure 6</ref> compares the receptive field range of each block. It has been shown that the proposed ScaleNets have much wider range of receptive field than others. Particularly, ScaleNet-50 reaches the resolution for classification and detection only in second and third block. On the one hand, ScaleNets potentially aggregate rich representations with large range of scales. On the other hand, they can extract global context information at very early stage (e.g., block 3) in one network. Together with data-driven neuron allocation, ScaleNets perform effectively and efficiently on various visual recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a scale aggregation block with data-driven neuron allocation. The SA block can replace 3×3 conv in ResNets to get ScaleNets. The data-driven neuron allocation can effectively allocate the neurons to suitable scale in each SA block. The proposed ScaleNets have wide range of receptive fields, and perform effectively and efficiently on image classification and object detection. We will test our ScaleNets on more computer vision tasks, such as segmentation <ref type="bibr" target="#b20">[22]</ref>, in the future.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Comparison with Pruning</head><p>To demonstrate the advantages comparing with pruning, we list the stage-of-art pruning methods and ScaleNet-50light in <ref type="table" target="#tab_12">Table 9</ref>.</p><p>top-1 acc.↑ FLOPs (10 9 )↓ CP-ResNet-50 [1, 10] -3.68 1.5 SSS-ResNet-50 <ref type="bibr" target="#b12">[14]</ref> -1.94 1.3 NISP-ResNet-50 <ref type="bibr" target="#b31">[33]</ref> -0.21 1.1 LCP-ResNet-50 <ref type="bibr" target="#b5">[7]</ref> +0.09 1.0 ScaleNet-50-light +0.98 1.2  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. GPU time</head><p>In <ref type="table" target="#tab_1">Table 10</ref> all networks were tested using Tensorflow with GTX 1060 GPU and i7 CPU at batch size 16 and image size 224. It has been shown that ScaleNet-50 achieves the best accuracy on ImageNet while with the least GPU running time.   <ref type="table" target="#tab_1">Table 1</ref>). Note that ScaleNets on CIFAR-100 have only three scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Allocated Neuron Numbers of ScaleNets</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the data-driven neuron allocation for the scale aggregation (SA) block. The proportion of output neurons (or channels) of different scales in an SA block is learned, and thus adaptively changes across layers in a network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>arXiv:1904.09460v1 [cs.CV] 20 Apr 2019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of the ScaleNets and modern architectures' top-1 error rates (single-crop testing) on the ImageNet validation dataset (left) and mAP on MS COCO mini-validation set (right) as a function of FLOPs during testing. ScaleNet-50-light indicates a light ScaleNet which is also constructed from ResNet-50. Architectures are given in the Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the SA block. The left shows the original residual block, and the right shows the module after replacing the 3 × 3 convolution by the SA block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Comparisons between even neuron allocation and datadriven neuron allocation on CIFAR-100 and ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Neuron proportion for scales in each SA block of ScaleNets on CIFAR-100 and ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Comparisons of receptive field of multi-branch networks as a function of block index. The shortcut branch and residual branch in each residual block of ResNets have the minimal and maximal receptive field respectively. The 1 × 1 conv branch, and 5×5 conv branch in each Inception block of GoogleNet have the minimal and maximal receptive field respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7</head><label>7</label><figDesc>compares the top-1 error rate of ResNets and ScaleNets on both ImageNet training and validation dataset. It has been shown that ScaleNets achieve much lower error rates than their counterpart ResNets during all training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of the training curves. The left compares scale ScaleNet-50 and ResNet-50 on ImageNet training set and validation set, while the right compares ScaleNet-101 and ResNet-101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>top-1 err.</cell><cell>top-5 err.</cell><cell>GFLOPs</cell></row><row><cell>ResNet-50</cell><cell>24.02</cell><cell>7.13</cell><cell>4.1</cell></row><row><cell cols="4">ScaleNet-50-light 23.04 (−0.98) 6.66 (−0.47) 2.9 (−1.2)</cell></row></table><note>Architectures of ScaleNets. D [1,2,4,7] indicates 1 × 1, 2 × 2, 4 × 4, and 7 × 7 downsampling layers. U [1,2,4,7] indicates 1 × 1, 2 × 2, 4 × 4, and 7 × 7 upsampling layers. We select 7 × 7 (but not 8×8) downsampling and upsampling layers since the spatial resolution of last stage of networks is 7 × 7. 3 × 3 conv [C 1 ,C 2 ,C 3 ,C 4 ] indicates 3 × 3 convolution layers with output channels of C1, C2, C3, and C4. Note that C1, C2, C3, and C4 are different from one SA block to another, and are detailed in the Appendix D.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Efficiency of ScaleNet-light. All results are evaluated with single crop on ImageNet validation set. ScaleNet-50-light indicates a light ScaleNet constructed from ResNet-50. Results of ResNet-50 are reimplemented with the same training strategy as ScaleNets for fair comparison.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b18">20</ref>.62 (−0.96) 5.34 (−0.41) 11.2</figDesc><table><row><cell>method</cell><cell></cell><cell>original</cell><cell></cell><cell cols="2">re-implementation</cell><cell></cell><cell>ScaleNet</cell></row><row><cell></cell><cell cols="6">top-1 err. top-5 err. top-1 err. top-5 err. GFLOPs top-1 err.</cell><cell>top-5 err.</cell><cell>GFLOPs</cell></row><row><cell>ResNet-50</cell><cell>24.7</cell><cell>7.8</cell><cell>24.02</cell><cell>7.13</cell><cell>4.1</cell><cell cols="2">22.20 (−1.82) 6.04 (−1.09) 3.8</cell></row><row><cell cols="2">ResNet-101 23.6</cell><cell>7.1</cell><cell>22.09</cell><cell>6.03</cell><cell>7.8</cell><cell cols="2">20.97 (−1.12) 5.58 (−0.45) 7.5</cell></row><row><cell cols="2">ResNet-152 23.0</cell><cell>6.7</cell><cell>21.58</cell><cell>5.75</cell><cell>11.5</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>. It has been shown that</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ScaleNets with different depths consistently improve their</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">baselines with impressive margins while using comparable</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparisons between ScaleNets and their baseline ResNets with single-crop error rates (%) on ImageNet validation set. The original column refers to the reported results in the original paper. For fair comparison, we retrain the baselines using the same strategy of training ScaleNet and report the results in the reimplementation column.</figDesc><table><row><cell>method</cell><cell cols="3">top-1 err. top-5 err. GFLOPs</cell></row><row><cell>ResNeXt-50</cell><cell>22.2</cell><cell>-</cell><cell>4.2</cell></row><row><cell>ResNeXt-101</cell><cell>21.2</cell><cell>5.6</cell><cell>8.0</cell></row><row><cell>SE-ResNet-50</cell><cell>23.29</cell><cell>6.62</cell><cell>4.1</cell></row><row><cell cols="2">SE-ResNet-101 22.38</cell><cell>6.07</cell><cell>7.8</cell></row><row><cell cols="2">SE-ResNet-152 21.57</cell><cell>5.73</cell><cell>11.5</cell></row><row><cell>DenseNet-121</cell><cell>25.02</cell><cell>7.71</cell><cell>2.9</cell></row><row><cell>DenseNet-169</cell><cell>23.8</cell><cell>6.85</cell><cell>3.4</cell></row><row><cell>DenseNet-201</cell><cell>22.58</cell><cell>6.34</cell><cell>4.3</cell></row><row><cell>ScaleNet-50</cell><cell>22.2</cell><cell>6.04</cell><cell>3.8</cell></row><row><cell>ScaleNet-101</cell><cell>20.97</cell><cell>5.58</cell><cell>7.5</cell></row><row><cell>ScaleNet-152</cell><cell>20.62</cell><cell>5.34</cell><cell>11.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Comparison with state-of-the-art architectures with</cell></row><row><cell>single-crop top-1 and top-5 error rates (%) on ImageNet valida-</cell></row><row><cell>tion set.</cell></row><row><cell>(or even a little less) computational complexity. Specifi-</cell></row><row><cell>cally, Compared with baselines, ScaleNet-50, 101, and 152</cell></row><row><cell>absolutely reduce the top-1 error rate by 1.82, 1.12 and 0.96,</cell></row><row><cell>the top-5 error rate by 1.09, 0.45, and 0.41 on ImageNet</cell></row><row><cell>respectively. ScaleNet-101 even outperforms ResNet-152,</cell></row><row><cell>although it has only 66% FLOPs (7.5 vs. 11.5). It suggests</cell></row><row><cell>that explicitly and effectively aggregating multi-scale repre-</cell></row><row><cell>sentations of ScaleNets can achieve considerably much per-</cell></row><row><cell>formance gain on image classification although deep CNNs</cell></row><row><cell>are robust against scale variance to some extent.</cell></row><row><cell>Comparisons with state-of-the-art architectures. We</cell></row><row><cell>next compare ScaleNets with ResNets, ResNeXts, SE-</cell></row><row><cell>ResNets, and DenseNets in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of the top-1 error rate on CIFAR-100 between ScaleNets and their baseline ResNets. All the results are the best of 5 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>AP m AP l mmAP AP s AP m AP l</figDesc><table><row><cell></cell><cell></cell><cell>600/1000</cell><cell></cell><cell></cell><cell>800/1200</cell><cell></cell></row><row><cell cols="3">mmAP AP s ResNet-50 31.7 12.6 35.9</cell><cell cols="2">48.3 32.6</cell><cell>15.9 36.7</cell><cell>46</cell></row><row><cell>ScaleNet-50</cell><cell>36.2</cell><cell>17.1 40.7</cell><cell cols="2">53.8 37.2</cell><cell>19.4 41.3</cell><cell>52.6</cell></row><row><cell>ResNet-101</cell><cell>34.1</cell><cell>13.8 38.6</cell><cell>51</cell><cell>35.9</cell><cell>17.7 39.9</cell><cell>51.6</cell></row><row><cell cols="2">ScaleNet-101 37.3</cell><cell>16.6 42.4</cell><cell cols="2">55.1 39.5</cell><cell>21.3 44</cell><cell>55.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparisons of mAP on MS COCO. mmAP indicates the results of mAP@IoU=[0.50:0.95]. Results of ResNets and ScaleNets are obtained by keeping all settings the same except backbone for fair comparison.</figDesc><table><row><cell></cell><cell cols="2">ImageNet</cell><cell>COCO</cell></row><row><cell></cell><cell cols="3">top-1 err. FLOPs mAP (600/1000)</cell></row><row><cell>ResNet-152</cell><cell>21.58</cell><cell>11.5</cell><cell>34.3</cell></row><row><cell>Xception</cell><cell>21.11</cell><cell>9.0</cell><cell>27.7</cell></row><row><cell>SE ResNet-152</cell><cell>21.07</cell><cell>11.5</cell><cell>37.1</cell></row><row><cell>ResNeXt-101</cell><cell>21.01</cell><cell>8.0</cell><cell>36.7</cell></row><row><cell>ScaleNet-101</cell><cell>20.97</cell><cell>7.5</cell><cell>37.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="2">: Comparisons of effectiveness of backbone for object de-</cell></row><row><cell cols="2">tection on MS COCO. All models are trained with the same strat-</cell></row><row><cell>egy for fair comparison.</cell><cell></cell></row><row><cell>method</cell><cell>top1 err.</cell></row><row><cell>stride 2 of 3 × 3 conv</cell><cell>26.19</cell></row><row><cell cols="2">stride 2 of 3 × 3 conv, dilated 2 25.42</cell></row><row><cell>2 × 2 average pool</cell><cell>24.58</cell></row><row><cell>2 × 2 max pool</cell><cell>24.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Top-1 error rate on CIFAR-100 with different downsampling methods. All the methods are of same FLOPs and record the best result in 5 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Comparison with state-of-the-art methods on ImageNet. ResNet-50 and ScaleNet-50-light are trained in same settings, and others are reported in their papers.</figDesc><table><row><cell></cell><cell cols="3">top-1 err. FLOPs(19 9 ) GPU time(ms)</cell></row><row><cell>ResNet-50</cell><cell>24.02</cell><cell>4.1</cell><cell>95</cell></row><row><cell>SE-ResNet-50</cell><cell>23.29</cell><cell>4.1</cell><cell>98</cell></row><row><cell>ResNeXt-50</cell><cell>22.2</cell><cell>4.2</cell><cell>147</cell></row><row><cell>ScaleNet-50</cell><cell>22.2</cell><cell>3.8</cell><cell>93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Comparison of GPU time (averaging on 1000 runs).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11</head><label>11</label><figDesc>lists the detailed allocated neuron numbers of 3 × 3 conv in SA block on ImageNet and CIFAR-100.</figDesc><table><row><cell></cell><cell cols="3">ScaleNets for CIFAR-100</cell><cell></cell><cell cols="2">ScaleNets for ImageNet</cell><cell></cell></row><row><cell>block index</cell><cell cols="4">38 layers 56 layers 101 layers 50 layers (light)</cell><cell>50 layers</cell><cell>101 layers</cell><cell>152 layers</cell></row><row><cell>1</cell><cell>6,2,14</cell><cell>14,6,3</cell><cell>10,6,7</cell><cell>30,8,10,16</cell><cell>62,9,5,12</cell><cell>61,11,7,7</cell><cell>39,27,10,14</cell></row><row><cell>2</cell><cell>15,6,1</cell><cell>10,10,3</cell><cell>5,5,13</cell><cell>30,9,9,16</cell><cell>55,27,5,1</cell><cell>56,23,4,3</cell><cell>45,32,8,5</cell></row><row><cell>3</cell><cell>16,5,1</cell><cell>9,3,11</cell><cell>8,4,11</cell><cell>30,27,7,0</cell><cell>59,26,0,3</cell><cell>59,24,3,0</cell><cell>46,36,8,0</cell></row><row><cell>4</cell><cell>16,6,0</cell><cell>11,3,9</cell><cell>12,8,3</cell><cell>59,55,13,1</cell><cell>125,41,6,3</cell><cell>123,41,1,6</cell><cell>55,26,35,63</cell></row><row><cell>5</cell><cell>30,12,1</cell><cell>14,9,0</cell><cell>11,6,6</cell><cell>59,43,8,18</cell><cell>90,39,9,37</cell><cell>126,38,1,6</cell><cell>89,44,19,27</cell></row><row><cell>6</cell><cell>24,15,4</cell><cell>15,8,0</cell><cell>9,9,5</cell><cell>59,57,12,0</cell><cell>106,56,4,9</cell><cell>127,41,3,0</cell><cell>93,62,14,10</cell></row><row><cell>7</cell><cell>26,16,1</cell><cell>28,15,3</cell><cell>9,11,3</cell><cell>59,59,9,1</cell><cell>116,56,3,0</cell><cell>127,41,3,0</cell><cell>110,43,12,14</cell></row><row><cell>8</cell><cell>27,13,3</cell><cell>26,13,7</cell><cell>13,8,2</cell><cell>117,65,71,3</cell><cell>223,71,55,0</cell><cell>220,86,35,0</cell><cell>109,54,13,3</cell></row><row><cell>9</cell><cell>47,17,22</cell><cell>29,15,2</cell><cell>12,9,2</cell><cell>107,16,33,100</cell><cell>196,104,44,5</cell><cell>186,64,55,36</cell><cell>119,59,0,1</cell></row><row><cell>10</cell><cell>30,38,18</cell><cell>26,16,4</cell><cell>15,8,0</cell><cell>111,49,62,34</cell><cell>195,98,52,4</cell><cell>156,25,53,107</cell><cell>106,70,3,0</cell></row><row><cell>11</cell><cell>26,52,8</cell><cell>30,14,2</cell><cell>16,7,0</cell><cell>106,61,61,28</cell><cell>155,128,66,0</cell><cell>191,44,52,54</cell><cell>114,65,0,0</cell></row><row><cell>12</cell><cell>23,51,12</cell><cell>12,25,9</cell><cell>30,14,2</cell><cell>99,71,59,27</cell><cell>134,129,86,0</cell><cell>181,53,83,24</cell><cell>224,102,31,1</cell></row><row><cell>13</cell><cell></cell><cell>50,19,22</cell><cell>23,14,9</cell><cell>76,50,67,63</cell><cell>120,127,98,4</cell><cell>221,82,34,4</cell><cell>163,49,68,78</cell></row><row><cell>14</cell><cell></cell><cell>52,34,5</cell><cell>28,14,4</cell><cell>141,182,189,0</cell><cell cols="3">237,354,106,0 177,62,90,12 115,65,73,105</cell></row><row><cell>15</cell><cell></cell><cell>25,47,19</cell><cell>23,15,8</cell><cell>83,9,185,235</cell><cell cols="3">172,435,90,0 130,75,102,34 143,107,71,37</cell></row><row><cell>16</cell><cell></cell><cell>24,59,8</cell><cell>26,17,3</cell><cell>77,16,184,235</cell><cell>138,462,97,0</cell><cell>206,71,55,9</cell><cell>144,97,92,25</cell></row><row><cell>17</cell><cell></cell><cell>17,57,17</cell><cell>26,16,4</cell><cell></cell><cell></cell><cell>203,83,53,2</cell><cell>195,87,60,16</cell></row><row><cell>18</cell><cell></cell><cell>2,58,31</cell><cell>21,22,3</cell><cell></cell><cell></cell><cell>207,73,54,7</cell><cell>198,77,62,21</cell></row><row><cell>19</cell><cell></cell><cell></cell><cell>24,22,0</cell><cell></cell><cell></cell><cell>245,84,12,0</cell><cell>168,139,43,8</cell></row><row><cell>20</cell><cell></cell><cell></cell><cell>19,18,9</cell><cell></cell><cell></cell><cell>221,103,17,0</cell><cell>80,80,71,127</cell></row><row><cell>21</cell><cell></cell><cell></cell><cell>28,18,0</cell><cell></cell><cell></cell><cell>221,100,20,0</cell><cell>138,88,93,39</cell></row><row><cell>22</cell><cell></cell><cell></cell><cell>22,20,4</cell><cell></cell><cell></cell><cell>158,99,84,0</cell><cell>107,93,65,93</cell></row><row><cell>23</cell><cell></cell><cell></cell><cell>53,15,24</cell><cell></cell><cell></cell><cell>220,106,15,0</cell><cell>230,103,25,0</cell></row><row><cell>24</cell><cell></cell><cell></cell><cell>30,43,19</cell><cell></cell><cell></cell><cell>173,92,73,3</cell><cell>182,132,40,4</cell></row><row><cell>25</cell><cell></cell><cell></cell><cell>61,27,4</cell><cell></cell><cell></cell><cell cols="2">135,122,84,0 179,114,53,12</cell></row><row><cell>26</cell><cell></cell><cell></cell><cell>52,35,5</cell><cell></cell><cell></cell><cell>109,71,132,29</cell><cell>220,76,53,9</cell></row><row><cell>27</cell><cell></cell><cell></cell><cell>42,45,5</cell><cell></cell><cell></cell><cell>147,94,93,7</cell><cell>227,118,13,0</cell></row><row><cell>28</cell><cell></cell><cell></cell><cell>43,45,4</cell><cell></cell><cell></cell><cell>191,108,42,0</cell><cell>196,110,51,1</cell></row><row><cell>29</cell><cell></cell><cell></cell><cell>43,47,2</cell><cell></cell><cell></cell><cell>127,95,113,6</cell><cell>232,118,8,0</cell></row><row><cell>30</cell><cell></cell><cell></cell><cell>6,50,36</cell><cell></cell><cell></cell><cell>203,117,21,0</cell><cell>224,114,20,0</cell></row><row><cell>31</cell><cell></cell><cell></cell><cell>4,51,37</cell><cell></cell><cell></cell><cell>282,377,23,0</cell><cell>214,100,43,1</cell></row><row><cell>32</cell><cell></cell><cell></cell><cell>22,57,13</cell><cell></cell><cell></cell><cell>279,388,15,0</cell><cell>139,114,97,8</cell></row><row><cell>33</cell><cell></cell><cell></cell><cell>9,60,23</cell><cell></cell><cell></cell><cell>84,442,155,1</cell><cell>198,113,47,0</cell></row><row><cell>34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>151,87,115,5</cell></row><row><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>171,103,83,1</cell></row><row><cell>36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>172,104,72,10</cell></row><row><cell>37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>205,88,65,0</cell></row><row><cell>38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>170,122,64,2</cell></row><row><cell>39</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>170,98,81,9</cell></row><row><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>223,101,32,2</cell></row><row><cell>41</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>192,114,52,0</cell></row><row><cell>42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>112,99,134,13</cell></row><row><cell>43</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>109,116,130,3</cell></row><row><cell>44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>110,90,118,40</cell></row><row><cell>45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>194,115,49,0</cell></row><row><cell>46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>178,135,45,0</cell></row><row><cell>47</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>209,135,14,0</cell></row><row><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>341,368,6,0</cell></row><row><cell>49</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>363,348,4,0</cell></row><row><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>311,398,6,0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table /><note>Learned neuron numbers in each SA block in ScaleNets on CIFAR-100 and ImageNet. These numbers indicates the output channel numbers for scale 1, 2, 3, and 4 (e.g. 3×3 conv [C 1 ,C 2 ,C 3 ,C 4 ] in</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the number indicates the layer number of their start points but not ScaleNets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by Beijing Municipal Science and Technology Commission (Z181100008918004) in part. We thank Zhanglin Peng, Youjiang Xu, Xinjiang Wang, and Huabin Zheng from SenseTime for the helpful suggestions and supports.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Training Curves on ImageNet</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning the number of neurons in neep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2270" to="2278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: A technique for the experimental evaluation of dependability in modern computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Madeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="136" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive multi-scale information flow for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Layercompensated pruning for resource-constrained convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Morphnet: Fast &amp; simple resourceconstrained structure learning of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="770" to="778" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-scale dense networks for resource efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-driven sparse structure selection for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="304" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast convnets using groupwise brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2554" to="2564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fpn feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tech</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I C</forename><surname>Onvolutions</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Parsenet looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional neural fabrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4053" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning time/emory-efficient deep architectures with budgeted super networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Veniat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian regularization and pruning using a laplace prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="143" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nisp: Pruning networks using neuron importance score propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
