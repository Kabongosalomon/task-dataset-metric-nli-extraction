<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance Credibility Inference for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
							<email>yanweifu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance Credibility Inference for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot learning (FSL) aims to recognize new objects with extremely limited training data for each category. Previous efforts are made by either leveraging meta-learning paradigm or novel principles in data augmentation to alleviate this extremely data-scarce problem. In contrast, this paper presents a simple statistical approach, dubbed Instance Credibility Inference (ICI) to exploit the distribution support of unlabeled instances for few-shot learning. Specifically, we first train a linear classifier with the labeled few-shot examples and use it to infer the pseudolabels for the unlabeled data. To measure the credibility of each pseudo-labeled instance, we then propose to solve another linear regression hypothesis by increasing the sparsity of the incidental parameters and rank the pseudo-labeled instances with their sparsity degree. We select the most trustworthy pseudo-labeled instances alongside the labeled examples to re-train the linear classifier. This process is iterated until all the unlabeled samples are included in the expanded training set, i.e. the pseudo-label is converged for unlabeled data pool. Extensive experiments under two fewshot settings show that our simple approach can establish new state-of-the-arts on four widely used few-shot learning benchmark datasets including miniImageNet, tieredIm-ageNet, CIFAR-FS, and CUB. Our code is available at: https://github.com/Yikai-Wang/ICI-FSL</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning from one or few examples is an important ability for humans. For example, children have no problem forming the concept of "giraffe" by only taking a glance from a picture in a book, or hearing its description as looking like a deer with a long neck <ref type="bibr" target="#b58">[58]</ref>. In contrast, the most successful recognition systems <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref> still highly * Corresponding author. rely on an avalanche of labeled training data. This thus increases the burden in rare data collection (e.g. accident data in the autonomous driving scenario) and expensive data annotation (e.g. disease data for medical diagnose), and more fundamentally limits their scalability to open-ended learning of the long tail categories in the real-world.</p><p>Motivated by these observations, there has been a recent resurgence of research interest in few-shot learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b52">53]</ref>. It aims to recognize new objects with extremely limited training data for each category. Basically, a few-shot learning model has the chance to access the source/base dataset with many labeled training instances for model training and then is able to generalize to a disjoint but relevant target/novel dataset with only scarce labeled data. A simplest baseline to transfer learned knowledge to the novel set is fine-tuning <ref type="bibr" target="#b57">[57]</ref>. However, it would cause severely overfitting as one or a few instances are insufficient to model the data distributions of the novel classes. Data augmentation and regularization techniques can alleviate overfitting in such a limited-data regime, but they do not solve it. Several recent efforts are made in leveraging learning to learn, or meta-learning paradigm by simulating the few-shot scenario in the training process <ref type="bibr" target="#b23">[24]</ref>. However, Chen et al. <ref type="bibr" target="#b6">[7]</ref> empirically argue that such a learning paradigm often results in inferior performance compared to a simple baseline with a linear classifier coupled with a deep feature extractor.</p><p>Given such a limited-data regime (one or few labeled examples per category), one of the fundamental problems for few-shot learning is that one can hardly estimate the data distribution without introducing the inductive bias. To address this problem, two types of strategy resort to model the data distribution of novel category beyond traditional inductive few-shot learning: (i) semi-supervised few-shot learning (SSFSL) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref> supposes that we can utilize unlabeled data (about ten times more than labeled data) to help to learn the model; furthermore, (ii) transductive inference <ref type="bibr" target="#b17">[18]</ref> for few-shot learning (TFSL) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref> assumes  <ref type="figure">Figure 1</ref>. Schematic illustration of our proposed framework. In the inference process of N -way-m-shot FSL task with unlabeled data, we embed each instance, inference each unlabeled data and use ICI to select the most trustworthy subset to expand the support set. This process is repeated until all unlabeled data are included in the support set.</p><p>we can access to all the test data, rather than evaluate them one by one in the inference process. In other words, the few-shot learning model can utilize the data distributions of testing examples.</p><p>Self-taught learning <ref type="bibr" target="#b34">[35]</ref> is one of the most straightforward ways in leveraging the information of unlabeled data. Typically, a trained classifier infers the labels of unlabeled data, which are further taken to update the classifier. Nevertheless, the inferred pseudo-labels may not be always trustworthy; the wrongly labeled instances may jeopardize the performance of the classifier. It is thus essential to investigate the labeling confidence of each unlabeled instance.</p><p>To this end, we present a simple statistical approach, dubbed Instance Credibility Inference (ICI) to exploit the distribution support of unlabeled instances for few-shot learning. Specifically, we first train a linear classifier (e.g., logistic regression) with the labeled few-shot examples and use it to infer the pseudo-labels for the unlabeled data. Our model aims to iteratively select the most trustworthy pseudo-labeled instances according to their credibility measured by the proposed ICI to augment the training set. The classifier thus can be progressively updated and further infer the unlabeled data. We iterate this process until all the unlabeled samples are included in the expanded training set, i.e. the pseudo-label is converged for unlabeled data pool. The schematic illustration is shown in <ref type="figure">Figure 1</ref>.</p><p>Basically, we re-purpose the standard self-taught learning algorithm by our ICI algorithm. How to select the pseudo-labeled data to exclude the wrong-predicted samples, i.e., excluding the noise introduced by the self-taught learning strategy? Our intuition is that the algorithm of sample selection can neither rely only on the label space (e.g. based on the probability of each class given by the classifier) nor the feature space (e.g. select samples most similar to training data). Instead, we introduce a linear regression hypothesis by regressing each instance (labeled and pseudolabeled) from feature to label space and increase the sparsity of the incidental parameter <ref type="bibr" target="#b8">[9]</ref> until it vanishes. Thus we can rank pseudo-labeled instances with sparsity degree as their credibility. We conduct extensive experiments on major few-shot learning datasets to validate the effectiveness of our proposed algorithm.</p><p>The contributions of this work are as follows: (i) We present a simple statistical approach, dubbed Instance Credibility Inference (ICI) to exploit the distribution support of unlabeled instances for few-shot learning. Specifically, our model iteratively selects the pseudo-labeled instances according to its credibility measured by the proposed ICI for classifier training. (ii) We re-purpose the standard selftaught learning algorithm <ref type="bibr" target="#b34">[35]</ref> by our proposed ICI. To measure the credibility of each pseudo-labeled instance, we solve another linear regression hypothesis by increasing the sparsity of the incidental parameter <ref type="bibr" target="#b8">[9]</ref> and rank the sparsity degree as the credibility for each pseudo-labeled instance. (iii) Extensive experiments under two few-shot settings show that our simple approach can establish new state-ofthe-arts on four widely used few-shot learning benchmark datasets including miniImageNet, tieredImageNet, CIFAR-FS, and CUB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Semi-supervised learning.</p><p>Semi-supervised learning (SSL) aims to improve the learning performance with limited labeled data by exploiting large amount of unlabeled data. Conventional approaches focus on finding the low-density separator within both labeled and unlabeled data <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>, and avoid to learn the "wrong" knowledge from the unlabeled data <ref type="bibr" target="#b25">[26]</ref>. Recently, semi-supervised learning with deep learning models use consistency regularization <ref type="bibr" target="#b20">[21]</ref>, moving average technique <ref type="bibr" target="#b47">[48]</ref> and adversarial perturbation regularization <ref type="bibr" target="#b28">[29]</ref> to train the model with large amount of unlabeled data. The key difference between semi-supervised learning and few-shot learning with unlabeled data is that the unlabeled data is still limited in the latter. To some extent, the low-density assumption widely utilized in SSL is hard to achieve in the few-shot scenario, making SSFSL a more difficult problem.</p><p>Self-taught learning <ref type="bibr" target="#b34">[35]</ref>, also known as selftraining <ref type="bibr" target="#b54">[55]</ref>, is a traditional semi-supervised strategy of utilizing unlabeled data to improve the performance of classifiers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>. Typically, an initially trained classifier predicts class labels of unlabeled instances; the unlabeled data with pseudo-labels are further selected to update the classifier. <ref type="bibr" target="#b21">[22]</ref>. Current algorithms based on self-taught learning includes training neural networks using labeled data and pseudo-labeled data jointly <ref type="bibr" target="#b21">[22]</ref>, using mix-up between unlabeled data and labeled data to reduce the influence of noise <ref type="bibr" target="#b1">[2]</ref>, using label propagation for pseudolabeling based on a nearest-neighbor graph and measuring the credibility using entropy <ref type="bibr" target="#b16">[17]</ref>, and re-weighting the pseudo-labeled data based on the cluster assumption on the feature space <ref type="bibr" target="#b39">[40]</ref>. Unfortunately, the predicted pseudolabels may not be trustworthy. Different and orthogonal to previous re-weighting or mix-up works, we design a statistical algorithm in estimating the credibility of each instance assigned with its corresponding pseudo-label. Only the most confident instances are employed to update the classifier.</p><p>Few-shot learning. Recent efforts on FSL are made towards the following aspects. (1) Metric learning methods, putting emphasis on finding better distance metrics, include weighted nearest neighbor classifier (e.g. Matching Network <ref type="bibr" target="#b52">[53]</ref>), finding prototype for each class (e.g. Prototypical Network <ref type="bibr" target="#b42">[43]</ref>), or learning specific metric for each task (e.g. TADAM <ref type="bibr" target="#b32">[33]</ref>); (2) Meta learning methods, such as Meta-Critic <ref type="bibr" target="#b46">[47]</ref>, MAML <ref type="bibr" target="#b9">[10]</ref>, Meta-SGD <ref type="bibr" target="#b26">[27]</ref>, Reptile <ref type="bibr" target="#b31">[32]</ref>, and LEO <ref type="bibr" target="#b38">[39]</ref>, optimize the models for the capacity of rapidly adapted to new tasks. (3) Data augmentation algorithms enlarge available data to alleviate the lack of data in the image level <ref type="bibr" target="#b7">[8]</ref> or the feature level <ref type="bibr" target="#b36">[37]</ref>. Additional, SNAIL <ref type="bibr" target="#b29">[30]</ref> utilizes the sequence modeling to create a new framework. The proposed statistical algorithm is orthogonal but potentially useful to improve these algorithms -it is always worth increasing the training set by utilizing the unlabeled data with confidently predicted labels.</p><p>Few-shot learning with unlabeled data. Recently ap-proaches tackle few-shot learning problems by resorting to additional unlabeled data. Specifically, in semi-supervised few-shot learning settings, recent works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28]</ref> enables unlabeled data from the same categories to better handle the true distribution of each class. Furthermore, transductive settings have also been considered recently. For example, LST <ref type="bibr" target="#b44">[45]</ref> utilizes self-taught learning strategy in a metalearning manner. Different from these methods, this paper presents a conceptually simple statistical approach derived from self-taught learning; our approach, empirically and significantly improves the performance of FSL on several benchmark datasets, by only using very simple classifiers, e.g., logistic regression, or Support Vector Machine (SVM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem formulation</head><p>We introduce the formulation of few-shot learning here. Assume a base category set C base , and a novel category set C novel with C base C novel = ∅. Accordingly, the base and novel datasets are For evaluation, we adopt the standard N -way-m-shot classification as <ref type="bibr" target="#b52">[53]</ref> on D novel . Specifically, in each episode, we randomly sample N classes L ∼ C novel ; and m and q labeled images per class are randomly sampled in L to construct the support set S and the query set Q, respectively. Thus we have |S| = N × m and |Q| = N × q. The classification accuracy is averaged on query sets Q of many meta-testing episodes. In addition, we have unlabeled data of novel categories U novel = {I u }.</p><formula xml:id="formula_0">D base = {(I i , y i ) , y i ∈ C base }, and D novel = {(I i , y i ) , y i ∈ C novel },</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-taught learning from unlabeled data</head><p>In general, labeled data for machine learning is often very difficult and expensive to obtain, while the unlabeled data can be utilized for improving the performance of supervised learning. Thus we recap the self-taught learning formalism -one of the most classical semi-supervised methods for few-shot learning <ref type="bibr" target="#b34">[35]</ref>. Particularly, assume f (·) is the feature extractor trained on the base dataset D base . One can train a supervised classifier g (·) on the support set S, and pseudo-labeling unlabeled data,ŷ i = g (f (I u )) with corresponding confidence p i given by the classifier. The most confident unlabeled instances will be further taken as additional data of corresponding classes in the support set S. Thus we obtain the updated supervised classifierĝ (·). To this end, few-shot classifier acquires additional training instances, and thus its performance can be improved.</p><p>However, it is problematic if directly utilizing self-taught learning in one-shot cases. Particularly, the supervised clas-sifier g (·) is only trained by few instances. The unlabeled instances with high confidence may not be correctly categorized, and the classifier will be updated by some wrong instances. Even worse, one can not assume the unlabeled instances follows the same class labels or generative distribution as the labeled data. Noisy instances or outliers may also be utilized to update the classifiers. To this end, we propose a systematical algorithm: Instance Credibility Inference (ICI) to reduce the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Instance credibility inference (ICI)</head><p>To measure the credibility of predicted labels over unlabeled data, we introduce a hypothesis of linear model by regressing each instance from feature to label spaces. Particularly, given n instances of N classes, S =</p><formula xml:id="formula_1">{(I i , y i , x i ) , y i ∈ C novel },</formula><p>where y i is the ground truth when I i come from the support set, or the pseudo-label when I i come from the unlabeled set, we employ a simple linear regression model to "predict" the class label,</p><formula xml:id="formula_2">y i = x i β + γ i + i ,<label>(1)</label></formula><p>where β ∈ R d×N is the coefficient matrix for classification; x i ∈ R d×1 is the feature vector of instance i; y i is N dimension one-hot vector denoting the class label of instance i. Note that to facilitate the computations, we employ PCA <ref type="bibr" target="#b49">[50]</ref> to reduce the dimension of extracted features f (I i ) to d. ij ∼ N 0, σ 2 is the Gaussian noise of zero mean and σ variance. Inspired by incidental parameters <ref type="bibr" target="#b8">[9]</ref>, we introduce γ i,j to amend the chance of instance i belonging to class y j . Larger γ i,j , the higher difficulty in attributing instance i to class y j . Write Eq. 1 in a matrix form for all instances, we are thus solving the problem of:</p><formula xml:id="formula_3">β ,γ = arg min β,γ Y − Xβ − γ 2 F + λR (γ) ,<label>(2)</label></formula><p>where · 2 F denotes the Frobenius norm. Y = [y i ] ∈ R n×N and X = x i ∈ R n×d indicate label and feature input respectively. γ = [γ i ] ∈ R n×N is the incidental matrix, with the penalty R (γ) = n i=1 γ i 2 . λ is the coefficient of penalty. To solve Eq. 2, we re-write the function as</p><formula xml:id="formula_4">L (β, γ) = Y − Xβ − γ 2 F + λR (γ) .</formula><p>Let ∂L ∂β = 0, we havê</p><formula xml:id="formula_5">β = X X † X (Y − γ) ,<label>(3)</label></formula><p>where (·) † denotes the Moore-Penrose pseudo-inverse.</p><p>Note that (1) we are interested in utilizing γ to measure the credibility of each instance along its regularization path, rather than estimatingβ, since the linear regression model Algorithm 1 Inference process of our algorithm.</p><formula xml:id="formula_6">Input:support data{(X i , y i )} N ×K i=1 , query data X t = {X j } M j=1 , unlabeled data X u = {X k } U k=1 Initialization: support set (X s , y s ) = {(X i , y i )} N ×K i=1 , fea- ture matrix X N ×K+U,d = [X s ; X u ], classifier Repeat:</formula><p>Train classifier using (X s , y s ); Get pseudo-label y u for X u by classifier; Rank (X, y) = (X, [y s ; y u ]) by ICI; Select a subset (X sub , y sub ) into (X s , y s ); Until Converged. Inference: Train classifier using (X s , y s ); Get pseudo-label y t for X t by classifier;</p><formula xml:id="formula_7">Output: inference labels y t = {ŷ j } M j=1</formula><p>is not good enough for classification in general. (2) theβ also relies on the estimation of γ. To this end, we take Eq. 3 into L (·) and solve the problem as,</p><formula xml:id="formula_8">arg min γ∈R n×N Y − H (Y − γ) − γ 2 F + λR (γ) ,<label>(4)</label></formula><p>where H = X X X † X is the hat matrix of X. We further defineX = (I − H) andỸ =XY . Then the above equation can be simplified as arg min</p><formula xml:id="formula_9">γ∈R n×N Ỹ −Xγ 2 F + λR (γ) ,<label>(5)</label></formula><p>which is a multi-response regression problem. We seek the best subset by checking the regularization path, which can be easily configured by a blockwise descent algorithm implemented in Glmnet <ref type="bibr" target="#b40">[41]</ref>. Specifically, we have a theoretical value of λ max = max i X ·iỸ 2 /n <ref type="bibr" target="#b40">[41]</ref> to guarantee the solution of Eq. 5 all 0. Then we can get a list of λs from 0 to λ max . We solve a specific Eq. 5 with each λ, and get the regularization path of γ along the way. Particularly, we regard γ as a function of λ. When λ changes from 0 to ∞, the sparsity of γ is increased until all of its elements are forced to be vanished. Further, our penalty R (γ) encourages γ vanishes row by row, i.e., instance by instance. Moreover, the penalty will tend to vanish the subset ofX with the lowest deviations, indicating less discrepancy between the prediction and the ground truth. Hence we could rank the pseudo-labeled data by their λ value when the corresponding γ i vanishes. As shown in one toy example of <ref type="figure" target="#fig_2">Figure 2</ref>, the γ value of the instance denoted by the red line vanishes first, and thus it is the most trustworthy sample by our algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Self-taught learning with ICI</head><p>The proposed ICI can thus be easily integrated to improve the self-taught learning algorithm. Particularly, the initialized classifier can predict the pseudo-labels of unlabeled instances; and we further employ the ICI algorithm to select the most confident subset of unlabeled instances, to update the classifier. The whole algorithm can be iteratively updated, as summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>Our experiments are conducted on several widely few-shot learning benchmark datasets for general object recognition and fine-grained classification, including miniImageNet <ref type="bibr" target="#b35">[36]</ref>, tieredImageNet <ref type="bibr" target="#b36">[37]</ref>, CIFAR-FS <ref type="bibr" target="#b4">[5]</ref> and CUB <ref type="bibr" target="#b53">[54]</ref>. miniImageNet consists of 100 classes with 600 labeled instances in each category. We follow the split proposed by <ref type="bibr" target="#b35">[36]</ref>, using 64 classes as the base set to train the feature extractor, 16 classes as the validation set and report performance on the novel set which consists of 20 classes. tieredImageNet is a larger dataset compared with miniImageNet, and its categories are selected with hierarchical structure to split base and novel datasets semantically. We follow the split introduced in <ref type="bibr" target="#b36">[37]</ref> with base set of 20 superclasses (351 classes), validation set of 6 superclasses (97 classes) and novel set of 8 superclasses (160 classes). Each class contains 1281 images on average. CUB is a fine-grained dataset of 200 bird categories with 11788 images in total. Following the previous setting in <ref type="bibr" target="#b14">[15]</ref>, we use 100 classes as the base set, 50 for validation and 50 as the novel set. To make a fair comparison, we crop all images with the bounding box provided by <ref type="bibr" target="#b50">[51]</ref>. CIFAR-FS is a dataset with lower-resolution images derived from CIFAR-100 <ref type="bibr" target="#b18">[19]</ref> . It contains 100 classes with 600 instances in each class. We follow the split given by <ref type="bibr" target="#b4">[5]</ref>, using 64 classes to construct the base set, 16 for validation and 20 as the novel set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setup.</head><p>Unless otherwise specified, we use the following settings and implementation in the experiments for our approach to make a fair comparison. As in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23]</ref>, we use ResNet-12 <ref type="bibr" target="#b12">[13]</ref> with 4 residual blocks as the feature extractor in our experiments. Each block consists of three 3 × 3 convolutional layers, each of which followed by a BatchNorm layer and a LeakyReLu(0.1) activation. In the end of each block, a 2 × 2 max-pooling layer is utilized to reduce the output size. The number of filters in each block is 64, 128, 256 and 512 respectively. Specifically, referring to <ref type="bibr" target="#b22">[23]</ref>, we adopt the Dropout <ref type="bibr" target="#b43">[44]</ref> in the first two block to vanish 10% of the output, and adopt DropBlock <ref type="bibr" target="#b10">[11]</ref> in the latter two blocks to vanish 10% of output in channel level. Finally, an average-pooling layer is employed to produce the input feature embedding. We select 90% images from each training class (e.g., 64 categories for miniImageNet) to construct our training set for training the feature extractor and use the remaining 10% as the validation set to select the best model. We use SGD with momentum as the optimizer to train the feature extractor from scratch. Momentum factor and L 2 weight decay is set to 0.9 and 1e − 4, respectively. All inputs are resized to 84 × 84. We set the initial learning rate of 0.1, decayed by 10 after every 30 epochs. The total training epochs is 120 epochs. In all of our experiments, we normalize the feature with L 2 norm and reduce the feature dimension to d = 5 using PCA <ref type="bibr" target="#b49">[50]</ref>. Our model and all baselines are evaluated over 600 episodes with 15 test samples from each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semi-supervised few-shot learning</head><p>Settings. In the inference process, the unlabeled data from the corresponding category pool is utilized to help FSL. In our experiments, we report following settings of SSFSL: (1) we use 15 unlabeled samples for each class, the same as TFSL, to compare our algorithm in SSFSL and TFSL settings; (2) we use 30 unlabeled samples in 1-shot task, and 50 unlabeled samples in 5-shot task, the same as current SSFSL approaches <ref type="bibr" target="#b44">[45]</ref>; (3) we use 80 unlabeled samples, to show the effectiveness of ICI compared with FSL algorithms with a larger network and higher-resolution inputs. We denote these as <ref type="bibr">(15/15)</ref>  <ref type="table">Table 1</ref>. Test accuracies over 600 episodes on several datasets. Results with (·) 1 are reported in <ref type="bibr" target="#b6">[7]</ref>, with (·) 2 are reported in <ref type="bibr" target="#b44">[45]</ref>, with (·) 3 are reported in <ref type="bibr" target="#b22">[23]</ref>. (·) 4 is our implementation with the official code of <ref type="bibr" target="#b27">[28]</ref>. Methods denoted by (·) * denotes ResNet-18 with input size 224 × 224, while (·) † denotes ResNet-18 with input size 84 × 84. Our method and other alternatives use ResNet-12 with input size 84 × 84. In. and Tran. indicate inductive and transductive setting, respectively. Semi. denotes semi-supervised setting where (·/·) shows the number of unlabeled data available in 1-shot and 5-shot experiments. so sufficient samples in each class, so we simply choose 5 as support set, 15 as query set and other samples as unlabeled set (about 39 samples on average) in the 5-shot task in the latter two settings. For all settings, we select 5 samples for every class in each iteration. The process is finished when at most five instances for each class are excluded from the expanded support set. i.e., select (10/10), (25/45), (75/75) unlabeled instances in total. Further, we utilize Logistic Regression (denoted as LR) and linear Support Vector Machine (denoted as SVM) to show the robustness of ICI against different linear classifiers.</p><p>Competitors. We compare our algorithm with current approaches in SSFSL. TPN <ref type="bibr" target="#b27">[28]</ref> uses labeled support set and unlabeled set to propagate label to one query sample each time. LST <ref type="bibr" target="#b44">[45]</ref> also uses self-taught learning strategy to pseudo-label data and select confident ones, but they do this by a neural network trained in the meta-learning manner for many iterations. Other approaches include Masked Soft k-Means <ref type="bibr" target="#b36">[37]</ref> and a combination of MTL with TPN and Masked Soft k-Means reported by LST.</p><p>Results. are shown in <ref type="table">Table 1</ref> where denoted as Semi. in the first column. Analysis from the experimental results, we can find that: (1) Compare SSFSL with TFSL with the same number of unlabeled data, we can see that our SSFSL results are only reduced by a little or even beat TFSL results, which indicates that the information we got from the unlabeled data are robust and we can indeed handle the true distribution with unlabeled data practically. <ref type="formula" target="#formula_3">(2)</ref> The more unlabeled data we get, the better performance we have. Thus we can learn more knowledge with more unla-beled data almost consistently using a linear classifier (e.g. logistic regression). When lots of unlabeled data are accessible, ICI achieves state-of-the-art in all experiments even compared with competitors which use bigger network and higher-resolution inputs.</p><p>(3) Compared with other SSFSL approaches, ICI also achieves varying degrees of improvements in almost all tasks and datasets. These results further indicate the robustness of our algorithm. Compared logistic regression with SVM, the robustness of ICI still holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transductive few-shot learning</head><p>Settings. In transductive few-shot learning setting, we have chance to access the query data in the inference stage. Thus the unlabeled set and the query dataset are the same.</p><p>In our experiments, we select 5 instances for each class in each iteration and repeat our algorithm until all the expected query samples are included, i.e., each class will be expanded by at most 15 images. We also utilize both Logistic Regression and SVM as our classifier, respectively.</p><p>Competitors. We compare ICI with current TFSL approaches. TPN <ref type="bibr" target="#b27">[28]</ref> constructs a graph and uses label propagation to transfer label from support samples to query samples and learn their framework in a meta-learning way. TEAM <ref type="bibr" target="#b33">[34]</ref> utilizes class prototypes with a data-dependent metric to inference labels of query samples.</p><p>Results. are shown in <ref type="table">Table 1</ref> where denoted as Tran. in the first column. Experiments cross four benchmark datasets indicate that: (1) Compared with basic linear classifier, ICI enjoys consistently improvements, especially in the 1-shot setting where the labeled data is extremely limited and such improvements are robust regardless of utilizing which linear classifiers. Further, compared results between miniImageNet and tieredImageNet, we can find that the improvement margin is in the similar scale, indicating that the improvement of ICI does not rely on the semantic relationship between base set and novel set. Hence the effectiveness and robustness of ICI is confirmed practically.</p><p>(2) Compared with current TFSL approaches, ICI also achieves the state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>Effectiveness of ICI. To show the effectiveness of ICI, we visualize the regularization path of γ in one episode of inference process in <ref type="figure" target="#fig_3">Figure 3</ref> where red lines are instances that are correct-predicted while black lines are wrong-predicted ones. It is obvious that that most of the correct-predicted instances lie in the lower-left part. Since ICI will select samples whose norm will vanish in a lower λ. We could get more correct-predicted instances than wrong-predicted instances in a high ratio.  Compare to baselines. To further show the effectiveness of ICI, we compare ICI with other sample selection strategies under the self-taught learning pipeline. One simple strategy is randomly sampling the unlabeled data into the expanded support set in each iteration, denoted as ra. Another is selecting the data based on the confidence given by the classifier, denoted by co. In this strategy, the more confident the classifier is to one sample, the more trustworthy that sample is. The last one is replacing our algorithm of computing credibility by choosing the nearest-neighbor of each class in the feature space, denoted as nn. In this part, we have 15 unlabeled instances for each class and select 5 to re-train the classifier by different methods for Semi. and Tran. task on miniImageNet. From  belled data for classifier training. Select all the unlabelled data in one go cannot take the distribution, or the credibility of the unlabeled data into account, and thus produce more noise labels to hurt the performance of the model. The classifier thus be trained with its prediction, resulting in no improvements in TFSL setting. We briefly validate this as ICI <ref type="bibr" target="#b14">(15)</ref> in <ref type="figure" target="#fig_4">Figure 4</ref> whilst ICI obtained better accuracy with iterative selection manner. For example, select 6 images with two iterations (ICI <ref type="formula" target="#formula_5">(3)</ref>) is superior to select 8 images in one iteration (ICI <ref type="formula">(8)</ref>).  <ref type="table">Table 3</ref>. We run 600 episodes, with each episode training an initial classifier. We denote "Acc" as the accuracy intervals; and "b/T" as the number of classifiers experienced improvement v.s. total classifiers in this accuracy interval.</p><p>Robustness against initial classifier. What are the requirements for the initial linear classifier? Is it necessary to satisfy that the accuracy of the initial linear classifier is higher than 50% or even higher? The answer is no. As long as the initial linear classifier can be trained, theoretically our method should work. It thus is a future open question of how the initial classifier affects. We briefly validate it in <ref type="table">Table 3</ref>. We run 600 episodes, with each episode training an initial classifier with different classification accuracy. <ref type="table">Table 3</ref> shows that most classifiers can get improved by ICI regardless of the initial accuracy (even with accuracy of 30-40%).</p><p>Influence of reduced dimension. In this part, we study the influence of reduced dimension d in our algorithm on 5-way 1-shot miniImageNet experiments. The results with reduced dimension 2, 5, 10, 20, 50, and without dimensionality reduction i.e., d = 512, are shown in dimension is much smaller than the number of instances (i.e., d n), which is consistent with the theoretical property <ref type="bibr" target="#b8">[9]</ref>. Moreover, we can observe that our model achieves the best accuracy 66.80% when d = 5. Practically, we adopt d = 5 in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of dimension reduction algorithms.</head><p>Furthermore, we study the robustness of ICI to different dimension reduction algorithms. We compare Isomap <ref type="bibr" target="#b48">[49]</ref>, principal components analysis <ref type="bibr" target="#b49">[50]</ref> (PCA), local tangent space alignment <ref type="bibr" target="#b59">[59]</ref> (LTSA), multi-dimensional scaling <ref type="bibr" target="#b5">[6]</ref> (MDS), locally linear embedding <ref type="bibr" target="#b37">[38]</ref> (LLE) and spectral embedding <ref type="bibr" target="#b2">[3]</ref> (SE) on 5-way 1-shot miniImageNet experiments. From <ref type="table">Table 4</ref> we can observe that ICI is robust across most of the dimensionality reduction algorithms (from LTAS 64.61% to SE 67.7%) except MDS (59.99%). We adopt PCA for dimension reduction in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a simple method, called Instance Credibility Inference (ICI) to exploit the distribution support of unlabeled instances for few-shot learning. The proposed ICI effectively select the most trustworthy pseudo-labeled instances according to their credibility to augment the training set. In order to measure the credibility of each pseudo-labeled instance, we propose to solve a linear regression hypothesis by increasing the sparsity of the incidental parameters <ref type="bibr" target="#b8">[9]</ref> and rank the pseudo-labeled instance with their sparsity degree. Extensive experiments show that our simple approach can establish new state-ofthe-arts on four widely used few-shot learning benchmark datasets including miniImageNet, tieredImageNet, CIFAR-FS, and CUB.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>respectively. In few-shot learning, the recognition models on D base should be generalized to the novel category C novel with only one or few training examples per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Regularization path of λ on ten samples. Red line is corresponding to the most trustworthy sample suggested by our ICI algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Regularization path of λ. Red lines are correct-predicted instances while black lines are wrong-predicted ones. ICI will choose instances in the lower-left subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Variation of accuracy as the selected samples increases over 600 episodes on miniImageNet. "ICI (n)": select n samples per class in each iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Compare to baselines on miniImageNet under several settings.</figDesc><table><row><cell>Model</cell><cell>1shot</cell><cell>Tran.</cell><cell>5shot</cell><cell>1shot</cell><cell>Semi.</cell><cell>5shot</cell></row><row><cell>LR</cell><cell>56.06</cell><cell></cell><cell>75.43</cell><cell>56.06</cell><cell></cell><cell>75.43</cell></row><row><cell>+ ra</cell><cell>59.01</cell><cell></cell><cell>76.38</cell><cell>59.46</cell><cell></cell><cell>76.58</cell></row><row><cell>+ nn</cell><cell>63.24</cell><cell></cell><cell>77.63</cell><cell>63.10</cell><cell></cell><cell>77.75</cell></row><row><cell>+ co</cell><cell>63.29</cell><cell></cell><cell>77.92</cell><cell>63.57</cell><cell></cell><cell>77.71</cell></row><row><cell>ICI</cell><cell>65.32</cell><cell></cell><cell>78.30</cell><cell>64.60</cell><cell></cell><cell>77.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>, we observe that ICI outperforms all the baselines in all settings. Effectiveness of iterative manner. Our intuition is the proposed ICI learns to generate a set of trustworthy unla-</figDesc><table><row><cell>68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>60 64 Test accuracy</cell><cell></cell><cell></cell><cell></cell><cell>ICI(3) ICI(5) ICI(8) ICI(15)</cell><cell></cell></row><row><cell>56</cell><cell>0</cell><cell>3</cell><cell>6 Selected number per class 9</cell><cell>12</cell><cell>15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 4 .</head><label>44</label><figDesc>Our algorithm achieves better performance when the reduced Influence of dimensionality reduction dimensions and algorithms.</figDesc><table><row><cell>d</cell><cell>Acc (%)</cell><cell>Alg.</cell><cell>Acc (%)</cell></row><row><cell>2</cell><cell>63.71 ± 1.025</cell><cell>Isomap [49]</cell><cell>66.53 ± 1.073</cell></row><row><cell>5</cell><cell>66.80 ± 1.096</cell><cell>PCA [50]</cell><cell>66.80 ± 1.096</cell></row><row><cell>10</cell><cell>66.25 ± 1.048</cell><cell>LTSA [59]</cell><cell>64.61 ± 1.058</cell></row><row><cell>20</cell><cell>64.98 ± 1.049</cell><cell>MDS [6]</cell><cell>59.99 ± 0.941</cell></row><row><cell>50</cell><cell>61.54 ± 0.980</cell><cell>LLE [38]</cell><cell>67.59 ± 1.120</cell></row><row><cell>512</cell><cell>57.41 ± 0.877</cell><cell>SE [3]</cell><cell>67.70 ± 1.117</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Massih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pseudo-labeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02983</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kristin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayhan</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demiriz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modern multidimensional scaling: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingwer</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Groenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image deformation meta-networks for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runlong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.6950</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Partial consistency with sparse incidental parameters</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Few-shot learning with metric-agnostic conditional embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Hilliard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Howland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artëm</forename><surname>Yankov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Courtney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">O</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hodas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04376</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Metalearning: a survey of trends and technologies. Artificial intelligence review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Lemke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Budka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Gabrys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for fewshot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Towards making unlabeled data never hurt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Metasgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Virtual adversarial training for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miayto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09926</idno>
		<title level="m">Rapid adaptation with conditionally shifted neurons</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodríguez López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-taught learning: Transfer learning from unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00676</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Nonlinear dimensionality reduction by locally linear embedding. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sygnowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zhiheng MaXiaoyu Tao, and Nanning Zheng. Transductive semisupervised deep learning using min-max features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A blockwise descent algorithm for group-penalized multiresponse and multinomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.6529</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to self-train for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibao</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00562</idno>
	</analytic>
	<monogr>
		<title level="m">Tat-Seng Chua, and Bernt Schiele</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning to learn: Meta-critic networks for sample efficient learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09529</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A global geometric framework for nonlinear dimensionality reduction. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Joshua B Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher M</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Few-shot learning through an information retrieval lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Statistical learning theory wiley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlamimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04252</idno>
		<title level="m">Self-training with noisy student improves imagenet classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sung Whan Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tapnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06549</idno>
		<title level="m">Neural network augmented with task-adaptive projection for few-shot learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Principal manifolds and nonlinear dimensionality reduction via tangent space alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on scientific computing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
