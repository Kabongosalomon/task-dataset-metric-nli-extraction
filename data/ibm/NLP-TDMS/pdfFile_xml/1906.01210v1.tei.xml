<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attributed Graph Clustering via Adaptive Graph Convolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attributed Graph Clustering via Adaptive Graph Convolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attributed graph clustering is challenging as it requires joint modelling of graph structures and node attributes. Recent progress on graph convolutional networks has proved that graph convolution is effective in combining structural and content information, and several recent methods based on it have achieved promising clustering performance on some real attributed networks. However, there is limited understanding of how graph convolution affects clustering performance and how to properly use it to optimize performance for different graphs. Existing methods essentially use graph convolution of a fixed and low order that only takes into account neighbours within a few hops of each node, which underutilizes node relations and ignores the diversity of graphs. In this paper, we propose an adaptive graph convolution method for attributed graph clustering that exploits high-order graph convolution to capture global cluster structure and adaptively selects the appropriate order for different graphs. We establish the validity of our method by theoretical analysis and extensive experiments on benchmark datasets. Empirical results show that our method compares favourably with state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Attributed graph clustering <ref type="bibr" target="#b1">[Cai et al., 2018]</ref> aims to cluster nodes of an attributed graph where each node is associated with a set of feature attributes. Attributed graphs widely exist in real-world applications such as social networks, citation networks, protein-protein interaction networks, etc. Clustering plays an important role in detecting communities and analyzing structures of these networks. However, attributed graph clustering requires joint modelling of graph structures and node attributes to make full use of available data, which presents great challenges.</p><p>Some classical clustering methods such as k-means only deal with data features. In contrast, many graph-based clustering methods <ref type="bibr" target="#b5">[Schaeffer, 2007]</ref> only leverage graph con-nectivity patterns, e.g., user friendships in social networks, paper citation links in citation networks, and genetic interactions in protein-protein interaction networks. Typically, these methods learn node embeddings using Laplacian eigenmaps <ref type="bibr" target="#b4">[Newman, 2006]</ref>, random walks <ref type="bibr" target="#b4">[Perozzi et al., 2014]</ref>, or autoencoder <ref type="bibr" target="#b8">[Wang et al., 2016a]</ref>. Nevertheless, they usually fall short in attributed graph clustering, as they do not exploit informative node features such as user profiles in social networks, document contents in citation networks, and protein signatures in protein-protein interaction networks.</p><p>In recent years, various attributed graph clustering methods have been proposed, including methods based on generative models <ref type="bibr" target="#b1">[Chang and Blei, 2009</ref>], spectral clustering <ref type="bibr" target="#b11">[Xia et al., 2014]</ref>, random walks <ref type="bibr" target="#b12">[Yang et al., 2015]</ref>, nonnegative matrix factorization <ref type="bibr" target="#b9">[Wang et al., 2016b]</ref>, and graph convolutional networks (GCN) <ref type="bibr" target="#b3">[Kipf and Welling, 2017]</ref>. In particular, GCN based methods such as GAE <ref type="bibr" target="#b2">[Kipf and Welling, 2016]</ref>, MGAE <ref type="bibr" target="#b10">[Wang et al., 2017]</ref>, <ref type="bibr">ARGE [Pan et al., 2018]</ref> have demonstrated state-of-the-art performance on several attributed graph clustering tasks.</p><p>Although graph convolution has been shown very effective in integrating structural and feature information, there is little study of how it should be applied to maximize clustering performance. Most existing methods directly use GCN as a feature extractor, where each convolutional layer is coupled with a projection layer, making it difficult to stack many layers and train a deep model. In fact, <ref type="bibr">ARGE [Pan et al., 2018]</ref> and MGAE <ref type="bibr" target="#b10">[Wang et al., 2017]</ref> use a shallow two-layer and threelayer GCN respectively in their models, which only take into account neighbours of each node in two or three hops away and hence may be inadequate to capture global cluster structures of large graphs. Moreover, all these methods use a fixed model and ignore the diversity of real-world graphs, which can lead to suboptimal performance.</p><p>To address these issues, we propose an adaptive graph convolution (AGC) method for attributed graph clustering. The intuition is that neighbouring nodes tend to be in the same cluster and clustering will become much easier if nodes in the same cluster have similar feature representations. To this end, instead of stacking many layers as in GCN, we design a k-order graph convolution that acts as a low-pass graph filter on node features to obtain smooth feature representations, where k can be adaptively selected using intra-cluster distance. AGC consists of two steps: 1) conducting k-order graph convolution to obtain smooth feature representations; 2) performing spectral clustering on the learned features to cluster the nodes. AGC enables an easy use of high-order graph convolution to capture global cluster structures and allows to select an appropriate k for different graphs. Experimental results on four benchmark datasets including three citation networks and one webpage network show that AGC is highly competitive and in many cases can significantly outperform state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph-based clustering methods can be roughly categorized into two branches: structural graph clustering and attributed graph clustering. Structural graph clustering methods only exploit graph structures (node connectivity). Methods based on graph Laplacian eigenmaps <ref type="bibr" target="#b4">[Newman, 2006]</ref> assume that nodes with higher similarity should be mapped closer. Methods based on matrix factorization <ref type="bibr" target="#b1">[Cao et al., 2015;</ref><ref type="bibr">Nikolentzos et al., 2017]</ref> factorize the node adjacency matrix into node embeddings. Methods based on random walks <ref type="bibr" target="#b4">[Perozzi et al., 2014;</ref><ref type="bibr" target="#b1">Grover and Leskovec, 2016]</ref> learn node embeddings by maximizing the probability of the neighbourhood of each node. Autoencoder based methods <ref type="bibr" target="#b8">[Wang et al., 2016a;</ref> find low-dimensional node embeddings with the node adjacency matrix and then use the embeddings to reconstruct the adjacency matrix.</p><p>Attributed graph clustering <ref type="bibr" target="#b11">[Yang et al., 2009]</ref> takes into account both node connectivity and features. Some methods model the interaction between graph connectivity and node features with generative models <ref type="bibr" target="#b1">[Chang and Blei, 2009;</ref><ref type="bibr" target="#b12">Yang et al., 2013;</ref><ref type="bibr">He et al., 2017;</ref><ref type="bibr">Bojchevski and Günnemann, 2018]</ref>. Some methods apply nonnegative matrix factorization or spectral clustering on both the underlying graph and node features to get a consistent cluster partition <ref type="bibr" target="#b11">[Xia et al., 2014;</ref><ref type="bibr" target="#b9">Wang et al., 2016b;</ref><ref type="bibr" target="#b4">Li et al., 2018;</ref><ref type="bibr" target="#b12">Yang et al., 2015]</ref>. Some most recent methods integrate node relations and features using GCN <ref type="bibr" target="#b3">[Kipf and Welling, 2017]</ref>. In particular, graph autoencoder (GAE) and graph variational autoencoder (VGAE) <ref type="bibr" target="#b2">[Kipf and Welling, 2016]</ref> learn node representations with a two-layer GCN and then reconstruct the node adjacency matrix with autoencoder and variational autoencoder respectively. Marginalized graph autoencoder (MGAE) <ref type="bibr" target="#b10">[Wang et al., 2017]</ref> learns node representations with a threelayer GCN and then applies marginalized denoising autoencoder to reconstruct the given node features. Adversarially regularized graph autoencoder (ARGE) and adversarially regularized variational graph autoencoder (ARVGE) <ref type="bibr" target="#b4">[Pan et al., 2018]</ref> learn node embeddings by GAE and VGAE respectively and then use generative adversarial networks to enforce the node embeddings to match a prior distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Given a non-directed graph G = (V, E, X), where V = {v 1 , v 2 , ..., v n } is a set of nodes with |V| = n, E is a set of edges that can be represented as an adjacency matrix A = {a ij } ∈ R n×n , and X is a feature matrix of all the nodes, i.e., X = [x 1 , x 2 , · · · , x n ] ∈ R n×d , where x i ∈ R d is a realvalued feature vector of node v i . Our goal is to partition the nodes of the graph G into m clusters C = {C 1 , C 2 , · · · , C m }. Note that we call v j a k-hop neighbour of v i , if v j can reach v i by traversing k edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Convolution</head><p>To formally define graph convolution, we first introduce the notions of graph signal and graph filter <ref type="bibr" target="#b5">[Shuman et al., 2013]</ref>. A graph signal can be represented as a vector f = [f (v 1 ), · · · , f (v n )] , where f : V → R is a real-valued function on the nodes of a graph. Given an adjacency matrix A and the degree matrix D = diag(d 1 , · · · , d n ), the symmetrically normalized graph Laplacian</p><formula xml:id="formula_0">L s = I − D − 1 2 AD − 1 2</formula><p>can be eigen-decomposed as L s = U ΛU −1 , where Λ = diag(λ 1 , · · · , λ n ) are the eigenvalues in increasing order, and U = [u 1 , · · · , u n ] are the associated orthogonal eigenvectors. A linear graph filter can be represented as a matrix</p><formula xml:id="formula_1">G = U p(Λ)U −1 ∈ R n×n , where p(Λ) = diag(p(λ 1 ), · · · , p(λ n ))</formula><p>is called the frequency response function of G. Graph convolution is defined as the multiplication of a graph signal f with a graph filter G:f = Gf , (1) wheref is the filtered graph signal.</p><p>Each column of the feature matrix X can be considered as a graph signal. In graph signal processing <ref type="bibr" target="#b5">[Shuman et al., 2013]</ref>, the eigenvalues (λ q ) 1≤q≤n can be taken as frequencies and the associated eigenvectors (u q ) 1≤q≤n are considered as Fourier basis of the graph. A graph signal f can be decomposed into a linear combination of the eigenvectors, i.e.,</p><formula xml:id="formula_2">f = U z = n q=1 z q u q ,<label>(2)</label></formula><p>where z = [z 1 , · · · , z n ] and z q is the coefficient of u q . The magnitude of the coefficient |z q | indicates the strength of the basis signal u q presented in f . A graph signal is smooth if nearby nodes on the graph have similar features representations. The smoothness of a basis signal u q can be measured by Laplacian-Beltrami operator Ω(·) [Chung and Graham, 1997], i.e.,</p><formula xml:id="formula_3">Ω(u q ) = 1 2 (vi,vj )∈E a ij u q (i) √ d i − u q (j) d j 2 2 = u q L s u q = λ q ,<label>(3)</label></formula><p>where u q (i) denotes the i-th element of the vector u q . <ref type="formula" target="#formula_3">(3)</ref> indicates that the basis signals associated with lower frequencies (smaller eigenvalues) are smoother, which means that a smooth graph signal f should contain more low-frequency basis signals than high-frequency ones. This can be achieved by performing graph convolution with a low-pass graph filter G, as shown below. By (2), the graph convolution can be written as</p><formula xml:id="formula_4">f = Gf = U p(Λ)U −1 · U z = n q=1 p(λ q )z q u q . (4)</formula><p>In the filtered signalf , the coefficient z q of the basis signal u q is scaled by p(λ q ). To preserve the low-frequency basis signals and remove the high-frequency ones in f , the graph filter G should be low-pass, i.e., the frequency response function p(·) should be decreasing and nonnegative. A low-pass graph filter can take on many forms. Here, we design a low-pass graph filter with the frequency response function</p><formula xml:id="formula_5">p(λ q ) = 1 − 1 2 λ q .<label>(5)</label></formula><p>As shown by the red line in <ref type="figure" target="#fig_0">Figure 1</ref>(a), one can see that p(·) in <ref type="formula" target="#formula_5">(5)</ref> is decreasing and nonnegative on [0, 2]. Note that all the eigenvalues λ q of the symmetrically normalized graph Laplacian L s fall into interval [0, 2] [Chung and <ref type="bibr" target="#b1">Graham, 1997]</ref>, which indicates that p(·) in <ref type="formula" target="#formula_5">(5)</ref> is low-pass. The graph filter G with p(·) in (5) as the frequency response function can then be written as</p><formula xml:id="formula_6">G = U p(Λ)U −1 = U (I − 1 2 Λ)U −1 = I − 1 2 L s .<label>(6)</label></formula><p>By performing graph convolution on the feature matrix X, we obtain the filtered feature matrix:</p><formula xml:id="formula_7">X = GX,<label>(7)</label></formula><p>whereX = [x 1 ,x 2 , · · · ,x n ] ∈ R n×d is the filtered node features after graph convolution. Applying such a low-pass graph filter on the feature matrix makes adjacent nodes have similar feature values along each dimension, i.e., the graph signals are smooth. Based on the cluster assumption that nearby nodes are likely to be in the same cluster, performing graph convolution with a low-pass graph filter will make the downstream clustering task easier. Note that the proposed graph filter in <ref type="formula" target="#formula_6">(6)</ref> is different from the graph filter used in GCN. The graph filter in GCN is G = I − L s with the frequency response function p(λ q ) = 1 − λ q <ref type="bibr" target="#b4">[Li et al., 2019]</ref>, which is clearly not low-pass as it is negative for λ q ∈ (1, 2] .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>k-Order Graph Convolution</head><p>To make clustering easy, it is desired that nodes of the same class should have similar feature representations after graph filtering. However, the first-order graph convolution in <ref type="formula" target="#formula_7">(7)</ref> may not be adequate to achieve this, especially for large and sparse graphs, as it updates each node v i by the aggregation of its 1-hop neighbours only, without considering long-distance neighbourhood relations. To capture global graph structures and facilitate clustering, we propose to use k-order graph convolution.</p><p>We define k-order graph convolution as</p><formula xml:id="formula_8">X = (I − 1 2 L s ) k X,<label>(8)</label></formula><p>where k is a positive integer, and the corresponding graph filter is</p><formula xml:id="formula_9">G = (I − 1 2 L s ) k = U (I − 1 2 Λ) k U −1 .<label>(9)</label></formula><p>The frequency response function of G in <ref type="formula" target="#formula_9">(9)</ref> is</p><formula xml:id="formula_10">p(λ q ) = (1 − 1 2 λ q ) k .<label>(10)</label></formula><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a), p(λ q ) in (10) becomes more lowpass as k increases, indicating that the filtered node features X will be smoother. The iterative calculation formula of k-order graph convolution is</p><formula xml:id="formula_11">x (0) i = x i ,x (1) i = 1 2  x (0) i + (vi,vj )∈E a ij d i d jx (0) j   , · · · , x (k) i = 1 2  x (k−1) i + (vi,vj )∈E a ij d i d jx (k−1) j   ,<label>(11)</label></formula><p>and the finalx i isx (k) i . From <ref type="formula" target="#formula_11">(11)</ref>, one can easily see that k-order graph convolution updates the features of each node v i by aggregating the features of its k-hop neighbours iteratively. As k-order graph convolution takes into account long-distance data relations, it can be useful for capturing global graph structures to improve clustering performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Analysis</head><p>As k increases, k-order graph convolution will make the node features smoother on each dimension. In the following, we prove this using the Laplacian-Beltrami operator Ω(·) defined in (3). Denote by f a column of the feature matrix X, which can be decomposed as f = U z. Note that Ω(βf ) = β 2 Ω(f ), where β is a scalar. Therefore, to compare the smoothness of different graph signals, we need to put them on a common scale. In what follows, we consider the smoothness of a normalized signal f f 2 , i.e.</p><p>,</p><formula xml:id="formula_12">Ω f f 2 = f L s f f 2 2 = z Λz z 2 2 = n i=1 λ i z 2 i n i=1 z 2 i .<label>(12)</label></formula><p>Theorem 1. If the frequency response function p(λ) of a graph filter G is nonincreasing and nonnegative for all λ i , then for any signal f and the filtered signalf = Gf , we always have</p><formula xml:id="formula_13">Ω f f 2 ≤ Ω f f 2 .</formula><p>Proof. We first prove the following lemma by induction. The following inequality</p><formula xml:id="formula_14">T (n) c = n i=1 c i T i n i=1 c i ≤ n i=1 b i T i n i=1 b i = T (n) b<label>(13)</label></formula><p>holds, if T 1 ≤ · · · ≤ T n and c1 b1 ≥ · · · ≥ cn bn with ∀c i , b i ≥ 0. It is easy to validate that it holds when n = 2. Now assume that it holds when n = l − 1, i.e., T</p><formula xml:id="formula_15">(l−1) c ≤ T (l−1) b</formula><p>. Then, consider the case of n = l and we have</p><formula xml:id="formula_16">l i=1 c i T i l i=1 c i = l−1 i=1 c i T i + c l T l l−1 i=1 c i + c l = ( l−1 i=1 c i )T (l−1) c + c l T l l−1 i=1 c i + c l ≤ ( l−1 i=1 c i )T (l−1) b + c l T l l−1 i=1 c i + c l .<label>(14)</label></formula><p>Since</p><formula xml:id="formula_17">T (l−1) b = l−1 i=1 biTi l−1 i=1 bi ≤ l−1 i=1 biT l−1 l−1 i=1 bi = T l−1 , we have T (l−1) b ≤ T l . Also note that l−1 i=1 ci l−1 i=1 bi ≥ c l b l .</formula><p>Since the lemma holds when n = 2, we have</p><formula xml:id="formula_18">( l−1 i=1 c i )T (l−1) b + c l T l l−1 i=1 c i + c l ≤ ( l−1 i=1 b i )T (l−1) b + b l T l l−1 i=1 b i + b l = l i=1 b i T i l i=1 b i ,<label>(15)</label></formula><p>which shows that the inequality (13) also holds when n = l. By induction, the above lemma holds for all n.</p><p>We can now prove Theorem 1 using this lemma. For convenience, we arrange the eigenvalues λ i of L s in increasing order such that 0 ≤ λ 1 ≤ · · · ≤ λ n . Since p(λ) is nonincreasing and nonnegative, p(λ 1 ) ≥ · · · ≥ p(λ n ) ≥ 0. Theorem 1 can then be proved with the above lemma by letting</p><formula xml:id="formula_19">T i = λ i , b i = z 2 i , c i = p 2 (λ i )z 2 i .<label>(16)</label></formula><p>Assuming that f andf are obtained by (k − 1)-order and k-order graph convolution respectively, one can immediately infer from Theorem 1 thatf is smoother than f . In other words, k-order graph convolution will produce smoother features as k increases. Since nodes in the same cluster tend to be densely connected, they are likely to have more similar feature representations with large k, which can be beneficial for clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Clustering via Adaptive Graph Convolution</head><p>We perform the classical spectral clustering method <ref type="bibr" target="#b4">[Perona and Freeman, 1998;</ref><ref type="bibr" target="#b7">Von Luxburg, 2007]</ref> on the filtered feature matrixX to partition the nodes of V into m clusters, similar to <ref type="bibr" target="#b10">[Wang et al., 2017]</ref>. Specifically, we first apply the linear kernel K =XX T to learn pairwise similarity between nodes, and then we calculate W = 1 2 (|K| + |K |) to make sure that the similarity matrix is symmetric and nonnegative, where | · | means taking absolute value of each element of the matrix. Finally, we perform spectral clustering on W to obtain clustering results by computing the eigenvectors associated with the m largest eigenvalues of W and then applying the k-means algorithm on the eigenvectors to obtain cluster partitions.</p><p>The central issue of k-order graph convolution is how to select an appropriate k. Although k-order graph convolution can make nearby nodes have similar feature representations, k is definitely not the larger the better. k being too large will lead to over-smoothing, i.e., the features of nodes in different clusters are mixed and become indistinguishable.  <ref type="bibr" target="#b6">Van der Maaten and Hinton, 2008]</ref> and nodes of the same class are indicated by the same colour. It can be seen that the node features become similar as k increases. The data exhibits clear cluster structures with k = 12. However, with k = 100, the features are over-smoothed and nodes from different clusters are mixed together.</p><p>To adaptively select the order k, we use the clustering performance metric -internal criteria based on the information intrinsic to the data alone [Aggarwal and <ref type="bibr" target="#b0">Reddy, 2014]</ref>. Here, we consider intra-cluster distance (intra(C) for a given cluster partition C), which represents the compactness of C:</p><formula xml:id="formula_20">intra(C) = 1 |C| C∈C 1 |C|(|C| − 1) vi,vj ∈C, vi =vj x i −x j 2 . (17)</formula><p>Note that inter-cluster distance can also be used to measure clustering performance given fixed data features, and a good cluster partition should have a large inter-cluster distance and a small intra-cluster distance. However, by Theorem 1, the node features become smoother as k increases, which could significantly reduce both intra-cluster and inter-cluster distances. Hence, inter-cluster distance may not be a reliable metric for measuring the clustering performance w.r.t. different k, and so we propose to observe the variation of intracluster distance for choosing k.</p><p>Our strategy is to find the first local minimum of intra(C) w.r.t. k. Specifically, we start from k = 1 and increment it by 1 iteratively. In each iteration t, we first obtain the cluster partition C (t) by performing k-order (k = t) graph convolution and spectral clustering, then we compute intra(C (t) ).</p><p>Once intra(C (t) ) is larger than intra(C (t−1) ), we stop the iteration and set the chosen k = t − 1. More formally, consider d intra(t − 1) = intra(C (t) ) − intra(C (t−1) ), the criterion for stopping the iteration is d intra(t − 1) &gt; 0, i.e., stops at the first local minimum of intra(C (t) ). So, the final choice of Algorithm 1 AGC Input: Node set V, adjacency matrix A, feature matrix X, and maximum iteration number max iter. Output: Cluster partition C.</p><p>1: Initialize t = 0 and intra(C (0) ) = +∞. Compute the symmetrically normalized graph Laplacian Ls = I − D − 1 2 AD − 1 2 , where D is the degree matrix of A. 2: repeat 3:</p><p>Set t = t + 1 and k = t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Perform k-order graph convolution by Eq. (8) and getX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Apply the linear kernel K =XX T , and calculate the similarity matrix W = 1 2 (|K| + |K T |).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Obtain the cluster partition C (t) by performing spectral clustering on W . 7:</p><p>Compute intra(C (t) ) by Eq. (17). 8: until d intra(t − 1) &gt; 0 or t &gt; max iter 9: Set k = t − 1 and C = C (t−1) . cluster partition is C (t−1) . The benefits of this selection strategy are two-fold. First, it ensures finding a local minimum for intra(C) that may indicate a good cluster partition and avoids over-smoothing. Second, it is time efficient to stop at the first local minimum of intra(C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Algorithm Procedure and Time Complexity</head><p>The overall procedure of AGC is shown in Algorithm 1. Denote by n the number of nodes, d the number of attributes, m the number of clusters, and N the number of nonzero entries of the adjacency matrix A. Note that for a sparse A, N &lt;&lt; n 2 . The time complexity of computing L s in the initialization is O(N ). In each iteration, for a sparse L s , the fastest way of computing k-order graph convolution in (8) is to left multiply X by I − 1 2 L s repeatedly for k times, which has the time complexity O(N dk). The time complexity of performing spectral clustering onX in each iteration is O(n 2 d + n 2 m). The time complexity of computing intra(C) in each iteration is O( 1 m n 2 d). Since m is usually much smaller than n and d, the time complexity of each iteration is approximately O(n 2 d+N dk). If Algorithm 1 iterates t times, the overall time complexity of AGC is O(n 2 dt + N dt 2 ). Unlike existing GCN based clustering methods, AGC does not need to train the neural network parameters, which makes it time efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our method AGC on four benchmark attributed networks. Cora, Citeseer and Pubmed <ref type="bibr" target="#b2">[Kipf and Welling, 2016]</ref> are citation networks where nodes correspond to publications and are connected if one cites the other. Wiki <ref type="bibr" target="#b12">[Yang et al., 2015]</ref> is a webpage network where nodes are webpages and are connected if one links the other. The nodes in Cora and Citeseer are associated with binary word vectors, and the nodes in Pubmed and Wiki are associated with tf-idf weighted word vectors. <ref type="table" target="#tab_0">Table 1 summarizes the details of the datasets.   Dataset  #Nodes #Edges #Features #Classes   Cora  2708  5429  1433  7  Citeseer  3327  4732  3703  6  Pubmed  19717  44338  500  3  Wiki  2405  17981  4973  17   Table 1</ref>: Dataset statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and Evaluation Metrics</head><p>We compare AGC with three kinds of clustering methods. 1) Methods that only use node features: k-means and spectral clustering (Spectral-f) that constructs a similarity matrix with the node features by linear kernel. 2) Structural clustering methods that only use graph structures: spectral clustering (Spectral-g) that takes the node adjacency matrix as the similarity matrix, DeepWalk <ref type="bibr" target="#b4">[Perozzi et al., 2014]</ref>, and deep neural networks for graph representations (DNGR) . 3) Attributed graph clustering methods that utilize both node features and graph structures: graph autoencoder (GAE) and graph variational autoencoder (VGAE) <ref type="bibr" target="#b2">[Kipf and Welling, 2016]</ref>, marginalized graph autoencoder (MGAE) <ref type="bibr" target="#b10">[Wang et al., 2017]</ref>, and adversarially regularized graph autoencoder (ARGE) and variational graph autoencoder (ARVGE) <ref type="bibr" target="#b4">[Pan et al., 2018]</ref>.</p><p>To evaluate the clustering performance, we adopt three widely used performance measures [Aggarwal and <ref type="bibr" target="#b0">Reddy, 2014]</ref>: clustering accuracy (Acc), normalized mutual information (NMI), and macro F1-score (F1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Settings</head><p>For AGC, we set max iter = 60. For other baselines, we follow the parameter settings in the original papers. In particular, for DeepWalk, the number of random walks is 10, the number of latent dimensions for each node is 128, and the path length of each random walk is 80. For DNGR, the autoencoder is of three layers with 512 neurons and 256 neurons in the hidden layers respectively. For GAE and VGAE, we construct encoders with a 32-neuron hidden layer and a 16-neuron embedding layer, and train the encoders for 200 iterations using the Adam optimizer with learning rate 0.01. For MGAE, the corruption level p is 0.4, the number of layers is 3, and the parameter λ is 10 −5 . For ARGE and ARVGE, we construct encoders with a 32-neuron hidden layer and a 16-neuron embedding layer. The discriminators are built by two hidden layers with 16 neurons and 64 neurons respectively. On Cora, Citeseer and Wiki, we train the autoencoderrelated models of ARGE and ARVGE for 200 iterations with the Adam optimizer, with the encoder learning rate and the discriminator learning rate both as 0.001; on Pubmed, we train them for 2000 iterations with the encoder learning rate 0.001 and the discriminator learning rate 0.008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Result Analysis</head><p>We run each method 10 times for each dataset and report the average clustering results in   a very large margin, due to the clear reason that AGC makes a better use of available data by integrating both kinds of information, which can complement each other and greatly improve clustering performance.</p><p>2) AGC consistently outperforms existing attributed graph clustering methods that use both node features and graph structures. This is because AGC can better utilize graph information than these methods. In particular, GAE, VGAE, ARGE and ARVGE only exploit 2-hop neighbourhood of each node to aggregate information, and MGAE only exploits 3-hop neighbourhood. In contrast, AGC uses k-order graph convolution with an automatically selected k to aggregate information within k-hop neighbourhood to produce better feature representations for clustering.</p><p>3) AGC outperforms the strongest baseline MGAE by a considerable margin on Cora, Citeseer and Pubmed, and is comparable to MGAE on Wiki. This is probably because Wiki is more densely connected than others and aggregating information within 3-hop neighbourhood may be enough for feature smoothing. But it is not good enough for larger and sparser networks such as Citeseer and Pubmed, on which the performance gaps between AGC and MGAE are wider. AGC deals with the diversity of networks well via adaptively selecting a good k for different networks.</p><p>To demonstrate the validity of the proposed selection crite-rion d intra(t − 1) &gt; 0, we plot d intra(k) and the clustering performance w.r.t. k on Cora and Wiki respectively in <ref type="figure">Figure</ref> 2. The curves of Citeseer and Pubmed are not plotted due to space limitations. One can see that when d intra(k) &gt; 0, the corresponding Acc, NMI and F1 values are the best or close to the best, and the clustering performance declines afterwards. It shows that the selection criterion can reliably find a good cluster partition and prevent over-smoothing. The selected k for Cora, Citeseer, Pubmed and Wiki are 12, 55, 60, and 8 respectively, which are close to the best k ∈ [0, 60] -12, 35, 60, and 6 on these datasets respectively, demonstrating the effectiveness of the proposed selection criterion. AGC is quite stable. The standard deviations of Acc, NMI and F1 are 0.17%, 0.42%, 0.01% on Cora, 0.24%, 0.36%, 0.19% on Citeseer, 0.00%, 0.00%, 0.00% on Pubmed, and 0.79%, 0.17%, 0.20% on Wiki, all very small.</p><p>The running time of AGC (in Python, with NVIDIA Geforce GTX 1060 6GB GPU) on Cora, Citeseer, Pubmed and Wiki is 5.78, 62.06, 584.87, and 10.75 seconds respectively. AGC is a little slower than GAE, VGAE, ARGE and ARVGE on Citeseer, but is more than twice faster on the other three datasets. This is because AGC does not need to train the neural network parameters as in these methods, and hence is more time efficient even with a relatively large k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a simple and effective method for attributed graph clustering. To make a better use of available data and capture global cluster structures, we design a k-order graph convolution to aggregate long-range data information. To optimize clustering performance on different graphs, we design a strategy for adaptively selecting an appropriate k. This enables our method to achieve competitive performance compared to classical and state-of-the-art methods. In future work, we plan to improve the adaptive selection strategy to make our method more robust and efficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Frequency response functions. (b-e) t-SNE visualization of the raw and filtered node features of Cora with different k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1(b-e) visualizes the raw and filtered node features of the Cora citation network with different k, where the features are projected by t-SNE [</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>d intra(k) and clustering performance w.r.t. k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>, where the top 2 results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Clustering performance.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the grants 1-ZVJJ and G-YBXV funded by the Hong Kong Polytechnic University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aleksandar Bojchevski and Stephan Günnemann. Bayesian robust attributed graph clustering: Joint learning of partial anomalies and group structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>Boca Raton</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Data Clustering: Algorithms and Applications</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
	</analytic>
	<monogr>
		<title level="m">Dongxiao He, Zhiyong Feng, Di Jin, Xiaobao Wang, and Weixiong Zhang. Joint identification of network communities and semantics via integrative modeling of network topologies and node contents</title>
		<imprint>
			<publisher>Grover and Leskovec</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="116" to="124" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Giannis Nikolentzos, Polykarpos Meladianos, and Michalis Vazirgiannis. Matching node embeddings for graph similarity</title>
	</analytic>
	<monogr>
		<title level="m">Online learning of social representations. In KDD</title>
		<imprint>
			<publisher>Perona and Freeman</publisher>
			<date type="published" when="1901" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Satu Elisa</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Computer Science Review</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; L</forename><forename type="middle">J P</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ulrike Von Luxburg. A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic community identification in large attribute networks</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mgae: Marginalized graph autoencoder for graph clustering</title>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining link and content for community detection: a discriminative approach</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="927" to="936" />
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
	<note>ICDM</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep autoencoder-like nonnegative matrix factorization for community detection</title>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1393" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
