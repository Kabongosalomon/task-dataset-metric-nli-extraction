<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Wen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Mitash</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baozhang</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
						</author>
						<title level="a" type="main">se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tracking the 6D pose of objects in video sequences is important for robot manipulation. This task, however, introduces multiple challenges: (i) robot manipulation involves significant occlusions; (ii) data and annotations are troublesome and difficult to collect for 6D poses, which complicates machine learning solutions, and (iii) incremental error drift often accumulates in long term tracking to necessitate re-initialization of the object's pose. This work proposes a data-driven optimization approach for long-term, 6D pose tracking. It aims to identify the optimal relative pose given the current RGB-D observation and a synthetic image conditioned on the previous best estimate and the object's model. The key contribution in this context is a novel neural network architecture, which appropriately disentangles the feature encoding to help reduce domain shift, and an effective 3D orientation representation via Lie Algebra. Consequently, even when the network is trained only with synthetic data can work effectively over real images. Comprehensive experiments over benchmarks -existing ones as well as a new dataset with significant occlusions related to object manipulation -show that the proposed approach achieves consistently robust estimates and outperforms alternatives, even though they have been trained with real images. The approach is also the most computationally efficient among the alternatives and achieves a tracking frequency of 90.9Hz. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Robotic tasks, such as object manipulation, often require to track the pose of an object. Pose estimation from a single snapshot can initiate a manipulation pipeline and has been studied extensively <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b8">[9]</ref>. Purposeful manipulation, however, such as placement and especially within-hand reorientation <ref type="bibr" target="#b9">[10]</ref>, critically depends on online tracking <ref type="bibr" target="#b10">[11]</ref>. Some pose estimation approaches are relatively fast and can re-estimate pose from scratch for every frame <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. This can be redundant, however, and often leads to less coherent estimations over consecutive frames, which negatively impact manipulation.</p><p>Temporal tracking of object poses over sequences of images can greatly improve speed while maintaining or even improving pose quality <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Nevertheless, many traditional methods, which depend on hand-crafted likelihood functions and features, require extensive hyper-parameter tuning when adapt to novel object categories or environments. On the other hand, data-driven techniques <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> require real-world training data, which are difficult to acquire and label in the context of 6D poses.</p><p>Work by the authors has been supported by NSF awards 1723869 and 1734492. The authors are with the Computer Science Department of Rutgers University in Piscataway, New Jersey, 08854, USA. Email: {bw344, cm1074, kb572}@rutgers.edu <ref type="bibr" target="#b0">1</ref> Code, data and supplementary video for this project are available at https://github.com/wenbowen123/iros20-6d-pose-tracking  <ref type="bibr" target="#b0">[1]</ref>. The proposed approach is able to perform more accurate tracking while being significantly faster than alternatives. Bottom: Pose predicted by the se(3)-TrackNet without any re-initialization, which is able to recover from complete occlusion.</p><p>This work proposes a data-driven optimization strategy to keep long-term track of an object's pose robustly, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The contributions are the following: 1. A novel deep neural network that learns to predict the relative pose between the current observation and the synthetic model rendering at the previous prediction. A smart featureencoding disentanglement technique enables more efficient sim-to-real transfer. 2. A Lie Algebra representation of 3D orientations, which allows effective learning of the residual pose transforms given a proper loss function. 3. A training pipeline over synthetic data that employs domain randomization <ref type="bibr" target="#b17">[18]</ref> in the context of pose tracking. Automatic training data generation significantly reduces the manual effort in collecting and labeling videos for tracking 6D poses. 4. A novel benchmarking dataset for 6D pose tracking in the context of multiple different robotics manipulation tasks. It was collected with various robotic end-effectors and YCB objects, where 6D object pose annotations are provided in every frame of the video.</p><p>Experiments indicate that the proposed network achieves state-of-art results on the YCB-Video benchmark without reinitialization in contrast to prior work <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>. It is also significantly faster at 90.9Hz. This allows use in real-time scenarios such as robot manipulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Data-driven 6D Pose Estimation: Learning-based techniques have shown promise in directly regressing the 6D object pose from image data <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Nevertheless, given the complexity of the 6D challenge, a large amount of pose annotated training data is required to achieve satisfactory results in practice. This is often more challenging than labeling for object classification or detection. Some datadriven techniques combine deep learning with traditional approaches like PnP <ref type="bibr" target="#b11">[12]</ref>. Although more data-efficient, this could be problematic under severe occlusions, and often requires precisely calibrated camera parameters for the PnP step. Given that a pose is re-estimated in every frame, estimation techniques often trade-off speed for accuracy <ref type="bibr" target="#b0">[1]</ref>. This might not be desirable for manipulation. In contrast, the current work exploits temporal information to achieve higher accuracy and faster response than state-of-art singleimage pose estimation methods while using only synthetic data for training. 6D Pose Tracking: For setups where CAD object models are available, the approaches can be generally categorized to probabilistic and optimization-based. Probabilistic Tracking: An efficient particle filtering framework <ref type="bibr" target="#b18">[19]</ref> harnesses the computational power of GPU, where likelihood is computed based on color, distance and normals. Nevertheless, the handdesigned likelihood functions can hardly generalize to different lighting conditions or scenes with challenging clutter. An alternative <ref type="bibr" target="#b13">[14]</ref> explicitly models occlusions and shows success in terms of robustness but the pose accuracy is not precise enough for certain manipulation tasks. To ameliorate this problem, follow up work <ref type="bibr" target="#b4">[5]</ref> applied Gaussian Filtering and achieves promising pose accuracy but introduces more frequent tracking loss. Recent work <ref type="bibr" target="#b1">[2]</ref> proposed a Rao-Blackwellized particle filter, which decouples the translational and rotational uncertainty, achieving state-of-art 6D pose tracking performance on the YCB Video benchmark. In the case of severe occlusions, however, re-initialization of the pose estimation is required. Optimization-based Tracking: A number of methods proposed objective functions, which capture the discrepancy between the current observation and the previous state. They compute relative transformations based on the minima of the residual function <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref>. In particular, methods combine the optical/AR flow and point-to-plane distance to solve tracking in a least-squares sense <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>. SIFT features and optical flow noise, however, limit performance and often require extensive hyperparameter tuning to adapt to new scenarios. The most related effort to the current paper leverages the FlowNetSimple network <ref type="bibr" target="#b23">[24]</ref> to refine pose outputs of any 6D object pose detection approach and can also be extended to tracking <ref type="bibr" target="#b15">[16]</ref>. It requires, however, occasional re-initialization and has to be trained at least partially with real data. Simulation to Reality: Training on synthetically generated datasets allows faster, more scalable, and lower-cost data collection <ref type="bibr" target="#b17">[18]</ref>. The discrepancy between the synthetic and real data, however, often results in significant performance drop. Domain adaptation techniques like Gradient-reversal <ref type="bibr" target="#b24">[25]</ref> and utilizing Generative Adversarial Networks (GANs) for input space domain alignment <ref type="bibr" target="#b25">[26]</ref> help bridge this gap. These methods, however, often assume the source domain already resembles the target domain to certain extent, which could not be trivially satisfied in practice. Advancements in computer graphics, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> have shown the benefits of performing photo-realistic renderings on tasks. Achieving such photo-realism, however, often introduces another source of human involvement and expert domain knowledge. Domain randomization efforts impose that the rendering settings of the simulator are randomized and certain transferability to real world has been demonstrated <ref type="bibr" target="#b17">[18]</ref>. This work inherits the idea of domain randomization but also pursues physical plausibility. Together with feature encoding disentanglement, the proposed network can be trained exclusively over synthetic data and is shown to generalize to the real world. III. APPROACH The objective of this work is to compute the 6D pose of an object T t ∈ SE(3) at any time t &gt; 0, given as input:</p><p>• a 3D CAD model of the object, • its initial pose, T 0 ∈ SE(3), computed by any singleimage based 6D pose estimation technique, and a • sequence of RGB-D images O τ , τ ∈ {0, 1, ...,t −1} from previous time stamps and the current observation O t . This work proposes a data-driven optimization technique to track the object pose over RGB-D image sequences. The cost function for the optimization is encoded and learned by a novel neural network architecture, trained with only synthetically generated data. In every time step, the proposed approach, computes a residual over the pose computed for the object model in the previous frame as indicated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The details of this formulation, the neural network architecture and the data generation pipeline are provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tracking on SE(3) Manifolds with Residuals</head><p>Optimization in this domain operates over cost functions defined for the object posesξ , ξ that measure the discrepancy  ε between the features extracted from the images:</p><formula xml:id="formula_0">ε = ρ(φ I 1 (ξ ) − φ I 2 (ξ )),</formula><p>where ρ is a predefined robust loss function, and φ (·) can be direct pixel intensity values, such as in <ref type="bibr" target="#b28">[29]</ref>, point-to-point discrepancy or its variations <ref type="bibr" target="#b29">[30]</ref>, pre-designed features <ref type="bibr" target="#b30">[31]</ref> or the combinations from any of the above <ref type="bibr" target="#b19">[20]</ref>. Given the current observation O t , and the pose computed in the previous timestamp ξ t−1 , the goal of this work is to find a relative transformation ∆ξ that takes the object from ξ t−1 to the pose captured by the current observation. This can be formulated as an optimization problem. Let R denote the image corresponding to the object model's rendering at the given pose. Then, the optimal relative transform is:</p><formula xml:id="formula_1">∆ξ * = argmin ∆ξ {ρ(φ O t (ξ t ) − φ R (ξ t−1 ∆ξ ))}</formula><p>A general approach to solve this is to perform Taylor expansion around ξ , which reformulates the equation as <ref type="formula">(3)</ref>, such that its exponential mapping lies in the Lie Group ∆T =</p><formula xml:id="formula_2">φ R (ξ t−1 ∆ξ ) = φ R (ξ t−1 ) + J(ξ t−1 )∆ξ , where J is the Ja- cobian matrix of φ R w.r.t. ξ . Now ξ is locally parametrized in its tangent space, specifically ξ = (t, w) T ∈ se</formula><formula xml:id="formula_3">exp(∆ξ ) = R t 0 1 ∈ SE(3), where R = I 3×3 + [w] × |w| sin(|w|)+ [w] 2 × |w| 2 (1 − cos(|w|)) and [w] × is the skew-symmetric matrix.</formula><p>In the case of L 2 loss function without loss of generality:</p><formula xml:id="formula_4">∆ξ = (J T J) −1 J T ||(φ O t (ξ t ) − φ R (ξ t−1 ∆ξ ))||.</formula><p>Solving the expression by explicitly deriving the Jacobian matrix and iteratively updating often requires a formalized cost function or the features extracted from the observations to be differentiable w.r.t. ξ , and appropriate choice of a robust cost function and hand-crafted features. Another problem arises when different modalities are involved. In such cases, another hyper-parameter controlling the importance of each modality (e.g., RGB-D) has to be introduced <ref type="bibr" target="#b19">[20]</ref> and could become non-trivial to tune for all different scenarios.</p><p>Instead, this work proposes a novel neural network architecture that implicitly learns to calibrate the residual between the features extracted from the current observation and the rendered image conditioned on previous pose estimate to resolve the relative transform in the tangent space ∆ξ ∈ se(3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Network Design</head><p>The proposed neural network is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The network takes as input a pair of images, I t−1 : rendered from the previous pose estimation, and I t : the current observation. The images are 4-channel RGB-D data. Depth is often available in robotics. Nevertheless, it complicates learning due to the additional domain-gap between synthetic and real depth images. In addition, not all neural network architectures are well suited to encode RGBD features, such as the FlowNetSimple architecture <ref type="bibr" target="#b23">[24]</ref> used in <ref type="bibr" target="#b15">[16]</ref>.</p><p>During training, both inputs are synthetically generated images φ (I t−1 syn,train ; I t syn,train ), while for testing the current timestamp input comes from a real sensor, φ (I t−1 syn,test ; I t real,test ). The se(3)-TrackNet uses two separate input branches for I t−1 and I t . The weights of the feature encoders are not shared so as to disentangle feature encoding. This is different from related work <ref type="bibr" target="#b15">[16]</ref>, where the two images are concatenated into a single input. A shared feature extractor worked in the previous work when both real and synthetic data are available during training. The property of the latent space φ (I A relative transform can be predicted by the network via end-to-end training. The transformation is represented by Lie algebra as ∆ξ = (t, w) T ∈ se <ref type="formula">(3)</ref>, where the prediction of w and t are disentangled into separate branches and trained by L 2 loss:</p><formula xml:id="formula_5">L = λ 1 ||w −w|| 2 + λ 2 ||t −t|| 2</formula><p>where λ 1 and λ 2 has been simply set to 1 in experiments.</p><p>Given ∆ξ , the current pose estimate is computed as T t = exp(∆ξ ) · T t−1 , as described in Sec. III-A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Synthetic Data Generation via PPDR</head><p>The purpose of domain randomization is to provide enough simulated variability at training time, such that at test time, the model is able to generalize to real-world data <ref type="bibr" target="#b17">[18]</ref>. Prior work implements the idea of domain randomization by randomly changing the number of objects, poses, textures, lighting, etc, where object poses are usually directly sampled from some predetermined distribution <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Although it is non-trivial to align some complex physical properties between the simulator and real world, such as lighting and camera properties, certain physical properties, for instance gravity and collision can be effortlessly preserved <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. This makes domain invariant features more tractable to be captured by the neural network. It is especially important in the current framework when depth modality is additionally employed and unrealistic object penetration which never occurs in real world introduces undesired bias to depth data distribution during learning.</p><p>This work therefore leverages the complementary attributes of domain randomization and physically-consistent simulation for the synthetic data generation process. The goal is to combine the two ideas such that the synthetic training data holds a diverse distribution for the network to generalize to the target domain with different environments, while being more data-efficient. We refer to this idea as PPDR (Physically Plausible Domain Randomization). More concretely, object poses are initialized randomly where collision between objects or distractors could occur, which is then followed by a number of physics simulation steps so that objects are separated or fall onto the table without collision. Other complex or intractable physical properties such as lighting, number of objects, distractor textures are randomized. A comparison between Domain Randomization against PPDR is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>Once the synthetic image is generated for the entire scene, paired data I t−1 syn,train and I t syn,train are extracted and utilized as the input to the network. I t syn,train is obtained by cropping the image given the target object's dimension and zoomed into a fixed resolution 176 × 176 before feeding into the network -similar to prior work <ref type="bibr" target="#b15">[16]</ref>. I t−1 syn,train is obtained by randomly sampling a perturbated pose T t t−1 where its translation's direction is uniformly sampled and its norm follows a Gaussian distribution |t| ∼ |N(0, σ t )| . The rotation is locally parameterized in the tangent space w ∈ so(3) as discussed above and the direction of w is also uniformly sampled, while its norm is sampled from a Gaussian distribution |w| ∼ |N(0, σ w )|, w ∈ R 3 , similar to t.</p><p>The next step within this context is to bridge the domain gap of depth data via bidirectional alignment. Similar to the case of RGB, sim-to-real gap also arise in terms of the depth data, especially for those captured by a commercial-level depth sensor. However, there has been less evidence about how the depth domain gap could be resolved in a general way, especially that it could be partly dependent on the specific depth sensor. In this work, a bidirectional alignment is performed between the synthetic depth data during training time and real depth data during test time. Specifically, during training time, two additional data augmentation steps are applied to the synthetic depth data D t syn,train at branch B. First, random Gaussian noise is added to the pixels with a valid depth value, which is then followed by a depthmissing procedure by randomly changing part of pixels with valid depth into invalid so as to resemble a real corrupted depth image captured by commercial-level depth sensors. In contrast, during test time, a bilateral filtering is carried out on the real depth image so as to smooth sensor noise and fill holes to be aligned with the synthetic domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>This section evaluates the proposed approach and compares against state-of-the-art 6D pose tracking methods as well as single-image pose estimation methods on a public benchmark. It also introduces a new benchmark developed as part of this work, which corresponds to robot manipulation scenarios. Extensive experiments are performed over diverse object categories and various robotics manipulation scenarios (moving camera or moving objects). Both quantitative and qualitative results demonstrate the advantages of the proposed method in terms of accuracy and speed, while using only synthetic training data. Except for training, all experiments are conducted on a standard desktop with Intel Xeon(R) E5-1660 v3@3.00GHz processor. Neural network training and inference are performed on a NVIDIA RTX 2080 Ti GPU and NVIDIA Tesla K40c GPU respectively.</p><p>The synthetic data generation pipeline is implemented in Blender 2 . To render images, the camera's pose is randomly sampled from a sphere of radius between 0.6 to 1.3 m, followed by an additional rotation along camera z-axis sampled between 0 to 360 • . The number of external lighting sources (lamps) is sampled within 0 to 2 with varying poses. The strength and color of the environment and the lights are randomized. Object poses are randomly initialized, followed by physics simulation, which is terminated after 50 steps to ensure objects have been separated without collisions or fallen onto the table. For YCB-Video, table textures are randomly selected from <ref type="bibr" target="#b33">[34]</ref>. For YCBInEOAT, tables are removed and background is replaced by images captured in the same location, where the data were collected. For each pair I t−1 syn,train and I t syn,train , their relative transformation T t t−1 is sampled following a Gaussian distribution as described  <ref type="bibr" target="#b34">[35]</ref> arranged on table-tops. Objects' groundtruth 6D poses are annotated in every frame. The various properties of different objects exhibit challenges to both RGB and depth modalities. The evaluation closely follows the protocols adopted in comparison methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref> and reports the AUC (Area Under Curve) results on the keyframes in 12 video test sequences evaluated by the metrics of ADD = 1 m ∑ x∈M ||Rx + T − (Rx + T )|| which performs exact model matching, and ADD-</p><formula xml:id="formula_6">S = 1 m ∑ x 1 ∈M min x 2 ∈M ||Rx 1 + T − (Rx 2 +T )|| [1]</formula><p>designed for evaluating symmetric objects, of which the matching between points can be ambiguous for some views.</p><p>Although this dataset contains pose annotated training and validation data collected in real world, the proposed approach does not use any of them but is trained only on synthetic data generated by aforementioned pipeline. YCBInEOAT Dataset There have been several public benchmarks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[19]</ref> where videos are collected by placing the objects statically on a table-top while a camera is moved around to imitate a 6D object pose tracking scenario. This can be limiting for evaluating 6D pose tracking since in a static environment, the entire image can be leveraged to solve for the trajectory of the camera, from which object pose can be inferred <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Additionally, in such scenarios, extreme object rotations, such as out-of-image-plane flipping are less likely to happen than a free moving object in front of the camera. Thus, exclusive evaluation on such datasets cannot entirely reflect the attributes of a 6D object pose tracking approach. Other datasets <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b37">[38]</ref> collected video sequences where objects are manipulated by a human hand. Nevertheless, human arm and hand motions can greatly vary from those of robots.</p><p>Therefore, in this work a novel dataset, referred to as "YCBInEOAT Dataset", is developed in the context of robotic manipulation, where various robot end-effectors are included: a vacuum gripper, a Robotiq 2F-85 gripper, and a Yale T42 Hand <ref type="bibr" target="#b38">[39]</ref>. The manipulation sequences consider 5 YCB objects, given that they are widely accessible. The data collection setup and objects are shown in <ref type="figure">Fig. 5</ref>. Each video sequence is collected from a real manipulation performed with a dual-arm Yaskawa Motoman SDA10f. In general, there vaccum gripper Robotiq 2F-85 Yale T42 Azure Kinect <ref type="figure">Fig. 5</ref>. Left: The dataset collection setup, where manipulation task is performed on a Yaskawa Motoman SDA10f. Right: Different end-effector modalities and YCB objects that have been used for manipulation. are 3 types of manipulation tasks performed: (1) single arm pick-and-place, (2) within-hand manipulation, and (3) pick to hand-off between arms to placement. RGB-D images are captured by an Azure Kinect sensor mounted statically on the robot with a frequency of 20 to 30 Hz. Similar to the YCB-Video, ADD and ADD-S metrics are adopted for evaluation. Ground-truth 6D object poses in camera's frame have been accurately annotated manually for each frame of the video. The extrinsic parameters of the camera in the frame of the robot has been obtained by a calibration procedure. This dataset is available for future benchmarking. 3 <ref type="table" target="#tab_5">Table I</ref> and <ref type="figure" target="#fig_0">Fig. 1</ref> present the evaluation over the YCB-Video dataset. The proposed approach is compared with other state-of-art 6D object pose detection approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref> and 6D pose tracking approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, where publicly available source code 4 is used to evaluate <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>, while other results are adopted from the respective publications. All the compared tracking methods except PoseRBPF are using ground-truth pose for initialization. PoseRBPF <ref type="bibr" target="#b1">[2]</ref> is the only one that is initialized using predicted poses from PoseCNN <ref type="bibr" target="#b0">[1]</ref>. For fairness, two additional experiments using the same initial pose as PoseRBPF 5 are performed and presented in the rightmost two columns of <ref type="table" target="#tab_5">Table I</ref>, one is without any re-initialization, and the other allows re-initialization by PoseCNN twice (same as in PoseRBPF) after heavy occlusions. The prior work <ref type="bibr" target="#b15">[16]</ref> was originally proposed to refine the pose output from any 6D pose estimation detection method, but also extends to RGB-based tracking. It has to be re-initialized by PoseCNN when the last 10 frames have an average rotation greater than 10 degrees or an average translation greater than 1 cm, which happens every 340 frames on average as reported <ref type="bibr" target="#b15">[16]</ref>. The initial pose is from ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on YCB-Video</head><p>In practice, re-initialization can be quite expensive in robotics applications given the slower running speed of 6D pose detection approaches, which can interrupt and adversely affect other components of the system, such as planning and control. It can also introduce new source of error. In contrast, DOPE <ref type="bibr" target="#b11">[12]</ref> DenseFusion <ref type="bibr" target="#b6">[7]</ref> PoseCNN+ICP+DeepIM <ref type="bibr" target="#b15">[16]</ref> DeepIM tracking <ref type="bibr" target="#b15">[16]</ref> RGF <ref type="bibr" target="#b4">[5]</ref> Wthrich's <ref type="bibr" target="#b13">[14]</ref> se <ref type="formula">(</ref>  <ref type="table">Objects  ADD  ADD-S  ADD  ADD-S  ADD  ADD-S  ADD  ADD-S  ADD  ADD-S  ADD  ADD-S  ADD  ADD-S  ADD  ADD-S  ADD  ADD-S  ADD  ADD-S  002 master chef</ref>   the proposed network performs long-term, accurate tracking with less frequent or even no re-initialization while trained using only on synthetic data. Additionally, the proposed method generalizes to different lighting conditions and a variety of objects with different properties, such as scissors, and clamps, which are challenging to alternatives. Another important aspect is that the proposed approach is able to achieve 93.05% on ADD metric, outperforming all comparison methods by a large margin. This can be attributed to its implicitly learnt residual estimator, which not only captures the discrepancy of geometry but also semantic textures by considering both the RGB and Depth modalities. <ref type="table" target="#tab_5">Table II</ref> shows the quantitative results evaluated by the area under the curve for ADD and ADD-S on the developed YCBInEOAT dataset. On this benchmark, the tracking approaches with publicly available source code could be directly evaluated <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Pose is initialized with ground-truth in the first frame and no re-initialization is allowed. Forward kinematics are not leveraged in order to solely evaluate tracking quality. Example qualitative results are demonstrated by <ref type="figure">Fig. 6</ref> where a vacuum gripper is performing a pick-and-place manipulation task. Abrupt motions, extreme rotations and slippage within the end-effector are introduced, which are challenging for 6D object pose tracking. Nevertheless, the proposed approach is sufficiently robust to provide long-term reliable pose estimation until the end of the manipulation. An ablation study investigates the importance of different modules of the proposed approach. It is performed for the large clamp object from the YCB-Video dataset and is presented in the accompanying table. The initial pose is given by ground-truth and no re-initialization is allowed. No physics implies that domain randomization is employed during synthetic data generation without any physics simulation. For No depth, the depth modality is removed in both training and inference stage to study its importance. Shared encoder means the two feature encoders for I t and I t−1 share the same architecture and weights. This corresponds to the one used for I t−1 in the original setup. Quaternion implements the rotation via a quaternion representation q = (x, y, z, w), where w = 1 − x 2 − y 2 − z 2 is forced to be non-negative so as to avoid the ambiguity of q and −q. The network is trained by L 2 loss over the representation. Shape-Match Loss is a popular loss function in 6D pose estimation task that does not require the specification of symmetries <ref type="bibr" target="#b0">[1]</ref>. It loses track, however, of the object very early in the current setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on YCBInEOAT-Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This work presents a framework for efficient and robust long-term 6D object pose tracking. A novel neural network architecture se(3)-TrackNet is proposed that allows training on synthetic datasets that transfers robustly to real world data. A combination of design choices for the network and the Lie Algebra representation for learning residual poses during pose tracking result in highly desirable performance validated by extensive experiments. The pose tracking process operates at approximately 90.90 fps, which is significantly higher than alternatives. An additional dataset is proposed to address the lack of an object tracking benchmark in the robotics manipulation context. se(3)-TrackNet is shown to be robust under large occlusions and sudden re-orientations introduced in the dataset, which challenge competing approaches. Despite these desirable properties for the proposed network, a limitation is that an object CAD model is required. The future objective is to achieve similar performance for category-level 6D pose tracking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Top: Performance w.r.t. computation time evaluated on the YCB-Video dataset according to the area under the curve (AUC) metric for the ADD and ADD-S objectives</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overview: At any given time t the current observation O t and the rendering R t−1 of the object model based on the previously computed pose ξ t−1 are passed to the se(3)-TrackNet. The network computes the relative pose ∆ξ t , which is then propagated forward to compute ξ t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Proposed se(3)-TrackNet architecture: It takes as input RGB-D images corresponding to the current observation and a rendering of the object model at the previous timestamp, into two separate feature encoders φ B and φ A respectively. Both inputs are synthetic during training while at test time, the input to φ B is a real image. The encoders' outputs are concatenated and used to predict the relative pose between the two images, with decoupled translation and rotational.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of Domain Randomization (DR) against PhysicallyPlausible Domain Randomization (PPDR). (a) DR directly renders using a sampled object pose. Notice the penetration between objects, which can introduce undesired bias to depth data. (b) In PPDR, a randomly sampled pose serves as initialization for physics simulation. Rendering is performed over the stable object pose. The domain invariant, penetration-free property can then help to effectively align the synthetic and real domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This representation, however, does not generalize to training exclusively on synthetic data. The latent space features trained on purely synthetic data are denoted as φ A (I t−1 syn,train ) and φ B (I t syn,train ). When tested on real world data, the latent space features are φ A (I t−1 syn,test ) and φ B (I t real,test ). By this feature encoding disentanglement, domain gap reduces to be between φ B (I t syn,train ) and φ B (I t real,test ), while φ A (I t−1 syn,train ) and φ A (I t−1 syn,test ) can be effortlessly aligned between the training and test phase without the need for tackling the domain gap problem.</figDesc><table><row><cell>−1 syn,train ; I t real,train ) could still</cell></row><row><cell>be partly preserved when tested on real world test scenarios</cell></row><row><cell>φ (I t−1 syn,test ; I t real,test ).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>in Sec. III-B, where σ t and σ w are empirically set to 2 cm and 0.262 rad (= 15 • ) respectively. 200k data points (image pairs) are used for training the training set. The network is trained with Adam optimizer for 300 epochs with a batch size of 200. Learning rate starts from 0.001 and is scaled by 0.1 at epochs 100 and 200. Input RGB-D images are resized to 176 × 176 before sending to the network.</figDesc><table><row><cell>Data</cell></row><row><cell>augmentations including random HSV shift, Gaussian noise,</cell></row><row><cell>Gaussian blur are added only to I t syn,train . Additional depth-missing corruption augmentation is applied to D t syn,train as</cell></row><row><cell>described in Sec. III-B by a missing percentage between 0</cell></row><row><cell>to 0.4. For both training and inference, rendering of I t−1 is</cell></row><row><cell>implemented in C++ OpenGL.</cell></row><row><cell>A. Datasets</cell></row><row><cell>YCB-Video Dataset This dataset [1] captures 92 RGB-D</cell></row><row><cell>video sequences over 21 YCB Objects</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table I :</head><label>I</label><figDesc>Comparing the performance of se(3)-TrackNet (Gray) with state-of-the-art techniques on the YCB Video. The approach significantly outperforms the competing approaches over the ADD metric, which considers semantic information during pose evaluation. It also achieves the highest success rate over the ADD-S metric both in cases of initialization with the ground-truth pose and when initialized with the output of PoseCNN<ref type="bibr" target="#b0">[1]</ref> (rightmost two columns). Left: Qualitative results for tracking the "large-clamp" object in the YCB-Video dataset. Right: Tracking results for "bleach-cleanser" being manipulated by a vacuum gripper in the YCBInEOAT dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>time</cell><cell></cell><cell></cell><cell>time</cell></row><row><cell>Input Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input Image</cell></row><row><cell>se(3)-TrackNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>se(3)-TrackNet</cell></row><row><cell>PoseRBPF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Wuhrich's</cell></row><row><cell>Fig. 6. Objects</cell><cell cols="2">RGF [5]</cell><cell cols="2">Wthrich's [14]</cell><cell cols="2">se(3)-TrackNet</cell></row><row><cell></cell><cell>ADD</cell><cell>ADD-S</cell><cell>ADD</cell><cell>ADD-S</cell><cell>ADD</cell><cell>ADD-S</cell></row><row><cell>003 cracker box</cell><cell>34.78</cell><cell>55.44</cell><cell>79.00</cell><cell>88.13</cell><cell>90.76</cell><cell>94.06</cell></row><row><cell>021 bleach cleanser</cell><cell>29.40</cell><cell>45.03</cell><cell>61.47</cell><cell>68.96</cell><cell>89.58</cell><cell>94.44</cell></row><row><cell>004 sugar box</cell><cell>15.82</cell><cell>16.87</cell><cell>86.78</cell><cell>92.75</cell><cell>92.43</cell><cell>94.80</cell></row><row><cell>005 tomato soup can</cell><cell>15.13</cell><cell>26.44</cell><cell>63.71</cell><cell>93.17</cell><cell>93.40</cell><cell>96.95</cell></row><row><cell>006 mustard bottle</cell><cell>56.49</cell><cell>60.17</cell><cell>91.31</cell><cell>95.31</cell><cell>97.00</cell><cell>97.92</cell></row><row><cell>ALL</cell><cell>29.98</cell><cell>39.90</cell><cell>78.28</cell><cell>89.18</cell><cell>92.66</cell><cell>95.53</cell></row><row><cell cols="7">Table II: Results evaluated on YCBInEOAT-dataset by AUC (Area Under</cell></row><row><cell cols="2">Curve) for ADD and ADD-S.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.blender.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/wenbowen123/iros20-6d-pose-tracking 4 https://github.com/bayesian-object-tracking/dbot<ref type="bibr" target="#b4">5</ref> We thank the authors for providing the initial pose they used in the original paper<ref type="bibr" target="#b1">[2]</ref> </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poserbpf: A rao-blackwellized particle filter for 6d object pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bretl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scene-level pose estimation for multiple instances of densely packed objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mitash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bekris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boularias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Self-supervised 6d object pose estimation for robot manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eppner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bretl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ICRA</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Depth-based object tracking using a robust gaussian filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Issac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wüthrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Cifuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trimpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Belief-space planning using learned models with application to underactuated hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sintov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boularias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Taskdriven perception and manipulation for constrained placement of unknown objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mitash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boularias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bekris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Robust, occlusion-aware pose estimation for objects grasped by adaptive hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mitash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sintov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ICRA</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Probabilistic object tracking using a range camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wüthrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dart: Dense articulated real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep 6-dof tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2410" to="2418" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rgb-d object tracking: A particle filter approach on gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Simtrack: A simulation-based framework for scalable real-time object pose detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A versatile learningbased 3d temporal tracker: Scalable, robust, online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. Joseph</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A robust monocular 3d object tracking method combining statistical and photometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A regionbased gauss-newton approach to real-time monocular multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tjaden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schwanecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schömer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks?&quot; in NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">How useful is photorealistic rendering for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor fusion IV: control paradigms and data structures. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combined shape, appearance and silhouette for simultaneous manipulator and object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bajracharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burdick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA 2012</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Falling things: A synthetic dataset for 3d object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2038" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A self-supervised learning system for object detection using physics simulation and multi-view pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mitash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boularias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE IROS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Random phase textures: Theory and synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galerne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICAR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hodaň</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haluza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Š</forename><surname>Obdržálek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zabulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE WACV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">6-dof model based tracking via object coordinate regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ihrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Open-loop precision grasping with underactuated hands inspired by a human manipulation strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Odhner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
