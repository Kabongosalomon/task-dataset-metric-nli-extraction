<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Detection of Subjective Bias using Contextualized Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanvi</forename><surname>Dadu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikey</forename><surname>Pant</surname></persName>
							<email>kartikey.pant@research.iiit.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Mamidi</surname></persName>
							<email>radhika.mamidi@iiit.ac.in</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NSIT</orgName>
								<address>
									<settlement>Delhi</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Detection of Subjective Bias using Contextualized Word Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Subjective bias detection is critical for applications like propaganda detection, content recommendation, sentiment analysis, and bias neutralization. This bias is introduced in natural language via inflammatory words and phrases, casting doubt over facts, and presupposing the truth. In this work, we perform comprehensive experiments for detecting subjective bias using BERT-based models on the Wiki Neutrality Corpus(WNC). The dataset consists of 360k labeled instances, from Wikipedia edits that remove various instances of the bias. We further propose BERT-based ensembles that outperform state-of-the-art methods like BERT l ar дe by a margin of 5.6 F1 score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In natural language, subjectivity refers to the aspects of communication used to express opinions, evaluations, and speculations <ref type="bibr" target="#b6">[7]</ref>, often influenced by one's emotional state and viewpoints. Writers and editors of texts like news and textbooks try to avoid the use of biased language, yet subjective bias is pervasive in these texts. More than 56% of Americans believe that news sources do not report the news objectively 1 , thus implying the prevalence of the bias. Therefore, when presenting factual information, it becomes necessary to differentiate subjective language from objective language.</p><p>There has been considerable work on capturing subjectivity using text-classification models ranging from linguistic-feature-based models <ref type="bibr" target="#b4">[5]</ref> to finetuned pre-trained word embeddings like BERT <ref type="bibr" target="#b3">[4]</ref>. The detection of bias-inducing words in a Wikipedia statement was explored in <ref type="bibr" target="#b4">[5]</ref>. The authors propose the "Neutral Point of View" (NPOV ) corpus made using Wikipedia revision history, containing Wikipedia edits that are specifically designed to remove subjective bias. They use logistic regression with linguistic features, including factive verbs, hedges, and subjective intensifiers to detect bias-inducing words. In <ref type="bibr" target="#b3">[4]</ref>, the authors extend this work by mitigating subjective bias after detecting bias-inducing words using a BERT-based model. However, they primarily focused on detecting and mitigating subjective bias for single-word edits. We extend their work by incorporating multi-word edits by detecting bias at the sentence level. We further use their version of the NPOV corpus called Wiki Neutrality Corpus(WNC) for this work.</p><p>The task of detecting sentences containing subjective bias rather than individual words inducing the bias has been explored in <ref type="bibr" target="#b1">[2]</ref>. However, they conduct majority of their experiments in controlled settings, limiting the type of articles from which the revisions were extracted. Their attempt to test their models in a general setting is dwarfed by the fact that they used revisions from a single Wikipedia * The first two authors contributed equally to the work. 1 https://news.gallup.com/opinion/gallup/235796/americans-misinformation-biasinaccuracy-news.aspx article resulting in just 100 instances to evaluate their proposed models robustly. Consequently, we perform our experiments in the complete WNC corpus, which consists of 423, 823 revisions in Wikipedia marked by its editors over a period of 15 years, to simulate a more general setting for the bias.</p><p>In this work, we investigate the application of BERT-based models for the task of subjective language detection 2 . We explore various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers. We propose an ensemble model exploiting predictions from these models using multiple ensembling techniques. We show that our model outperforms the baselines by a margin of 5.6 of F1 score and 5.95% of Accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BASELINES AND APPROACH</head><p>In this section, we outline baseline models like BERT l ar дe . We further propose three approaches: optimized BERT-based models, distilled pretrained models, and the use of ensemble methods for the task of subjectivity detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baselines</head><p>(1) FastText <ref type="bibr" target="#b2">[3]</ref>: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently. (2) BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline. (3) BERT [1]: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large 3.3B word corpus. We use the BERT l ar дe model finetuned on the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed Approaches</head><p>(1) Optimized BERT-based models: We use BERT-based models optimized as in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref>, pretrained on a dataset as large as twelve times as compared to BERT l ar дe , with bigger batches, and longer sequences. ALBERT, introduced in [9], uses factorized embedding parameterization and cross-layer parameter sharing for parameter reduction. These optimizations have led both the models to outperform BERT l ar дe in various benchmarking tests, like GLUE for text classification and SQuAD for Question Answering. (2) Distilled BERT-based models: Secondly, we propose to use distilled BERT-based models, as introduced in <ref type="bibr" target="#b5">[6]</ref>. They are smaller general-purpose language representation model, pretrained by leveraging distillation knowledge. This results in significantly smaller and faster models with performance comparable to their undistilled versions. We finetune these pretrained distilled models on the training corpus to efficiently detect subjectivity. (3) BERT-based ensemble models: Lastly, we use the weightedaverage ensembling technique to exploit the predictions made by different variations of the above models. Ensembling methodology entails engendering a predictive model by utilizing predictions from multiple models in order to improve Accuracy and F1, decrease variance, and bias. We experiment with variations of RoBERT a l ar дe , ALBERT x xl ar дe .v2 , DistilRoBERT a and BERT and outline selected combinations in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS 3.1 Dataset and Experimental Settings</head><p>We perform our experiments on the WNC dataset open-sourced by the authors of <ref type="bibr" target="#b3">[4]</ref>. It consists of aligned pre and post neutralized sentences made by Wikipedia editors under the neutral point of view. It contains 180k biased sentences, and their neutral counterparts crawled from 423, 823 Wikipedia revisions between 2004 and 2019. We randomly shuffled these sentences and split this dataset into two parts in a 90 : 10 Train-Test split and perform the evaluation on the held-out test dataset. For all BERT-based models, we use a learning rate of 2 * 10 −5 , a maximum sequence length of 50, and a weight decay of 0.01 while finetuning the model. We use FastText's recently open-sourced automatic hyperparameter optimization functionality while training the model. For the BiLSTM baseline, we use a dropout of 0.05 along with a recurrent dropout of 0.2 in two 64 unit sized stacked BiLSTMs, using softmax activation layer as the final dense layer. <ref type="table" target="#tab_0">Table 1</ref> shows the performance of different models on the WNC corpus evaluated on the following four metrics: Precision, Recall, F1, and Accuracy. Our proposed methodology, the use of finetuned optimized BERT based models, and BERT-based ensemble models outperform the baselines for all the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>Among the optimized BERT based models, RoBERT a l ar дe outperforms all other non-ensemble models and the baselines for all metrics. It further achieves a maximum recall of 0.681 for all the proposed models. We note that DistillRoBERTa, a distilled model, performs competitively, achieving 69.69% accuracy, and 0.672 F1 score. This observation shows that distilled pretrained models can replace their undistilled counterparts in a low-computing environment.</p><p>We further observe that ensemble models perform better than optimized BERT-based models and distilled pretrained models. Our proposed ensemble comprising of RoBERT a l ar дe , ALBERT x xl ar дe .v2 , DistilRoBERT a and BERT outperforms all the proposed models obtaining 0.704 F1 score, 0.733 precision, and 71.61% Accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In this paper, we investigated BERT-based architectures for sentence level subjective bias detection. We perform our experiments on a general Wikipedia corpus consisting of more than 360k pre and post subjective bias neutralized sentences. We found our proposed architectures to outperform the existing baselines significantly. BERTbased ensemble consisting of RoBERTa, ALBERT, DistillRoBERTa, and BERT led to the highest F1 and Accuracy. In the future, we would like to explore document-level detection of subjective bias, multi-word mitigation of the bias, applications of detecting the bias in recommendation systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental Results for the Subjectivity Detection Task</figDesc><table><row><cell></cell><cell>Models/Metrics</cell><cell cols="2">Precision Recall F1</cell><cell>Acc</cell></row><row><cell></cell><cell>FastText</cell><cell>0.613</cell><cell cols="2">0.612 0.613 61.24%</cell></row><row><cell>Baselines</cell><cell>BiLSTM+GloVe</cell><cell>0.648</cell><cell cols="2">0.647 0.648 64.76%</cell></row><row><cell></cell><cell>BERT l ar дe</cell><cell>0.681</cell><cell cols="2">0.587 0.631 65.66%</cell></row><row><cell></cell><cell>ALBERT x xl ar дe .v2</cell><cell>0.667</cell><cell cols="2">0.579 0.620 64.56%</cell></row><row><cell>Single Model</cell><cell>DistillBERT DistillRoBERTa</cell><cell>0.731 0.730</cell><cell cols="2">0.608 0.664 69.28% 0.623 0.672 69.69%</cell></row><row><cell></cell><cell>RoBERT a l ar дe</cell><cell>0.723</cell><cell cols="2">0.681 0.702 71.09%</cell></row><row><cell></cell><cell>BERT Ensembl e +DistillBERT</cell><cell>0.731</cell><cell cols="2">0.610 0.665 69.36%</cell></row><row><cell>Ensemble model</cell><cell>RoBERT a Ensembl e RoBERTa+ALBERT +DistillRoBERTa+BERT</cell><cell>0.732 0.733</cell><cell cols="2">0.679 0.704 71.57% 0.677 0.704 71.61%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Made available at https://github.com/tanvidadu/Subjective-Bias-Detection arXiv:2002.06644v1 [cs.CL] 16 Feb 2020 Dadu, Pant, and Mamidi</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Based Statement Classification for Biased Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Hube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Besnik</forename><surname>Fetahu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bag of Tricks for Efficient Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>arxiv:1607.01759</idno>
		<ptr target="http://arxiv.org/abs/1607.01759cite" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Pryzant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Diehl</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Dass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1911.09709</idno>
		<title level="m">Automatically Neutralizing Subjective Bias in Text</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Linguistic Models for Analyzing and Detecting Biased Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/acl/acl2013-1.html#RecasensDJ13" />
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1650" to="1659" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1910.01108</idno>
		<title level="m">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tracking Point of View in Narrative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<ptr target="https://openreview.net/forum?id=SyxS0T4tvSunderreview" />
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
		<editor>Naman Goyal Jingfei Du Mandar Joshi Danqi Chen Omer Levy Mike Lewis Luke Zettlemoyer Veselin Stoyanov Yinhan Liu, Myle Ott. 2020</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<ptr target="https://openreview.net/forum?id=H1eA7AEtvSunderreview" />
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
		<editor>Sebastian Goodman Kevin Gimpel Piyush Sharma Radu Soricut Zhenzhong Lan, Mingda Chen</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
