<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HAA500: Human-Centric Atomic Action Dataset with Curated Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Hsin</forename><surname>Wuu</surname></persName>
							<email>cwuu@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>3 Kuaishou</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsuan-Ru</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
							<email>cktang@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HAA500: Human-Centric Atomic Action Dataset with Curated Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We contribute HAA500 1 , a manually annotated humancentric atomic action dataset for action recognition on 500 classes with over 591k labeled frames. Unlike existing atomic action datasets, where coarse-grained atomic actions were labeled with action-verbs, e.g., "Throw", HAA500 contains fine-grained atomic actions where only consistent actions fall under the same label, e.g., "Baseball Pitching" vs "Free Throw in Basketball", to minimize ambiguities in action classification. HAA500 has been carefully curated to capture the movement of human figures with less spatio-temporal label noises to greatly enhance the training of deep neural networks. The advantages of HAA500 include: 1) human-centric actions with a high average of 69.7% detectable joints for the relevant human poses; 2) each video captures the essential elements of an atomic action without irrelevant frames; 3) fine-grained atomic action classes. Our extensive experiments validate the benefits of human-centric and atomic characteristics of HAA, which enables the trained model to improve prediction by attending to atomic human poses. We detail the HAA500 dataset statistics and collection methodology, and compare quantitatively with existing action recognition datasets.</p><p>Recently, FineGym <ref type="bibr" target="#b35">[35]</ref> has been introduced to solve the aforementioned limitations by proposing fine-grained action annotations, e.g. Balance Beam-Dismount-Salto forward Tucked. But due to expensive data collection process, they only contain 4 events with atomic action annotations 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Observe the coarse annotation provided by commonlyused action recognition datasets such as <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b44">44]</ref>, where the same action label was assigned to a given complex video action sequence (e.g. Play Soccer, Play Baseball) typically lasting 10 seconds or 300 frames, thus introducing a lot of ambiguities during training as two or more action categories may contain the same atomic action (e.g., Run is one of the atomic actions for both Play Soccer and Play Baseball).</p><p>Recently, atomic action datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b35">35]</ref> have been introduced in an attempt to resolve the aforementioned issue. Google's AVA actions dataset <ref type="bibr" target="#b15">[15]</ref> provides dense an-1 HAA500 dataset can be downloaded at https://www.cse.ust.hk/haa notations of 80 atomic visual actions in 430 fifteen-minute video clips where actions are localized in space and time. AVA spoken activity dataset <ref type="bibr" target="#b32">[32]</ref> contains temporally labeled face tracks in videos, where each face instance is labeled as speaking or not, and whether the speech is audible. The something-something dataset <ref type="bibr" target="#b14">[14]</ref> contains clips of humans performing pre-defined basic actions with daily objects.</p><p>However, some of their actions are still coarse which can be further split into atomic classes with significantly different motion gestures. e.g., AVA <ref type="bibr" target="#b15">[15]</ref> and somethingsomething <ref type="bibr" target="#b14">[14]</ref> contain Play Musical Instrument and Throw Something as a class, respectively, where the former should be further divided into sub-classes such as Play Piano and Play Cello, and the latter into Soccer Throw In and Pitch Baseball, etc, because each of these atomic actions has significantly different gestures. Encompassing different visual postures into a single class poses a deep neural network almost insurmountable challenge to properly learn the pertinent atomic action, which probably explains the prevailing low performance employing even the most state-of-the-art architecture, SlowFast (mAP: 34.25%) <ref type="bibr" target="#b10">[10]</ref>, in AVA <ref type="bibr" target="#b15">[15]</ref>, despite only having 80 classes.</p><p>The other problem with existing action recognition video datasets is that their training examples contain actions irrelevant to the target action. Video datasets tend to have fixed clip length, allowing unrelated video frames to be easily included during the data collection stage. Kinetics 400 dataset <ref type="bibr" target="#b19">[19]</ref>, with a fixed 10 second clip length, contains a lot of irrelevant actions, e.g., showing the audience before the main violin playing, or a person takes a long run before kicking the ball. Another problem is having too limited or too broad field-of-view, where a video only exhibits a part of a human interacting with an object <ref type="bibr" target="#b14">[14]</ref>, or a single video contains multiple human figures with different actions present <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b44">44]</ref>. (Balance Beam, Floor Exercise, Uneven Bars, and Vault-Women), and their clips were extracted from professional gymnasium videos in athletic or competitive events. In this paper, we contribute Human-centric Atomic Action dataset (HAA500) which has been constructed with carefully curated videos with an average of 69.7% detectable joints, where a dominant human figure is present to perform the labeled action. The curated videos have been annotated with fine-grained labels to avoid ambiguity, and with dense per-frame action labeling and no unrelated frames being included in the collection as well as annotation. HAA500 contains wide-variety of atomic actions, ranging from athletic atomic action (Figure Skating -Ina Bauer) to daily atomic action (Eating a Burger). The clips are class-balanced and contain clear visual signals with little occlusion. As opposed to "in the wild" atomic action datasets, our "cultivated" clean, class-balanced dataset provides an effective alternative to advance research in atomic visual actions recognition and thus video understanding. An example of the collected atomic action is shown in <ref type="figure">Figure 1</ref>. <ref type="table" target="#tab_1">Table 1</ref> summarizes of representative action recognition datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Action Recognition Dataset</head><p>Composite Action Dataset Representative action recognition datasets, such as HMDB51 <ref type="bibr" target="#b23">[23]</ref>, UCF101 <ref type="bibr" target="#b38">[38]</ref>, Hollywood-2 <ref type="bibr" target="#b27">[27]</ref>, ActivityNet <ref type="bibr" target="#b8">[8]</ref>, and Kinetics <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">19]</ref>x consist of short clips which are manually trimmed to capture a single action. These datasets are ideally suited for training fully-supervised, whole-clip video classifiers. A few datasets used in action recognition research, such as MSR Actions <ref type="bibr" target="#b43">[43]</ref>, UCF Sports <ref type="bibr" target="#b30">[30]</ref> and JHMDB <ref type="bibr" target="#b17">[17]</ref>, provide spatio-temporal annotations in each frame for short videos, but they only contain few actions. Aside from the subcate-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Videos Actions Atomic KTH <ref type="bibr" target="#b33">[33]</ref> 600 6 Weizmann <ref type="bibr" target="#b0">[1]</ref> 90 10 UCF Sports <ref type="bibr" target="#b30">[30]</ref> 150 10 Hollywood-2 <ref type="bibr" target="#b27">[27]</ref> 1,707 12 HMDB51 <ref type="bibr" target="#b23">[23]</ref> 7,000 51 UCF101 <ref type="bibr" target="#b38">[38]</ref> 13,320 101 DALY <ref type="bibr" target="#b40">[40]</ref> 510 10 AVA <ref type="bibr" target="#b15">[15]</ref> 57,600 80 Kinetics 700 <ref type="bibr" target="#b1">[2]</ref> 650,317 700 HACS <ref type="bibr" target="#b44">[44]</ref> 1,550,000 200 Moments in Times <ref type="bibr" target="#b29">[29]</ref> 1,000,000 339 FineGym <ref type="bibr" target="#b35">[35]</ref> 32,687 530 HAA500 10,000 500 gory of shortening the video length, recent extensions such as UCF101 <ref type="bibr" target="#b38">[38]</ref>, DALY <ref type="bibr" target="#b40">[40]</ref>, and Hollywood2Tubes <ref type="bibr" target="#b28">[28]</ref> evaluate spatio-temporal localization in untrimmed videos, resulting in a performance drop due to the more difficult nature of the task. One common issue on these aforementioned datasets is that they are annotated with composite action classes (e.g. tennis), thus different human action gestures (e.g., backhand swing, forehand swing) are annotated under a single class. Another issue is that they tend to capture in wide field-of-view and thus include multiple human figures (e.g. tennis player, referee, audience) with different actions in a single frame, which inevitably introduce confusion to action analysis and recognition.</p><p>Atomic Action Dataset To model finer-level events, the AVA dataset <ref type="bibr" target="#b15">[15]</ref> was introduced to provide person-centric spatio-temporal annotations on atomic actions similar to some of the earlier works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">33]</ref>. Other specialized datasets such as Moments in Times <ref type="bibr" target="#b29">[29]</ref>, HACS <ref type="bibr" target="#b44">[44]</ref>, Something-Something <ref type="bibr" target="#b14">[14]</ref>, and Charades-Ego <ref type="bibr" target="#b36">[36]</ref> provide classes for  <ref type="table">Table 2</ref>. Performance of previous works on Kinetics 400 <ref type="bibr" target="#b19">[19]</ref>, Something-Something <ref type="bibr" target="#b14">[14]</ref>, and NTU-RGB+D <ref type="bibr" target="#b34">[34]</ref> dataset. We evaluate on both cross subject (X-Sub) and cross view (X-View) benchmarks for NTU-RGB+D. For fair comparison, in this paper we use <ref type="bibr" target="#b19">[19]</ref> rather than <ref type="bibr" target="#b1">[2]</ref> where 400 classes <ref type="bibr" target="#b19">[19]</ref> are still used and at the time of writing, representative action recognition models still use <ref type="bibr" target="#b19">[19]</ref> for pre-training or benchmarking.</p><p>atomic actions but none of them are human-centric atomic action, where some of the video frames are ego-centric which only show part of a human body (e.g. hand), or no human action at all. Atomic action datasets <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b29">29]</ref> tend to have atomicity under English linguistics, e.g. In Moments in Times <ref type="bibr" target="#b29">[29]</ref> open is annotated on video clips with a tulip opening, an eye opening, a person opening a door, or a person opening a package, which are fundamentally different actions only sharing the verb open, thus giving the possibility of finer division.</p><p>Fine-Grained Action Dataset A different approach to designing fine-grained action datasets has been used to tackle the same problem. These methods (e.g., <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35]</ref>) use systematic action labeling to annotate finegrained labels on a small domain of actions. Breakfast <ref type="bibr" target="#b22">[22]</ref>, MPII Cooking 2 <ref type="bibr" target="#b31">[31]</ref>, and EPIC-KITCHENS <ref type="bibr" target="#b5">[5]</ref> offer finegrained actions for cooking and preparing dishes, e.g., twist milk bottle cap <ref type="bibr" target="#b22">[22]</ref>. JIGSAWS <ref type="bibr" target="#b12">[12]</ref>, Diving48 <ref type="bibr" target="#b24">[24]</ref>, and FineGym <ref type="bibr" target="#b35">[35]</ref> respectively offer fine-grained action dataset for surgery, diving, and gymnastics. While existing finegrained action datasets are well suited for benchmarks, due to their low-variety and the narrow domain of the classes, they cannot be extended easily in a general-purpose action recognition. Our HAA500 dataset differs from all of the aforementioned datasets as we provide a wide variety of 500 finegrained atomic human action classes in various domains, where videos in each class only exhibit the relevant human atomic actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Action Recognition Architectures</head><p>Current action recognition architectures can be categorized into two major approaches: 2D-CNN and 3D-CNN.  2D-CNN <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b45">45]</ref> based models utilize imagebased 2D-CNN models on a single frame where features are aggregated to predict the action. While some methods (e.g., <ref type="bibr" target="#b7">[7]</ref>) use RNN modules for temporal aggregation over visual features, TSN <ref type="bibr" target="#b39">[39]</ref> shows that simple average pooling can be an effective method to cope with temporal aggregation. To incorporate temporal information to 2D-CNN, twostream structure <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b37">37]</ref> has been proposed to use RGBframes and optical flow as separate inputs to convolutional networks. 3D-CNN [3, 10, 18] takes a more natural approach by incorporating spatio-temporal filters to the image frames. Inspired from <ref type="bibr" target="#b37">[37]</ref>, two-streamed inflated 3D-CNN (I3D) <ref type="bibr" target="#b2">[3]</ref> incorporates two-stream structure on 3D-CNN. SlowFast <ref type="bibr" target="#b10">[10]</ref> improves from I3D by showing that the accuracy increases when the 3D kernels are used only in the later layers of the model. A different approach is adopted TPN <ref type="bibr" target="#b42">[42]</ref> where a high-level structure is designed adopting a temporal pyramid network which can use either 2D-CNN or 3D-CNN as a backbone. Some models <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b41">41]</ref> use alternative information to predict video action. Specifically, ST-GCN <ref type="bibr" target="#b41">[41]</ref> uses a graph convolutional network to predict video action from pose estimation. However, their pose-based models cannot demonstrate better performance than RGB-frame based models. <ref type="table">Table 2</ref> tabulates the performance of representative action recognition models on video action datasets, where 2D-skeleton based models <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b41">41]</ref> show considerably low accuracy in Kinetics 400 <ref type="bibr" target="#b19">[19]</ref>.</p><p>3. HAA500</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection</head><p>The annotation of HAA500 consists of two stages: vocabulary collection and video clip selection. While the bottom-up approach which annotates action labels on selected long videos was often used in atomic/fine-grained action datasets <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b35">35]</ref>, we aim to build a clean and finegrained dataset for atomic action recognition, thus the video clips are collected based on pre-defined atomic actions following a top-down approach.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Vocabulary Collection</head><p>To make the dataset as clean as possible and useful for recognizing fine-grained atomic actions, we narrowed down the scope of our super-classes into 4 areas; Sport/Athletics, Musical Instruments, Games and Hobbies, and Daily Actions, where future extension beyond the existing classes is feasible. We select action labels where the variations within a class are typically indistinguishable. For example, instead of Hand Whistling, we have Whistling with One Hand and Whistling with Two Hands, as the variation is large and distinguishable. Our vocabulary collection methodology makes the dataset hierarchical where atomic actions may be combined to form a composite action, e.g., Whistling or Soccer.</p><p>Consequently, HAA500 contains 500 atomic action classes, where 212 are Sport/Athletics, 51 are Musical Instruments, 82 are Games and Hobbies and the rest are Daily Actions. <ref type="figure" target="#fig_1">Figure 2</ref> shows the hierarchy of HAA500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Video Clip Selection</head><p>To ensure our dataset is clean and class-balanced, all the video clips are collected from YouTube with the majority having a resolution of at least 720p, and each class of atomic action containing 16 training clips. We manually select the clips with apparent human-centric actions where the personof-interest is the only dominant person in the frame at the center with their body clearly visible. To increase diversity among the video clips and avoid unwanted bias, all the clips were collected from different YouTube videos, with different environment settings so that the action recognition task cannot be trivially reduced to identifying the corresponding backgrounds. Clips are properly trimmed in a frameaccurate manner to cover the desired actions, while assuring every video clip to have compatible actions within each class (e.g. every video in the class salute starts on the exact frame where the person is standing still before moving the arm, and the video ends when the hand goes next to the eyebrow). <ref type="figure">Figure 1</ref> shows examples of the selected videos. <ref type="table" target="#tab_4">Table 3</ref> summarizes the HAA500 statistics. HAA500 includes 500 atomic action classes where each class contains 20 clips, with an average length of 2.12 seconds. Each clip was annotated with meta-information which contains the following two fields: the number of dominant people in the video and the camera movement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Training/Validation/Test Sets</head><p>Since the clips in different classes are mutually exclusive, all clips appear only in one split. The 10,000 clips are split as 16:1:3, resulting in segments of 8,000 training, 500 validation, and 1,500 test clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Properties and Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Clean Labels for Every Frame</head><p>Most video datasets <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b38">38]</ref> show strong label noises, due to the difficulties of collecting clean video action dataset. Some <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b38">38]</ref> often focus on the "Scene" of Dataset Clip Length Irr. Actions Camera Cuts UCF101 <ref type="bibr" target="#b38">[38]</ref> Varies × × HMDB51 <ref type="bibr" target="#b23">[23]</ref> Varies the video clip, neglecting the human "action" thus including irrelevant actions or frames with visible camera cuts in the clip. Video action datasets <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b44">44]</ref> also have a fixed length of video clips, where irrelevant frames are inevitable for shorter or longer actions. Our properly trimmed video collection guarantees clean label for every frame. <ref type="table" target="#tab_5">Table 4</ref> tabulates clip lengths and label noises of video action datasets. <ref type="figure" target="#fig_2">Figure 3</ref> shows examples of label noises. As HAA500 are constructed gearing to accurate temporal annotation, we are almost free from any adverse effect due to these noises.</p><formula xml:id="formula_0">× • AVA [15] 1 second • • HACS [44] 2 second • × K.400 [19] 10 second • • M.i.T. [29] 3 second × × HAA500 Just Right × ×</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Human-Centric</head><p>One of the difficulties in action recognition is that the neural network tends to predict by trivially comparing the background scene in the video, or detecting key elements in a frame (e.g., a basketball to detect Playing Basketball) rather than analyzing human gesture, thus causing the action recognition to have no better performance improvements over scene/object recognition. The other difficulty stems from the video action datasets where videos captured in wide field-of-view contain multiple people in a single frame <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b44">44]</ref>, while videos captured using narrow field-of-view only exhibit very little body part in interaction with an object <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b29">29]</ref>. In <ref type="bibr" target="#b15">[15]</ref> attempts were made to overcome this issue through spatial annotation of each individual in a given frame; this introduces another problem of action localization and thus further complicating the dif-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Detectable Joints Kinetics 400 <ref type="bibr" target="#b19">[19]</ref> 41.0% UCF101 <ref type="bibr" target="#b38">[38]</ref> 37.8% HMDB51 <ref type="bibr" target="#b23">[23]</ref> 41.8% FineGym <ref type="bibr" target="#b35">[35]</ref> 44.7% HAA500 69.7% <ref type="table">Table 5</ref>. Detectable joints of video action datasets. We use Alpha-Pose <ref type="bibr" target="#b9">[9]</ref> to detect the largest person in the frame, and count the number of joints with a score higher than 0.5. ficult recognition task. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates example frames of different video action datasets.</p><p>HAA500 contributes a curated dataset where each human joint can be clearly detected over any given frame, thus allowing the model to benefit from learning human movements than just performing scene recognition. As tabulated in <ref type="table">Table 5</ref>, HAA500 have high detectable joints <ref type="bibr" target="#b9">[9]</ref> of 69.7%, well above other representative action datasets.  <ref type="table">Table 6</ref>. Left : HAA500 trained over different models. Right : Composite action detection accuracy of different models when they are trained with/without atomic action detection. Numbers are bolded when the difference is larger than 1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Atomic</head><p>Existing atomic action datasets such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b29">29]</ref> is limited by English linguistics, where action verbs (e.g., walk, throw, pull, etc) are decomposed. Such classification does not fully eliminate the aforementioned problems of composite action dataset. <ref type="figure" target="#fig_4">Figure 5</ref> shows cases of different atomic action datasets where a single action class contains fundamentally different actions. On the other hand, our fine-grained atomic actions contain only a single type of action under each class, e.g. Baseball -Pitch, Yoga -Tree, Hopscotch -Spin, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical Studies</head><p>We study HAA500 and representative action datasets over multiple aspects using widely used action recognition models. Left of <ref type="table">Table 6</ref> shows the performance of the model when they are trained with HAA500. For fair comparison between RGB frame based models with optical flow <ref type="bibr" target="#b16">[16]</ref> or pose <ref type="bibr" target="#b9">[9]</ref> based models, all the experiments have been done without any ImageNet <ref type="bibr" target="#b6">[6]</ref> pre-training. For Pose models except ST-GCN <ref type="bibr" target="#b41">[41]</ref>, we use three-channel pose joint heatmaps <ref type="bibr" target="#b9">[9]</ref> to train pose models. RGB, Flow and Pose all show relatively similar performance in HAA500, without none of them showing superior performance than the other. Given that the information of pose heatmap is far less than the image information given from RGB frames or optical flow frames, we expect that easily detectable joints of HAA500 is benefiting the pose-based model performance.</p><p>Furthermore, we study the benefits of atomic action annotation on video recognition, as well as the importance of human-centric characteristics of HAA500. In this paper, we use I3D-RGB <ref type="bibr" target="#b2">[3]</ref> with 32 frames for all of our experiments unless otherwise specified. We use AlphaPose <ref type="bibr" target="#b9">[9]</ref> for the models that require human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Atomic Action</head><p>We have previously discussed that modern action recognition datasets introduce ambiguities where two or more composite actions sharing the same atomic actions, while a single composite action class containing multiple distinguishable actions. HAA500 addresses this issue by providing fine-grained atomic action labels that distinguish similar atomic action in different composite actions. To study the benefits of atomic action labels, specifically, how it helps on composite action detection for ambiguous classes, we selected two areas from HAA500, Sports/Athletics and Musical Instruments, in which composite actions contain strong ambiguities with other actions in the area. We compare models trained with two different types of labels: 1) only composite labels and 2) atomic + composite labels, then we evaluate the performance on composite action detection. Results are tabulated on the right of <ref type="table">Table 6</ref>. Accuracy from the models trained with only composite labels are under Inst. and Sport column, and the accuracy of composite action detection while trained with atomic action detection is listed on the other columns.</p><p>We can observe improvements in composite action detection when atomic action detection is incorporated. The fine-grained action decomposition in HAA enables the models to resolve ambiguities of similar atomic actions and helps the model to learn the subtle differences in the atomic actions across different composite actions. This demonstrates the importance of a proper label of fine-grained atomic action which can increase the performance for composite action detection without change of the model architecture or the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Human-Centric</head><p>HAA500 is designed to contain action clips with a high percentage of detectable human-figures. To study the importance of human-pose in fine-grained atomic action recognition, we compare the performance of HAA500 and FineGym when both RGB and pose estimation are given as input. For pose estimation, we obtain the 17 joint heatmaps from AlphaPose <ref type="bibr" target="#b9">[9]</ref> and merge them into 3 channels; head, upper-body, and lower-body. <ref type="bibr">RGB</ref>   <ref type="table">Table 7</ref>. Atomic action detection accuracy when both RGB image and pose estimation are given as an input. We also show performance when they are trained separately for comparison. <ref type="table">Table 7</ref> tabulates the results. In three out of four areas of HAA500, I3D-RGB shows better performance than I3D-Pose, due to the vast amount of information given to the model. I3D-Pose shows the highest performance on Sports/Athletics with vibrant and distinctive action, while I3D-Pose fails to show comparable performance on Musical Instrument area, where predicting the atomic action from only 17 joints is quite challenging. Nonetheless, our experiments show a performance boost when both pose estimation and RGB frame are fed to the atomic action detection model, implicating the importance of human action in HAA500 action detection. For FineGym -Gym288, due to the rapid athletic movements resulting in blurred frames, human pose is not easily recognizable which accounts for the observed low accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Observations</head><p>We present notable characteristics observed from HAA500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Fine-Tuning over HAA500</head><p>Here, we test how to exploit the curated HAA500 dataset to detect action in "in the wild" action datasets. We pre-train using HAA500 and other datasets on I3D-RGB <ref type="bibr" target="#b2">[3]</ref> and freeze all the layers except for the last three for feature extraction. We then finetune on the last three layers with "in the wild" composite action datasets <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b38">38]</ref>. <ref type="table">Table 8</ref> tabulates the fine-tuning result. Our dataset is carefully curated to have a high variety of backgrounds and people while having consistent actions over each class. While being comparably smaller and "human-centric" than other action recognition datasets, HAA500's cleanness and UCF101 <ref type="bibr" target="#b38">[38]</ref> ActNet 100 <ref type="bibr" target="#b8">[8]</ref> HMDB51 <ref type="bibr" target="#b23">[23]</ref> Pre <ref type="table" target="#tab_1">-trained  Top-1  Top-1  Top-1  None  58.87%  35.68%</ref> 28.56% AVA <ref type="bibr" target="#b15">[15]</ref> 48.54% 21.10% 25.28% Gym288 <ref type="bibr" target="#b35">[35]</ref> 69.94% 29.88% 36.24% UCF101 <ref type="bibr" target="#b38">[38]</ref> -33.56% 32.37% ActNet 100 <ref type="bibr" target="#b8">[8]</ref> 58.90% -27.71% HMDB51 <ref type="bibr" target="#b23">[23]</ref> 53.36% 28.60% -HAA500</p><p>68.70% 36.53% 40.45% <ref type="table">Table 8</ref>. Fine-tuning Performance on I3D.</p><p>high-variety makes it easily transferable to different tasks and datasets.</p><p>Effects of Scale Normalization HAA500 has high diversity in human positions across the video collection. Here, we choose an area of HAA500, Musical Instruments, to investigate the effect of human-figure normalization on detection accuracy. We have manually annotated the bounding box of the person-of-interest in each frame and cropped them for the model to focus on the human action. In <ref type="table" target="#tab_8">Table 9</ref> we test models that were trained to detect the composite actions or both composite and atomic actions. While HAA500 is highly human-centric with person-ofinterest as the most dominant figure of the frame, action detections on the normalized frames still show considerable improvements when trained on either atomic action annotations or composite action annotations. This indicates that spatial annotation is an important information in video action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Object Detection</head><p>In most video action dataset, non-human objects exist as a strong bias to the classes (e.g. basketball in Playing Basketball). When the action dataset annotates highly diverse actions (e.g. Shooting a Basketball, Dribbling a Basketball, etc.) under a single class, general deep-learning models tend to suffer from the bias and will learn to detect the easiest common factor (Basketball) among the video clips, rather than "seeing" the human action. Poorly designed video action dataset encourages the action detection model to trivially become an object detection model.</p><p>In HAA500, every video clip in the same class contains compatible actions, making the common factor to be the "action", while objects are ambiguity that spreads among different classes (e.g. basketball exists in both Shooting a Basketball and Dribbling a Basketball). To test the influence of "object" in HAA500, we design an experiment similar to investigating the effect of human poses, as presented in <ref type="table">Table 7</ref>, while we use object detection heatmap instead.</p><p>Here we use Fast RCNN <ref type="bibr" target="#b13">[13]</ref> trained with COCO <ref type="bibr" target="#b26">[26]</ref> dataset to generate the object heatmap. Among 80 detectable objects in COCO, we select 5 types of objects (sports equipments, food, animals, cutleries, and vehicles) to draw 5-channel heatmap. Similar to <ref type="table">Table 7</ref>, the heatmap channel is appended to the RGB channel as an input. <ref type="bibr">RGB</ref>   <ref type="table" target="#tab_1">Table 10</ref>. Accuracy of I3D when it is trained with object heatmap. HAA-COCO denotes 147 classes of HAA500 expected to have objects that were detected in the experiment. <ref type="table" target="#tab_1">Table 10</ref> tabulates the negligible effect of object detection in atomic action detection of HAA500, including the classes that are expected to use the selected objects (HAA-COCO), while UCF101 shows improvements when object heatmap is used as a visual cue. Given the negligible effect of object heatmaps, we believe that fine-grained annotation of actions can effectively eliminates unwanted bias ("objects") from affecting the prediction, while in UCF101 (composite action dataset), "objects" can still affect the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Dense Temporal Sampling</head><p>The top of Table 11 tabulates the performance difference of HAA500 and other datasets over the number of frames that are used during training and testing. The bottom of <ref type="table" target="#tab_1">Table 11</ref> tabulates the performance with varying strides with a window size of 32 frames, except AVA which we test with 16 frames. Top-1 accuracies on action recognition are shown except AVA which shows mIOU due to its multi-labeled nature of the dataset.</p><p>As expected, most datasets show the best performance when 32 frames are fed. AVA shows a drop in performance due to the irrelevant frames (e.g., action changes, camera cuts, etc.) included in the wider window. While all the datasets show comparable accuracy when the model only uses a single frame (i.e. when the problem has been reduced to "Scene Recognition" problem), both HAA500 and Gym288 show a significant drop compared to their accuracy in 32 frames. While having an identical background contributes to the performance difference for Gym288, from # of frames HAA500 UCF101 <ref type="bibr" target="#b38">[38]</ref> AVA <ref type="bibr" target="#b15">[15]</ref> Gym288 <ref type="bibr">[</ref>  <ref type="table" target="#tab_1">Table 11</ref>. Performance comparison on I3D-RGB over the number of frames and strides, where in the latter a window size of 32 frames is used except AVA which we test with 16 frames.</p><p>HAA500, we see how temporal action movements are crucial for the detection of atomic actions, and they cannot be trivially detected using a simple scene detecting model. We also see that the density of the temporal window is another important factor in atomic action detection. We see that in both HAA500 and Gym288, which are fine-grained action datasets, show larger performance drops when the frames have been sampled with strides of 2 or more, reflecting the importance of short temporal action movements in fine-grained action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper introduces HAA500, a new human action dataset with fine-grained atomic action labels and humancentric clip annotations, where the videos are carefully selected such that the relevant human poses are apparent and detectable. With carefully curated action videos, HAA500 does not suffer from irrelevant frames, where videos clips only exhibit the annotated action. With a small number of clips per class, HAA500 is highly scalable to include more action classes. We have demonstrated the efficacy of HAA500 where action recognition can be greatly benefited from our clean, highly diversified, class-balanced finegrained atomic action dataset which is human-centric with high percentage of detectable pose. On top of HAA500, we have also empirically investigated several important factors that can affect the performance of action recognition. We hope HAA500 and our findings could facilitate new advances in the field of action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Header Dribble Shoot Play Guitar Play Cello ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Example of HAA500 hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Different types of label noise in action recognition datasets. (a): Kinetics400 has a fixed video length of 10 seconds which cannot accurately annotate quick actions like "Shooting Basketball" where the irrelevant action of dribbling the ball is included in the clip. (b): A camera cut can be seen, showing unrelated frames (audience) after the main action. (c): By not having a frame-accurate clipping, the clip starts with a person-of-interest in the midair, and quickly disappears after few frames, causing the rest of the video clip to not have any person in action. (d): Our HAA500 accurately annotates the full motion of "Uneven Bars -Land" without any irrelevant frames. All the videos in the class starts with the exact frame an athlete puts the hand off the bar, to the exact frame when he/she finishes the landing pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The video clips in AVA, HACS, and Kinetics 400 contain multiple human figures with different actions in the same frame. Something-Something focuses on the target object and barely shows any human body parts. In contrast, all video clips in HAA500 (inFigure 1) are carefully curated where each video shows either a single person or the person-of-interest as the most dominant figure in a given frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Coarse-grained atomic action datasets label different actions under a single English action verb. HAA500 (Bottom) has fine-grained classes where the action ambiguities are eliminated as much as possible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Summary of representative action recognition datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Summary of HAA500</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table /><note>Clip length and irrelevent frames of video action datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>RGB 66.01% 56.86% 75.82% 77.12% I3D-Flow 73.20% 77.78% 75.16% 74.51% 2-Stream 77.78% 80.39% 83.01% 80.39% Accuracy improvements on person-of-interest normalization. Numbers indicate the composite action detection accuracy.</figDesc><table><row><cell>Original</cell><cell>Normalized</cell></row><row><cell cols="2">Composite Both Composite Both</cell></row><row><cell>I3D-</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE International Conference on Computer Vision (ICCV 2005)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liat</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaver</surname></persName>
		</author>
		<imprint>
			<pubPlace>Rebecca Marvin, Caroline Pantofaru, Nathan Reale, Loretta Guarino Reid, Kevin W</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ava-speech: A densely labeled dataset of speech activity in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bernard Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1812.03982</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset for human motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaroop</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><forename type="middle">E</forename><surname>Reiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamın</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Béjar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Miccai workshop: M2cai</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The&quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision (ICCV 2013)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">STM: spatiotemporal and motion encoding for action recognition. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ferdous Sohel, and Farid Boussaid. A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senjian</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE conference on computer vision and pattern recognition workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomaso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Computer Vision (ICCV 2011)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="513" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 European Conference on Computer Vision (ECCV 2016)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<idno>abs/1801.03150</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action MACH a spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2008)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing fine-grained and composite activities using hand-centric features and script data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="373" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ava active speaker: An audio-visual dataset for active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liat</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharadh</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Xi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local SVM approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schüldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 IEEE International Conference on Pattern Recognition (CVPR 2004)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Finegym: A hierarchical video dataset for fine-grained action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS 2014)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Towards weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno>abs/1605.05197</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Discriminative subvolume search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hacs: Human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09374</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
