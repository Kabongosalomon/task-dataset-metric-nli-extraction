<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A person is commonly described by attributes like height, build, cloth color, cloth type, and gender. Such attributes are known as soft biometrics. They bridge the semantic gap between human description and person retrieval in surveillance video. The paper proposes a deep learning-based linear filtering approach for person retrieval using height, cloth color, and gender. The proposed approach uses Mask R-CNN for pixel-wise person segmentation. It removes background clutter and provides precise boundary around the person. Color and gender models are fine-tuned using AlexNet and the algorithm is tested on SoftBioSearch dataset. It achieves good accuracy for person retrieval using the semantic query in challenging conditions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Surveillance systems are deployed at many places for security. Commonly, person retrieval is done by manually searching through videos. Its primary goal is to localize the person(s) using semantic queries e.g., a tall male with a black t-shirt and blue jeans. Such descriptions are easy to understand and commonly used by an eye-witness to describe the person. However, such description cannot be fed to an automatic person retrieval system and they should be converted to a compatible representation.</p><p>The task of person retrieval in the video is very challenging due to occlusion, light condition, camera quality, pose, and zoom. However, attributes like height, cloth color, gender can be deduced from low-quality surveillance video at a distance without cooperation from the subject. Such attributes are known as soft biometrics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">21]</ref>. A single soft biometric cannot uniquely identify the individual. Therefore, it is important to find the most discriminative soft biometrics for person retrieval. A study identifies 13 commonly used soft biometric attributes <ref type="bibr" target="#b1">[2]</ref>.</p><p>Person retrieval using semantic description has been widely studied in recent years. Jain et al. <ref type="bibr" target="#b2">[3]</ref> propose the integration of ethnicity, gender and height to improve performance of the traditional biometric system. Park et al. <ref type="bibr" target="#b3">[4]</ref> develops visual search engine using dominant color, build and height to find the person. Techniques in <ref type="bibr">[5 -8]</ref> use soft biometrics for person re-identification which aims at locating a person of interest in camera network. The appearance-based model proposal by Farenzena et al. <ref type="bibr" target="#b4">[5]</ref> uses three complementary aspects of the human appearance; the overall chromatic content, the spatial arrangement of colors into stable regions, and the presence of recurrent textures. Bazzani et al. <ref type="bibr" target="#b5">[6]</ref> suggest an appearance-based model that incorporates a global and local statistical feature in the form of an HSV histogram and epitomic analysis respectively. However, techniques <ref type="bibr">[5 -8]</ref> are unsuitable for automatic identification and retrieval.</p><p>Description based person retrieval <ref type="bibr">[9 -11]</ref> uses an avatar generated from a semantic query. The avatar incorporates height and cloth color and search the frame using particle filter <ref type="bibr" target="#b8">[9]</ref>. Denman et al. <ref type="bibr" target="#b9">[10]</ref> generate a search query in the form of channel representation using height, dominant color (torso and leg), and clothing type (torso and leg).</p><p>Convolutional Neural Network (CNN) based approaches <ref type="bibr">[12 -14]</ref> are becoming popular for person attribute recognition. Multi-label convolutional neural network (MLCNN) <ref type="bibr" target="#b11">[12]</ref> predicts gender, age, and clothing together with pedestrian attributes. Person's full body image is split into 15 overlapping 32Ã—32 sized parts which are filtered and combined in the cost layer. Dangwei Li et al. <ref type="bibr" target="#b14">[14]</ref> studies the limitation of handcrafted features (e.g., color histograms) and focuses on the relationship between different attributes. They propose deep learning based single attribute recognition model (DeepSAR) to independently identify the attribute. Another model (DeepMAR) exploits the relationship between the attributes to jointly recognize multiple attributes.</p><p>The approaches based on avatar <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, channel representation <ref type="bibr" target="#b10">[11]</ref> and CNN <ref type="bibr">[12 -14]</ref> considers bounding box around the person as potential area. This creates the following problems:</p><p>1. The bounding box with cluttered background may impact the person retrieval accuracy specifically for low-resolution video, occlusion, and multiple views. This paper proposes a deep learning-based approach. It uses height, cloth color, and gender as linear filters for person retrieval. Such descriptors are chosen due to their discriminative ability and they also commonly occur in the investigation report. For example, height is view and distance invariant. As discussed in <ref type="bibr" target="#b15">[15]</ref>, the color prediction is also invariant to direction and view angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person Retrieval in Surveillance Video using Height, Color and Gender</head><p>The height of the person is estimated using given camera calibration parameters while cloth color and gender are detected using CNN. Main contributions of this work are:</p><p>1. Use of semantic segmentation (pixel level segmentation) <ref type="bibr" target="#b16">[16]</ref> to detect a person which has the following advantages: a. It removes unwanted background clutter. b. The precision of the segmented boundary yields accurate head and feet points. This results in a better estimate of the real-world height. c. It extracts precise patch for torso color classification. 2. The height can also be used to discriminate between the upright and sitting position of the person. This reduces search space for the person of interest in the upright position. 3. The paper proposes a generalized framework to deal with low resolution, view and pose variance.</p><p>The remainder of the paper is organized as follows. Section 2 describes models used for training data and fusion of various soft biometrics attributes to localize the person. Section 3 discusses the test results and the accuracy of the approach. Section 4 concludes the paper and discusses future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed approach</head><p>This section introduces person retrieval based on height, cloth color, and gender. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates a flow diagram of the proposed framework. Each frame of video is given to state-of-the-art Mask R-CNN <ref type="bibr" target="#b16">[16]</ref> for detection and pixel-wise segmentation of person(s). Head and feet points are extracted for all segmented persons. Using camera calibration parameters their respective height is computed which is compared with height given in the semantic query. Thus, height acts as a filter to reduce the number of persons in the frame. In case of multiple matches, further filtering is done using torso color. The use of semantic segmentation allows background free extraction of the color patch from the torso. The number of subjects is further reduced by matching the semantic query color with the classified color of extracted patches. The exactness of the final output is improved by using gender classification. Next subsections describe the process of height, color and gender estimation for person retrieval.</p><p>Height estimation: Person height is view-invariant and it also helps to discriminate between upright and sitting position of the person. Height is estimated using Tsai camera calibration approach <ref type="bibr" target="#b20">[20]</ref>, which translates the image coordinate to real-world coordinate. SoftBioSearch dataset <ref type="bibr" target="#b9">[10]</ref> provides 6 calibrated cameras for calculation of real-world coordinates. Person's head and feet points are estimated from the semantic segmentation ( <ref type="figure" target="#fig_0">Figure 1</ref> During training, annotated head and feet points are used to compute height from all frames of the video sequence. The height is then averaged (Havg) over all frames. Over the same training sequence, it was observed that average height computed from automated head and feet point is larger than Havg. This difference yields the wrong estimation, therefore; it is subtracted from average estimated height during testing to normalize the error.</p><p>Torso color prediction: Mask R-CNN generates a class label with probability score and semantic segmentation. The torso and leg regions are extracted using the golden ratio for height. The upper 20%-50% portion is classified as the torso, while the lower 50%-100% represents the legs of the person. The torso and leg segmentation are shown in <ref type="figure">Figure 2</ref>. Semantic segmentation is used to obtain the torso region without background to improve color classification. Color patch for prediction is extracted from region marked by points 'a', 'b', 'c', and 'd' (ref. <ref type="figure" target="#fig_0">Figure 1 -color filter)</ref>. The extracted color patch is used to fine-tune AlexNet <ref type="bibr" target="#b18">[18]</ref>, to predict the probability score. The dataset <ref type="bibr" target="#b9">[10]</ref> annotations contain two colors Torso Color and Torso Second Color for each subject. In case of multiple matches due to Torso Color, the algorithm will refine the result using Torso Second Color if present. This feature helps to further narrow down the search space.</p><p>Gender classification: The proposed algorithm accurately retrieves the person for most cases using height and cloth color. But, in the case of multiple matches, the algorithm uses gender classification. The AlexNet is fine-tuned using full body images for male and female gender classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Implementation details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">AlexNet training</head><p>The training is accomplished on a workstation with Intel Xeon core processor and accelerated by NVIDIA Quadro K5200 of 8 GB GPU. Color and gender models are fine-tuned using AlexNet which is pretrained on ImageNet <ref type="bibr" target="#b19">[19]</ref> dataset. with illumination changes, these patches are augmented by increasing brightness with a gamma factor of 1.5. Approximately 17000 color patches are generated and further divided into 80% for training and 20% for the validation set. The last four layers of AlexNet namely (Conv5, fc6, fc7, and fc8) are fine-tuned for color training. It is trained for 30 epochs with a learning rate of 0.001, dropout set to 0.50 and effective batch size of 128. Above hyper-parameters led to an accuracy of 71.2% on the validation set.</p><p>Gender training: The data augmentation generated 105980 images for gender training which is approximately 13 times larger than the original training set (8577). The validation set has 20% of the total images. Gender training is accomplished by fine-tuning the last 3 layers (fc6, fc7, and fc8) of AlexNet. The model is fine-tuned for 20 epochs with learning rate 0.0001, batch size 64 images and a dropout rate of 0.40. This results in the accuracy of 68% on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results and discussion</head><p>This section covers the qualitative and quantitative experimental results derived on test data set of 41 subjects. The ground truth (i.e., a person of interest) is established by manually mapping semantic test query to video frames of the test set. The ground truth is established after the first 30 initialization frames of each subject. <ref type="figure">Figure 3</ref> shows true positive cases of the person retrieval using semantic queries. Abbreviations in the caption are interpreted as follows; e.g., Seq.10, F.59 indicates sequence number 10 with frame number 59 in the test set. In <ref type="figure">Figure 3</ref>, images from left to right indicate the input test frame, the output of the height filter, the output of color filter and gender filter respectively. <ref type="figure">Figure 3</ref>   of Seq.4, F.76 with semantic query height (160 -180 cm), torso color (pink) and gender (female) is shown in <ref type="figure">Figure 3</ref>(b). It can be observed that multiple persons are detected with the same height class (ref. middle image in 3(b)). The correct person of interest is retrieved by adding torso color query to the height. <ref type="figure">Figure 3</ref>(c) shows a person of Seq.8, F.31 with semantic query height (130 -160 cm), torso color (pink) and gender (female) in which algorithm utilizes all 3 semantic attributes i.e., height, torso color, and gender to retrieve person of interest. Thus, algorithm retrieves a person with a rank-1 match by utilizing a minimum number of attributes and in case of multiple matches, it uses additional soft biometrics to retrieve the person. <ref type="figure">Figure 4</ref> shows the results when the algorithm fails to retrieve the person correctly. It also indicates challenging conditions in the test dataset. <ref type="figure">Figure 4(a)</ref> shows a person of Seq.16, F.31 with semantic query height (160 -180 cm), torso color (pink) and gender (female). It shows incorrect torso color classification with pink color classified as black (Torso Color) and brown (Torso Second color). It could be due to the presence of brown hair at the back side of the person. <ref type="figure">Figure 4</ref>(b) contains occlusion due to multiple persons. As a result, Mask R-CNN creates a single bounding box for Seq.25, F.34 with semantic query height (150 -170 cm), torso color (green) and gender (female). Algorithm fails to retrieve person uniquely when multiple persons with the same height, torso color and gender are present, e.g., Seq.18, F.31 with semantic query height (180 -210 cm), torso color (white) and gender (male) (Ref. <ref type="figure">Figure 4(c)</ref>). <ref type="figure">Figure 4</ref>(d) (Seq.1, F.73) shows the scenario where the person of interest is merged with a black background (except black and white torso region) due to poor illumination. Mask R-CNN fails to detect the person for such noisy scenarios. Person retrieval can be improved by incorporating additional attributes in the retrieval process. E.g., <ref type="figure" target="#fig_6">Figure 5</ref> shows the improved result (Ref. <ref type="figure">Figure 4(c)</ref>) where the person is correctly retrieved using the leg color.</p><p>In the proposed approach, each attribute is a filter which reduces the search space for person retrieval. The use of a linear filter for attribute classification has a potential weakness. The classification error due to first filter (height) will propagate to subsequent filters (color and gender).</p><p>True Positive (TP) rate is used as a qualitative measure of accuracy for a testing dataset of 41 persons. It is computed as follows:</p><formula xml:id="formula_0">% . . 1</formula><p>The person localization accuracy for all frames is evaluated by computing an intersection-over-union (IoU) given by Eq. 2. Ground truth bounding box is constructed using the top of the head, the lowest of the feet, and the two most extreme locations from either feet, shoulder, waist or neck annotations.</p><formula xml:id="formula_1">D âˆ© GT D âˆª GT 2</formula><p>where, D is the bounding box due to algorithm and GT is ground truth bounding box.</p><p>The algorithm correctly retrieves 28 out of 41 persons with varying TP rate. <ref type="figure" target="#fig_7">Figure 6</ref> shows the TP rate (%) as well as IoU (Y-axis) and index of a correctly retrieved person (X-axis). Average TP rate of the algorithm is 65.8% and the resulting average IoU for correct retrieval is 0.458. Among 28 persons, 19 are retrieved with TP rate greater than 60%, 5 with TP rate between 30% -60% and 4 persons with TP rate between 0% -30%. For 19 persons (TP rate &gt; 60%), height and color model works extremely well. The TP rate and IoU are very poor for some sequences. For example, in Seq.39 torso color (brown) is incorrectly classified which results in a TP rate of 8.32% and corresponding IoU as 0.101. This is due to a low number of color patches used in training and therefore, the model is unable to classify true color. The color classification can be improved by adding more color patches. In many frames, a person of interest is occluded e.g., Seq. 25 and semantic segmentation could not extract a precise boundary around the person. Similarly, in the Seq.1 person of interest is merged with background due to poor illumination resulting in poor detection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>The proposed approach retrieves the person in surveillance video using a semantic query based on height, cloth color, and gender. Use of semantic segmentation allows better height estimation and precise color patch extraction from the torso. The algorithm correctly recovers 28 persons out of 41 in a very challenging dataset with soft biometric attributes. The algorithm achieves an average IoU of 0.36 on the test dataset. It achieves an IoU of greater than or equal to 0.4 for 52.2% of frames. Future work will focus on improving results by incorporating other soft biometric attributes (e.g., leg color, torso texture, body accessory) and investigate mechanism to enhance robustness of the proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Proposed approach of person retrieval using height, cloth color and gender.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>This section covers details about the dataset, data augmentation, and AlexNet training. The proposed approach uses the weights of Mask R-CNN pretrained on Microsoft COCO [17] for detection and semantic segmentation. It has the Average Precision (AP) of 35.7 on COCO test set. Dataset overview: This paper uses the SoftBioSearch database [10] which includes 110 unconstrained training video sequences, recorded using 6 stationary calibrated cameras. Each of the sequences contains 16 annotated soft biometric attributes and 9 body markers for subject localization. The 9 body markers are top of the head, left and right neck, left and right shoulder, left and right waist, approximate toe position of the feet. The test dataset contains video sequences of 41 subjects with the semantic query. The sequence length has 21 to 290 frames for training subjects. Discarding the frames with partial occlusion, the resulting training set has 8577 images from 110 subjects. Data augmentation: It is a practice in deep learning to augment the training samples for improvement in performance and robustness. E.g., training AlexNet with 8577 images may result into over-fitting, which is avoided using data augmentation. Each training image is horizontally and vertically flipped, rotated with 10 angles {1 o , 2 o , 3 o , 4 o , 5 o , -1 o , -2 o , -3 o , -4 o , -5 o } and brightness increased with gamma factor of 1.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>20 %Figure 2 :</head><label>202</label><figDesc>Color training: The SoftBioSearch [10] database contains 1704 color patches divided into 12 culture colors. Additional patches for color training are extracted using 4 body markers namely left and right shoulder, left and right waist from the training dataset. In order to deal 50 % Extraction of torso and leg region from body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) shows a person of Seq.10, F.59 with semantic query height (170 -190 cm), torso color (black) and gender (male). A person is retrieved using only a single soft biometric attribute i.e., height. Person (a) Seq.10, F.59: height (170 -190 cm), torso color (black) and gender (male). Person retrieved using only height. (b) Seq.04, F.76: height (160 -180 cm), torso color (pink) and gender (female). Person retrieved using height and torso color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(c) Seq.8, F.31: height (130 -160 cm), torso color (pink) and gender (female). Person retrieved using height, torso color and gender.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>True positive cases of person retrieval with semantic query. False negative cases of person retrieval. (a) incorrect color classification with multiple persons, (b) multiple persons with occlusion, (c) multiple persons with same torso color and height class and (d) person detection fails.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Use of leg color for improving retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>True Positive (TP) rate and IoU for the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Estimated height helps in reducing the search space within the test frame based on the description (e.g., 150 -170 cm). Test frame would now contain only the person(s) which matches height description.</figDesc><table><row><cell></cell><cell cols="3">). Steps for height estimation are</cell></row><row><cell>as follows:</cell><cell></cell><cell></cell></row><row><cell cols="4">1. Intrinsic parameters matrix (k), rotation matrix (R)</cell></row><row><cell cols="4">and a translation vector (t) are calculated from given</cell></row><row><cell cols="2">calibration parameters.</cell><cell></cell></row><row><cell cols="4">2. The perspective transformation matrix is calculated</cell></row><row><cell>as</cell><cell>| .</cell><cell></cell></row><row><cell cols="4">3. Head and feet points are undistorted using radial</cell></row><row><cell cols="2">distortion parameters.</cell><cell></cell></row><row><cell cols="2">4. World coordinate for feet is set as</cell><cell>0</cell><cell>,</cell></row><row><cell cols="4">world coordinates are derived using inverse</cell></row><row><cell cols="2">transformation of C.</cell><cell></cell></row><row><cell>5. Use ,</cell><cell cols="3">coordinate (of step-4) to calculate Z</cell></row><row><cell cols="4">coordinate of the head which also represent height.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors would like to thank the Board of Research in Nuclear Sciences (BRNS) for a generous grant (36(3)/14/20/2016-BRNS/36020). We acknowledge the support of NVIDIA Corporation for a donation of the Quadro K5200 GPU used for this research. We would also like to thank the SAIVT Research Labs at Queensland University of Technology (QUT) for SAIVT-SoftBioSearch database.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What Else Does Your Biometric Data Reveal? A Survey on Soft Biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Elia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="467" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adult eyewitness testimony: Current trends and developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Frowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Shepherd</surname></persName>
		</author>
		<editor>D. F. Ross, J. D. Read, &amp; M. P. Toglia</editor>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="125" to="143" />
			<pubPlace>New York, US</pubPlace>
		</imprint>
	</monogr>
	<note>Whole body information: Its relevance to eyewitnesses</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Soft biometric traits for personal recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Dass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nandakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometric Authentication</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="731" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vise: Visual search engine using multiple networked cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kitahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kogure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hagita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18 th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1204" to="1207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple-shot person re-identification by HPE signature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20 th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1413" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person reidentification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human reidentification by matching compositional template with cluster sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3152" to="3159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Can you describe him for me? A technique for semantic person search in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halstead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bialkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Locating people in video from semantic descriptions: a new database and approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halstead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4501" to="4506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Searching for people using semantic soft biometric descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halstead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="306" to="315" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-label CNN Based Pedestrian Attribute Learning for Soft Biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="535" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sudowe</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person Attribute Recognition with a Jointly-Trained Holistic CNN Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="329" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-attribute Learning for Pedestrian Attribute Recognition in Surveillance Scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="111" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Description Based Person Identification: Use of Clothes Color and Type</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Raval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galiyawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)</title>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
	<note>IIT Mandi</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014-09-06" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06-20" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A versatile camera calibration technique for high-accuracy 3D machine vision metrology using offthe-shelf TV cameras and lenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="323" to="344" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Digital Video Forensics: Description Based Person Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CSI Communications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9" to="11" />
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
