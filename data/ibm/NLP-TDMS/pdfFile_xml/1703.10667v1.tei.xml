<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for Activity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
							<email>cyma@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
							<email>zkira@gatech.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Georgia Tech Research Institution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
							<email>alregib@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for Activity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent two-stream deep Convolutional Neural Networks (ConvNets) have made significant progress in recognizing human actions in videos. Despite their success, methods extending the basic two-stream ConvNet have not systematically explored possible network architectures to further exploit spatiotemporal dynamics within video sequences. Further, such networks often use different baseline two-stream networks. Therefore, the differences and the distinguishing factors between various methods using Recurrent Neural Networks (RNN) or convolutional networks on temporallyconstructed feature vectors (Temporal-ConvNet) are unclear. In this work, we first demonstrate a strong baseline two-stream ConvNet using ResNet-101. We use this baseline to thoroughly examine the use of both RNNs and Temporal-ConvNets for extracting spatiotemporal information. Building upon our experimental results, we then propose and investigate two different networks to further integrate spatiotemporal information: 1) temporal segment RNN and 2) Inception-style Temporal-ConvNet. We demonstrate that using both RNNs (using LSTMs) and Temporal-ConvNets on spatiotemporal feature matrices are able to exploit spatiotemporal dynamics to improve the overall performance. However, each of these methods require proper care to achieve state-of-the-art performance; for example, LSTMs require pre-segmented data or else they cannot fully exploit temporal information. Our analysis identifies specific limitations for each method that could form the basis of future work. Our experimental results on UCF101 and HMDB51 datasets achieve state-of-the-art performances, 94.1% and 69.0%, respectively, without requiring extensive temporal augmentation. * equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human action recognition is a challenging task and has been researched for years. Compared to singleimage recognition, the temporal correlations between image frames of a video provide additional motion information for recognition. At the same time, the task is much more computationally demanding since each video contains hundreds of image frames that need to be processed individually.</p><p>Encouraged by the success of using Convolutional Neural Networks (ConvNets) on still images, many researchers have developed similar methods for video understanding and human action recognition <ref type="bibr" target="#b9">[13,</ref><ref type="bibr" target="#b23">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">30,</ref><ref type="bibr">16,</ref><ref type="bibr" target="#b15">20]</ref>. Most of the recent works were inspired by two-stream ConvNets proposed by Simonyan et al. <ref type="bibr" target="#b9">[13]</ref>, which incorporate spatial and temporal information extracted from RGB and optical flow images. These two image types are fed into two separate networks, and finally the prediction score from each of the streams are fused.</p><p>However, traditional two-stream ConvNets are unable to exploit the most critical component in action recognition, e.g. visual appearance across both spatial and temporal streams and their correlations are not considered. Several works have explored the stronger use of spatiotemporal information, typically by taking frame-level features and integrating them using Long short-term memory (LSTM) cells, temporal feature pooling <ref type="bibr" target="#b23">[28,</ref><ref type="bibr" target="#b5">6]</ref> and temporal segments <ref type="bibr" target="#b20">[25]</ref>. However, these works typically try individual methods with little analysis of whether and how they can successfully use temporal information. Furthermore, each individual work uses different networks for the baseline two-stream approach, with varied performance depending on training and testing procedure as well as the optical flow method used. Therefore it is unclear how much improvement resulted from a different use of temporal information.</p><p>In this paper, we would like to answer the question: given the spatial and motion features representations over  time , what is the best way to exploit the temporal information? We thoroughly evaluate design choices with respect to a baseline two-stream ConvNet and two proposed methods for exploiting spatiotemporal information. We show that both can achieve high accuracy individually, and our approach using LSTMs with temporal segments improves upon the current state of the art. In this work, we aim to investigate several different ways to model the temporal dynamics of activities with feature representations from image appearance and motion. We encode the spatial and temporal features via a highdimensional feature space, examining various training practices for developing two-stream ConvNets for action recognition. Using this and other related work as a baseline, we then make the following contributions:</p><p>1. Temporal Segment LSTM (TS-LSTM): we revisit the use of LSTMs to fuse high-level spatial and temporal features to learn hidden features across time. We adapt the temporal segment method by Wang et al. <ref type="bibr" target="#b20">[25]</ref> and exploit it with LSTM cells. We show that directly using LSTM performed only similar to naive temporal pooling methods, e.g. mean or max pooling, and the integration of both yields better performance.</p><p>2. Temporal-ConvNet: We propose to use stacked temporal convolution kernels to explore temporal information at multiple scales. The proposed architecture can be extended to an Inception-style Temporal-ConvNet. We show that by properly exploiting the temporal information, Temporal-Inception can achieve state-ofthe-art performance even when taking feature vectors as inputs (i.e. without using feature maps).</p><p>By thoroughly exploring the space of architectural designs within each method, we clarify the contribution of each decision and highlight their implication in terms of current limitations of methods such as LSTMs to fully exploit temporal information without manipulating its inputs. Our approaches are implemented using Torch7 <ref type="bibr" target="#b2">[3]</ref> and are publicly available at GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Many of the action recognition methods extract highdimensional features that can be used within a classifier. These features can be hand-crafted <ref type="bibr" target="#b16">[21,</ref><ref type="bibr" target="#b17">22,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b18">23]</ref> or learned, and in many instances, frame-level features are then combined in some form.</p><p>3D ConvNets. The early work from Karpathy et al. <ref type="bibr" target="#b8">[9]</ref> stacked consecutive video frames and extended the first convolutional layer to learn the spatiotemporal features while exploring different fusion approaches, including early fusion and slow fusion. Another proposed method, C3D <ref type="bibr" target="#b15">[20]</ref>, took this idea one step further by replacing all of the 2D convolutional kernels with 3D kernels at the expense of GPU memory. To avoid high complexity when training 3D convolutional kernels, Sun et al. <ref type="bibr">[16]</ref> factorize the original 3D kernels into 2D spatial and 1D temporal kernels and achieve comparable performance. Instead of using only one layer like [16], we demonstrate that multiple layers can extract temporal correlations at different time scales and provide better capability to distinguish different types of actions.</p><p>ConvNets with RNNs. Instead of integrating temporal information via 3D convolutional kernels, Donahue et al. <ref type="bibr" target="#b3">[4]</ref> fed spatial features extracted from each time step to a recurrent network with LSTM cells. In contrast to the traditional models which can only take a fixed number of temporal inputs and have limited spatiotemporal receptive fields, the proposed Long-term Recurrent Convolutional Networks (LRCN) can directly take variable length inputs and learn long-term dependencies.</p><p>Two-stream ConvNets. Another branch of research in action recognition extracts temporal information from traditional optical flow images . This approach was pioneered by <ref type="bibr" target="#b9">[13]</ref>. The proposed two-stream ConvNets demonstrated that the stacked optical flow images solely can achieve comparable performance despite the limited training data. Currently, the two-stream ConvNet is the most popular and effective approach for action recognition. A few works have proposed to extend the approach. Ng et al. <ref type="bibr" target="#b23">[28]</ref> take advantage of both two-stream ConvNets and LRCN, in which not only the spatial features are fed into the LSTM but also the temporal features from optical flow images. Our work shows that combining a ConvNet with vanilla LSTM results in limited performance improvement when there are not many temporal variances. Feichtenhofer et al. <ref type="bibr" target="#b5">[6]</ref> proposed to fuse the spatiotemporal streams via 3D convolutional kernels and 3D pooling. Our proposed Temporal-ConvNet is different because we only use the feature vector representations instead of feature maps. We also show that by properly leveraging temporal information, our proposed Temporal-Inception can achieve state-of-the-art results only using feature vector representations.</p><p>Similar to the above works, many other approaches exist. Wang et al. <ref type="bibr" target="#b20">[25]</ref> proposed the temporal segment network (TSN), which divides the input video into several segments and extracts two-stream features from randomly selected snippets. <ref type="bibr" target="#b22">[27]</ref> incorporate semantic information into the two-stream ConvNets. Zhu et al. <ref type="bibr" target="#b26">[31]</ref> propose a key volume mining deep framework to identify key volumes that are associated with discriminative actions. Wang et al. <ref type="bibr" target="#b21">[26]</ref> proposed to use a two-stream Siamese network to model the transformation of the state of the environment.</p><p>In this work, we aim to create a common and strong baseline two-stream ConvNet and extend the two-stream Con-vNet to provide in-depth analysis of design decisions for both RNN and Temporal-ConvNet. We demonstrate that both methods can achieve state of the art but proper care must be given. For instance, LSTMs require pre-segmented data or else they cannot fully exploit temporal information. Our analysis identifies specific limitations for each method that could form the basis of future work.</p><p>In the following sections, we first discuss our proposed approach in section 3. In each of the subsections, the intuition and reasoning behind our approaches will be elaborated upon. Finally, we describe experimental results and analysis that validates our approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Overview. We build on the traditional two-stream ConvNet and explore the correlations between spatial and temporal streams by using two different proposed fusion frameworks. We specifically focus on two models that can be used to process temporal data: Temporal Segment LSTMs (TS-LSTM) which leverage recurrent networks and convolution over temporally-constructed feature matrices (Temporal-ConvNet). Both methods achieve state of art performance, and the TS-LSTM surpasses existing methods. However, the details of the architectures matter significantly, and we show that the methods only work when used in particular ways. We perform significant experimentation to elucidate which design decisions are important. <ref type="figure" target="#fig_0">Figure 1</ref> schematically illustrates our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Two-stream ConvNets</head><p>The two-stream ConvNet is constructed by two individual spatial-stream and temporal-stream ConvNets. The spatial-stream network takes RGB images as input, while the temporal-stream network takes stacked optical flow images as inputs. A great deal of literature has shown that using deeper ConvNets can improve overall performance for two-stream methods. In particular, the performance from VGG-16 <ref type="bibr" target="#b10">[14]</ref>, GoogLeNet <ref type="bibr" target="#b13">[18]</ref>, and BN-Inception <ref type="bibr" target="#b7">[8]</ref> on both spatial and temporal streams are reported <ref type="bibr" target="#b20">[25,</ref><ref type="bibr" target="#b23">28,</ref><ref type="bibr" target="#b20">25]</ref>. Since ResNet-101 <ref type="bibr" target="#b6">[7]</ref> has demonstrated its capability in capturing still image features, we chose ResNet-101 as our ConvNet for both the spatial and temporal streams. We demonstrate that this can result in a strong baseline. Feichtenhofer et al. <ref type="bibr" target="#b5">[6]</ref> experiment with different fusion stages for the spatial and temporal ConvNets. Their results indicate that fusion can achieve the best performance using late fusions. Fusion conducted at early layers results in lower performance, though require less number of parameters. For this reason, we aim at exploring feature fusion using the last layer from both spatial-stream and temporal-stream Con-vNets. In our framework, the two-stream ResNets serve as high-dimensional feature extractors. The output feature vectors at time step t from the spatial-stream and temporalstream ConvNets can be represented as f S t ∈ R n S and f T t ∈ R n T , respectively. The input feature vector x t ∈ R n S +n T for our proposed temporal segment LSTM and Temporal-ConvNet is the concatenation of f S t and f T t . In our case, n S and n T are both 2048.</p><p>Spatial stream. Using a single RGB image for the spatial stream has been shown to achieve fairly good performance. Experimenting with the stacking of RGB difference is beyond the scope of this work, but can potentially improve performance <ref type="bibr" target="#b20">[25]</ref>. The ResNet-101 spatial-stream ConvNet is pre-trained on ImageNet and fine-tuned on RGB images extracted from UCF101 dataset with classification loss for predicting activities.</p><p>Temporal stream. Stacking 10 optical flow images for the temporal stream has been considered as a standard for two-stream ConvNets <ref type="bibr" target="#b9">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">28,</ref><ref type="bibr" target="#b20">25,</ref><ref type="bibr" target="#b22">27]</ref>. We follow the standard to show how each of our framework design and training practices can improve the classification accuracy. In particular, using a pre-trained network and fine-tuning has been confirmed to be extremely helpful despite differences in the data distributions between RGB and optical flow. We follow the same pre-train procedure shown by Wang et al. <ref type="bibr" target="#b20">[25]</ref>. The effectiveness of the pre-trained model on temporal-stream ConvNet is in <ref type="table" target="#tab_1">Table 1</ref> in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Segment LSTM (TS-LSTM)</head><p>The variations between each of the image frames within a video may contain additional information that could be useful in determining the human action in the whole video. One of the most straightforward ways to incorporate and exploit sequences of inputs using neural networks is through a Recurrent Neural Network (RNN). RNNs can learn temporal dynamics from a sequence of inputs by mapping the inputs to hidden states, and from hidden states to outputs. The objective of using RNNs is to learn how the representations change over time for activities. However, several previous works have shown limited ability of directly using a ConvNet and RNN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">28,</ref><ref type="bibr">11,</ref><ref type="bibr" target="#b0">1]</ref>. We therefore adapted temporal segments <ref type="bibr" target="#b20">[25]</ref> for use with RNNs and provide segmental consensus via temporal pooling and LSTM cells. In our work, the input of the RNN in each time step t is a highlevel feature representation x t ∈ R 4096 .</p><p>LSTM cells have been adapted for the action recognition problem, but so far have shown limited improvement <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">28]</ref>. In particular, Ng et al. <ref type="bibr" target="#b23">[28]</ref> use five stacked LSTM layers each with 512 memory cells. We argue that deeper LSTM layers do not necessarily help in achieving better action recognition performance, since the baseline two-stream ConvNet has already learned to become more robust in effectively representing frame-level images (see section 4.2).</p><p>Temporal segments and pooling. In addressing the limited improvement of using LSTM, we follow the intuition of the temporal segment by Wang et al. <ref type="bibr" target="#b20">[25]</ref> and divide the sampled video frames into several segments. A temporal pooling layer is applied to extract distinguishing features from each of the segments, and an LSTM layer is used to extract the embedded features from all segments. The proposed TS-LSTM is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>First, the concatenated spatial and temporal features will be pooled through temporal pooling layers, e.g. mean or max pooling. In practice, the temporal mean pooling performed similarly with max pooling. We use max pooling in our experiments. The temporal pooling layer takes the feature vectors concatenated from spatial and temporal streams and extracts distinguishing feature elements. The recur- rent LSTM module then learns the final embedded features for the entire video. The proposed TS-LSTM module essentially serves as a mechanism that learns the non-linear feature combination and its segmental representation over time. We discuss implications of the success of TS-LSTMs in the sections describing experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal-ConvNet</head><p>Spatiotemporal correlations. One of the main drawbacks of baseline two-stream ConvNets is that the network only learns the spatial correlation within a single frame instead of leveraging the temporal relation across different frames. To address this issue, [16, 6] adopted the concept of 3D kernels to exploit the pixel-wise correlation between spatial and temporal feature maps. However, both approaches applied convolution kernels of one scale to extract the temporal features with fixed temporal receptive fields, and they did so on the full feature maps which results in more than 40 times the parameters compared to using feature vectors (see supplementary material). In contrast to these approaches, we focus on designing a more efficient architecture with multiple convolution layers to explore the temporal information only using feature vectors.</p><p>In addition to using RNNs to learn the temporal dynamics, we adapt the ConvNet architecture on feature matrices</p><formula xml:id="formula_0">x = {x 1 , x 2 , ..., x t , ..., x N } ∈ R 4096×N ,</formula><p>where N is the number of sampled frames in one video. Different from natural images, the elements in each x t have little spatial dependency, but have temporal correlation across different x t . With the ConvNet architecture, we explore the temporal correlation of the feature elements, and then distinguish different categories of actions.</p><p>The overall architecture of the Temporal-ConvNet is composed of multiple Temporal-ConvNet layers (TCLs), as shown in <ref type="figure" target="#fig_2">Figure 3</ref>, and can be formulated as follows:</p><formula xml:id="formula_1">T emConv(x) = H(G(F (F (F (F (x, W 1 ), W 2 ), W 3 ), W 4 ))) where F (x, W i ) = (T CL 1 (x; W i1 ), T CL 2 (x; W i2 ))</formula><p>G maps the output of the final TCL to a 1024-dimension vector. H is the Softmax function. W i j denotes the parameter set used for TCL, where i denotes layer number and j represents the index of TCL in a multi-flow module. Temporal-Inception model. After obtaining the feature matrix x, we apply 1D kernels to specifically encode temporal information in different scales and reduce the temporal dimension because the spatial information is already encoded from the two-stream ConvNet. Applying convolution along the spatial direction may alter the learned spatial characteristics. In addition, we adapt the inception module <ref type="bibr" target="#b13">[18]</ref> into our architecture and note it as the multi-flow module, which consists of different convolution kernel sizes, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Multiple multi-flow modules are used to hierarchically encode temporal information and reduce the temporal dimension. However, the filter dimension gradually increases because of the concatenation in each multiflow module. To avoid potential overfitting issues, we convolve the concatenated feature vectors with a set of filters</p><formula xml:id="formula_2">f i ∈ R 1×1×Di1×Di2 , D i2 &lt; D i1</formula><p>, to reduce the filter dimension. We note this architecture as Temporal-Inception in this paper. The rationale behind Temporal-Inception is that different types of action have different temporal characteristics, and different kernels in different layers essentially search for different actions by exploiting different receptive fields to encode the temporal characteristics.</p><p>The current state-of-the-art method from Wang et al. <ref type="bibr" target="#b20">[25]</ref> and our Temporal-Inception both explore the temporal information but with different perspectives and can be complementary. <ref type="bibr" target="#b20">[25]</ref> focuses on designing novel and effective sampling approaches for temporal information exploration while we focus on designing the architecture to extract the temporal convolutional features given the sampled frames.</p><p>3.4. Implementation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Training and testing practice</head><p>Two-stream inputs. We use both RGB and optical flow images as inputs to two-stream ConvNets. For generating the optical flow images, literature have different choices for optical flow algorithms. Although most of the works used either Brox <ref type="bibr" target="#b1">[2]</ref> or TV-L1 <ref type="bibr" target="#b24">[29]</ref>, within each of the different optical flow algorithms there are still some variations in how the optical flow images are thresholded and normalized. We summarize the prediction accuracy of different optical flow methods from recent works in <ref type="table" target="#tab_1">Table 1</ref>. Note that we thresholded the absolute value of motion magnitude to 20 and rescale the horizontal and vertical components of the optical flow to the range [0, 255] for TV-L1. From <ref type="table" target="#tab_1">Table  1</ref>, we can conclude that both Brox and TV-L1 can achieve state-of-the-art performance, but from our experiments TV-L1 is slightly better than Brox. Thus, unless specified we use TV-L1 as input for the temporal-stream ConvNet.</p><p>Hyper-parameter optimization. The learning rate of the spatial-stream ConvNet is initially set to 5 × 10 −6 , and divided by 10 when the accuracy is saturated. The weight decay is set to be 1 × 10 −4 , and momentum is 0.9. On the other hand, the learning rate of the temporal-stream Con-vNet is initially set to 5 × 10 −3 , and divided by 10 when the accuracy is saturated. The weight decay and momentum are the same as the spatial-stream ConvNet. The batch sizes for both ConvNets are 64. Both Temporal Segment LSTM and Temporal-ConvNets are trained with ADAM optimizer. The learning rate is set to 5 × 10 −5 for training Temporal Segment LSTM. For Temporal-ConvNets, we use 1 × 10 −4 for learning rate and 1 × 10 −1 for weight decay.</p><p>Data augmentation. Data augmentation has been very helpful especially when the training data are limited. During training, a sub-image with size 256 x 256 is first randomly cropped using a smaller image region (between 0.08 to 1 of the original image area). Second, the cropped image was randomly scaled between 3/4 and 4/3 of its size. Fi-nally, the cropped and scaled image will be scaled again to 224 x 224. The same data augmentation is applied when training both spatial-and temporal-stream ConvNets. Note that we use additional color jittering for the spatialstream ConvNet, but not temporal-stream ConvNet. Wang et al. <ref type="bibr" target="#b20">[25]</ref> argued that a corner cropping strategy, which only crops 4 corners and center of the image, can prevent overfitting. However, in our implementation, the corner cropping strategy actually makes the ConvNet converge faster and leads to overfitting.</p><p>Testing. We followed the work from Simonyan and Zisserman <ref type="bibr" target="#b9">[13]</ref> to use 25 frames for testing. Each of the 25 frames is sampled equally across each of the videos. During testing, many works averaged predictions from the RNN on all 25 frames. We did not average the prediction because it will be biased towards the average representation of the video and neglect the temporal dynamics learned by LSTM cells. However, without temporal segments, since LSTM cells fail to learn the temporal dynamics, it will benefit from averaging the predictions. Yet, the final prediction accuracy is significantly worse than temporal segments LSTM without averaging prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>We adapt our proposed model to two of the most widely used action recognition benchmarks: UCF101 <ref type="bibr" target="#b11">[15]</ref> and HMDB51 <ref type="bibr">[10]</ref>. We used the first split of UCF101 for validating our proposed models. We used the same parameters and architectures obtained from UCF101 split 1 directly for the other two splits. The model pre-trained on UCF101 is fine-tuned for the HMDB51 dataset. <ref type="table" target="#tab_2">Table 2</ref> shows our experiment results for the spatialstream, temporal-streams, and two-stream on three different splits in the UCF101 and HMDB51 datasets. Our performance on the two-stream model is obtained by taking the mean values of the prediction probabilities from both spatial and temporal-stream ConvNets. Our two proposed methods leverage the baseline two-stream ConvNet and show significant improvement by modeling temporal dynamics. For comparison of our baseline method with others, please refer to supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baseline Two-stream ConvNets.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance of TS-LSTM</head><p>In the following section, we discuss various experiments and individual performance associated with different architectural designs for TS-LSTM. We thoroughly explore different network designs and conclude that: (i) using temporal segments perform better than no segments (ii) adding a FC layer before temporal pooling layer can learn the spatial and temporal feature integration and reduce the feature dimension for each segment. (iii) LSTMs effectively replace the needs for the first FC layer and learns the spatial and temporal feature integration while reducing the feature dimension. (iv) deep LSTM layers do not necessarily help and often lead to overfitting These experiments lead us to conclude that despite their theoretical ability to extract temporal patterns over many scales and lengths, in this case their inputs must be pre-segmented, demonstrating a limitation of our current usage of LSTMs. The summarization of our experiments on TS-LSTM is shown in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>Temporal segments and pooling. Temporal segments have been shown to be useful in end-to-end frameworks <ref type="bibr" target="#b20">[25]</ref>. In our experiments, we demonstrate that even with pre-saved feature vectors extracted from equally sampled images in the videos, temporal segments can help in improving the classification accuracy. The difference of using three or five segments is statistically insignificant and may highly depend on the types of action performed in the video. Temporal mean pooling performed similarly with max pooling. We use max pooling for the rest of experiments.</p><p>Spatial and temporal feature integration. Since the features from each segment are concatenated together, the model suffers from overfitting when the number of segments increases, as can be seen from <ref type="table" target="#tab_3">Table 3</ref> with five segments. By adding an FC layer before the temporal max pooling layer, we can effectively control the dimensionality of the embedded features, exploit the spatial and temporal correlation, and prevent overfitting.</p><p>Vanilla LSTM. LSTM cells have the ability to model temporal dynamics, but only shown limited improvement from previous works. Our experimental results shown that there is only a 0.2% improvement over two-stream Con-vNet, which is consistent with <ref type="bibr" target="#b23">[28]</ref> (88.0 to 88.6%) and <ref type="bibr" target="#b3">[4]</ref> (69.0 to 71.1% on RGB, 72.2 to 77.0% on flow), and <ref type="bibr" target="#b0">[1]</ref> (63.3 to 64.5%). The performance gained from LSTM cells decreases when the baseline two-stream ConvNet is stronger. Thus, we observed only 0.2% improvement when using a baseline two-stream ConvNet achieving 92.6% accuracy, as opposed to 2-5% improvement from a baseline of 70% accuracy <ref type="bibr" target="#b3">[4]</ref>. Note that using vanilla LSTM performed only similar to naive temporal max pooling. TS-LSTM. By combining the temporal segments and LSTM cells, we can leverage the temporal dynamics across each temporal segment, and significantly boost the prediction accuracy. This finding suggests that carefully rethinking and understanding how LSTMs model temporal information is necessary. Our experimental results indicate that using deeper LSTM layers is prone to overfitting on the UCF101 dataset. This is probably because our features generated from spatial and temporal ConvNets were fine-tuned to identify video classes at the frame level. The dynamics of feature representations over time is not as complicated as other sequential data, e.g. speech and text. Thus, increasing the number of stacked LSTM layers tends to overfit the data in UCF101. Our experiments on HMDB51 also confirm this hypothesis. The prediction accuracy on the HMDB51 dataset using two-layer LSTM increased from 68.7% to 69.0% over single LSTM layer, since the baseline model has not yet learned to robustly identify video classes at the frame level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance of Temporal-ConvNet</head><p>In this Section, we discuss different factors for designing the architecture of the Temporal-ConvNet. We conclude that: (i) applying multiple TCLs performs better than using single or double TCLs. (ii) concatenating the outputs of each multi-flow module is the better way to fuse different flows. (iii) with proper fusion methods, the multi-flow architecture has better capability to explore the temporal information than the single-flow architecture. (iv) adding batch normalization and dropout layers can further improve the performance. The summarization of our experiments on Temporal-ConvNet is shown in <ref type="table" target="#tab_8">Table 8</ref>. Based on these experiments, we can conclude that convolution across time can effectively extract temporal patterns, but as with other applications the specific architecture is crucial and lessons learned from other architectures and tasks can inform successful designs.</p><p>Temporal-ConvNet layer. One of the most important components in the Temporal-ConvNet is the TCL, and the convolutional kernel size is the most critical part since it directly affects how the network learns the temporal correlation. The different kernel sizes essentially correspond to actions with different temporal duration and period. We set the temporal convolutional kernel size to 5 for our experiments. On the other hand, the number of TCLs also plays an important role, because the TCLs are used to gradually reduce the dimension in the temporal direction, i.e. we map the feature matrices to a feature vector. Applying only single or double TCLs will still leave the temporal direction to have a high dimensionality, and thus result in larger numbers of parameters and cause overfitting. <ref type="table" target="#tab_8">Table 8</ref> shows the results from different numbers of TCLs.</p><p>Multi-flow architecture There are two main questions in optimizing the multi-flow architecture: 1) How to combine multiple flows? 2) How many flows should we have? For the first question, we propose two approaches. One is our Temporal-Inception, and the other one is the multi-flow version of Temporal-VGG. The difference between these two approaches is where we place the concatenation layers, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Regarding the second question, increasing the flow number provides a better capability to describe actions with different temporal scales, but it also greatly increases the chance to overfit the data. In addition, with the multi-flow architecture, the size of the temporal convolutional kernel is also important. From our experiments, kernel sizes of 5 and 7 achieved the best prediction accuracy.</p><p>The illustration of Temporal-VGG, Multi-flow Temporal-VGG, and Temporal-Inception are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. The prediction accuracy on UCF-101 split 1 is shown in Table 8. The Temporal-Inception has better performance than the multi-flow version of Temporal-VGG, since Temporal-Inception effectively explores and combines temporal information obtained through various temporal receptive fields.</p><p>Batch normalization and dropout To overcome the overfitting and internal covariate shift issues, we add a batch normalization layer right before each ReLU layer, and add dropout layers before and after the FC-1024 layer. We use Temporal-Inception to demonstrate how batch normalization and dropout improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Final Performance</head><p>The results from the proposed Temporal Segment LSTM and Temporal-Inception on both UCF101 and HMDB51 are  shown in <ref type="table" target="#tab_5">Table 5</ref>. While TSN <ref type="bibr" target="#b20">[25]</ref> achieved significant progress on human action recognition, it requires significant temporal augmentation and it is unclear how such augmentation and temporal segments each contributed to the results. Our proposed methods explore temporal segments and demonstrate that, without tedious randomly sampled snippets from video in each training step, a simple temporal pooling layer and LSTM cells trained on a fixed sampled video can achieve better accuracy on both UCF101 and HMDB51.</p><p>Modeling temporal dynamics. To further validate that our methods can model the temporal dynamics, we train both TS-LSTM and Temporal-Inception using only a maximum of the first 10 seconds of videos, e.g. 250 frames per video. Note that the number of frames in the UCF101 dataset ranges from 30 to 1700. About 21% of videos are longer than 10 seconds. Our experiments show that by only  <ref type="bibr" target="#b9">[13]</ref> 88.0 59.4 LSTM <ref type="bibr" target="#b23">[28]</ref> 88.6 -Transformation <ref type="bibr" target="#b21">[26]</ref> 92.4 63.4 Convolutional Two-stream <ref type="bibr" target="#b5">[6]</ref> 92.5 65.4 SR-CNN <ref type="bibr" target="#b22">[27]</ref> 92.6 -Key volume <ref type="bibr" target="#b26">[31]</ref> 93.1 67.2 ST-ResNet <ref type="bibr" target="#b4">[5]</ref> 93.4 -TSN (2 modalities) <ref type="bibr" target="#b20">[25]</ref> 94.0 68.5 TS-LSTM 94.1 69.0 Temporal-Inception 93.9 67.5</p><p>using the first 10 seconds of the video, the baseline twostream ConvNets achieve 92.9% accuracy which is slightly better than when using the full length of the videos. On the other hand, when the proposed methods see more frames of each video, both of the methods achieved better prediction accuracy (TS-LSTM: 93.7 to 94.1%; Temporal-Inception: 93.2 to 93.9%). This verifies that our proposed methods can successfully leverage the temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion &amp; Discussion</head><p>Two-stream ConvNets have been widely used in video understanding, especially for human action recognition. Recently, several works have explored various methods to exploit spatiotemporal information. However, these works have tried individual methods with little analysis of whether and how they can successfully model dynamic temporal information, and often multiple differences obfuscate the exact cause for better performance. In this paper, we thoroughly explored two methods to model dynamic temporal information: Temporal Segment LSTM and Temporal-ConvNet. We showed that naive temporal max pooling performed similar to the vanilla LSTM. By integrating temporal segments and LSTM, the proposed method achieved state-of-the-art accuracy on both UCF101 and HMDB51. Our proposed Temporal-ConvNet performs convolutions on temporally-constructed feature vectors to learn global video-level representations. We further investigated different VGG-and Inception-style Temporal-ConvNets, and demonstrated that the proposed Temporal-Inception can achieve state-of-the-art performance using only high-level feature vector representations equally sampled from each of the videos. Combined, we show that both RNNs and convolutions across time are able to model temporal dynamics, but that care must be given to account for strengths and weaknesses in each approach. Our findings using temporal segments with LSTMs suggests that there may be low-hanging fruit in carefully re-thinking and understand-ing how LSTMs model temporal information. On the other hand, it is clear that convolutional networks can exploit information across time as well, and that design choices used for spatial tasks can transfer to temporal tasks. In the future, we plan to pursue these directions on bigger datasets as well as investigate how to better regularize LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Two-stream ConvNets Comparison on UCF101</head><p>A great deal of literature has shown that using deeper ConvNets can improve overall performance for two-stream methods. In particular, the performance of VGG-16 <ref type="bibr" target="#b10">[14]</ref>, GoogLeNet <ref type="bibr" target="#b13">[18]</ref>, and BN-Inception <ref type="bibr" target="#b7">[8]</ref> on both spatial and temporal streams are reported <ref type="bibr" target="#b20">[25,</ref><ref type="bibr" target="#b23">28]</ref>. <ref type="table" target="#tab_6">Table 6</ref> compares the baseline performance using different ConvNets for training spatial-and temporal-stream ConvNet. We demonstrate a strong baseline performance using ResNet-101 <ref type="bibr" target="#b6">[7]</ref>. We expect to observe better performance by training with Inception-v3 or v4 models <ref type="bibr" target="#b12">[17,</ref><ref type="bibr" target="#b14">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Complete Experimental Results for TS-LSTM</head><p>We demonstrate the effectiveness of the proposed TS-LSTM and its comparison with various networks and dimensions we used in developing TS-LSTM. We show how vanilla LSTM can be properly trained and largely benefited by using batch normalization, but the vanilla LSTM still overfits the training samples and sometimes results in lower accuracy than the baseline two-stream method (92.6%). A naive temporal pooling method can perform similarly with vanilla LSTM. We can further increase the accuracy by integrating temporal segments with LSTMs. We also show how a different number of temporal segments affects the prediction accuracy when using different dimension and the depth of the LSTM cells. Using three or five temporal segments results in similar performances in our experiments on UCF101 split 1. It is worth mentioning that the number of temporal segments might depend on the type of video classification problem we are trying to solve. Different types of video or actions may benefit from employing more temporal segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Complete Experimental Results for Temporal-ConvNet</head><p>We claim that by properly leveraging temporal information, we can achieve state-of-the-art results only using feature vector representations. <ref type="table" target="#tab_8">Table 8</ref> shows all of the architectures with different designs of Temporal-ConvNet layers (TCLs). There are several interesting findings: First, the Multi-flow architecture does not guarantee better performance. If we only apply single or double TCL, the overall dimension number is still large and can cause over-fitting problems. Applying the multi-flow modules will reduce the performance. However, Temporal-Inception uses four TCLs to reduce the temporal dimension to avoid the overfitting problem, so the performance boosts as expected. Secondly, different multi-flow approaches also affect the results. Temporal-Inception fuses the outputs of flows in each layer instead of fusing at last as Temporal-VGG does. In this way, the architecture can effectively exploit and combine temporal information obtained through various temporal receptive fields, and properly increases the accuracy. Finally, the dimension of last full-connected layer also play an important role. To meet the balance between the capability of discrimination and over-fitting, we choose 1024 as our final feature dimension.</p><p>Although TCLs can reduce the temporal dimension, the filter dimension will increase because of the concatenation of multi-flow modules. There are two different approaches to reduce the filter dimension: (i) reduce the dimension for each multi-flow module (ii) reduce the dimension after all the multi-flow modules. Average pooling, max pooling and Conv fusion (convolve with a set of filters to reduce the filter dimension.) are used as the dimension-reduction methods. <ref type="table" target="#tab_9">Table 9</ref> shows the experiment results. The accuracy drops dramatically if applying the above methods for each module because we partially lose information for each dimensionreduction process. We also found that using multiple convolution layers to gradually reduce the dimension is better than directly applying average or max pooling.</p><p>One of the most important components in the Temporal-ConvNet is the TCL, and the convolutional kernel size is the most critical part since it directly affects how the network learns the temporal correlation. Larger kernel sizes are used for actions with longer temporal duration. In our experiments, combining the Temporal-Inception architecture with the convolution kernel size 5 and 7 provides the best capability to represent different kinds of actions. <ref type="table" target="#tab_1">Table 10</ref> also shows the results for other kernel sizes. We also found that the factorization concept in Inception-v3 <ref type="bibr" target="#b14">[19]</ref> does not fit our architecture. Finally, in addition to one-stride convolution followed by max pooling, two-stride convolution  could be a better alternative to reduce the dimension since we may lose part of the information by max pooling. However, it is not the case for the Temporal-Inception. Although 2-stride convolution improves the performance on split 1, the overall accuracy is still not better. We performed an experiment where we limited the baseline and the two proposed methods to only access the first 10 seconds of each video, e.g. maximum 250 frames per video. Out of 13320 videos in UCF101, there are 2805 videos that are longer than 250 frames. About 21% of the videos are cut short to demonstrate how the length of videos affects final prediction accuracy. Our experiment shows that the proposed methods can effectively leverage the temporal information since performance can continue to improve as they process additional temporal data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Statistics of the UCF101 dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Video Analysis of the UCF101 dataset</head><p>We use some examples to show how our approaches work. Our first example is HighJump. All the videos in this category can be divided into two parts: running and jumping. The baseline is a frame-based method, which does not make the prediction using the cross-frame information. Therefore, by applying the baseline approach, some videos are misclassified as other categories including running and jumping. For example, <ref type="figure">Figure 6</ref>  Another example is PizzaTossing. In <ref type="figure">Figure 7</ref>, we show three different variations of this category. <ref type="figure">Figure 7</ref>(a) is misclassified as Punch since both videos include two people and their arm motion are also similar. <ref type="figure">Figure 7(b)</ref> is misclassified as Nunchucks because the people in both videos are making an object spinning. Finally, <ref type="figure">Figure 7</ref>(c) is misclassified as SalsaSpin since in both videos, there are two objects spinning quickly. Those three examples are also correctly classified by both of our approaches because we take the spatial and temporal correlation into account to distinguish different categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. t-SNE Visualization</head><p>We visualize the feature vectors from baseline twostream ConvNet, TS-LSTM, and Temporal-Inception methods. From the visualizations in <ref type="figure">Fig. 8</ref>, we can see that both of the proposed methods are able to group the test samples into more distinct clusters. Thus, after the last fullyconnected layer, both the proposed methods can achieve better classification results.</p><p>By superimpose the snap image from each of the video on the data points optimized from t-SNE, the visualization of the videos in the high-dimensional space are shown in <ref type="figure" target="#fig_0">Fig. 10, 11</ref>, and 12. We can further observe the difference from each of the class by zoom-in these figures, as shown in <ref type="figure">Fig. 9</ref>. It is clear that after using the proposed TS-LSTM and Temporal-Inception, the data points from the same class were pushed toward each other in the highdimensional space. For example, both HighJump and Piz-zaTossing videos were originally scattered around the center of the figures, but were able to grouped together by the two proposed methods. Note that the accuracy of individual methods are: 62.2% (baseline), 97.3% (TS-LSTM), and 94.6% (Temporal-Inception) for HighJump; 66.7% (base-line), 90.9% (TS-LSTM), and 97.0% (Temporal-Inception) for PizzaTossing.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the proposed framework. Spatial and temporal features were extracted from a two-stream ConvNet using ResNet-101 pre-trained on ImageNet, and fine-tuned for single-frame activity prediction. Spatial and temporal features are concatenated and temporally-constructed into feature matrices. The constructed feature matrices are then used as input to both of our proposed methods: Temporal Segment LSTM (TS-LSTM) and Temporal-Inception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Temporal Segment LSTM (TS-LSTM) first divides the feature matrix into several temporal segments. Each temporal segment is then pooled via mean or max pooling layers, and their outputs are fed into the LSTM layer sequentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Overall Temporal-Inception architecture. The input of Temporal-Inception is a 2D matrix composed of feature vectors across different time steps. In each multiflow module, there are two TCLs with different convolution kernels. Each multi-flow module reduces the temporal dimension by half. Therefore, with multiple TCLs and two fully-connected layers, the input feature matrices are mapped to the class prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of three different architecture of Temporal-ConvNet. Temporal-Inception has the best performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Statistics of video length of the UCF101 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>depicts the length of each video from the UCF101 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a)(b) are misclassified as JavelinThrow and LongJump, and Figure 6(c)(d) are misclassified as FloorGymnastics and PoleVault. How-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :Figure 8 :Figure 9 :</head><label>6789</label><figDesc>The category (HighJump) that is misclassified by the baseline approach but correctly classified by TS-LSTM and Temporal-Inception. 1st row: four example videos with the ground truth labels. 2nd row: the incorrectly predicted labels by the baseline approach and the examples of those labels. Another category (PizzaTossing) that is misclassified by the baseline approach but correctly classified by TS-LSTM and Temporal-Inception. 1st row: 3 example videos with the ground truth labels. 2nd row: the incorrectly predicted labels by the baseline approach and the examples of those labels.ference on Computer Vision and Pattern Recognition, pages 1725-1732, 2014. 2 [10] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. Hmdb: a large video database for human motion recognition. In 2011 International Conference on Computer Vision, pages 2556-2563. IEEE, 2011. 6, 8 [11] Y. Pan, T. Mei, T. Yao, H. Li, and Y. Rui. Jointly modeling embedding and translation to bridge video and language. arXiv preprint arXiv:1505.01861, 2015. 4 [12] X. Peng, L. Wang, X. Wang, and Y. Qiao. Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice. Computer Vision and Image t-SNE visualization of the last feature vector representation from baseline two-stream ConvNet, TS-LSTM, and Temporal-Inception on UCF101 split 1. The feature representations from the two proposed methods show the data points are easier to separate and classify. (a) Baseline two-stream ConvNet (b) TS-LSTM (c) Temporal-Inception (d) Baseline two-stream ConvNet (e) TS-LSTM (f) Temporal-Inception By zooming in the t-SNE visualization of baseline two-stream ConvNet, TS-LSTM, and Temporal-Inception on UCF101 split 1, we can see specific example of how videos in different video classes were scattered before and grouped together by both proposed TS-LSTM and Temporal-Inception methods. Top tow: HighJump video class in UCF101. Bottom tow: PizzaTossing video class in UCF101. Note that the recognition accuracy of HighJump using individual methods are: 62.2% (baseline), 97.3% (TS-LSTM), and 94.6% (Temporal-Inception), and the accuracy of PizzaTossing are: 66.7% (baseline), 90.9% (TS-LSTM), and 97.0% (Temporal-Inception) Figure 10: t-SNE visualization of baseline two-stream Con-vNet. Each of the data points are replaced by the snap shot of each test video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 : 8 Figure 12</head><label>11812</label><figDesc>t-SNE visualization of TS-LSTM. Each of the data points are replaced by the snap shot of each test video. preprint arXiv:1212.0402, 2012. 6, 8 [16] L. Sun, K. Jia, D.-Y. Yeung, and B. E. Shi. Human action recognition using factorized spatio-temporal convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4597-4605, 2015. 1, 2, 4, : t-SNE visualization of Temporal-Inception. Each of the data points are replaced by the snap shot of each test video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Optical flow algorithms and Temporal-stream Con-vNet performance comparison.</figDesc><table><row><cell></cell><cell>Optical flow</cell><cell>ConvNet</cell><cell cols="2">Fine-tune Accuracy</cell></row><row><cell>Two-stream [13]</cell><cell>Brox</cell><cell>CNN M 2048</cell><cell>N</cell><cell>81.0</cell></row><row><cell>Convolutional</cell><cell>Brox</cell><cell>VGG-M</cell><cell>Y</cell><cell>82.3</cell></row><row><cell>Two-stream [6]</cell><cell>[-20,20]</cell><cell>VGG-16</cell><cell>Y</cell><cell>86.3</cell></row><row><cell>TSN [25]</cell><cell>TV-L1</cell><cell>VGG-16 BN-Inception</cell><cell>Y Y</cell><cell>85.7 87.2</cell></row><row><cell>SR-CNNs [27]</cell><cell>TV-L1</cell><cell>VGG-16</cell><cell>Y</cell><cell>85.3</cell></row><row><cell>Ours</cell><cell>TV-L1 [-20,20]</cell><cell>ResNet-101</cell><cell>Y</cell><cell>86.2</cell></row><row><cell>Ours</cell><cell>Brox</cell><cell>ResNet-101</cell><cell>Y</cell><cell>84.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance from spatial and temporal-stream ConvNets, and two-stream ConvNet on three different splits of the UCF101 and HMDB51 datasets.</figDesc><table><row><cell></cell><cell cols="4">Spilit 1 Split 2 Split 3 Mean</cell></row><row><cell></cell><cell cols="2">Spatial-stream</cell><cell></cell><cell></cell></row><row><cell>UCF101</cell><cell>86.1</cell><cell>83.6</cell><cell>85.3</cell><cell>85.0</cell></row><row><cell>HMDB51</cell><cell>51.9</cell><cell>49.7</cell><cell>49.7</cell><cell>50.4</cell></row><row><cell></cell><cell cols="2">Temporal-stream</cell><cell></cell><cell></cell></row><row><cell>UCF101</cell><cell>86.2</cell><cell>87.0</cell><cell>88.4</cell><cell>87.2</cell></row><row><cell>HMDB51</cell><cell>60.3</cell><cell>59.0</cell><cell>60.0</cell><cell>59.7</cell></row><row><cell></cell><cell cols="2">Two-stream</cell><cell></cell><cell></cell></row><row><cell>UCF101</cell><cell>92.6</cell><cell>92.2</cell><cell>92.9</cell><cell>92.6</cell></row><row><cell>HMDB51</cell><cell>66.4</cell><cell>64.8</cell><cell>92.6</cell><cell>64.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of each component in Temporal Segment LSTM on UCF101 split 1. TS: number of temporal segments. Max: temporal max pooling layer. 512: dimension of LSTM cell. For a more complete version, please refer to supplementary material.</figDesc><table><row><cell cols="2">TS BN FC</cell><cell>Temporal Pooling</cell><cell>BN FC</cell><cell>Acc</cell></row><row><cell></cell><cell></cell><cell>LSTM</cell><cell></cell></row><row><cell>1</cell><cell>BN</cell><cell>512</cell><cell cols="2">BN 101 92.8</cell></row><row><cell>1</cell><cell>BN</cell><cell>(512,512)</cell><cell cols="2">BN 101 92.5</cell></row><row><cell>1</cell><cell>BN</cell><cell>(512,512,512)</cell><cell cols="2">BN 101 92.1</cell></row><row><cell></cell><cell cols="3">Temporal segment &amp; Batch normalization</cell></row><row><cell>1</cell><cell>BN</cell><cell>Max</cell><cell cols="2">BN 101 92.8</cell></row><row><cell>3</cell><cell>BN</cell><cell>Max</cell><cell cols="2">BN 101 93.4</cell></row><row><cell></cell><cell cols="3">Feature integration &amp; dimension reduction</cell></row><row><cell>3</cell><cell>BN 512</cell><cell>Max</cell><cell cols="2">BN 101 93.9</cell></row><row><cell></cell><cell></cell><cell cols="2">Temporal Segment LSTM</cell></row><row><cell>3</cell><cell>BN</cell><cell>Max + 512</cell><cell cols="2">BN 101 94.3</cell></row><row><cell>3</cell><cell>BN</cell><cell>Max + (512,512)</cell><cell cols="2">BN 101 94.2</cell></row><row><cell>3</cell><cell>BN</cell><cell cols="3">Max + (512,512,512) BN 101 93.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of Temporal-ConvNet on UCF101 split 1. 1L: single TCL. 2L: double TCL. BN: Batch Normalization. FC: fully-connected layer. In the column "Architecture", "T" is denoted as one TCL. {} denote as the stacked architecture. () denotes as the wide (parallel) architecture. The illustration of the last three methods are shown inFigure 4.</figDesc><table><row><cell>Architecture</cell><cell>BN Dropout</cell><cell>FC</cell><cell>Acc</cell></row><row><cell></cell><cell>1L</cell><cell></cell><cell></cell></row><row><cell>T</cell><cell cols="3">BN Dropout 1024 93.6</cell></row><row><cell>(T,T)</cell><cell cols="3">BN Dropout 1024 92.6</cell></row><row><cell></cell><cell>2L</cell><cell></cell><cell></cell></row><row><cell>{T,T}</cell><cell cols="3">BN Dropout 1024 93.1</cell></row><row><cell>{(T,T),(T,T)}</cell><cell cols="3">BN Dropout 1024 92.7</cell></row><row><cell cols="2">Temporal-VGG</cell><cell></cell><cell></cell></row><row><cell>{T,T,T,T}</cell><cell cols="3">BN Dropout 1024 94.0</cell></row><row><cell cols="2">Multi-flow Temporal-VGG</cell><cell></cell><cell></cell></row><row><cell>({T,T,T,T},{T,T,T,T})</cell><cell cols="3">BN Dropout 1024 93.4</cell></row><row><cell cols="2">Temporal-Inception</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">1024 93.3</cell></row><row><cell></cell><cell cols="3">Dropout 1024 93.4</cell></row><row><cell cols="2">{(T,T),(T,T),(T,T),(T,T)} BN</cell><cell cols="2">1024 93.4</cell></row><row><cell></cell><cell cols="3">BN Dropout 2048 93.7</cell></row><row><cell></cell><cell cols="3">BN Dropout 1024 94.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>State-of-the-art action recognition comparison on the UCF101 [15] and HMDB51 [10] datasets.</figDesc><table><row><cell>Methods</cell><cell cols="2">UCF101 HMDB51</cell></row><row><cell>FstCN [16]</cell><cell>88.1</cell><cell>59.1</cell></row><row><cell>Two-stream</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Two-stream ConvNet comparison on the UCF101 dataset.</figDesc><table><row><cell></cell><cell cols="3">Spatial-stream ConvNet Temporal-stream ConvNet Two-stream ConvNet</cell></row><row><cell>GoogLeNet [13]</cell><cell>77.1</cell><cell>83.9</cell><cell>89.0</cell></row><row><cell>VGG-16 [24]</cell><cell>78.4</cell><cell>87.0</cell><cell>91.4</cell></row><row><cell>BN-Inception [25]</cell><cell>84.5</cell><cell>87.2</cell><cell>92.0</cell></row><row><cell>ResNet-101</cell><cell>85.0</cell><cell>87.2</cell><cell>92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Complete performance comparison of Temporal Segment LSTM on UCF101 split 1. () denote as stacked LSTM.</figDesc><table><row><cell cols="2">TS BN FC</cell><cell>Temporal Pooling</cell><cell>BN</cell><cell>FC</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell cols="2">ConvNet + LSTM</cell><cell></cell></row><row><cell>1</cell><cell></cell><cell>LSTM-512</cell><cell></cell><cell>FC-101</cell><cell>87.0</cell></row><row><cell>1</cell><cell></cell><cell>LSTM-1024</cell><cell></cell><cell>FC-101</cell><cell>86.2</cell></row><row><cell>1</cell><cell></cell><cell>LSTM-2048</cell><cell></cell><cell>FC-101</cell><cell>85.8</cell></row><row><cell></cell><cell></cell><cell cols="2">Batch Normalization + LSTM</cell><cell></cell></row><row><cell>1</cell><cell>BN</cell><cell>LSTM-512</cell><cell cols="2">BN FC-101</cell><cell>92.8</cell></row><row><cell>1</cell><cell>BN</cell><cell>LSTM-1024</cell><cell cols="2">BN FC-101</cell><cell>91.2</cell></row><row><cell>1</cell><cell>BN</cell><cell>LSTM-2048</cell><cell cols="2">BN FC-101</cell><cell>91.8</cell></row><row><cell>1</cell><cell>BN</cell><cell>LSTM-(512, 512)</cell><cell cols="2">BN FC-101</cell><cell>92.6</cell></row><row><cell>1</cell><cell>BN</cell><cell>LSTM-(1024, 512)</cell><cell cols="2">BN FC-101</cell><cell>91.9</cell></row><row><cell>1</cell><cell>BN</cell><cell>LSTM-(512,512,512)</cell><cell cols="2">BN FC-101</cell><cell>92.1</cell></row><row><cell></cell><cell cols="5">Temporal Segment + Max Pooling + Batch Normalization</cell></row><row><cell>1</cell><cell></cell><cell>Max</cell><cell></cell><cell>FC-101</cell><cell>89.0</cell></row><row><cell>1</cell><cell></cell><cell>Max</cell><cell cols="2">BN FC-101</cell><cell>92.8</cell></row><row><cell>1</cell><cell>BN</cell><cell>Max</cell><cell cols="2">BN FC-101</cell><cell>92.8</cell></row><row><cell>3</cell><cell>BN</cell><cell>Max</cell><cell cols="2">BN FC-101</cell><cell>93.4</cell></row><row><cell>5</cell><cell>BN</cell><cell>Max</cell><cell cols="2">BN FC-101</cell><cell>93.2</cell></row><row><cell></cell><cell cols="4">Feature integration + Dimension Reduction</cell></row><row><cell>3</cell><cell>BN 512</cell><cell>Max</cell><cell cols="2">BN FC-101</cell><cell>93.9</cell></row><row><cell>5</cell><cell>BN 512</cell><cell>Max</cell><cell cols="2">BN FC-101</cell><cell>93.7</cell></row><row><cell></cell><cell></cell><cell cols="3">Temporal Segment + Max + LSTM</cell></row><row><cell>3</cell><cell>BN</cell><cell>Max + 512</cell><cell cols="2">BN FC-101</cell><cell>94.3</cell></row><row><cell>3</cell><cell>BN</cell><cell>Max + 1024</cell><cell cols="2">BN FC-101</cell><cell>94.0</cell></row><row><cell>3</cell><cell>BN</cell><cell>Max + 2048</cell><cell cols="2">BN FC-101</cell><cell>94.1</cell></row><row><cell>5</cell><cell>BN</cell><cell>Max + 512</cell><cell cols="2">BN FC-101</cell><cell>94.2</cell></row><row><cell>5</cell><cell>BN</cell><cell>Max + 1024</cell><cell cols="2">BN FC-101</cell><cell>94.1</cell></row><row><cell>5</cell><cell>BN</cell><cell>Max + 2048</cell><cell cols="2">BN FC-101</cell><cell>94.2</cell></row><row><cell></cell><cell cols="4">Temporal Segment + Max + stacked LSTM</cell></row><row><cell>3</cell><cell>BN</cell><cell>Max + (512, 512)</cell><cell cols="2">BN FC-101</cell><cell>94.2</cell></row><row><cell>3</cell><cell>BN</cell><cell>Max + (1024, 512)</cell><cell cols="2">BN FC-101</cell><cell>94.1</cell></row><row><cell>3</cell><cell>BN</cell><cell cols="3">Max + (512, 512, 512) BN FC-101</cell><cell>93.9</cell></row><row><cell>5</cell><cell>BN</cell><cell>Max + (512, 512)</cell><cell cols="2">BN FC-101</cell><cell>94.2</cell></row><row><cell>5</cell><cell>BN</cell><cell>Max + (1024, 512)</cell><cell cols="2">BN FC-101</cell><cell>94.0</cell></row><row><cell>5</cell><cell>BN</cell><cell cols="3">Max + (512, 512, 512) BN FC-101</cell><cell>93.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Complete Performance of different architectures in Temporal-ConvNet on UCF101 split 1. 1L: single TCL. 2L: double TCL. BN: Batch Normalization. FC: fully-connected layer. In the column "Architecture", "T" is denoted as one TCL. {} denote as the stacked architecture. () denotes as the wide (parallel) architecture.</figDesc><table><row><cell>Architecture</cell><cell>BN Dropout</cell><cell>FC</cell><cell>Accuracy</cell></row><row><cell></cell><cell>1L</cell><cell></cell><cell></cell></row><row><cell>T</cell><cell cols="2">BN Dropout 1024</cell><cell>93.6</cell></row><row><cell>(T,T)</cell><cell cols="2">BN Dropout 1024</cell><cell>92.6</cell></row><row><cell></cell><cell>2L</cell><cell></cell><cell></cell></row><row><cell>{T,T}</cell><cell cols="2">BN Dropout 1024</cell><cell>93.1</cell></row><row><cell>{(T,T),(T,T)}</cell><cell cols="2">BN Dropout 1024</cell><cell>92.7</cell></row><row><cell></cell><cell>Temporal-VGG</cell><cell></cell><cell></cell></row><row><cell></cell><cell>BN Dropout</cell><cell>512</cell><cell>93.5</cell></row><row><cell>{T,T,T,T}</cell><cell cols="2">BN Dropout 1024</cell><cell>94.0</cell></row><row><cell></cell><cell cols="2">BN Dropout 2048</cell><cell>93.8</cell></row><row><cell></cell><cell cols="2">BN Dropout 4096</cell><cell>93.2</cell></row><row><cell cols="2">Multi-flow Temporal-VGG</cell><cell></cell><cell></cell></row><row><cell></cell><cell>BN Dropout</cell><cell>512</cell><cell>93.6</cell></row><row><cell>({T,T,T,T},{T,T,T,T})</cell><cell cols="2">BN Dropout 1024</cell><cell>93.4</cell></row><row><cell></cell><cell cols="2">BN Dropout 2048</cell><cell>93.5</cell></row><row><cell></cell><cell cols="2">BN Dropout 4096</cell><cell>93.2</cell></row><row><cell cols="2">Temporal-Inception</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1024</cell><cell>93.3</cell></row><row><cell></cell><cell cols="2">Dropout 1024</cell><cell>93.4</cell></row><row><cell></cell><cell>BN</cell><cell>1024</cell><cell>93.4</cell></row><row><cell cols="2">{(T,T),(T,T),(T,T),(T,T)} BN Dropout</cell><cell></cell><cell>92.7</cell></row><row><cell></cell><cell>BN Dropout</cell><cell>512</cell><cell>93.1</cell></row><row><cell></cell><cell cols="2">BN Dropout 1024</cell><cell>94.2</cell></row><row><cell></cell><cell cols="2">BN Dropout 2048</cell><cell>93.7</cell></row><row><cell></cell><cell cols="2">BN Dropout 4096</cell><cell>93.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Dimension reduction methods for Temporal-ConvNet. The performance is shown on UCF101 split1. Conv1, n: convolution with the kernel size 1×1 and the output filter dimension is n.</figDesc><table><row><cell cols="2">Reduce the dimension for each multi-flow module</cell><cell></cell></row><row><cell></cell><cell>Average pooling</cell><cell>92.8</cell></row><row><cell></cell><cell>Max pooling</cell><cell>92.9</cell></row><row><cell></cell><cell>Conv fusion (Conv1, 1)</cell><cell>92.8</cell></row><row><cell cols="2">Reduce the dimension after all the multi-flow modules</cell><cell></cell></row><row><cell>Average pooling</cell><cell></cell><cell>93.0</cell></row><row><cell>Max pooling</cell><cell></cell><cell>93.1</cell></row><row><cell>Conv fusion</cell><cell>Conv1, 1</cell><cell>92.8</cell></row><row><cell>Conv fusion</cell><cell>Conv1, 2 Conv1, 1</cell><cell>92.9</cell></row><row><cell>Conv fusion</cell><cell>Conv1, 4 Conv1, 1</cell><cell>92.5</cell></row><row><cell>Conv fusion</cell><cell>Conv1, 8 Conv1, 1</cell><cell>93.3</cell></row><row><cell>Conv fusion</cell><cell>Conv1, 4 Conv1, 2 Conv1, 1</cell><cell>94.2</cell></row><row><cell>Conv fusion</cell><cell>Conv1, 8 Conv1, 4 Conv1, 1</cell><cell>93.8</cell></row><row><cell>Conv fusion</cell><cell>Conv1, 8 Conv1, 2 Conv1, 1</cell><cell>92.7</cell></row><row><cell>Conv fusion</cell><cell cols="2">Conv1, 8 Conv1, 4 Conv1, 2 Conv1, 1 93.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Convolution methods for Temporal-ConvNet. The performance is shown on UCF101 split1. Conv1, n: convolution with the kernel size 1×1 and the output filter dimension is n.</figDesc><table><row><cell>Architecture</cell><cell>First flow</cell><cell>Second flow</cell><cell>Accuracy</cell></row><row><cell>1-stride Conv + Max pooling</cell><cell>Conv3, 1</cell><cell>Conv5, 1</cell><cell>92.0</cell></row><row><cell>1-stride Conv + Max pooling</cell><cell>Conv3, 1</cell><cell>Conv7, 1</cell><cell>93.1</cell></row><row><cell>1-stride Conv + Max pooling</cell><cell>Conv3, 1</cell><cell>Conv9, 1</cell><cell>93.8</cell></row><row><cell>1-stride Conv + Max pooling</cell><cell>Conv5, 1</cell><cell>Conv9, 1</cell><cell>92.8</cell></row><row><cell>1-stride Conv + Max pooling</cell><cell>Conv7, 1</cell><cell>Conv9, 1</cell><cell>93.9</cell></row><row><cell>1-stride Conv + Max pooling</cell><cell>Conv5, 1</cell><cell>Conv7, 1</cell><cell>94.2 (3 splits: 93.9)</cell></row><row><cell></cell><cell cols="2">stacked Conv3 to replace Conv5 &amp; Conv7</cell><cell></cell></row><row><cell cols="3">1-stride Conv + Max pooling Conv3, 1 -Conv3, 1 Conv3, 1 -Conv3, 1 -Conv3, 1</cell><cell>93.3</cell></row><row><cell cols="3">Use 2-stride Conv to reduce the temporal dimension</cell><cell></cell></row><row><cell>2-stride Conv</cell><cell>Conv5, 1</cell><cell>Conv7, 1</cell><cell>94.4 (3 splits: 93.6)</cell></row><row><cell cols="2">ever, those examples are correctly classified by TS-LSTM</cell><cell></cell><cell></cell></row><row><cell cols="2">and Temporal-Inception. This shows both our approaches</cell><cell></cell><cell></cell></row><row><cell cols="2">can effectively extract the temporal information.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Youtube-8m: A large-scale video classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06573</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<editor>D. Blei and F. Bach</editor>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE con-Understanding</title>
		<meeting>the IEEE con-Understanding</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<title level="m">Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567v3</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2009-British Machine Vision Conference</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="124" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<title level="m">Towards good practices for very deep two-stream convnets</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, pages ***-***</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Actions transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two-stream sr-cnns for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC, pages ***-***</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploiting image-trained cnn architectures for unconstrained video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04144</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A key volume mining deep framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
