<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning an Animatable Detailed 3D Face Model from In-The-Wild Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
							<email>yfeng@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Max Planck ETH Center for Learning System</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Feng</surname></persName>
							<email>hfeng@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
							<email>tbolkart@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning an Animatable Detailed 3D Face Model from In-The-Wild Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While current monocular 3D face reconstruction methods can recover fine geometric details, they suffer several limitations. Some methods produce faces that cannot be realistically animated because they do not model how wrinkles vary with expression. Other methods are trained on high-quality face scans and do not generalize well to inthe-wild images. We present the first approach to jointly learn a model with animatable detail and a detailed 3D face regressor from in-the-wild images that recovers shape details as well as their relationship to facial expressions. Our DECA (Detailed Expression Capture and Animation) model is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. We introduce a novel detail-consistency loss to disentangle person-specific details and expression-dependent wrinkles. This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged. DECA achieves stateof-the-art shape reconstruction accuracy on two benchmarks. Qualitative results on in-the-wild data demonstrate DECA's robustness and its ability to disentangle identity and expression dependent details enabling animation of reconstructed faces. The model and code are publicly available at https://github.com/YadiraF/DECA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Two decades have passed since the seminal work of Vetter and Blanz <ref type="bibr" target="#b74">[76]</ref> that first showed how to reconstruct 3D facial geometry from a single image. Since then, 3D face reconstruction methods have rapidly advanced (for a comprehensive overview see <ref type="bibr" target="#b83">[85]</ref>) enabling applications such * equal contribution <ref type="figure">Figure 1</ref>: DECA. Example images (row 1), the regressed coarse shape (row 2), detail shape (row 3) and reposed coarse shape (row 4), and reposed with person-specific details (row 5). DECA is robust to occlusion and captures person-specific details as well as expression wrinkles that appear in regions like forehead and mouth. Our novelty is that this detail shape can be reposed (animated) such that the wrinkles are specific to the source shape and expression.</p><p>as 3D avatar creation for VR/AR <ref type="bibr" target="#b30">[32]</ref>, video editing <ref type="bibr" target="#b69">[71]</ref>, face recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b52">54]</ref>, virtual make-up <ref type="bibr" target="#b57">[59]</ref>, or speechdriven facial animation <ref type="bibr" target="#b15">[17]</ref>. To make the problem tractable, most existing methods incorporate prior knowledge about geometry or appearance by leveraging pre-computed 3D face models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">20]</ref>. These models reconstruct the coarse face shape but are unable to capture geometric details such as expression-dependent wrinkles, which are essential for realism and for analysing human emotion.</p><p>Several methods recover detailed facial geometry <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b71">73,</ref><ref type="bibr" target="#b72">74]</ref>, however, they require high-quality training scans <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">15]</ref> or lack robustness to occlusions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b51">53]</ref>. None of these works explore how the recovered wrinkles change with varying expressions. Previous methods that learn expression-dependent detail models <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b80">82]</ref> either use detailed 3D scans as training data and, hence, do not generalize to unconstrained images <ref type="bibr" target="#b80">[82]</ref>, or model expressiondependent details as part of the appearance map rather than the geometry <ref type="bibr" target="#b12">[14]</ref>, preventing realistic mesh relighting.</p><p>We introduce DECA (Detailed Expression Capture and Animation), which learns an animatable displacement model from in-the-wild images without 2D-to-3D supervision. In contrast to prior work, these animatable expressiondependent wrinkles are specific to an individual and are regressed from an image. Specifically, DECA jointly learns 1) a geometric detail model that generates a UV displacement map from a low-dimensional representation that consists of subject-specific detail parameters and expression parameters, and 2) a regressor that predicts subject-specific detail, albedo, shape, expression, pose, and lighting parameters from an image. The detail model builds upon FLAME's <ref type="bibr" target="#b40">[42]</ref> coarse geometry, and we formulate the displacements as a function of subject-specific detail parameters and FLAME's jaw pose and expression parameters.</p><p>To gain control over expression-dependent wrinkles of the reconstructed face, while preserving person-specific details (i.e. moles, pores, eyebrows, and expressionindependent wrinkles), the person-specific details and expression-dependent wrinkles must be disentangled. Our key contribution is a novel detail consistency loss that enforces this disentanglement. Given two images of the same person with different expressions, we observe that their 3D face shape and their person-specific details are the same in both images, but the expression and the intensity of the wrinkles differ with expression. During training, this observation is exploited by swapping the detail codes between different images of the same identity and enforcing the newly rendered results to look similar to the original input images. Once trained, DECA reconstructs a detailed 3D face from a single image ( <ref type="figure">Fig. 1</ref> third row) in real time (about 120fps on a Nvidia Quadro RTX 5000), and is able to animate the reconstruction with realistic adaptive expression wrinkles ( <ref type="figure">Fig. 1 bottom)</ref>.</p><p>In summary, our main contributions are: 1) The first approach to learn an animatable displacement model from inthe-wild images that can synthesize plausible geometric details by varying expression parameters. 2) A novel detail consistency loss that disentangles identity-dependent and expression-dependent facial details. 3) Reconstruction of geometric details that is, unlike most competing methods, robust to occlusions, poses, and illumination variation. This is enabled by our low-dimensional detail representation, the detail disentanglement, and training from a large dataset of in-the-wild images. 4) State-of-the-art shape reconstruction accuracy on two different benchmarks. 5) Code and model will be made publicly available for research purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The reconstruction of 3D faces from visual input has received significant attention over the last decades after the pioneering work of Parke <ref type="bibr" target="#b45">[47]</ref>, the first method to reconstruct 3D faces from multi-view images. While a large body of related work aims to reconstruct 3D faces from various input modalities such as multi-view images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b48">50]</ref>, video data <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b64">66]</ref>, RGB-D data <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b68">70,</ref><ref type="bibr" target="#b78">80]</ref> or subject-specific image collections <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b54">56]</ref>, our main focus is on methods that use only a single RGB image. For a more comprehensive overview, see Zollhöfer et al. <ref type="bibr" target="#b83">[85]</ref>. Coarse reconstruction: Many monocular 3D face reconstruction methods follow Vetter and Blanz <ref type="bibr" target="#b74">[76]</ref> by estimating coefficients of pre-computed statistical models in an analysis-by-synthesis fashion. Such methods can be categorized into optimization-based <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b69">71]</ref>, or learning-based methods <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b70">72,</ref><ref type="bibr" target="#b73">75]</ref>. These methods estimate parameters of a statistical face model with a fixed linear shape space, which captures only low-frequency shape information. This results in overlysmooth reconstructions.</p><p>Several works are model-free and directly regress 3D faces (i.e. voxels <ref type="bibr" target="#b32">[34]</ref> or meshes <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b77">79]</ref>) and hence could capture more variation than the model-based methods. However, all these methods require explicit 3D supervision, which is provided either by an optimization-based model fitting <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b77">79]</ref> or by synthetic data generated by sampling a statistical face model <ref type="bibr" target="#b17">[19]</ref> and therefore also only capture coarse shape variations.</p><p>Instead of capturing high-frequency geometric details, some methods reconstruct coarse facial geometry along with high-fidelity textures <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b79">81]</ref>. As this "bakes" shading details into the texture, lighting changes do not affect these details. To enable animation and relighting, DECA captures these details as part of the geometry. Detail reconstruction: Another body of work aims to reconstruct faces with "mid-frequency" details. Common optimization-based methods fit a statistical face model to images to obtain a coarse shape estimate, followed by a shape from shading (SfS) method to reconstruct facial details from monocular images <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b41">43]</ref> or videos <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b64">66]</ref>. Unlike DECA, these approaches are slow, the results lack robustness to occlusions, and the coarse model fitting step requires facial landmarks, making them error-prone for large viewing angles and occlusions.</p><p>Most regression-based approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b71">73]</ref> follow a similar approach by first reconstructing the parameters of a statistical face model to obtain a coarse shape, followed by a refinement step to capture localized details. Chen et al. <ref type="bibr" target="#b13">[15]</ref> and Cao et al. <ref type="bibr" target="#b8">[9]</ref> compute local wrinkle statistics from high-resolution scans and leverage these to constrain the fine-scale detail reconstruction from images <ref type="bibr" target="#b13">[15]</ref> or videos <ref type="bibr" target="#b8">[9]</ref>. Guo et al. <ref type="bibr" target="#b28">[30]</ref> and Richardson et al. <ref type="bibr" target="#b51">[53]</ref> directly regress per-pixel displacement maps. All these methods only reconstruct fine-scale details in nonoccluded regions, causing visible artifacts in the presence of occlusions. Tran et al. <ref type="bibr" target="#b71">[73]</ref> gain robustness to occlusions by applying some face segmentation method <ref type="bibr" target="#b44">[46]</ref> to determine occluded regions, and employ an example-based hole filling of the occluded regions. Further, model-free methods exist that directly reconstruct detailed meshes <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b81">83]</ref> or surface normals that add detail to coarse reconstructions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b59">61]</ref>.</p><p>Tran et al. <ref type="bibr" target="#b72">[74]</ref> and Tewari et al. <ref type="bibr" target="#b65">[67,</ref><ref type="bibr" target="#b66">68]</ref> jointly learn a statistical face model and reconstruct 3D faces from images. While offering more flexibility than fixed statistical models, these methods capture limited geometric details compared to other detail reconstruction methods.</p><p>Unlike DECA, none of these detail reconstruction methods offer animatable details after reconstruction. Animatable detail reconstruction: Most relevant to DECA are methods that reconstruct detailed faces while allowing animation of the result. Golovinski et al. <ref type="bibr" target="#b25">[27]</ref>, Shin et al. <ref type="bibr" target="#b61">[63]</ref> and FaceScape <ref type="bibr" target="#b80">[82]</ref> learn correlations between wrinkles and factors like age and gender <ref type="bibr" target="#b25">[27]</ref> or expression <ref type="bibr" target="#b61">[63,</ref><ref type="bibr" target="#b80">82]</ref> from high-quality face scans. In contrast, DECA learns an animatable detail model solely from inthe-wild images without paired 3D training data. While FaceScape <ref type="bibr" target="#b80">[82]</ref> predicts an animatable 3D face from a single image, the method is not robust to occlusions. This is due to a two step reconstruction process: first optimize the coarse shape, then predict a displacement map from the texture map extracted with the coarse reconstruction.</p><p>Chaudhuri et al. <ref type="bibr" target="#b12">[14]</ref> learn identity and expression corrective blendshapes with dynamic (expression-dependent) albedo maps <ref type="bibr" target="#b43">[45]</ref>. They model geometric details as part of the albedo map, and therefore, the shading of these details does not adapt with varying lighting. This results in unrealistic renderings. In contrast, DECA models details as geometric displacements, which look natural when re-lit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Geometry prior: FLAME <ref type="bibr" target="#b40">[42]</ref> is a statistical 3D head model that combines separate linear identity shape and expression spaces with linear blend skinning (LBS) and posedependent corrective blendshapes to articulate the neck, jaw, and eyeballs. Given parameters of facial identity β ∈ R |β| , pose θ ∈ R 3k+3 (with k = 4 joints for neck, jaw, and eyeballs), and expression ψ ∈ R |ψ| , FLAME outputs a mesh with n = 5023 vertices. The model is defined as</p><formula xml:id="formula_0">M (β, θ, ψ) = W (T P (β, θ, ψ), J(β), θ, W),<label>(1)</label></formula><p>with the blend skinning function W (T, J, θ, W) that rotates the vertices in T ∈ R 3n around joints J ∈ R 3k , linearly smoothed by blendweights W ∈ R k×n . The joint locations J are defined as a function of the identity β. Further,</p><formula xml:id="formula_1">T P (β, θ, ψ) = T+B S (β; S)+B P (θ; P)+B E (ψ; E) (2)</formula><p>denotes the mean template T in "zero pose" with added shape blendshapes B S (β; S) : R |β| → R 3n , pose correctives B P (θ; P) : R 3k+3 → R 3n , and expression blendshapes B E (ψ; E) : R |ψ| → R 3n , with the learned identity, pose, and expression bases S, P and E. See <ref type="bibr" target="#b40">[42]</ref> for details. Appearance model: FLAME does not have an appearance model, hence we convert Basel Face Model's PCA albedo space <ref type="bibr" target="#b47">[49]</ref> into the FLAME UV layout to make it compatible with FLAME. The appearance model outputs a UV albedo map A(α) ∈ R d×d×3 for albedo parameters α ∈ R |α| . Camera model: Photographs in existing in-the-wild face datasets are often taken from a distance. We, therefore, use an orthographic camera model c to project the 3D mesh into image space. Face vertices are projected into the image as</p><formula xml:id="formula_2">v = sΠ(M i ) + t, where M i ∈ R 3 is a vertex in M , Π ∈ R 2×3</formula><p>is the orthographic 3D-2D projection matrix, and s ∈ R and t ∈ R 2 denote isotropic scale and 2D translation, respectively. The parameters s, and t are summarized as c.</p><p>Illumination model: For face reconstruction, the most frequently-employed illumination model is based on Spherical Harmonics (SH) <ref type="bibr" target="#b42">[44]</ref>. By assuming that the light source is distant and the face's surface reflectance is Lambertian, the shaded face image is computed as:</p><formula xml:id="formula_3">B(α, l, N uv ) i,j = A(α) i,j 9 k=1 l k H k (N i,j ),<label>(3)</label></formula><p>where the albedo, A, surface normals, N , and shaded texture, B, are represented in UV coordinates and where</p><formula xml:id="formula_4">B i,j ∈ R 3 , A i,j ∈ R 3 , and N i,j ∈ R 3 denote pixel (i, j)</formula><p>in the UV coordinate system. The SH basis and coefficients are defined as H k : R 3 → R and l = [l T 1 , · · · , l T 9 ] T , with l k ∈ R 3 , and denotes the Hadamard product. Texture rendering: Once we have the geometry parameters (β, θ, ψ), albedo (α), lighting (l) and camera information c, we can recover the 2D image I r by rendering as</p><formula xml:id="formula_5">I r = R(M, B, c),</formula><p>where R denotes the rendering function. FLAME is able to generate a face geometry with various poses, shapes and expressions from a low-dimensional latent space. However, the representational power of the model is limited by the low mesh resolution and therefore mid-frequency details are mostly missing in FLAME's surface. The next section introduces our expression-dependent displacement model that augments FLAME with midfrequency details, and it demonstrates how to reconstruct detailed geometry from a single image and animate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed method</head><p>The goal of DECA is to learn a parameterized face model with geometric detail solely from in-the-wild images <ref type="figure">(</ref>  During training, DECA estimates parameters to reconstruct face shape for each image and, at the same time, learns an expression-conditioned displacement model by leveraging the shape and detail consistency information from multiple images of the same individual (see Sec. 4.3 for details, the yellow box region is further illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>). Once trained, DECA animates a face (right) by combining the reconstructed source identity's shape, head pose, and detail code, with the reconstructed source expression's jaw pose and expression parameters to obtain an animated coarse shape and an animated displacement map. Finally, DECA outputs an animated detail shape. left). Once trained, DECA reconstructs the 3D head with detailed face geometry from a single face image I. The learned parametrization of the reconstructed details enables us then to animate the detail reconstruction by controlling FLAME's expression and jaw pose parameters ( <ref type="figure" target="#fig_1">Fig. 2  right)</ref>. This synthesizes new wrinkles while keeping personspecific details unchanged. Key idea: The key idea of DECA is grounded in the observation that an individual's face shows different details (i.e. wrinkles), depending on their facial expressions but that other properties of their shape remain unchanged. Consequently, facial details should be separated into static person-specific details and dynamic expression-dependent details such as wrinkles <ref type="bibr" target="#b38">[40]</ref>. However, disentangling static and dynamic facial details is a non-trivial task. Static facial details are different across people, whereas dynamic expression dependent facial details even vary for the same person.</p><p>Thus, DECA learns an expression-conditioned detail model to infer facial details from both the person-specific detail latent space and the expression space.</p><p>The main difficulty of learning a detail displacement model is the lack of training data. Prior work uses specialized camera systems to scan people in a controlled environment to obtain detailed facial geometry. However, this approach is expensive and impractical for capturing large numbers of identities with varying expressions and diversity in ethnicity and age. Therefore we propose an approach to learn detail geometry from in-the-wild images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Coarse reconstruction</head><p>We first learn a coarse reconstruction (i.e. in FLAME's model space) in an analysis-by-synthesis way: given a 2D image I as input, we encode the image to a latent code, decode this to synthesize a 2D image I r , and minimize the difference between the synthesized image and the input. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we train an encoder E c , which consists of a ResNet50 <ref type="bibr" target="#b29">[31]</ref> network followed by a fully connected layer, to regress a low-dimensional latent code. This latent code consists of FLAME parameters β, ψ, θ (i.e. representing the coarse geometry), albedo coefficients α, camera c, and lighting parameters l. More specifically, the coarse geometry uses the first 100 FLAME shape parameters (β), 50 expression parameters (ψ), and 50 albedo parameters (α). In total, E c predicts a 236 dimensional latent code.</p><p>Given a dataset of 2D face images I i with multiple images per subject, corresponding identity labels c i , and 68 2D keypoints k i per image, the coarse reconstruction branch is trained by minimizing</p><formula xml:id="formula_6">L coarse = L lmk + L eye + L pho + L id + L sc + L reg , (4)</formula><p>with landmark loss L lmk , eye closure loss L eye , photometric loss L pho , identity loss L id , shape consistency loss L sc and regularization L reg . Landmark re-projection loss: The landmark loss measures the difference between ground-truth 2D face landmarks k i and the corresponding landmarks in the FLAME's surface M i ∈ R 3 , projected into the image by the estimated camera model. The landmark loss is defined as</p><formula xml:id="formula_7">L lmk = 68 i=1 k i − sΠ(M i ) + t 1 .<label>(5)</label></formula><p>Eye closure loss: The eye closure loss computes the relative offset of landmarks k i and k j on the upper and lower eyelid, and measures the difference to the offset of the corresponding landmarks in the FLAME's surface M i and M j projected into the image. Formally, the loss is given as</p><formula xml:id="formula_8">L eye = (i,j)∈E k i − k j − sΠ(M i − M j ) 1 ,<label>(6)</label></formula><p>where E is the set of upper/lower eyelid landmark pairs. Photometric loss: The photometric loss computes the error between the input image I and the rendering I r as L pho = V I (I − I r ) 1,1 . Here, V I is a face mask with value 1 in the face skin region, and value 0 elsewhere obtained by an existing face segmentation method <ref type="bibr" target="#b44">[46]</ref>, and denotes the Hadamard product. Computing the error in the face region only provides robustness to common occlusions by e.g. hair, clothes, sunglasses, etc. Identity loss: Recent 3D face reconstruction methods demonstrate the effectiveness of utilizing an identity loss to produce more realistic face shapes <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b22">24]</ref>. Motivated by this, we also use a pretrained face recognition network <ref type="bibr" target="#b9">[10]</ref>, to employ an identity loss during training.</p><p>The face recognition network f outputs feature embeddings of the rendered images and the input image, and the identity loss then measures the cosine similarity between the two embeddings. Formally, the loss is defined as</p><formula xml:id="formula_9">L id = 1 − f (I)f (I r ) f (I) 2 · f (I r ) 2 .<label>(7)</label></formula><p>Shape consistency loss: Given two images I i and I j of the same subject (i.e. c i = c j ), the coarse encoder E c should output the same shape parameters (i.e. β i = β j ). Previous work encourages shape consistency by enforcing the distance between β i and β j to be smaller by a margin than the distance to the shape coefficients corresponding of a different subject <ref type="bibr" target="#b56">[58]</ref>. However, choosing this fixed margin is challenging in practice. Instead, we propose a different strategy by replacing β i with β j while keeping all other parameters unchanged. Given that β i and β j represent the same subject, this new set of parameters must reconstruct I i well. Formally, we minimize </p><formula xml:id="formula_10">L sc = L coarse (I i , R(M (β j , θ i , ψ i ), A(α i ), l i , c i )). (8) Regularization: L reg regularizes shape E β = β 2 2 , ex- pression E ψ = ψ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Detail reconstruction</head><p>The detail reconstruction aims at augmenting the coarse FLAME geometry with a detailed UV displacement map D ∈ [−0.01, 0.01] d×d (see <ref type="figure" target="#fig_1">Fig. 2</ref>). Similar to the coarse reconstruction, we train an encoder E d (with the same architecture as E c ) to encode I to a 128-dimensional latent code δ, representing subject-specific details. The latent code δ is then concatenated with FLAME's expression ψ and jaw pose parameters θ jaw , and decoded by F d to D. Detail decoder: The detail decoder is defined as</p><formula xml:id="formula_11">D = F d (δ, ψ, θ jaw ),<label>(9)</label></formula><p>where the detail code δ ∈ R 128 controls the static personspecific details. We leverage the expression ψ ∈ R 50 and jaw pose parameters θ jaw ∈ R 3 from the coarse reconstruction branch to capture the dynamic expression wrinkle details. For rendering, D is converted to a normal map.</p><p>Detail rendering: The detail displacement model allows us to generate images with fine-scale surface details. To reconstruct the detailed geometry M , we convert M and its surface normals N to UV space, denoted as M uv ∈ R d×d×3 and N uv ∈ R d×d×3 , and combine them with D as</p><formula xml:id="formula_12">M uv = M uv + D N uv .<label>(10)</label></formula><p>By calculating normal N from M , we obtain the detail rendering I r by rendering M with applied normal map as</p><formula xml:id="formula_13">I r = R(M, B(α, l, N ), c).<label>(11)</label></formula><p>The detail reconstruction is trained by minimizing</p><formula xml:id="formula_14">L detail = L phoD + L mrf + L sym + L dc + L regD ,<label>(12)</label></formula><p>with photometric detail loss L phoD , ID-MRF loss L mrf , soft symmetry loss L sym , and detail regularization L regD . Detail photometric losses: With the applied detail displacement map, the rendered images I r contain some geometric details. Equivalent to the coarse rendering, we use a photometric loss L phoD = V I (I − I r ) 1,1 , where, recall, V I is a mask representing the visible skin pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID-MRF loss:</head><p>We add an Implicit Diversified Markov Random Fields (ID-MRF) loss <ref type="bibr" target="#b76">[78]</ref> to reconstruct geometric details. Given two images of the same person, the ID-MRF loss extracts feature patches from different layers of a pretrained network, and then minimizes the difference between corresponding nearest neighbor feature patches from both images. Following Wang et al. <ref type="bibr" target="#b76">[78]</ref>, the loss is computed on layers conv3 2 and conv4 2 of VGG19 <ref type="bibr" target="#b62">[64]</ref> as</p><formula xml:id="formula_15">L mrf = 2L M (conv4 2) + L M (conv3 2),<label>(13)</label></formula><p>where L M (layer th ) denotes the ID-MRF loss which is employed on the feature patches extracted from I r and I with layer layer th of VGG19. As for the photometric losses, we compute L mrf only for the face skin region in UV space. Soft symmetry loss: To add robustness to occlusions, we add a soft symmetry loss to regularize non-visible face parts. Specifically, we minimize</p><formula xml:id="formula_16">L sym = V uv (D − f lip(D)) 1,1 ,<label>(14)</label></formula><p>where V uv denotes the face skin mask in UV space, and f lip is the horizontal flip operation.</p><p>Detail regularization: The detail displacements are regularized by L regD = D 1,1 to reduce noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Detail disentanglement</head><p>Optimizing L detail enables us to reconstruct faces with mid-frequency details. Making these detail reconstructions animatable however requires us to disentangle person specific details (i.e. moles, pores, eyebrows, and expressionindependent wrinkles) controlled by δ from expressiondependent wrinkles (i.e. wrinkles that change for varying facial expression) controlled by FLAME's expression and jaw pose parameters, ψ and θ jaw . Our key observation is that the same person in two images should have both similar coarse geometry and personalized details. So for the rendered detail image, exchanging the detail codes between two images of the same subject should have no effect on the rendered image. Detail consistency loss: Given two images I i and I j of the same subject (i.e. c i = c j ), the loss is defined as</p><formula xml:id="formula_17">L dc = L detail (I i , R(M (β i , θ i , ψ i ), A(α i ), F d (δ j , ψ i , θ jaw,i ), l i , c i )),<label>(15)</label></formula><p>where β i , θ i , ψ i , θ jaw,i , α i , l i , and c i are the parameters of I i , while δ j is the detail code of I j (see <ref type="figure" target="#fig_2">Fig. 3</ref>). We show the necessity and effectiveness of L dc in Sec. 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>Data: We train DECA on three publicly available datasets: VGGFace2 <ref type="bibr" target="#b9">[10]</ref>, BUPT-Balancedface <ref type="bibr" target="#b75">[77]</ref> and VoxCeleb2 <ref type="bibr" target="#b14">[16]</ref>. VGGFace2 <ref type="bibr" target="#b9">[10]</ref> contains images of over 8k subjects, with an average of more than 350 images per subject. BUPT-Balancedface <ref type="bibr" target="#b75">[77]</ref> offers 7k subjects per ethnicity (i.e. Caucasian, Indian, Asian and African), and VoxCeleb2 <ref type="bibr" target="#b14">[16]</ref> contains 145k videos of 6k subjects. In total, DECA is trained on 2 Million images. All datasets provide an identity label for each image. We use FAN <ref type="bibr" target="#b7">[8]</ref> to predict 68 2D landmarks k i on each face. To improve the robustness of the predicted landmarks, we run FAN for each image twice with different face crops, and discard all images with non-matching landmarks. See Sup. Mat. for details on data selection and data cleaning. Implementation details: DECA is implemented in Py-Torch <ref type="bibr" target="#b46">[48]</ref>, using the differentiable rasterizer from Py-torch3D <ref type="bibr" target="#b49">[51]</ref> for rendering. We use Adam <ref type="bibr" target="#b37">[39]</ref> as optimizer with a learning rate of 1e − 4. The input image size is 224 2 and UV space size d = 256. See Sup. Mat. for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Qualitative evaluation</head><p>Reconstruction: Given a single face image, DECA reconstructs the 3D face shape with mid-frequency geometry details. The second row of <ref type="figure">Fig. 1</ref> shows that the coarse shape (i.e. in FLAME space) well represents the overall face shape, and the learned DECA detail model reconstructs subject-specific details and wrinkles of the input identity ( <ref type="figure">Fig. 1 row three)</ref>, while being robust to partial occlusions. <ref type="figure">Figure 4</ref> qualitatively compares DECA results with stateof-the-art coarse face reconstruction methods, namely PR-Net <ref type="bibr" target="#b19">[21]</ref>, RingNet <ref type="bibr" target="#b56">[58]</ref>, Deng et al. <ref type="bibr" target="#b16">[18]</ref>, FML <ref type="bibr" target="#b65">[67]</ref> and 3DDFA-V2 <ref type="bibr" target="#b27">[29]</ref>. Compared to these methods, DECA better reconstructs the overall face shape with details like the nasolabial fold (rows 1, 2, 3, 4, and 6) and forehead wrinkles (row 3). DECA better reconstructs the mouth shape and the eye region than all other methods. DECA further reconstructs a full head while PRNet <ref type="bibr" target="#b19">[21]</ref>, Deng et al. <ref type="bibr" target="#b16">[18]</ref>, FML <ref type="bibr" target="#b65">[67]</ref> and 3DDFA-V2 <ref type="bibr" target="#b27">[29]</ref> reconstruct tightly cropped faces. While RingNet <ref type="bibr" target="#b56">[58]</ref>, like DECA, is based on FLAME <ref type="bibr" target="#b40">[42]</ref>, DECA better reconstructs the face shape and the facial expression. <ref type="figure" target="#fig_4">Figure 5</ref> compares DECA visually to existing detail face reconstruction methods, namely Extreme3D <ref type="bibr" target="#b71">[73]</ref>, Crossmodal <ref type="bibr" target="#b0">[1]</ref>, and FaceScape <ref type="bibr" target="#b80">[82]</ref>. Extreme3D <ref type="bibr" target="#b71">[73]</ref> and Crossmodal <ref type="bibr" target="#b0">[1]</ref> reconstruct more details than DECA but at the cost of being less robust to occlusions (rows 1, 2, 3). Unlike DECA, Extreme3D and Cross-modal only reconstruct static details. However, using static details instead of DECA's animatable details leads to visible artifacts when animating the face (see <ref type="figure" target="#fig_5">Fig. 6</ref>). While FaceScape <ref type="bibr" target="#b80">[82]</ref> provides animatable details, unlike DECA, the method is trained on high-resolution scans while DECA is solely trained on inthe-wild images. Also, with occlusion, FaceScape produces artifacts (rows 1, 2) or effectively fails (row 3).</p><p>In summary, DECA produces high-quality reconstructions, outperforming previous work in terms of robustness, while enabling animation of the detailed reconstruc- <ref type="figure">Figure 4</ref>: Comparison to other coarse reconstruction methods, from left to right: PRNet <ref type="bibr" target="#b19">[21]</ref>, RingNet <ref type="bibr" target="#b56">[58]</ref> Deng et al. <ref type="bibr" target="#b16">[18]</ref>, FML <ref type="bibr" target="#b65">[67]</ref>, 3DDFA-V2 <ref type="bibr" target="#b27">[29]</ref>, DECA (ours). tion. To demonstrate the quality of DECA and the robustness to variations in head pose, expression, occlusions, image resolution, lighting conditions, etc., we show results for 200 randomly selected ALFW2000 <ref type="bibr" target="#b82">[84]</ref> images in the Sup. Mat. along with more qualitative coarse and detail reconstruction comparisons to the state-of-the-art. Detail animation: DECA models detail displacements as a function of subject-specific detail parameters δ and FLAME's jaw pose θ jaw and expression parameters ψ. This formulation allows us to animate detailed facial geometry such that wrinkles are specific to the source shape and expression as shown in <ref type="figure">Fig. 1</ref>. Using static details instead of DECA's animatable details (i.e. by using the reconstructed details as a static displacement map) and animating only the coarse shape by changing the FLAME parameters results in visible artifacts as shown in <ref type="figure" target="#fig_5">Fig. 6 (top)</ref>, while animatable details (middle) look similar to the reference shape (bottom) of the same identity. The Sup. Mat. shows more comparisons of animatable and static details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Quantitative evaluation</head><p>We compare DECA with publicly available methods, namely 3DDFA-V2 <ref type="bibr" target="#b27">[29]</ref>, Deng et al. <ref type="bibr" target="#b16">[18]</ref>, RingNet <ref type="bibr" target="#b56">[58]</ref>, PRNet <ref type="bibr" target="#b19">[21]</ref>, 3DMM-CNN <ref type="bibr" target="#b70">[72]</ref> and Extreme3D <ref type="bibr" target="#b71">[73]</ref>. NoW benchmark: The NoW challenge <ref type="bibr" target="#b56">[58]</ref> consists of 2054 face images of 100 subjects, split into a validation set (20 subjects) and a test set (80 subjects), with a reference 3D face scan per subject. The images consist of indoor and outdoor images, neutral expression and expressive face images, partially occluded faces, and varying viewing angles ranging from frontal view to profile view, and selfie images.  The challenge provides a standard evaluation protocol that measures the distance from all reference scan vertices to the closest point in the reconstructed mesh surface, after rigidly   aligning scans and reconstructions. For details, see <ref type="bibr">[12]</ref>. We found that the tightly cropped face meshes predicted by Deng et al. <ref type="bibr" target="#b16">[18]</ref> are smaller than the NoW reference scans, which would result in a high reconstruction error in the missing region. For a fair comparison to the method of Deng et al. <ref type="bibr" target="#b16">[18]</ref>, we use the Basel Face Model (BFM) <ref type="bibr" target="#b47">[49]</ref> parameters they output, reconstruct the complete BFM mesh, and get the NoW evaluation for these complete meshes. As shown in Tab. 1 and the cumulative error plot in the Sup. Mat., DECA gives state-of-the-art results on NoW, providing the reconstruction error with the lowest mean, median, and standard deviation.  <ref type="bibr" target="#b20">[22]</ref> contains 2000 face images of 135 subject, and a reference 3D face scan for each subject. The benchmark consists of 1344 low-quality (LQ) images extracted from videos, and 656 high-quality (HQ) images taken in controlled scenarios. A protocol similar to NoW is used for evaluation that measures the distance between all reference scan vertices to the closest points on the reconstructed mesh surface, after rigidly aligning scan and reconstruction. As shown in Tab. 2 and the cumulative error plot in the Sup. Mat., DECA provides state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation experiment</head><p>Detail consistency loss: To evaluate the importance of our novel detail consistency loss L dc (Eq. 15), we train DECA with and without L dc . <ref type="figure" target="#fig_7">Figure 7</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and discussion</head><p>We have presented DECA, which enables detailed expression capture and animation from single images by learning an animatable detail model from in-the-wild images. In total, DECA is trained from about 2M in-the-wild face images without 2D-to-3D supervision. DECA reaches state-of-the-art shape reconstruction performance enabled by a shape consistency loss. A novel detail consistency loss helps DECA to disentangle expression-dependent wrinkles from person-specific details. The low-dimensional detail latent space makes the fine-scale reconstruction robust to noise and occlusions, and the novel loss leads to disentanglement of identity and expression-dependent wrinkle details. This enables applications like animation, shape change, wrinkle transfer, etc. DECA is publicly available for research purposes. Due to the reconstruction accuracy, the reliability, and the speed, DECA is useful for applications like face reenactment or virtual avatar creation.</p><p>DECA opens the door for future work. First, our albedo model is dependent on the Basel face model, which lacks ethnic diversity and facial hair. This pushes skin tone into the lighting model and causes facial hair to be explained by shape deformations. We believe that we can learn a more diverse albedo model from in-the-wild images using our system. Second, we want to extend the model over time, both for tracking and to learn more personalized models of individuals from video where we could enforce continuity of intrinsic wrinkles over time. Third, while robust, our method can still fail due to extreme head pose and lighting. This suggests the need for more diverse training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head><p>We thank S. Sanyal for providing us the RingNet Py-Torch implementation, support with paper writing, and fruitful discussions, and M. Kocabas, N. Athanasiou, and V. Fernández Abrevaya for the helpful suggestions. We further thank all Perceiving Systems department members for the feedback. This work was partially supported by the Max Planck ETH Center for Learning Systems. Disclosure: MJB has received research gift funds from Intel, Nvidia, Adobe, Facebook, and Amazon. While MJB is a part-time employee of Amazon, his research was performed solely at, and funded solely by, MPI. MJB has financial interests in Amazon and Meshcapade GmbH. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Data: DECA is trained on 2 Million images from VGGFace2 <ref type="bibr" target="#b9">[10]</ref> and BUPT-Balancedface <ref type="bibr" target="#b75">[77]</ref> and Vox-Celeb2 <ref type="bibr" target="#b14">[16]</ref>. From VGGFace2 <ref type="bibr" target="#b9">[10]</ref>, we randomly select 950k images such that 750K images are of resolution higher than 224 × 224, and 200K are of lower resolution. From BUPT-Balancedface <ref type="bibr" target="#b75">[77]</ref> we randomly sample 550k with Asian or African ethnicity labels to reduce the ethnicity bias of VGGFace2. From VoxCeleb2 <ref type="bibr" target="#b14">[16]</ref> we choose 500k frames, with multiple samples from the same video clip per subject to obtain data with variation only in the facial expression and head pose. We also sample 50k images from the VGGFace2 <ref type="bibr" target="#b9">[10]</ref>  </p><formula xml:id="formula_18">i D(k 2 i − − k 1 i ) ≥ 0.1,</formula><p>where k 2 i and k 1 i are the ith landmarks for the original and the shifted bounding box, respectively, and D denote the normalization matrix diag(b w , b h ) −1 . Training details: We pre-train the coarse model (i.e. E c ) for two epochs with a batch size of 64 with λ lmk = 1e − 4, λ eye = 1.0, λ β = 1e − 4, and λ ψ = 1e − 4. Then, we train the coarse model for 1.5 epochs with a batch size of 32, with 4 images per subject with λ pho = 2.0, λ id = 0.2, λ sc = 1.0, λ lmk = 1.0, λ eye = 1.0, λ β = 1e − 4, and λ ψ = 1e − 4. The landmark loss uses different weights for individual landmarks, the mouth corners and the nose tip landmarks are weighted by a factor of 3, other mouth and nose landmarks with a factor of 1.5, and all remaining landmarks have a weight of 1.0. This is followed by training the detail model (i.e. E d and F d ) on VGGFace2 and Vox-Celeb2 with a batch size of 6, with 3 images per subject, and parameters λ phoD = 2.0, λ mrf = 5e − 2, λ sym = 5e − 3, λ dc = 1.0, and λ regD = 5e − 3. The coarse model is fixed while training the detail model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Detail animation</head><p>As described in Section 6.1 and shown in <ref type="figure" target="#fig_5">Figure 6</ref> of the main paper, using a static displacement map to model geometric details instead of DECA's animatable details results in visible artifacts. <ref type="figure" target="#fig_8">Figure 8</ref> shows more examples where using static details results in artifacts in the mouth corner or the forehead region, while DECA's animated results look plausible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Quantitative evaluation</head><p>As described in Section 6.2 of the main paper, we quantitatively compare DECA with publicly available methods, namely 3DDFA-V2 <ref type="bibr" target="#b27">[29]</ref>, Deng et al. <ref type="bibr" target="#b16">[18]</ref>, RingNet <ref type="bibr" target="#b56">[58]</ref>, PRNet <ref type="bibr" target="#b19">[21]</ref>, 3DMM-CNN <ref type="bibr" target="#b70">[72]</ref> and Extreme3D <ref type="bibr" target="#b71">[73]</ref> on two existing 3D face reconstruction benchmarks, the NoW challenge <ref type="bibr" target="#b56">[58]</ref> and the Feng et al. <ref type="bibr" target="#b20">[22]</ref> benchmark. The left of <ref type="figure" target="#fig_9">Figure 9</ref> shows the cumulative errors for <ref type="table" target="#tab_1">Table 1</ref> of the main paper, the middle and right of <ref type="figure" target="#fig_9">Figure 9</ref> show the cumulative errors for <ref type="table" target="#tab_2">Table 2</ref> of the main paper. Note that in all cases, the DECA curve in dark blue is aboth that of the other methods. This demonstrates that DECA gives state-of-the-art reconstruction performance for both benchmarks. <ref type="figure">Figure 10</ref> shows additional qualitative comparisons to existing coarse and detail reconstruction methods. DECA better reconstructs the overall face shape than all existing methods, it reconstructs more details than existing coarse reconstruction methods (e.g. (b), (e), (f)), and it is more robust to occlusions compared to existing detail reconstruction methods (e.g. (c), (d), (g)). As promised in the main paper (e.g. Section 6.1), we show results for more than 200 randomly selected ALFW2000 <ref type="bibr" target="#b82">[84]</ref> samples in <ref type="bibr">Figures 11,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b15">and 17</ref>. For each sample, we compare the DECA's detail reconstruction (e) with the state-of-the-art coarse reconstruction method 3DDFA-V2 <ref type="bibr" target="#b27">[29]</ref> (see (b)) and existing detail re-construction methods, namely FaceScape <ref type="bibr" target="#b80">[82]</ref> (see (c)), and Extreme3D <ref type="bibr" target="#b71">[73]</ref> (see (e)). In total, DECA reconstructs more details then 3DDFA-V2, and it is more robust to occlusions than FaceScape and Extreme3D. Further, the DECA retargeting results appear realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Qualitative comparisons</head><p>(a) (b) (c) (d) (e) (f) (g) (h) (i) <ref type="figure">Figure 10</ref>: Comparison to previous work, from left to right: (a) Input image, (b) 3DDFA-V2 <ref type="bibr" target="#b27">[29]</ref>, (c) FaceScape <ref type="bibr" target="#b80">[82]</ref>, (d) Extreme3D <ref type="bibr" target="#b71">[73]</ref>, (e) PRNet <ref type="bibr" target="#b19">[21]</ref>, (f) Deng et al. <ref type="bibr" target="#b16">[18]</ref>, (g) Cross-modal <ref type="bibr" target="#b0">[1]</ref>, (h) DECA detail reconstruction, and (i) reposing (animation) of DECA's detail reconstruction to a common expression. The expression in (i) is from the source expression E in <ref type="figure" target="#fig_1">Figure 2</ref> of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh. <ref type="figure">Figure 11</ref>: Qualitative comparisons on random ALFW2000 <ref type="bibr" target="#b82">[84]</ref> samples. a) Input image, b) 3DDFA-V2 <ref type="bibr" target="#b27">[29]</ref>, c) FaceScape <ref type="bibr" target="#b80">[82]</ref>, d) Extreme3D <ref type="bibr" target="#b71">[73]</ref>, e) DECA detail reconstruction, and f) reposing (animation) of DECA's detail reconstruction to a common expression. The expression in (i) is from the source expression E in <ref type="figure" target="#fig_1">Figure 2</ref> of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh. <ref type="figure" target="#fig_2">Figure 13</ref>: Qualitative comparisons on random ALFW2000 <ref type="bibr" target="#b82">[84]</ref> samples. a) Input image, b) 3DDFA-V2 <ref type="bibr" target="#b27">[29]</ref>, c) FaceScape <ref type="bibr" target="#b80">[82]</ref>, d) Extreme3D <ref type="bibr" target="#b71">[73]</ref>, e) DECA detail reconstruction, and f) reposing (animation) of DECA's detail reconstruction to a common expression. The expression in (i) is from the source expression E in <ref type="figure" target="#fig_1">Figure 2</ref> of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh. , c) FaceScape <ref type="bibr" target="#b80">[82]</ref>, d) Extreme3D <ref type="bibr" target="#b71">[73]</ref>, e) DECA detail reconstruction, and f) reposing (animation) of DECA's detail reconstruction to a common expression. The expression in (i) is from the source expression E in <ref type="figure" target="#fig_1">Figure 2</ref> of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh. , c) FaceScape <ref type="bibr" target="#b80">[82]</ref>, d) Extreme3D <ref type="bibr" target="#b71">[73]</ref>, e) DECA detail reconstruction, and f) reposing (animation) of DECA's detail reconstruction to a common expression. The expression in (i) is from the source expression E in <ref type="figure" target="#fig_1">Figure 2</ref> of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh. <ref type="figure" target="#fig_5">Figure 16</ref>: Qualitative comparisons on random ALFW2000 <ref type="bibr" target="#b82">[84]</ref> samples. a) Input image, b) 3DDFA-V2 <ref type="bibr" target="#b27">[29]</ref>, c) FaceScape <ref type="bibr" target="#b80">[82]</ref>, d) Extreme3D <ref type="bibr" target="#b71">[73]</ref>, e) DECA detail reconstruction, and f) reposing (animation) of DECA's detail reconstruction to a common expression. The expression in (i) is from the source expression E in <ref type="figure" target="#fig_1">Figure 2</ref> of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh. , c) FaceScape <ref type="bibr" target="#b80">[82]</ref>, d) Extreme3D <ref type="bibr" target="#b71">[73]</ref>, e) DECA detail reconstruction, and f) reposing (animation) of DECA's detail reconstruction to a common expression. The expression in (i) is from the source expression E in <ref type="figure" target="#fig_1">Figure 2</ref> of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>DECA training and animation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Detail consistency loss. See Sec. 4.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 2 ,</head><label>2</label><figDesc>and albedo E α = α 2 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Comparison to other detail reconstruction methods, from left to right: Extreme3D<ref type="bibr" target="#b71">[73]</ref>, FaceScape<ref type="bibr" target="#b80">[82]</ref>, Cross-modal<ref type="bibr" target="#b0">[1]</ref>, DECA (ours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Effect of DECA's animatable details. Given images of source identity I and source expression E (left), DECA reconstructs the detail shapes (middle) and animates the detail shape of I with the expression of E (right, middle). This synthesized DECA expression appears identical to the reconstructed same subject's reference detail shape (right, bottom). Using the reconstructed details of I instead (i.e. static details) and animating the coarse shape only, results in visible artifacts (right, top). See Sec. 6.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Feng et al. benchmark: The Feng et al. challenge</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>(left) shows the DECA details for detail code δ I from the source identity, and expression ψ E and jaw pose parameters θ jaw,E from the source expression. For DECA trained with L dc (top), wrinkles appear in the forehead as a result of the raised eyebrows of the source expression, while for DECA trained without L dc (bottom), no such wrinkles appear. This indicates that with-Ablation experiments. Left: Effects of L dc on the animation of the source identity with the source expression visualized on a neutral expression template mesh. Without L dc , no wrinkles appear in the forehead due to the surprise source expression. Right: Effect of L mrf on the detail reconstruction. Without L mrf , less details are reconstructed. out L dc , person-specific details and expression-dependent wrinkles are not well disentangled. See Sup. Mat. for more disentanglement results. ID-MRF loss: Figure 7 (right) shows the effect of L mrf on the detail reconstruction. Without L mrf (middle), wrinkle details (e.g. in the forehead) are not reconstructed, resulting in an overly smooth result. With L mrf (right), DECA captures the details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Effect of DECA's animatable details. Given a single image (top), DECA reconstructs a detailed mesh (second row). Using static details and animating the coarse FLAME shape only (third row) results in visible artifacts as highlighted by the red boxes. Instead, reposing with DECA's animatable details (bottom) results in a more realistic mesh with geometric details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Quantitative comparison to state-of-the-art on two 3D face reconstruction benchmarks, namely the NoW<ref type="bibr" target="#b56">[58]</ref> challenge (left) and the Feng et al.<ref type="bibr" target="#b20">[22]</ref> benchmark for low-quality (middle) and high-quality (right) images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Qualitative comparisons on random ALFW2000<ref type="bibr" target="#b82">[84]</ref> samples. a) Input image, b) 3DDFA-V2<ref type="bibr" target="#b27">[29]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 :</head><label>15</label><figDesc>Qualitative comparisons on random ALFW2000<ref type="bibr" target="#b82">[84]</ref> samples. a) Input image, b) 3DDFA-V2<ref type="bibr" target="#b27">[29]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 :</head><label>17</label><figDesc>Qualitative comparisons on random ALFW2000<ref type="bibr" target="#b82">[84]</ref> samples. a) Input image, b) 3DDFA-V2<ref type="bibr" target="#b27">[29]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Reconstruction error on the NoW<ref type="bibr" target="#b56">[58]</ref> benchmark.</figDesc><table><row><cell>Method</cell><cell cols="4">Median (mm) Mean (mm)</cell><cell cols="2">Std (mm)</cell></row><row><cell></cell><cell>LQ</cell><cell>HQ</cell><cell>LQ</cell><cell>HQ</cell><cell>LQ</cell><cell>HQ</cell></row><row><cell cols="3">3DMM-CNN [72] 1.88 1.85</cell><cell>2.32</cell><cell>2.29</cell><cell>1.89</cell><cell>1.88</cell></row><row><cell>Extreme3D [73]</cell><cell cols="2">2.40 2.37</cell><cell>3.49</cell><cell>3.58</cell><cell>6.15</cell><cell>6.75</cell></row><row><cell>PRNet [21]</cell><cell cols="2">1.79 1.60</cell><cell>2.38</cell><cell>2.06</cell><cell>2.19</cell><cell>1.79</cell></row><row><cell>RingNet [58]</cell><cell cols="2">1.63 1.58</cell><cell>2.08</cell><cell>2.02</cell><cell>1.79</cell><cell>1.69</cell></row><row><cell>3DDFA-V2 [29]</cell><cell cols="2">1.62 1.49</cell><cell>2.10</cell><cell>1.91</cell><cell>1.87</cell><cell>1.64</cell></row><row><cell>DECA (ours)</cell><cell cols="2">1.48 1.44</cell><cell>1.91</cell><cell>1.89</cell><cell>1.68</cell><cell>1.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Feng et al. [22] benchmark performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>test set for validation. Data cleaning: We generate a different crop for the face image by shifting the provided bounding box by 5% to the bottom right (i.e. shift by = 1 20 (b w , b h ) T , where b w and b h denote the bounding box width and height). Then we expand the original and the shifted bounding boxes by 10% to the top, and by 20% to the left, right, and bottom. We run FAN [8], providing the expanded bounding boxes as input and discard all images with max</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-modal deep face normals with deactivable skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adnane</forename><surname>Victoria Fernández Abrevaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boukhayma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inverse rendering of faces with a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Aldrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1080" to="1093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fitting a 3D morphable model to edges: A comparison between hard and soft correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wuhrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="377" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-quality single-shot capture of facial geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face identification across different poses and illuminations with a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Volker Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Review of statistical shape spaces for 3D data with comparative analysis for human faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augusto</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Wuhrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2D &amp; 3D face alignment problem? (and a dataset of 230,000 3D facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time high-fidelity facial performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">VGGFace2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse photometric 3D face reconstruction guided by morphable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anpei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4635" to="4644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Expnet: Landmark-free, deep, 3D facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Ju</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="122" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Personalized face modeling for improved face reconstruction and motion retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bindita</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noranart</forename><surname>Vesdapunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06759</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Photo-realistic facial details synthesis from single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anpei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9429" to="9439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Voxceleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Capture, learning, and synthesis of 3D speaking styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cudeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassidy</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10101" to="10111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate 3D face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end 3D face reconstruction with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shishir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5908" to="5917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D morphable face models -past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wuhrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vetter</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint 3D face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluation of dense 3D reconstruction from 2D face images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rätsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reconstruction of personalized 3D face rigs from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GANFIT: generative adversarial network fitting for high fidelity 3D face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baris</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised training for 3D morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8377" to="8386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Morphable face models-an open framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Luthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Schönborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A statistical model for synthesis of detailed facial geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksey</forename><surname>Golovinskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1025" to="1034" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DenseReg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Riza Alp Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snape</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6799" to="6808" />
		</imprint>
	</monogr>
	<note>Stefanos Zafeiriou, and Iasonas Kokkinos</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards fast, accurate and stable 3D dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CNN-based real-time dense face reconstruction with inverse-rendered photo-realistic face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1294" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Avatar digitization from a single image for real-time rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Fursund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iman</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carrie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno>195:1-195:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic 3D avatar creation from hand-held video input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Alexandru Eugen Ichim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large pose 3D face reconstruction from a single image via direct volumetric CNN regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Aaron S Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1031" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dense 3D face alignment from 2D videos in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>László</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D face reconstruction with geometry details from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4756" to="4770" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face reconstruction in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1746" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inverse-FaceNet: deep monocular inverse face rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4625" to="4634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust single-view geometry and motion reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">175</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Realtime facial animation with on-the-fly correctives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="42" to="43" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning a model of facial shape and expression from 4D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature-preserving detailed 3D face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Visual Media Production</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Claus Müller. Spherical Harmonics. Springer Berlin Heidelberg</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">paGAN: real-time avatars using dynamic textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviral</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Fursund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno>258:1-258:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On face segmentation, face swapping, and face perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tran</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A parametric model for human faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parke</forename><surname>Frederick Ira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
		</imprint>
		<respStmt>
			<orgName>University of Utah</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Synthesizing realistic facial expressions from photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/pytorch3d,2020.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3D face reconstruction by learning from synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="460" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Face identification by fitting a 3D morphable model using linear shape and texture error functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Estimating 3D shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="986" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adaptive 3D face reconstruction from unconstrained photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4197" to="4206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Photorealistic facial texture inference using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5144" to="5153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to regress 3D face shape and expression from an image without 3D supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Computer-suggested facial makeup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hullin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="485" to="492" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unrestricted facial geometry reconstruction using image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1576" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">SfSNet: learning shape, reflectance and illuminance of faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6296" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Automatic acquisition of high-fidelity facial performances using monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">222</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Extraction and transfer of facial expression wrinkles for facial performance enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il-Kyu</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeon-Joong</forename><surname>Cengizöztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Mi</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Conference on Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">High quality facial surface and texture synthesis via generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Slossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Shamai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops (ECCV-W)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Total moving face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="796" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">FML: Face Model Learning from Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Self-supervised multi-level face model learning for monocular reconstruction at over 250 Hz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2549" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">MoFA: model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1274" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Real-time expression transfer for facial reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="183" to="184" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Face2Face: Real-time face capture and reenactment of RGB videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3D morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Extreme 3D face reconstruction: Seeing through occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Towards highfidelity nonlinear 3D face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Joint 3D face reconstruction and dense face alignment from a single image with 2D-assisted self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Estimating coloured 3D face models from single images: An example based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="499" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Racial faces in the wild: Reducing racial bias by information maximization adaptation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohai</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Image inpainting via generative multi-column convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05562</idno>
		<title level="m">3D dense face alignment via graph convolution networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Realtime performance-based facial animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Weise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">77</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">High-fidelity facial reflectance and geometry inference from an unconstrained image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugo</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno>162:1-162:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">FaceScape: a large-scale high quality 3D face dataset and detailed riggable 3D face prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">DF2Net: A dense-fine-finer network for detailed 3D face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">State of the art on monocular 3D face reconstruction, tracking, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics State of the Art Reports</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Computer Graphics Forum</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Input image, b) 3DDFA-V2 [29], c) FaceScape [82], d) Extreme3D [73], e) DECA detail reconstruction, and f) reposing (animation) of DECA&apos;s detail reconstruction to a common expression</title>
	</analytic>
	<monogr>
		<title level="m">Qualitative comparisons on random ALFW2000 [84] samples. a)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>The expression in (i) is from the source expression E in Figure 2 of the main paper. Blank entries indicate that the particular method did not return any reconstructed mesh</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
