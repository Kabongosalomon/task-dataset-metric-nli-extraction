<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Does BERT Understand Sentiment? Leveraging Comparisons Between Contextual and Non-Contextual Embeddings to Improve Aspect-Based Sentiment Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natesh</forename><surname>Reddy</surname></persName>
							<email>natesh@paralleldots.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Paralleldots, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranaydeep</forename><surname>Singh</surname></persName>
							<email>pranaydeep@paralleldots.com</email>
							<affiliation key="aff1">
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Muktabh</surname></persName>
							<email>muktabh@paralleldots.com</email>
							<affiliation key="aff2">
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
							<affiliation key="aff2">
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Does BERT Understand Sentiment? Leveraging Comparisons Between Contextual and Non-Contextual Embeddings to Improve Aspect-Based Sentiment Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When performing Polarity Detection for different words in a sentence, we need to look at the words around to understand the sentiment. Massively pretrained language models like BERT can encode not only just the words in a document but also the context around the words along with them. This begs the questions, "Does a pretrain language model also automatically encode sentiment information about each word?" and "Can it be used to infer polarity towards different aspects?". In this work we try to answer this question by showing that training a comparison of a contextual embedding from BERT and a generic word embedding can be used to infer sentiment. We also show that if we finetune a subset of weights the model built on comparison of BERT and generic word embedding, it can get state of the art results for Polarity Detection in Aspect Based Sentiment Classification datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect-based Sentiment Analysis (ABSA) has always been a topic of keen research interest due to the endless commercial applications. The ability to perceive polarities associated with particular entities has several key implications for openly available social media data as well as data from websites like Amazon. To associate fine-grained sentiment with certain aspects is of a much higher degree of difficulty than standard sentence-level or document-level sentiment analysis tasks.</p><p>ABSA is divided into two sub-tasks. Firstly, Aspect extraction, the identification of entities which represent certain properties of a subject. For example in a laptop review from Amazon "The battery is fantastic but the display is a bit underwhelming", the words battery and display represent the aspects of the laptop that we want to extract. The second task, Polarity detection, aims to understand the sentiment associated with each individual aspect. In this case, the polarity for battery would be positive, while the polarity for display would be negative. In this work, we aim to focus primarily on the second task of Polarity detection given a set of aspects and the associated text.</p><p>Deep Neural Networks paved the way for most major developments in NLP. Even though earlier feature-based methods <ref type="bibr" target="#b8">(Samha et al., 2014)</ref> could be tailored for particular domains, DNNs generalized better. Hybrid models <ref type="bibr" target="#b13">(Wallaart and Frasincar, 2019)</ref>, combining the two approaches also yielded significant improvements. Deep Contextual Embeddings though, brought out a whole new class of models that outperformed all previous approaches. Massively pre-trained Language Models like BERT <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref> and XLNet <ref type="bibr" target="#b15">(Yang et al., 2020)</ref>, yielded state-of-the-art results by simply transfer learning for downstream tasks like ABSA. Our contributions to this area are two fold:</p><p>1. We demonstrate that by training to compare Deep Contextual Embeddings with standard GloVe embeddings <ref type="bibr" target="#b3">(Pennington et al., 2014)</ref> for a particular aspect, one can obtain meaningful representations for polarity and result in a model that requires minimal compute and training time but beats baselines that take 3 times the amount of compute and training time.</p><p>2. We also demonstrate that when the above method is trained along with fine-tuning the last five layers of BERT, it is able to achieve state-of-the-art results on multiple standard datasets, while still being a significantly smaller and faster model when compared to recent approaches. arXiv:2011.11673v1 [cs.CL] 23 Nov 2020 With the onset of Large pre-trained Language Models, significant improvements were achieved by simply fine-tuning for downstream tasks like ABSA. Methods like BERT-QA  further exploit the generalization abilities of BERT by framing ABSA as a Question Answering task, thus also increasing training data via augmentations for QA. Further, methods like BAT <ref type="bibr" target="#b2">(Karimi et al., 2020)</ref> introduced the advantages of adversarial examples as a source of additional supervision.</p><p>In some datasets however, methods that used domain specific ontologies were still found to be superior over neural networks with millions of parameters. BERT-PT , therefore explored the advantages of domain specific pretraining for BERT embeddings to further improve domain-level context. These methods however fail to generalize on smaller domains due to the lack of pre-training data.</p><p>Our hypothesis in this work is that training to compare good contextual embeddings of aspect terms derived from BERT with generic noncontextual embeddings of the same aspects can be used to derive the sentiment the document has about the aspect terms. We use the GloVe Embedding <ref type="bibr" target="#b3">(Pennington et al., 2014)</ref> of an aspect term as an approximation of its non-contextual embedding. GloVe Embeddings aren't trained on domain specific data but cover data from a wide lexica. We thus assume that GloVe embeddings would provide a good enough generic non-contextual embedding. Training on a combination of BERT embeddings and GloVe embeddings is thus assumed to be able to decipher sentiment. We also explore the advantages of combining information from multiple layers of BERT using Interaction Layers like Au-toInt <ref type="bibr" target="#b9">(Song et al., 2019)</ref> to improve context resolution. We aim to achieve the above goals while keeping the trainable model size and training time very limited, in contrast to other recent approaches which extensively fine-tune BERT to increase performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We build on the hypothesis that Deep Contextual Embeddings like BERT capture some semantics related to the polarity associated with the aspect, by default. This is backed by the fact that BERT or XLNet Embeddings when fine-tuned for Aspect Polarity Detection achieve near state-of-theart results. We further hypothesize that the Polarity in the semantics can be better derived by comparing contextual GLoVE embeddings with neutral embeddings of the aspect word from standard non-Contextualized embeddings like GloVe and word2vec. To further improve the resolution of BERT Embeddings, we use output from multiple layers of BERT and aggregate them into a single  <ref type="bibr" target="#b9">(Song et al., 2019)</ref>. Given a sentence s and a set of aspects {a 1 , a 2 , a 3 ...a n }, we obtain Contextual Embeddings {c m1 , c m2 , c m3 ...c mn } for each aspect from the m th layer of BERT, while obtaining Non-Contextual Embeddings {g 1 , g 2 , g 3 ...g n } from a pre-trained GloVe model. We combine embeddings from the last 5 layers of BERT using muti-headed self-attention, as this has been shown to better combine higher order features than standard weighted averaging. The modified embedding after applying the self-attention weights for a single head can be represented as,</p><formula xml:id="formula_0">c mn = 5 k=1 α m,k (W value c kn )<label>(1)</label></formula><p>We combine outputs from 8 such heads and concatenate them, while also adding a residual connection to preserve raw features from the original BERT layers.</p><formula xml:id="formula_1">c mn = ReLU (c mn + W c mn )<label>(2)</label></formula><p>We combine the refined contextual embeddings from the 5 layers and project them to 512 dimensions along with the GloVe Embeddings g n for the aspect, and then concatenate the two, followed by a Classification Layer.</p><formula xml:id="formula_2">c 512 n = W T (c 1n ⊕ c 2n ⊕ c 3n ⊕ c 4n ⊕ c 5n ) (3) g 512 n = W T (g n )<label>(4)</label></formula><p>O n = Sof tM ax(W T (c 512 n ⊕ g 512 n )) (5)</p><p>where O, is of the size M × B, where M is the number of classes and B is the batch size. We use Cross-Entropy loss, thus minimizing,</p><formula xml:id="formula_3">Loss = − M c=1 Y c log(O n,c )<label>(6)</label></formula><p>where Y c is the true probability for class c, while O n,c is the predicted probability for class c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We carry out experiments primarily on 3 standard datasets: The details of the train-test splits and the perlabel distributions are summarized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>We jointly evaluate two models for all 4 of the data sets. The default model described in Section 3 is referred to as BERT-IL. The 768 dimensional BERT embeddings are obtained from the standard pretrained BERT-base uncased model, while the 300 dimensional GloVe embeddings are obtained from standard GLoVE embeddings pre-trained on the English Wikipedia dump from 2014. We also evaluate BERT-IL Finetuned, where the final 5 layers of BERT are jointly trained alongside the BERT-IL Model. While BERT-IL Finetuned is always expected to perform better than BERT-IL, it comes at the price of a higher compute cost and memory for training and deployment. We train both models with the Adam optimizer with a learning rate of 1e-5 and Dropout of 0.1, with a batch size of 8. We report 3-way percentage accuracy scores for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We compare our results with 2 baselines. The BERT baseline simply uses BERT pre-trained embeddings from the bert-base-uncased model with a classification layer. The BERT-Finetune model follows a similar approach but trains the final 5 layers of BERT, increasing the trainable parameters and also the accuracy significantly.  Results for the Restaurant dataset from SemEval 2014 Task 4 Sub-task 2 have been summarized in <ref type="table" target="#tab_2">Table 2</ref>. BERT-IL Finetuned achieved the best scores, improving the previous state-of-the-art of BAT by 0.2%, while BERT-IL beats the standard BERT baseline by a significant margin with a fraction of the training time and trainable model size. We compare with other BERT based models like BERT-PT which explores a novel post training approach for RRC (Review Reading Comprehension) and ABSA. The Interactive Multi-task Learning Network (IMN) <ref type="bibr" target="#b1">(He et al., 2019)</ref> explores the concept of exploiting joint training of Aspect Extraction and Polarity Detection to increase performance for both tasks. The previous state-of-theart approach BAT leverages adversarial training along with BERT post training. While there are approaches like BERT-ADA <ref type="bibr" target="#b6">(Rietzler et al., 2019)</ref> that beat the scores of BAT as well, they use domain specific adaptation and additional data for pre-training which are not effective techniques for niche domains with limited data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-Finetune</head><p>Results for the SemEval 2016 Task 5 Sub-task 1 results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Similar to SemEval 2014, BERT-IL Finetuned achieves the best scores, beating the previous state-of-the-art of HAABSA <ref type="bibr" target="#b13">(Wallaart and Frasincar, 2019)</ref>     <ref type="table" target="#tab_5">Table 4</ref>. BERT-IL Finetuned seems to underperform here compared to approaches like BERT-pair-QAM  which reconstructs ABSA as a QA task. The significant difference in performance can be explained by the data augmentations introduced, which result in a dataset almost 3 times larger than vanilla Sentihood. However, here too BERT-IL bests the BERT baseline with ease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Deployment and training resources have been a large factor in development of the next generation of NLP models. Large pre-trained Language Models even though state-of-the-art in most downstream tasks can become taxing to even fine-tune. Not only are larger models costly to deploy, in recent times concerns of carbon emissions due to the fine-tuning of large models have also been raised. We demonstrate that with the simple BERT-IL model, without fine-tuning BERT layers, we beat baseline scores while keeping the trainable model size and training time unprecedentedly low. While with BERT-IL Finetuned we achieve stateof-the-art results on 2 standard datasets, it still manages to keep a minimal memory footprint, upto 65%   lesser than similar models, while reducing training time and thus carbon emissions by upto 75%. <ref type="table" target="#tab_7">Table 5</ref> gives details of the Trainable Model Size(MB) and the time in seconds for a single forward and backward pass, of various models evaluated on the SemEval 14 Restaurant dataset. Since metrics like Trainable Model Size and Time for a single pass might ignore the performance of a model, we use a couple of metrics to compare the Economy and Green Scores for all the models listed. The comparisons are shown in <ref type="figure">Figure  2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We demonstrate that sentiment polarity for words in a document can be predicted by learning to compare its contextual BERT embeddings with standard Non-Contextual embeddings like GloVe. This method can be made to work just by learning a few parameters on features from pretrained models and if a few layers of the models are incrementally fine-tuned, this simple hypothesis can give competitive results. We achieve state-of-the-art results for two standard datasets with this methods, while also keeping the trainable model size and training time minimal compared to previous methods. While BERT-IL Finetuned beats previous state-of-the-art results, we also introduce the BERT-IL model which is deployment friendly and upto three times smaller and upto four times faster compared to previous methods. Because the polarity is being inferred just based on comparison of two different source of embeddings, future work on this method can also help in domain generalization of Aspect Based Sentiment Analysis tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Simplified overview of the comparison network for BERT and GloVe Embeddings2 Related ResearchWhile initial work in ABSA<ref type="bibr" target="#b8">(Samha et al., 2014)</ref> relied heavily on feature engineering and ontologies, deep neural networks quickly asserted their superiority. There was although a certain lack of domain specific supervision with DNNs. Hybrid approaches like HAABSA<ref type="bibr" target="#b13">(Wallaart and Frasincar, 2019)</ref> exploit domain ontologies along with deep nearal networks to get further improvements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Accuracy/Model Size(MB) Score Comparison (b) Accuracy/Time(s) Score Comparison Figure 2: Economy and Green Scores Comparison for various recent ABSA models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results for the Resturant domain data from SemEval 2014 Task 4 Subtask 2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">: Results for the SemEval 2016 Task 5 Sub-task</cell></row><row><cell>1 Data</cell><cell></cell></row><row><cell cols="2">Our simpler BERT-IL method here too, beats the</cell></row><row><cell>baseline of BERT.</cell><cell></cell></row><row><cell>BERT</cell><cell></cell></row><row><cell>BERT-Finetune</cell><cell></cell></row><row><cell>Sentic-LSTM</cell><cell>89.32</cell></row><row><cell>REN</cell><cell>91.0</cell></row><row><cell>BERT-pair-QAM</cell><cell>93.6</cell></row><row><cell>BERT-IL</cell><cell>85.8</cell></row><row><cell cols="2">BERT-IL Finetuned 90.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results for the SentiHood dataset Finally, we show results for the Sentihood dataset in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Occupied size on GPU in MBs and Time taken for a single forward and backward pass in seconds for Batch Size 1</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An interactive multi-task learning network for end-to-end aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlmeier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adversarial training for aspect-based sentiment analysis with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akbar</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Prati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 5: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Smadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orphée</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Véronique</forename><surname>De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kotelnikov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>Nuria Bel, Salud María Jiménez-Zafra, and Gülşen Eryigit; San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adapt or get left behind: Domain adaptation through bert language model finetuning for aspect-target sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rietzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stabinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Engl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SentiHood: Targeted aspect based sentiment analysis dataset for urban neighbourhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1546" to="1556" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Aspect-based opinion extraction from customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Samha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoint</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3357925</idno>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Utilizing bert for aspect-based sentiment analysis via constructing auxiliary sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Flavius Frasincar, and Rommert Dekker. 2020. A hybrid approach for aspect-based sentiment analysis using deep contextual word embeddings and hierarchical attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mihaela Trusca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wassenberg</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A Hybrid Approach for Aspect-Based Sentiment Analysis Using a Lexicalized Domain Ontology and Attentional Neural Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Wallaart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavius</forename><surname>Frasincar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-21348-0_24</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="363" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bert post-training for review reading comprehension and aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
