<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zigzag Learning for Weakly Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at San Antonio</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Zigzag Learning for Weakly Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses weakly supervised object detection with only image-level supervision at training stage. Previous approaches train detection models with entire images all at once, making the models prone to being trapped in sub-optimums due to the introduced false positive examples. Unlike them, we propose a zigzag learning strategy to simultaneously discover reliable object instances and prevent the model from overfitting initial seeds. Towards this goal, we first develop a criterion named mean Energy Accumulation Scores (mEAS) to automatically measure and rank localization difficulty of an image containing the target object, and accordingly learn the detector progressively by feeding examples with increasing difficulty. In this way, the model can be well prepared by training on easy examples for learning from more difficult ones and thus gain a stronger detection ability more efficiently. Furthermore, we introduce a novel masking regularization strategy over the high level convolutional feature maps to avoid overfitting initial samples. These two modules formulate a zigzag learning process, where progressive learning endeavors to discover reliable object instances, and masking regularization increases the difficulty of finding object instances properly. We achieve 47.6% mAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Current state-of-the-art object detection performance has been achieved with a fully supervised paradigm. However, it requires a large quantity of high-quality object-level annotations (i.e., object bounding boxes) at training stages <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, which are very costly to collect. Fortunately, the prevalence of image tags allows search engines to quickly provide a set of images related to the target category <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, making image-level annotations much easier to acquire. Hence it is more appealing to learn detection models from such weakly labeled images. In this paper, we focus on object detection under a weakly supervised paradigm, where  <ref type="figure" target="#fig_4">Figure 1</ref>. Object difficulty scores predicted by our proposed mEAS. Higher scores indicate the object is easier to localize. This paper proposes a zigzag learning based detector to progressively learn from object instances in the order according to mEAS, with a novel masking regularization to avoid overfitting initial samples.</p><p>only image-level labels indicating the presence of an object are available during training.</p><p>The main challenge in weakly supervised object detection is how to disentangle object instances from the complex backgrounds. Most previous methods model the missing object locations as latent variables, and optimize them via different heuristic methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Among them, a typical solution is alternating between model re-training and object re-localization, which shares a similar spirit with Multiple Instance Learning (MIL) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Nevertheless, such optimization is non-convex and easy to get stuck in local minimums if the latent variables are not properly initialized. Then mining object instances with only imagelevel labels becomes a classical chicken-and-egg problem: without an accurate detection model, object instances cannot be discovered, while an accurate detection model cannot be learned without appropriate object examples.</p><p>To solve this problem, this paper proposes a zigzag learning strategy for weakly supervised object detection, which aims at mining reliable object instances for model training, and meanwhile avoiding getting trapped in local minimums. As our first contribution, different from previous works which perform model training and object re-localization over the entire images all at once <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we progressively feed the images into the learning model in an easy-to-difficult order <ref type="bibr" target="#b12">[13]</ref>. To this end, we propose an effective criterion named mean Energy Accumulated Scores (mEAS) to automatically measure the difficulty of an image containing the target object, and progressively add samples during model training. As shown in <ref type="figure" target="#fig_4">Fig. 1</ref>, car and dog are simpler to localize while horse and sheep are more difficult. Intuitively, ignoring this discrepancy of object difficulty in localization would inevitably include many poorly localized samples, which deteriorates the trained model. On the other hand, processing easier images in the initial stages leads to better detection models, which in turn increases the probability of successfully localizing objects in difficult images.</p><p>Due to lack of object annotations, the mined object instances inevitably include false positive samples. Current approaches <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> simply treat these pseudo annotations as ground truth, which is suboptimal and easy to overfit the initial seeds. This is especially true for a deep network due to its high fitting capacity. As our second contribution, we design a novel masking strategy over the last convolutional feature maps, which randomly erases the discriminative regions during training. It prevents the model from concentrating on part details at earlier training, and induces the network to focus more on those less discriminative parts at current training. In this way, the model is able to discover more integrated objects as desired. Another advantage is that the proposed masking operation introduces many random occluded samples, which can be treated as data augmentation and enhances the generalization ability of the model.</p><p>Integrating the progressive learning and masking regularization formulates a zigzag learning process. The progressive learning endeavours to discover reliable object instances in an easy-to-difficult order, while the masking strategy increases the difficulty in a way favorable of object mining via introducing many random occluded samples. These two adversarial modules boost each other, and benefit both object instance mining and reducing model overfitting risks. The effectiveness of zigzag learning has been validated experimentally. On benchmark dataset PASCAL VOC 2007, we achieve an accuracy of 47.6% under weakly supervised paradigm, which surpasses the-state-of-the-arts by a large margin. To sum up, we make following contributions.</p><p>• We propose a new and effective criterion named mean Energy Accumulated Scores (mEAS) to automatically measure the difficulty of an image w.r.t. localizing a specific object. Based on mEAS, we train detection models via an easy-to-hard strategy. This kind of progressive learning is beneficial to finding reliable object instances especially for the difficult images.</p><p>• We introduce a feature masking strategy during an endto-end model learning, which not only forces the network to focus on less discriminative details during training, but also avoids model overfitting via introducing random occluded positive instances. Integrating these two components gives a novel zigzag learning method and achieves state-of-the-art performance for weakly supervised object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Our method is related with two fields: 1) image difficulty evaluation; 2) weakly supervised detection.</p><p>Evaluating image difficulty. Little literature has been devoted to evaluating the difficulty of an image. A preliminary work in <ref type="bibr" target="#b13">[14]</ref> estimates the image difficulty via analyzing some low-level cues such as edges, segments, and objectness scores. Similarly, <ref type="bibr" target="#b14">[15]</ref> assumes that image difficulty is most related with the object size, and builds a regression model to estimate the object size in an image. However, it needs extra object size annotations for training the regressor. In contrast, we propose an easy-to-compute criterion named mean Accumulated Energy Scores (mEAS) to automatically measure the difficulty of an image. The advantage is that the criterion is based on the network itself, and free of human interpretation.</p><p>Weakly supervised detection. It is intuitive to mine object instances from weakly labeled images <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and follow the pipeline of fully supervised detection based on the mined objects. Our proposed method is most related with <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, which try to obtain reliable object instances via an iterative updating strategy. However, these methods either detach the feature extraction and model training into separate steps <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, or simply utilize the high representation ability of CNN without considering model overfitting <ref type="bibr" target="#b10">[11]</ref>, which results in limited performance. Comparatively, we integrate model training and object mining into a unified framework, and propose a zigzag learning strategy to improve the generalization ability of the model. These modifications enable us to achieve superior detection accuracy under the weakly supervised paradigm.</p><p>Our method is also related with <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Oquab et al. <ref type="bibr" target="#b15">[16]</ref> proposed a weakly supervised object localization method by explicitly searching over candidate object locations at different scales during training. However, their localization result is limited since it only returns a center point for an object, not the tight bounding box. Bilen <ref type="bibr" target="#b16">[17]</ref> et al. proposed to model image-level loss as the accumulated scores over regions and performed detection based on the region scores. Nevertheless, this network is modeled as classification loss, which makes the detection model easily focus on object parts rather than the whole objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we elaborate on the proposed zigzag learning based weakly supervised detection model. Its overall architecture consists of three modules, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. The first module estimates image difficulty automatically via a backbone network <ref type="bibr" target="#b17">[18]</ref> trained with only image-level labels. The second module progressively adds samples to network training in an ascending order based on image difficulty. Third, we incorporate convolutional fea-  . Architecture of our proposed zigzag detection network. We first estimate the image difficulty with mean Accumulated Energy Scores (mEAS), organizing training images in an easy-to-difficult order. Then we introduce a masking strategy over the last convolutional feature maps of fast RCNN framework, which enhances the generalization ability of the model. ture masking into model training to regularize the high responsive patches during previous training and enhance the generalization ability of the model. In the following, we discuss these modules in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Estimating Image Difficulty</head><p>Images differ in their difficulty for localization, which comes from factors such as object size, background clutter, number of objects, and partial occlusion. For subjective evaluation, image difficulty can be quantified as the time needed by a human to determine the actual position of a given class <ref type="bibr" target="#b13">[14]</ref>. However, this brings about extra human efforts. In this subsection, we evaluate the image difficulty via diagnosing its localization outputs.</p><p>WSDDN framework. Our method needs a pretrained model to diagnose the localization outputs of an image. Without loss of generality, we use WSDDN <ref type="bibr" target="#b16">[17]</ref> as the baseline network, for its effectiveness and implementation convenience. WSDDN explicitly models image-level classification loss via aggregating region proposal scores. Specifically, given an image x with region proposals R, and image level labels y ∈ {1, −1} C , where y c = 1 (y c = −1) indicates the presence (absence) of an object class c. Denote the outputs of f c 8C and f c 8R layer as φ(x, f c 8C ) and φ(x, f c 8R ), respectively, which are with size C ×|R|. Here, C represents the number of categories and |R| denotes the number of regions. The score of region r corresponding to class c is the dot product of the two fully connected layers φ(x, f c 8C ) and φ(x, f c 8R ), normalized at different dimensions:</p><formula xml:id="formula_0">x cr = e φ cr (x,f c 8C ) C i=1 e φ ir (x,f c 8C ) . * e φ cr (x,f c 8R ) |R| j=1 e φ cj (x,f c 8R ) .<label>(1)</label></formula><p>Based on the region-level score x cr , the probability output y w.r.t. category c at image-level is defined as the sum of a series of region-level scores:</p><formula xml:id="formula_1">φ c (x, w cls ) = |R| j=1 x cj ,<label>(2)</label></formula><p>where w cls denotes the non-linear mapping from input x to classification stream output. This network is backpropagated via a binary log image-level loss, denoted as</p><formula xml:id="formula_2">L cls (x, y) = C i=1 log(y i (φ i (x, w cls ) − 1/2) + 1/2),<label>(3)</label></formula><p>and is able to automatically localize the regions which contribute most to the image level scores.</p><p>Mean Energy Accumulated Scores (mEAS). Benefiting from the competitive mechanism, WSDDN is able to pick out the most discriminative details for classification. These details sometimes fortunately correspond to the whole object, but in most cases only focus on object parts. We observe that the successfully localized objects usually appear in relatively simple, uniform background with only a few objects in the image. In order to pick out images that WSDDN localizes successfully, we propose an effective criterion named mean Energy Accumulated Scores (mEAS) to quantify the localization difficulty of each image.</p><p>If the target object is easy to localize, the regions that contribute most to the classification scores should be highly concentrated. To be specific, given an image x with labels y ∈ {1, −1} C , for each class y c = 1, we sort the region scores x cr (r ∈ {1, ..., |R|}) in a descending order, and obtain the sorted list x cr , where r is a permutation of {1, ..., |R|}. Then we compute the accumulated scores of x cr to obtain a monotonically increasing list X c ∈ R |R| , with each dimension denoted as  number of regions needed to make X c above a threshold t,</p><formula xml:id="formula_3">X cr = r (j) j=r (1) x cj / |R| j=1 x cj .<label>(4</label></formula><formula xml:id="formula_4">EAS(X c , t) = X cj [t] j [t] , j [t] = arg min j X cj ≥ t. (5)</formula><p>It is obvious that a larger EAS(X c , t) means that fewer regions will be needed to reach the target energy. Finally, we define the mean Energy Accumulated Scores (mEAS) as the mean scores at a set of eleven equally spaced energy levels [0, 0.1, ..., 1]:</p><formula xml:id="formula_5">mEAS(X c ) = 1 11 t∈{0,0.1,...,1} EAS(X c , t).<label>(6)</label></formula><p>Mining object instances. Once we obtain the image difficulty, the remaining task is to mine object instances from the images. A natural way is to directly choose the top scored region as the target object, which is used for localization evaluation in <ref type="bibr" target="#b17">[18]</ref>. However, since the whole network is trained with classification loss, which makes high scored regions tend to focus on object parts rather than the whole objects. To relieve this issue, we do not optimistically consider the top scored region to be accurate enough. In contrast, we consider them to be accurate enough as soft voters. To be specific, we compute the object heat map H c for class c, which collectively returns the confidence that pixel p lies in an object, i.e.,</p><formula xml:id="formula_6">H c (p) = r x cr D r (p)/Z,<label>(7)</label></formula><p>where D r (p) = 1 when the r-th region proposal contains pixel p, and Z is a normalization constant such that max H c (p) = 1. We binarize the heat map H c with threshold T (set as 0.5 in all experiments), and choose the tightest bounding box that encloses the largest connect component as the mined object instance. Analysis of mEAS. mEAS is an effective criterion to quantify the localization difficulty of an image. <ref type="figure" target="#fig_3">Fig. 3</ref> shows some image difficulty scores from mEAS on PAS-CAL VOC 2007 dataset, together with the mined object instances (top row) and object heat maps (bottom row). It can be seen that images with higher mEAS are easy to localize, and the corresponding heat maps exhibit excellent spatially convergence characteristics. In contrast, images with lower mEAS are usually hard to localize, and the corresponding heat maps are divergent. Comparing with the region scores in Eq. (1), mEAS is especially effective in filtering out the inaccurate localizations in these two cases:</p><p>• The top scored regions only focus on part of the object. This usually occurs on non-rigid objects such as cat and person (see the 6th column in <ref type="figure" target="#fig_3">Fig. 3</ref>). In this case, the less discriminative parts make the heat maps relatively divergent, and thus lower the mEAS.</p><p>• There exist multiple objects of the same class. They all contribute to the classification, which makes the object heat maps divergent (see the 7th column in <ref type="figure" target="#fig_3">Fig. 3</ref>).</p><p>In addition, based on the mEAS, we are also able to analyze image difficulty at the class level. We compute mEAS at the class level by averaging the scores of images that contain the target object. In <ref type="table" target="#tab_1">Table 1</ref>, we show the difficulty scores for all the 20 categories on PASCAL VOC 2007 trainval split, along with the localization performance <ref type="bibr" target="#b16">[17]</ref> in terms of CorLoc <ref type="bibr" target="#b18">[19]</ref>. We find that mEAS is highly related with the localization precision, with a correlation coefficient as high as 0.703. In this dataset, chair and table are the most difficult classes, containing cluttered scenes or partial occlusion. On the other hand, rigid objects such as bus and car are the easiest to localize, because these objects are usually large in images, or in relatively clean background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Progressive Detection Network</head><p>Given the image difficulty scores and the mined seed positive instances, we are able to organize our network training in a progressive learning mode. The detection network follows a fast-RCNN <ref type="bibr" target="#b0">[1]</ref> framework. Specifically, we split the training images D into K folds D = {D 1 , ..., D K }, which are in an easy-to-difficult order. Instead of training and relocalization on the entire images all at once, we progressively recruit samples in terms of image difficulty. The training process starts with running a fast-RCNN on the first fold D 1 , which contains the easiest images, and obtains a trained model M D1 . M D1 already has a good generalization ability since the trained object instances are highly reliable. Then we move on to the second fold D 2 , which contains relatively more difficult images. Instead of performing training and relocalization from scratch, we choose the trained model M D1 to discover object instances in fold D 2 . It is likely to find more reliable instances on D 1 D 2 . As the training process proceeds, more images are added in, which improves the localization ability of the network steadily. When reaching later folds, the learned model has been powerful enough for localizing these difficult images.</p><p>Weighted loss. Due to the high variation of image difficulty, the mined object instances used for training cannot be all reliable. It is suboptimal to treat all these instances equally important. Therefore, we penalize the output layers with a weighted loss, which considers the reliability of the mined instances. At each relocalization step, the network M k returns a detection score for each region, indicating its confidence of containing the target object. Formally, let </p><formula xml:id="formula_7">L cls (x o c , y o c , M k+1 ) = −φ c (x o c , M k ) log φ c (x o c , M k+1 ). (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convolutional Feature Masking Regularization</head><p>The above detector learning proceeds by alternating between model retraining and object relocalization, and is easy to get stuck in sub-optimums without proper initialization. Unfortunately, due to lack of object annotations, the initial seeds inevitably include inaccurate samples. As a result, the network tends to overfit those inaccurate instances during each iteration, leading to poor generalization. To solve this issue, we propose a regularization strategy to avoid the network from overfitting initial seeds in the proposed zigzag learning. Concretely, during network training, we randomly mask out those discriminative details at previous training, which enforces the network to focus on those less discriminative details, so that the current network can see a more holistic object.</p><p>The convolutional feature masking operation works as follows. Given an image x and the mined object In our experiments, T = 16 for all models. During each iteration, we randomly mask out the regions by setting φ(Ω, f conv ) = 0, and continue forward and backward propagation as usual. For simplicity, we keep the aspect ratio of the masked region Ω the same as the mined object x o c . The whole process is summarized in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our proposed zigzag learning for weakly supervised object detection, providing extensive ablation studies and making comparison with state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets and evaluation metrics. We evaluate our approach on PASCAL VOC 2007 <ref type="bibr" target="#b20">[21]</ref> and 2012 <ref type="bibr" target="#b21">[22]</ref> datasets. The VOC 2007 contains a total of 9,963 images spanning 20 object classes, of which 5,011 images are used for trainval and the rest 4,952 images for test. The VOC 2012 contains 11,540 images for trainval and 10,991 images for test. We choose the trainval split for network training. For performance evaluation, two kinds of measurements are used: 1) CorLoc <ref type="bibr" target="#b18">[19]</ref> evaluated on the trainval split; 2) the VOC protocol which measures the detection performance with average precision (AP) on the test split. Implementation details. We choose two CNN models to evaluate our approach: 1) CaffeNet <ref type="bibr" target="#b22">[23]</ref>, which we refer to as model S (meaning "small"), and 2) VGG-VD <ref type="bibr" target="#b23">[24]</ref> (the 16-layer model is used), which we call model L (meaning "large"). In progressive learning, the training is run for 12 epoches for each iteration, with learning rate 10 −4 for the first 6 epoches and 10 −5 for the last 6 epoches. We choose edge boxes <ref type="bibr" target="#b24">[25]</ref> to generate |R| ≈ 2000 region proposals per image on average. All experiments use single-scale (s = 600) for training and test. We denote the length of its shortest side as the scale s of an image. For data augmentation, we regard all proposals that have IoU ≥ 0.5 with the mined objects as positive. The proposals that have IoU ∈ [0.1, 0.5) are treated as hard negative samples.The mean outputs of the K models {M k } K k=1 are chosen for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>We first analyze the performance of our approach with different configurations. Then we evaluate the localization precision of different folds to validate the effectiveness of the mEAS. At last, we analyze the influences of two parameters: the progressive learning folds K and the masking ratio τ . Without loss of generality, all experiments here are conducted on PASCAL VOC 2007 with model S. • Component analysis. To reveal the contribution of each module, we test the detection performance with different configurations. These variants include: 1) using region scores (Eq. (1)) as image difficulty metric; 2) using the proposed mEAS for image difficulty measurement; 3) introducing weighted loss during model retraining; and 4) adding masking regularization. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>From the table we observe the following three aspects.</p><p>1) The mEAS is more effective than region scores from Eq. (1), with a gain up to about 3.2% (34.1% → 37.7%). The main reason is as follows. For deformable objects like bird and cat, the highest region scores may focus on object parts, thus the progressive learning chooses inaccurate object instances during initial training. In contrast, mEAS lowers those scores only concentrating on part of the objects by introducing convergent measurement, and avoids choosing these parts for initial detector training.</p><p>2) Introducing weighted loss brings about 1.4% gain. This demonstrates that considering the confidence of the mined object instances helps boost the performance.</p><p>3) The proposed masking strategy further boosts the performance to an accuracy of 40.7%, which is 1.6% better than the baseline. This demonstrates that the masking strategy can effectively prevent the model from ovetfitting and enhance its generalization ability.</p><p>• CorLoc versus fold iteration. In order to validate the effectiveness of mEAS, we test the localization performance during each iteration in terms of CorLoc. <ref type="table">Table 3</ref> shows the localization results on VOC 2007 trainval split when learning folds K = 3. During the first iteration (k = 1) for the easiest images, our method achieves an accuracy of 72.3%. When moving on to more difficult images (k = 2), the performance is decreased to 56.8%. It only achieves 44.3% for the most difficult image fold, even though we have a more powerful model when k = 3. The results demonstrate that mEAS is an effective criterion to measure the difficulty of an image w.r.t. localizing the corresponding object.</p><p>• Learning folds K. <ref type="figure" target="#fig_7">Fig. 4(a)</ref> shows the detection results w.r.t. different learning folds, where K = 1 means that the training process chooses entire images all at once, without using progressive learning. We find that the progressive learning strategy significantly improves the detection performance. The result is 39.1% for K = 3, i.e. about 3.2% gain over the baseline (35.9%). The performance tends to <ref type="bibr">Figure 5</ref>. Example detections on PASCAL VOC 2007 test split (47.6% mAP). The successful detections (IoU ≥ 0.5) are marked with green bounding boxes, and the failed ones are marked with red. We show all detections with scores ≥ 0.7 and use nms to remove duplicate detections. The failed detections often come from localizing object parts or grouping multiple objects from the same class.  be saturated as K increases and even slightly drops, mainly because too few images in initial stages degrade the model's detection power.</p><p>• Masking ratio τ . The masking ratio τ denotes the percentage of area Ω versus that of the mined object x o c . <ref type="figure" target="#fig_7">Fig.  4(b)</ref> shows the results as we mask out different ratios of the mined objects. With masking ratio τ = 0.1, the test performance reaches 40.7%, which surpasses the baseline without using masking by 1.6%. The improvement demonstrates that the proposed masking strategy is able to enhance the generalization ability of the trained model. As the masking ratio increases, the performance gradually drops, mainly because masking too many regions prevents the model from seeing true positive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with state-of-the-arts</head><p>We then compare our results with state-of-the-arts for weakly supervised detection. Our method is denoted as ZLDN, standing for Zigzag Learning Detection Network. Unless specified, all other results are based on model L. • CorLoc evaluation. <ref type="table">Table 4</ref> shows the localization results on PASCAL VOC 2007 trainval split in terms of Cor-Loc <ref type="bibr" target="#b18">[19]</ref>. Comparing with WSDDN <ref type="bibr" target="#b17">[18]</ref> (53.5%), our method brings 7.7% improvement, this mainly results from the zigzag learning. Our method achieves slightly better localization performance (61.2%) compared with previous best-performing method <ref type="bibr" target="#b10">[11]</ref> (60.6%). Similar results can be found in <ref type="table">Table 6</ref> which shows the localization performance on VOC 2012. Our method obtains an accuracy of 61.5%, which is comparable with the best performing method <ref type="bibr" target="#b10">[11]</ref> (62.1%). Note that the result of <ref type="bibr" target="#b10">[11]</ref> is based on multiple scales, while our result is simply from the last learning iteration, which is in single scale. • AP evaluation. <ref type="table">Table 5</ref> and <ref type="table">Table 7</ref> show the detection performance in average precision (AP) on PASCAL VOC 2007 and 2012 test split, respectively. Just using model S, our method achieves an accuracy of 40.7%, i.e. about 6.2% improvement over the best-performing method WS-DDN <ref type="bibr" target="#b16">[17]</ref> (34.5%) using the same model on VOC 2007. When switching to model L, the detection accuracy in-creases to 47.6% on VOC 2007, which is about 6% better than the best-performing result <ref type="bibr" target="#b11">[12]</ref> (41.7%). On PASCAL VOC 2012, the detection accuracy is 42.9%, which is 4.6% better than previous state-of-the-art result <ref type="bibr" target="#b11">[12]</ref> (38.3%).</p><p>• Error analysis and visualization. To show the performance of our model more detailedly, we use the analysis tool from <ref type="bibr" target="#b25">[26]</ref> to diagnose the detector error. <ref type="figure" target="#fig_8">Fig. 6</ref> shows the error analysis on PASCAL VOC 2007 test split with model L (mAP 47.6%). The classes are categorized into three categories, animals, vehicles, and furniture. Our method achieves promising results on categories animals and vehicles, with an average precision above 60%, but it does not work well on detecting furniture. This is mainly because furniture like chair and table are usually in cluttered scenes, thus very hard to pick out for model training.</p><p>On the other hand, the majority of error comes from inaccurate localization, which is around 30% for all categories. We show some detection results in <ref type="figure">Fig. 5</ref>. The correct detections are marked with green bounding boxes, while the failed ones are marked with red. It can be seen that the incorrect detections often come from detecting object parts, or grouping multiple objects from the same class.</p><p>Although our proposed method achieves better performance than previous works, it performs not very well on some categories, like chair and person. The reason is that the detection performance mainly dependents on the object instances obtained from the classification model, which is limited in correctly localizing these objects. Actually, localizing objects such as chair and person in cluttered backgrounds is the main challenge in weakly supervised detection, which remains a further research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposed a zigzag learning strategy for weakly supervised object detection. To develop such effective learning, we propose a new and effective criterion named mean Energy Accumulated Scores (mEAS) to automatically measure the difficulty of an image, and progressively recruit samples via mEAS for model training. Moreover, a masking strategy is incorporated into network training by randomly erasing the high responses over the last convolutional feature maps, which highlights the less discriminative parts and improves the network's generalization ability. Experiments conducted on PASCAL VOC benchmarks demonstrated the effectiveness of the proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Sheep: 0.02 (b) Dog: 0.44 (c) Horse: 0.29 (a) Car: 0.79</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure 2. Architecture of our proposed zigzag detection network. We first estimate the image difficulty with mean Accumulated Energy Scores (mEAS), organizing training images in an easy-to-difficult order. Then we introduce a masking strategy over the last convolutional feature maps of fast RCNN framework, which enhances the generalization ability of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Example image difficulty scores by the proposed mEAS metric. Top row: mined object instances and mEAS. Bottom row: corresponding object heat maps produced by Eq.<ref type="bibr" target="#b6">(7)</ref>. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>Zigzag Learning based Weakly Supervised Detection Network Input: Training setD = {x i } N i=1 with image-level labels Y = {y i } N i=1 ,iteration folds K, and masking ratio τ ; Estimating Image Difficulty: Given an image x with label y ∈ {1, −1} C and region proposals R: i). Obtain region scores x cr ∈ R C×|R| with WSDDN. ii). For each y c = 1, compute mEAS(X c ) with Eq. (6), and the object instance x o c with Eq. (7). Progressive Learning: Divide D into K folds D = {D 1 , ..., D K } according to mEAS. for fold k = 1 to K do i). Training detection model M k with current selection of object instances in k i=1 D i , a). given an image x, compute the last convolutional feature maps φ(x, f conv ). b). for each mined object instance x o c , randomly select regions {Ω| SΩ S x o c = τ }, and set φ(Ω, f conv ) = 0. c). continue forward and back propagation. ii). Relocalize object instances in folds k+1 i=1 D i using current detection model M k : end for Output: Detection models {M k } K k=1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>x o c be the relocalized object with instance label y o c = 1, and φ c (x o c , M k ) be the detection score returned by M k . The weighted loss w.r.t. region x o c in the next retraining step is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>x o c for each y c = 1 ,</head><label>1</label><figDesc>we randomly select region Ω ∈ x o c with S Ω /S x o c = τ , where S Ω denotes the area of region Ω. As x o c obtains the highest responses during previous iteration, Ω is among the most discriminative regions. For each pixel[u, v]  ∈ Ω, we project it onto the last convolutional feature maps φ(x, f conv ), such that the pixel[u, v]  in the image domain is closest to the receptive field of that feature map pixel[u , v ]. This mapping is complicated due to the padding operations among convolutional and pooling layers. To simplify the implementation, following<ref type="bibr" target="#b19">[20]</ref>, we pad p/2 pixels for each layer with a filter size of p. This establishes a rough correspondence between a response centered at [u , v ], and receptive field in the image domain centered at [T u , T v ], where T is the stride from the image to the target convolutional feature maps. The mapping of [u, v] to the feature map [u , v ] is simply conducted as u = round((u−1)/T+1), v = round((v−1)/T+1).<ref type="bibr" target="#b8">(9)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Detection performance on PASCAL VOC 2007 test split for different learning folds K (left) and masking ratio τ (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Detection error analysis<ref type="bibr" target="#b25">[26]</ref> of our proposed model on animals, vehicles, and furniture from VOC 2007 test split. The detections are categorized as correct (Cor), false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (Oth), and with background (BG).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Average mEAS per class versus the correct localization precision (CorLoc<ref type="bibr" target="#b18">[19]</ref>) on PASCAL VOC 2007 trainval split. The correlation coefficient of these two variables is 0.703.</figDesc><table><row><cell>Class</cell><cell>mEAS</cell><cell>CorLoc</cell><cell>Class</cell><cell>mEAS</cell><cell>CorLoc</cell></row><row><cell>bus</cell><cell>0.306</cell><cell>0.699</cell><cell>car</cell><cell>0.262</cell><cell>0.750</cell></row><row><cell>tv</cell><cell>0.254</cell><cell>0.582</cell><cell>aero</cell><cell>0.220</cell><cell>0.685</cell></row><row><cell>mbike</cell><cell>0.206</cell><cell>0.829</cell><cell>train</cell><cell>0.206</cell><cell>0.628</cell></row><row><cell>horse</cell><cell>0.195</cell><cell>0.672</cell><cell>cow</cell><cell>0.185</cell><cell>0.681</cell></row><row><cell>boat</cell><cell>0.177</cell><cell>0.343</cell><cell>sheep</cell><cell>0.176</cell><cell>0.719</cell></row><row><cell>bike</cell><cell>0.170</cell><cell>0.675</cell><cell>bird</cell><cell>0.170</cell><cell>0.567</cell></row><row><cell>sofa</cell><cell>0.165</cell><cell>0.620</cell><cell>plant</cell><cell>0.163</cell><cell>0.437</cell></row><row><cell>person</cell><cell>0.162</cell><cell>0.288</cell><cell>bottle</cell><cell>0.150</cell><cell>0.328</cell></row><row><cell>cat</cell><cell>0.143</cell><cell>0.457</cell><cell>dog</cell><cell>0.135</cell><cell>0.406</cell></row><row><cell>chair</cell><cell>0.093</cell><cell>0.171</cell><cell>table</cell><cell>0.052</cell><cell>0.305</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Detection performance comparison of model S with various configurations on PASCAL VOC 2007 test split.</figDesc><table><row><cell>Model S</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 6 .Table 7 .</head><label>367</label><figDesc>Localization precision (%) on PASCAL VOC 2007 trainval split at different fold iterations. The number of total folds is K = 3. 48.7 49.5 32.8 81.7 85.4 40.1 40.6 79.5 35.7 33.7 60.5 88.8 21.8 57.9 76.3 59.9 75.3 81.68.5 50.1 16.8 20.8 62.7 66.8 56.5 2.1 57.8 47.5 40.1 69.7 68.2 21.6 27.2 53.4 56.1 52.5 58.2 47.6 Localization precision (%) on PASCAL VOC 2012 trainval split in terms of CorLoc [19] metric. method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mean DSD [12] 82.4 68.1 54.5 38.9 35.9 84.7 73.1 64.8 17.1 78.3 22.5 57.0 70.8 86.6 18.7 49.7 80.7 45.3 70.1 77.3 58.8 OICR [11] 86.2 84.2 68.7 55.4 46.5 82.8 74.9 32.2 46.7 82.8 42.9 41.0 68.1 89.6 9.2 53.9 81.0 52.9 59.5 83.2 62.1 ZLDN-L 80.3 76.5 64.2 40.9 46.7 78.0 84.3 57.6 21.1 69.5 28.0 46.8 70.7 89.4 41.9 54.7 76.3 61.1 76.3 65.2 61.5 Detection average precision (%) on PASCAL VOC 2012 test split. method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP PDA [10] 62.9 55.5 43.7 14.9 13.6 57.7 52.4 50.9 13.3 45.4 4.0 30.2 55.6 67.0 3.8 23.1 39.4 5.5 50.7 29.3 35.9 DSD [12] 60.8 54.2 34.1 14.9 13.1 54.3 53.4 58.6 3.7 53.1 8.3 43.4 49.8 69.2 4.1 17.5 43.8 25.6 55.0 50.1 38.3 OICR [11] 67.7 61.2 41.5 25.6 22.2 54.6 49.7 25.4 19.9 47.0 18.1 26.0 38.9 67.7 2.0 22.6 41.1 34.3 37.9 55.3 37.9 ZLDN-L 54.3 63.7 43.1 16.9 21.5 57.8 60.4 50.9 1.2 51.5 44.4 36.6 63.6 59.3 12.8 25.6 47.8 47.2 48.9 50.6 42.9</figDesc><table><row><cell cols="2">Fold aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mean</cell></row><row><cell cols="2">k=1 87.3 90.0 81.8 56.7 69.1 85.5 88.9 62.5 27.0 80.9 61.2 53.2 85.4 92.6 36.1 62.7 78.1 81.6 79.3 85.9 72.3</cell></row><row><cell cols="2">k=2 72.5 76.8 60.9 23.0 20.7 67.7 83.2 61.1 12.8 78.7 48.5 51.8 74.8 88.9 27.4 35.4 64.5 54.6 63.4 67.4 56.8</cell></row><row><cell cols="2">k=3 64.6 40.7 38.2 28.3 24.7 46.8 68.8 58.0 7.4 55.3 26.9 58.2 58.3 77.1 30.2 27.7 51.5 44.7 32.2 45.9 44.3</cell></row><row><cell></cell><cell>Table 4. Localization precision (%) on PASCAL VOC 2007 trainval split in terms of CorLoc [19] metric.</cell></row><row><cell>method</cell><cell>aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mean</cell></row><row><cell>PLSA [8]</cell><cell>80.1 63.9 51.5 14.9 21.0 55.7 74.2 43.5 26.2 53.4 16.3 56.7 58.3 69.5 14.1 38.3 58.8 47.2 49.1 60.9 48.5</cell></row><row><cell cols="2">WSDDN [17] 65.1 58.8 58.5 33.1 39.8 68.3 60.2 59.6 34.8 64.5 30.5 43.0 56.8 82.4 25.5 41.6 61.5 55.9 65.9 63.7 53.5</cell></row><row><cell>PDA [10]</cell><cell>78.2 67.1 61.8 38.1 36.1 61.8 78.8 55.2 28.5 68.8 18.5 49.2 64.1 73.5 21.4 47.4 64.6 22.3 60.9 52.3 52.4</cell></row><row><cell>DSD [12]</cell><cell>72.7 55.3 53.0 27.8 35.2 68.6 81.9 60.7 11.6 71.6 29.7 54.3 64.3 88.2 22.2 53.7 72.2 52.6 68.9 75.5 56.1</cell></row><row><cell>OICR [11]</cell><cell>81.7 80.4 4 60.6</cell></row><row><cell>ZLDN-S</cell><cell>74.8 69.1 60.3 35.9 38.1 66.7 80.2 60.5 15.7 71.6 45.5 54.4 72.8 86.1 31.2 42.0 64.6 60.3 58.6 66.4 57.8</cell></row><row><cell>ZLDN-L</cell><cell>74.0 77.8 65.2 37.0 46.7 75.8 83.7 58.8 17.5 73.1 49.0 51.3 76.7 87.4 30.6 47.8 75.0 62.5 64.8 68.8 61.2</cell></row><row><cell></cell><cell>Table 5. Detection average precision (%) on PASCAL VOC 2007 test split.</cell></row><row><cell>method</cell><cell>aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP</cell></row><row><cell>pLSA [8]</cell><cell>48.8 41.0 23.6 12.1 11.1 42.7 40.9 35.5 11.1 36.6 18.4 35.3 34.8 51.3 17.2 17.4 26.8 32.8 35.1 45.6 30.9</cell></row><row><cell cols="2">WSDDN S [17] 42.9 56.0 32.0 17.6 10.2 61.8 50.2 29.0 3.8 36.2 18.5 31.1 45.8 54.5 10.2 15.4 36.3 45.2 50.1 43.8 34.5</cell></row><row><cell cols="2">WSDDN L [17] 39.4 50.1 31.5 16.3 12.6 64.5 42.8 42.6 10.1 35.7 24.9 38.2 34.4 55.6 9.4 14.7 30.2 40.7 54.7 46.9 34.8</cell></row><row><cell>PDA [10]</cell><cell>54.5 47.4 41.3 20.8 17.7 51.9 63.5 46.1 21.8 57.1 22.1 34.4 50.5 61.8 16.2 29.9 40.7 15.9 55.3 40.2 39.5</cell></row><row><cell>DSD [12]</cell><cell>52.2 47.1 35.0 26.7 15.4 61.3 66.0 54.3 3.0 53.6 24.7 43.6 48.4 65.8 6.6 18.8 51.9 43.6 53.6 62.4 41.7</cell></row><row><cell>OICR [11]</cell><cell>58.0 62.4 31.1 19.4 13.0 65.1 62.2 28.4 24.8 44.7 30.6 25.3 37.8 65.5 15.7 24.1 41.7 46.9 64.3 62.6 41.2</cell></row><row><cell>ZLDN-S</cell><cell>51.9 57.5 40.9 15.8 17.6 53.3 61.2 54.0 2.0 44.2 42.9 34.5 58.3 60.3 18.8 20.7 44.9 43.4 43.5 48.3 40.7</cell></row><row><cell>ZLDN-L</cell><cell>55.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">X c is in the range of [0 1] and can be regarded as an indicator depicting the convergence degree of the region scores. If the top scores only focus on a few regions, then X c converges quickly to 1. In this case, WSDDN is easy to pick out the target object.Inspired by the precision/recall metric, we introduce Energy Accumulated Scores (EAS) to quantify the convergence of X c . EAS is inversely proportional to the minimal</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual recognition by learning from web data: A weakly supervised domain generalization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2774" to="2783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keywords to visual categories: Multiple-instance learning forweakly supervised object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Image colocalization by mimicking a good detector&apos;s confidence score distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04619</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weaklysupervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-fold mil training for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2409" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2843" to="2850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How hard can it be? estimating the difficulty of visual search in an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization using size estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="105" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Weakly supervised localization using deep feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00489</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
