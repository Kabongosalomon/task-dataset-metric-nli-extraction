<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Digging Into Self-Supervised Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl√©ment</forename><surname>Godard</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Caltech 3 Niantic</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Caltech 3 Niantic</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Caltech 3 Niantic</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Caltech 3 Niantic</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Digging Into Self-Supervised Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods.</p><p>Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We seek to automatically infer a dense depth image from a single color input image. Estimating absolute, or even relative depth, seems ill-posed without a second input image to enable triangulation. Yet, humans learn from navigating and interacting in the real-world, enabling us to hypothesize plausible depth estimates for novel scenes <ref type="bibr" target="#b17">[18]</ref>.</p><p>Generating high quality depth-from-color is attractive because it could inexpensively complement LIDAR sensors used in self-driving cars, and enable new single-photo applications such as image-editing and AR-compositing. Solving for depth is also a powerful way to use large unlabeled image datasets for the pretraining of deep networks for downstream discriminative tasks <ref type="bibr" target="#b22">[23]</ref>. However, collecting large and varied training datasets with accurate ground truth depth for supervised learning <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b8">9]</ref> is itself a formidable challenge. As an alternative, several recent self-supervised approaches have shown that it is instead possible to train monocular depth estimation models using only synchronized stereo pairs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> or monocular video <ref type="bibr" target="#b75">[76]</ref>. Among the two self-supervised approaches, monocular video is an attractive alternative to stereo-based supervision, but it introduces its own set of challenges. In addition to estimating depth, the model also needs to estimate the egomotion between temporal image pairs during training. This typically involves training a pose estimation network that takes a finite sequence of frames as input, and outputs the corresponding camera transformations. Conversely, using stereo data for training makes the camera-pose estimation a one-time offline calibration, but can cause issues related to occlusion and texture-copy artifacts <ref type="bibr" target="#b14">[15]</ref>.</p><p>We propose three architectural and loss innovations that combined, lead to large improvements in monocular depth estimation when training with monocular video, stereo pairs, or both: (1) A novel appearance matching loss to address the problem of occluded pixels that occur when using monocular supervision. (2) A novel and simple automasking approach to ignore pixels where no relative camera Input Geonet <ref type="bibr" target="#b70">[71]</ref> (M) <ref type="bibr">Ranjan</ref>  <ref type="bibr" target="#b50">[51]</ref> (M) EPC++ <ref type="bibr" target="#b37">[38]</ref> (MS) Baseline (M) Monodepth2 (M) <ref type="figure">Figure 2</ref>. Moving objects. Monocular methods can fail to predict depth for objects that were often observed to be in motion during training e.g. moving cars -including methods which explicitly model motion <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51]</ref>. Our method succeeds here where others, and our baseline with our contributions turned off, fail.</p><p>motion is observed in monocular training. (3) A multi-scale appearance matching loss that performs all image sampling at the input resolution, leading to a reduction in depth artifacts. Together, these contributions yield state-of-the-art monocular and stereo self-supervised depth estimation results on the KITTI dataset <ref type="bibr" target="#b12">[13]</ref>, and simplify many components found in the existing top performing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review models that, at test time, take a single color image as input and predict the depth of each pixel as output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Supervised Depth Estimation</head><p>Estimating depth from a single image is an inherently illposed problem as the same input image can project to multiple plausible depths. To address this, learning based methods have shown themselves capable of fitting predictive models that exploit the relationship between color images and their corresponding depth. Various approaches, such as combining local predictions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b54">55]</ref>, non-parametric scene sampling <ref type="bibr" target="#b23">[24]</ref>, through to end-to-end supervised learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b9">10]</ref> have been explored. Learning based algorithms are also among some of the best performing for stereo estimation <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b24">25]</ref> and optical flow <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>Many of the above methods are fully supervised, requiring ground truth depth during training. However, this is challenging to acquire in varied real-world settings. As a result, there is a growing body of work that exploits weakly supervised training data, e.g. in the form of known object sizes <ref type="bibr" target="#b65">[66]</ref>, sparse ordinal depths <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b5">6]</ref>, supervised appearance matching terms <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b72">73]</ref>, or unpaired synthetic depth data <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b77">78]</ref>, all while still requiring the collection of additional depth or other annotations. Synthetic training data is an alternative <ref type="bibr" target="#b40">[41]</ref>, but it is not trivial to generate large amounts of synthetic data containing varied real-world appearance and motion. Recent work has shown that conventional structure-from-motion (SfM) pipelines can generate sparse training signal for both camera pose and depth <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b67">68]</ref>, where SfM is typically run as a pre-processing step decoupled from learning. Recently, <ref type="bibr" target="#b64">[65]</ref> built upon our model by incorporating noisy depth hints from traditional stereo algorithms, improving depth predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-supervised Depth Estimation</head><p>In the absence of ground truth depth, one alternative is to train depth estimation models using image reconstruction as the supervisory signal. Here, the model is given a set of images as input, either in the form of stereo pairs or monocular sequences. By hallucinating the depth for a given image and projecting it into nearby views, the model is trained by minimizing the image reconstruction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-supervised Stereo Training</head><p>One form of self-supervision comes from stereo pairs. Here, synchronized stereo pairs are available during training, and by predicting the pixel disparities between the pair, a deep network can be trained to perform monocular depth estimation at test time. <ref type="bibr" target="#b66">[67]</ref> proposed such a model with discretized depth for the problem of novel view synthesis. <ref type="bibr" target="#b11">[12]</ref> extended this approach by predicting continuous disparity values, and <ref type="bibr" target="#b14">[15]</ref> produced results superior to contemporary supervised methods by including a left-right depth consistency term. Stereo-based approaches have been extended with semi-supervised data <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39]</ref>, generative adversarial networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b47">48]</ref>, additional consistency <ref type="bibr" target="#b49">[50]</ref>, temporal information <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b2">3]</ref>, and for real-time use <ref type="bibr" target="#b48">[49]</ref>.</p><p>In this work, we show that with careful choices regarding appearance losses and image resolution, we can reach the performance of stereo training using only monocular training. Further, one of our contributions carries over to stereo training, resulting in increased performance there too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-supervised Monocular Training</head><p>A less constrained form of self-supervision is to use monocular videos, where consecutive temporal frames provide the training signal. Here, in addition to predicting depth, the network has to also estimate the camera pose between frames, which is challenging in the presence of object motion. This estimated camera pose is only needed during training to help constrain the depth estimation network.</p><p>In one of the first monocular self-supervised approaches, <ref type="bibr" target="#b75">[76]</ref> trained a depth estimation network along with a separate pose network. To deal with non-rigid scene motion, an additional motion explanation mask allowed the model to ignore specific regions that violated the rigid scene assumption. However, later iterations of their model available online disabled this term, achieving superior performance. Inspired by <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b60">[61]</ref> proposed a more sophisticated motion model using multiple motion masks. However, this was not fully evaluated, making it difficult to understand its utility. <ref type="bibr" target="#b70">[71]</ref> also decomposed motion into rigid and non-rigid components, using depth and optical flow to explain object motion. This improved the flow estimation, but they reported no improvement when jointly training for flow and depth estimation. In the context of optical flow estimation, <ref type="bibr" target="#b21">[22]</ref> showed that it helps to explicitly model occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I t I t-1 I t+1</head><p>Recent approaches have begun to close the performance gap between monocular and stereo-based self-supervision. <ref type="bibr" target="#b69">[70]</ref> constrained the predicted depth to be consistent with predicted surface normals, and [69] enforced edge consistency. <ref type="bibr" target="#b39">[40]</ref> proposed an approximate geometry based matching loss to encourage temporal depth consistency. <ref type="bibr" target="#b61">[62]</ref> use a depth normalization layer to overcome the preference for smaller depth values that arises from the commonly used depth smoothness term from <ref type="bibr" target="#b14">[15]</ref>. <ref type="bibr" target="#b4">[5]</ref> make use of pre-computed instance segmentation masks for known categories to help deal with moving objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appearance Based Losses</head><p>Self-supervised training typically relies on making assumptions about the appearance (i.e. brightness constancy) and material properties (e.g. Lambertian) of object surfaces between frames. <ref type="bibr" target="#b14">[15]</ref> showed that the inclusion of a local structure based appearance loss <ref type="bibr" target="#b63">[64]</ref> significantly improved depth estimation performance compared to simple pairwise pixel differences <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b75">76]</ref>. <ref type="bibr" target="#b27">[28]</ref> extended this approach to include an error fitting term, and <ref type="bibr" target="#b42">[43]</ref> explored combining it with an adversarial based loss to encourage realistic looking synthesized images. Finally, inspired by <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref> use ground truth depth to train an appearance matching term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Here, we describe our depth prediction network that takes a single color input I t and produces a depth map D t . We first review the key ideas behind self-supervised training for monocular depth estimation, and then describe our depth estimation network and joint training loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-Supervised Training</head><p>Self-supervised depth estimation frames the learning problem as one of novel view-synthesis, by training a net-work to predict the appearance of a target image from the viewpoint of another image. By constraining the network to perform image synthesis using an intermediary variable, in our case depth or disparity, we can then extract this interpretable depth from the model. This is an ill-posed problem as there is an extremely large number of possible incorrect depths per pixel which can correctly reconstruct the novel view given the relative pose between those two views. Classical binocular and multi-view stereo methods typically address this ambiguity by enforcing smoothness in the depth maps, and by computing photo-consistency on patches when solving for per-pixel depth via global optimization e.g. <ref type="bibr" target="#b10">[11]</ref>.</p><p>Similar to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b75">76]</ref>, we also formulate our problem as the minimization of a photometric reprojection error at training time. We express the relative pose for each source view I t , with respect to the target image I t 's pose, as T t‚Üít . We predict a dense depth map D t that minimizes the photometric reprojection error L p , where</p><formula xml:id="formula_0">L p = t pe(I t , I t ‚Üít ),<label>(1)</label></formula><p>and</p><formula xml:id="formula_1">I t ‚Üít = I t proj(D t , T t‚Üít , K) .<label>(2)</label></formula><p>Here pe is a photometric reconstruction error, e.g. the L1 distance in pixel space; proj() are the resulting 2D coordinates of the projected depths D t in I t and is the sampling operator. For simplicity of notation we assume the pre-computed intrinsics K of all the views are identical, though they can be different. Following <ref type="bibr" target="#b20">[21]</ref> we use bilinear sampling to sample the source images, which is locally sub-differentiable, and we follow <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b14">15]</ref> in using L1 and SSIM <ref type="bibr" target="#b63">[64]</ref> to make our photometric error function pe, i.e.</p><formula xml:id="formula_2">pe(I a , I b ) = Œ± 2 (1 ‚àí SSIM(I a , I b )) + (1 ‚àí Œ±) I a ‚àí I b 1 ,</formula><p>where Œ± = 0.85. As in <ref type="bibr" target="#b14">[15]</ref> we use edge-aware smoothness  where d * t = d t /d t is the mean-normalized inverse depth from <ref type="bibr" target="#b61">[62]</ref> to discourage shrinking of the estimated depth.</p><formula xml:id="formula_3">L s = |‚àÇ x d * t | e ‚àí|‚àÇxIt| + |‚àÇ y d * t | e ‚àí|‚àÇyIt| ,<label>(3)</label></formula><p>In stereo training, our source image I t is the second view in the stereo pair to I t , which has known relative pose. While relative poses are not known in advance for monocular sequences, <ref type="bibr" target="#b75">[76]</ref> showed that it is possible to train a second pose estimation network to predict the relative poses T t‚Üít used in the projection function proj. During training, we solve for camera pose and depth simultaneously, to minimize L p . For monocular training, we use the two frames temporally adjacent to I t as our source frames, i.e. I t ‚àà {I t‚àí1 , I t+1 }. In mixed training (MS), I t includes the temporally adjacent frames and the opposite stereo view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Improved Self-Supervised Depth Estimation</head><p>Existing monocular methods produce lower quality depths than the best fully-supervised models. To close this gap, we propose several improvements that significantly increase predicted depth quality, without adding additional model components that also require training (see <ref type="figure">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-Pixel Minimum Reprojection Loss</head><p>When computing the reprojection error from multiple source images, existing self-supervised depth estimation methods average together the reprojection error into each of the available source images.This can cause issues with pixels that are visible in the target image, but are not visible in some of the source images ( <ref type="figure">Fig. 3(c)</ref>). If the network predicts the correct depth for such a pixel, the corresponding color in an occluded source image will likely not match the target, inducing a high photometric error penalty. Such problematic pixels come from two main categories: out-of-view pixels due to egomotion at image boundaries, and occluded pixels. The effect of out-of-view pixels can be reduced by masking such pixels in the reprojection loss <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b60">61]</ref>, but this does not handle disocclusion, where average reprojection can result in blurred depth discontinuities.</p><p>We propose an improvement that deals with both issues <ref type="figure">Figure 5</ref>. Auto-masking. We show auto-masks computed after one epoch, where black pixels are removed from the loss (i.e. ¬µ = 0). The mask prevents objects moving at similar speeds to the camera (top) and whole frames where the camera is static (bottom) from contaminating the loss. The mask is computed from the input frames and network predictions using Eqn. 5.</p><p>at once. At each pixel, instead of averaging the photometric error over all source images, we simply use the minimum. Our final per-pixel photometric loss is therefore</p><formula xml:id="formula_4">L p = min t pe(I t , I t ‚Üít ).<label>(4)</label></formula><p>See <ref type="figure" target="#fig_1">Fig. 4</ref> for an example of this loss in practice. Using our minimum reprojection loss significantly reduces artifacts at image borders, improves the sharpness of occlusion boundaries, and leads to better accuracy (see <ref type="table">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auto-Masking Stationary Pixels</head><p>Self-supervised monocular training often operates under the assumptions of a moving camera and a static scene. When these assumptions break down, for example when the camera is stationary or there is object motion in the scene, performance can suffer greatly. This problem can manifest itself as 'holes' of infinite depth in the predicted test time depth maps, for objects that are typically observed to be moving during training <ref type="bibr" target="#b37">[38]</ref>  <ref type="figure">(Fig. 2</ref>). This motivates our second contribution: a simple auto-masking method that filters out pixels which do not change appearance from one frame to the next in the sequence. This has the effect of letting the network ignore objects which move at the same velocity as the camera, and even to ignore whole frames in monocular videos when the camera stops moving. Like other works <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b37">38]</ref>, we also apply a per-pixel mask ¬µ to the loss, selectively weighting pixels. However in contrast to prior work, our mask is binary, so ¬µ ‚àà {0, 1}, and is computed automatically on the forward pass of the network, instead of being learned or estimated from object motion. We observe that pixels which remain the same between adjacent frames in the sequence often indicate a static camera, an object moving at equivalent relative translation to the camera, or a low texture region. We therefore set ¬µ to only include the loss of pixels where the reprojection error of the warped image I t ‚Üít is lower than that of the original, unwarped source image I t , i.e.</p><formula xml:id="formula_5">¬µ = min t pe(I t , I t ‚Üít ) &lt; min t pe(I t , I t ) ,<label>(5)</label></formula><p>where [ ] is the Iverson bracket. In cases where the camera and another object are both moving at a similar velocity, ¬µ prevents the pixels which remain stationary in the image from contaminating the loss. Similarly, when the camera is static, the mask can filter out all pixels in the image ( <ref type="figure">Fig. 5</ref>). We show experimentally that this simple and inexpensive modification to the loss brings significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale Estimation</head><p>Due to the gradient locality of the bilinear sampler <ref type="bibr" target="#b20">[21]</ref>, and to prevent the training objective getting stuck in local minima, existing models use multi-scale depth prediction and image reconstruction. Here, the total loss is the combination of the individual losses at each scale in the decoder. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> compute the photometric error on images at the resolution of each decoder layer. We observe that this has the tendency to create 'holes' in large low-texture regions in the intermediate lower resolution depth maps, as well as texture-copy artifacts (details in the depth map incorrectly transferred from the color image). Holes in the depth can occur at low resolution in low-texture regions where the photometric error is ambiguous. This complicates the task for the depth network, now freed to predict incorrect depths. Inspired by techniques in stereo reconstruction <ref type="bibr" target="#b55">[56]</ref>, we propose an improvement to this multi-scale formulation, where we decouple the resolutions of the disparity images and the color images used to compute the reprojection error. Instead of computing the photometric error on the ambiguous low-resolution images, we first upsample the lower resolution depth maps (from the intermediate layers) to the input image resolution, and then reproject, resample, and compute the error pe at this higher input resolution ( <ref type="figure">Fig. 3 (d)</ref>). This procedure is similar to matching patches, as low-resolution disparity values will be responsible for warping an entire 'patch' of pixels in the high resolution image. This effectively constrains the depth maps at each scale to work toward the same objective i.e. reconstructing the high resolution input target image as accurately as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Training Loss</head><p>We combine our per-pixel smoothness and masked photometric losses as L = ¬µL p + ŒªL s , and average over each pixel, scale, and batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Additional Considerations</head><p>Our depth estimation network is based on the general U-Net architecture <ref type="bibr" target="#b52">[53]</ref>, i.e. an encoder-decoder network, with skip connections, enabling us to represent both deep abstract features as well as local information. We use a ResNet18 <ref type="bibr" target="#b16">[17]</ref> as our encoder, which contains 11M parameters, compared to the larger, and slower, DispNet and ResNet50 models used in existing work <ref type="bibr" target="#b14">[15]</ref>. Similar to <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16]</ref>, we start with weights pretrained on Ima-geNet <ref type="bibr" target="#b53">[54]</ref>, and show that this improves accuracy for our compact model compared to training from scratch ( <ref type="table">Table 2</ref>). Our depth decoder is similar to <ref type="bibr" target="#b14">[15]</ref>, with sigmoids at the output and ELU nonlinearities <ref type="bibr" target="#b6">[7]</ref> elsewhere. We convert the sigmoid output œÉ to depth with D = 1/(aœÉ + b), where a and b are chosen to constrain D between 0.1 and 100 units. We make use of reflection padding, in place of zero padding, in the decoder, returning the value of the closest border pixels in the source image when samples land outside of the image boundaries. We found that this significantly reduces the border artifacts found in existing approaches, e.g. <ref type="bibr" target="#b14">[15]</ref>.</p><p>For pose estimation, we follow <ref type="bibr" target="#b61">[62]</ref> and predict the rotation using an axis-angle representation, and scale the rotation and translation outputs by 0.01. For monocular training, we use a sequence length of three frames, while our pose network is formed from a ResNet18, modified to accept a pair of color images (or six channels) as input and to predict a single 6-DoF relative pose. We perform horizontal flips and the following training augmentations, with 50% chance: random brightness, contrast, saturation, and hue jitter with respective ranges of ¬±0.2, ¬±0.2, ¬±0.2, and ¬±0.1. Importantly, the color augmentations are only applied to the images which are fed to the networks, not to those used to compute L p . All three images fed to the pose and depth networks are augmented with the same parameters.</p><p>Our models are implemented in PyTorch <ref type="bibr" target="#b45">[46]</ref>, trained for 20 epochs using Adam <ref type="bibr" target="#b25">[26]</ref>, with a batch size of 12 and an input/output resolution of 640 √ó 192 unless otherwise specified. We use a learning rate of 10 ‚àí4 for the first 15 epochs which is then dropped to 10 ‚àí5 for the remainder. This was chosen using a dedicated validation set of 10% of the data. The smoothness term Œª is set to 0.001. Training takes 8, 12, and 15 hours on a single Titan Xp, for the stereo (S), monocular (M), and monocular plus stereo models (MS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Here, we validate that (1) our reprojection loss helps with occluded pixels compared to existing pixel-averaging, (2) our auto-masking improves results, especially when training on scenes with static cameras, and (3) our multi-scale appearance matching loss improves accuracy. We evaluate our models, named Monodepth2, on the KITTI 2015 stereo dataset <ref type="bibr" target="#b12">[13]</ref>, to allow comparison with previously published monocular methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">KITTI Eigen Split</head><p>We use the data split of Eigen et al. <ref type="bibr" target="#b7">[8]</ref>. All results here are presented without post-processing <ref type="bibr" target="#b14">[15]</ref>; see supplementary Section F for improved postprocessed results. While our contributions are designed for monocular training, we still gain high accuracy in the stereo-only category.</p><p>We additionally show we can get higher scores at a larger 1024 √ó 320 resolution, similar to <ref type="bibr" target="#b46">[47]</ref> -see supplementary Section G. These high resolution numbers are bolded if they beat all other models, including our low-res versions.</p><p>Legend D -Depth supervision D* -Auxiliary depth supervision S -Self-supervised stereo supervision M -Self-supervised mono supervision ‚Ä† -Newer results from github. + pp -With post-processing (monocular plus stereo), we set the transformation between the two stereo frames to be a pure horizontal translation of fixed length. During evaluation, we cap depth to 80m per standard practice <ref type="bibr" target="#b14">[15]</ref>. For our monocular models, we report results using the per-image median ground truth scaling introduced by <ref type="bibr" target="#b75">[76]</ref>. See also supplementary material Section D.2 for results where we apply a single median scaling to the whole test set, instead of scaling each image independently. For results that use any stereo supervision we do not perform median scaling as scale can be inferred from the known camera baseline during training. We compare the results of several variants of our model, trained with different types of self-supervision: monocular video only (M), stereo only (S), and both (MS). The results in <ref type="table" target="#tab_2">Table 1</ref> show that our monocular method outperforms all existing state-of-the-art self-supervised approaches. We also outperform recent methods ( <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b50">51]</ref>) that explicitly compute optical flow as well as motion masks. Qualitative results can be seen in <ref type="figure">Fig. 7</ref> and supplementary Section E. However, as with all image reconstruction based approaches to depth estimation, our model breaks when the scene contains objects that violate the Lambertian assumptions of our appearance loss <ref type="figure" target="#fig_3">(Fig. 8)</ref>.</p><p>As expected, the combination of M and S training data increases accuracy, which is especially noticeable on metrics that are sensitive to large depth errors e.g. RMSE. Despite our contributions being designed around monocular training, we find that the in the stereo-only case we still perform well. We achieve high accuracy despite using a lower resolution than <ref type="bibr" target="#b46">[47]</ref>'s 1024 √ó 384, with substantially less training time (20 vs. 200 epochs) and no use of postprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">KITTI Ablation Study</head><p>To better understand how the components of our model contribute to the overall performance in monocular training, in <ref type="table">Table 2</ref>(a) we perform an ablation study by changing various components of our model. We see that the baseline model, without any of our contributions, performs the worst. When combined together, all our components lead to a significant improvement (Monodepth2 (full)). More experiments turning parts of our full model off in turn are shown in supplementary material Section C.</p><p>Benefits of auto-masking The full Eigen <ref type="bibr" target="#b7">[8]</ref> KITTI split contains several sequences where the camera does not move between frames e.g. where the data capture car was stopped at traffic lights. These 'no camera motion' sequences can cause problems for self-supervised monocular training, and as a result, they are typically excluded at training time using expensive to compute optical flow <ref type="bibr" target="#b75">[76]</ref>. We report monocular results trained on the full Eigen data split in  <ref type="figure">Figure 6</ref>. Qualitative Make3D results. All methods were trained on KITTI using monocular supervision.</p><p>Further, in <ref type="table">Table 2</ref>(a), we replace our auto-masking loss with a re-implementation of the predictive mask from <ref type="bibr" target="#b75">[76]</ref>. We find that this ablated model is worse than using no masking at all, while our auto-masking improves results in all cases. We see an example of how auto-masking works in practice in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of ImageNet pretraining</head><p>We follow previous work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16]</ref> in initializing our encoders with weights pretrained on ImageNet <ref type="bibr" target="#b53">[54]</ref>. While some other monocular depth prediction works have elected not to use ImageNet pretraining, we show in <ref type="table" target="#tab_2">Table 1</ref> that even without pretraining, we still achieve state-of-the-art results. We train these 'w/o pretraining' models for 30 epochs to ensure convergence. <ref type="table">Table 2</ref> shows the benefit our contributions bring both to pretrained networks and those trained from scratch; see supplementary material Section C for more ablations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Additional Datasets KITTI Odometry</head><p>In Section A of the supplementary material we show odometry evaluation on KITTI. While our focus is better depth estimation, our pose network performs on par with competing methods. Competing methods typically feed more frames to their pose network which may improve their ability to generalize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI Depth Prediction Benchmark</head><p>We also perform experiments on the recently introduced KITTI Depth Prediction Evaluation dataset <ref type="bibr" target="#b58">[59]</ref>, which features more accurate ground truth depth, addressing quality issues with the stan-  <ref type="table">Table 3</ref>. Make3D results. All M results benefit from median scaling, while MS uses the unmodified network prediction. dard split. We train models using this new benchmark split, and evaluate it using the online server <ref type="bibr" target="#b26">[27]</ref>, and provide results in supplementary Section D.3. Additionally, 93% of the Eigen split test frames have higher quality ground truth depths provided by <ref type="bibr" target="#b58">[59]</ref>. Like <ref type="bibr" target="#b0">[1]</ref>, we use these instead of the reprojected LIDAR scans to compare our method against several existing baseline algorithms, still showing superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Make3D</head><p>In <ref type="table">Table 3</ref> we report performance on the Make3D dataset <ref type="bibr" target="#b54">[55]</ref> using our models trained on KITTI. We outperform all methods that do not use depth supervision, with the evaluation criteria from <ref type="bibr" target="#b14">[15]</ref>. However, caution should be taken with Make3D, as its ground truth depth and input images are not well aligned, causing potential evaluation issues. We evaluate on a center crop of 2 √ó 1 ratio, and apply median scaling for our M model. Qualitative results can be seen in <ref type="figure">Fig. 6</ref> and in supplementary Section E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a versatile model for self-supervised monocular depth estimation, achieving state-of-the-art depth predictions. We introduced three contributions: (i) a minimum reprojection loss, computed for each pixel, to deal Input Monodepth <ref type="bibr" target="#b14">[15]</ref> Zhou et al. <ref type="bibr" target="#b75">[76]</ref> DDVO <ref type="bibr" target="#b61">[62]</ref> GeoNet <ref type="bibr" target="#b70">[71]</ref> Zhan et al. <ref type="bibr" target="#b72">[73]</ref> Ranjan et al. <ref type="bibr" target="#b50">[51]</ref> 3Net -R50 <ref type="bibr" target="#b37">[38]</ref> EPC++ (MS) <ref type="bibr" target="#b37">[38]</ref>   <ref type="figure">Figure 7</ref>. Qualitative results on the KITTI Eigen split. Our models (MD2) in the last four rows produce the sharpest depth maps, which are reflected in the superior quantitative results in <ref type="table" target="#tab_2">Table 1</ref>. Additional results can be seen in the supplementary materiale Section E. with occlusions between frames in monocular video, (ii) an auto-masking loss to ignore confusing, stationary pixels, and (iii) a full-resolution multi-scale sampling method. We showed how together they give a simple and efficient model for depth estimation, which can be trained with monocular video data, stereo data, or mixed monocular and stereo data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Note on arXiv versions In an earlier pre-print of this paper, 1806.01260v1, we included a shared encoder for pose and depth. While this reduced the number of training parameters, we have since found that we can gain even higher results with a separate ResNet pose encoder which accepts a stack of two frames as input (see ablation study in Section H). Since v1, we have also introduced auto-masking to help the model ignore pixels that violate our motion assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Odometry Evaluation</head><p>In <ref type="table">Table 4</ref> we evaluate our pose estimation network following the protocol in <ref type="bibr" target="#b75">[76]</ref>. We trained our models on sequences 0-8 from the KITTI odometry split and tested on sequences 9 and 10. As in <ref type="bibr" target="#b75">[76]</ref>, the absolute trajectory error is then averaged over all overlapping five-frame snippets in the test sequences. Here, unlike <ref type="bibr" target="#b75">[76]</ref> and others who use custom models for the odometry task, we use the same architecture for this task as our other results, and simply train it again from scratch on these new sequences.</p><p>Baselines such as <ref type="bibr" target="#b75">[76]</ref> use a pose network which predicts transformations between sets of five frames simultaneously. Our pose network only takes two frames as input, and ouputs a single transformation between that pair of frames. In order to evaluate our two-frame model on the five-frame test sequences, we make separate predictions for each of the four frame-to-frame transformation for each set of five frames and combine them to form local trajectories. For completeness we repeat the same process with <ref type="bibr" target="#b75">[76]</ref> predicted poses, which we denote as 'Zhou * '. As we can see in <ref type="table">Table 4</ref>, our frame-to-frame poses come close to the accuracy of methods trained on blocks of five frames at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Details</head><p>Except where stated, for all experiments we use a standard ResNet18 <ref type="bibr" target="#b16">[17]</ref> encoder for both depth and pose networks. Our pose encoder is modified to accept a pair of frames, or six channels, as input. Our pose encoder therefore has convolutional weights in the first layer of shape 6√ó64√ó3√ó3, instead of the ResNet default of 3√ó64√ó3√ó3. When using pretrained weights for our pose encoder, we duplicate the first pretrained filter tensor along the channel dimension to make a filter of shape 6 √ó 64 √ó 3 √ó 3. This allows for a six-channel input image. All weights in this new expanded filter are divided by 2 to make the output of the convolution in the same numerical range as the original, one-image ResNet. In <ref type="table" target="#tab_5">Table 5</ref> we describe the parameters of each layer used in our depth decoder and pose network. Our pose network is larger and deeper than previous works <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b61">62]</ref>, and we only feed two frames at a time to the Sequence 09 Sequence 10 # frames ORB-Slam <ref type="bibr" target="#b43">[44]</ref> 0.014¬±0.008 0.012¬±0.011 -DDVO <ref type="bibr" target="#b61">[62]</ref> 0.045¬±0.108 0.033¬±0.074 3 Zhou* <ref type="bibr" target="#b75">[76]</ref> 0.050¬±0.039 0.034¬±0.028 5 ‚Üí 2 Zhou <ref type="bibr" target="#b75">[76]</ref> 0.021¬±0.017 0.020¬±0.015 5 Zhou <ref type="bibr" target="#b75">[76]</ref> ‚Ä† 0.016¬±0.009 0.013¬±0.009 5 Mahjourian <ref type="bibr" target="#b39">[40]</ref> 0.013¬±0.010 0.012¬±0.011 3 GeoNet <ref type="bibr" target="#b70">[71]</ref> 0.012¬±0.007 0.012¬±0.009 5 EPC++ M <ref type="bibr" target="#b37">[38]</ref> 0.013¬±0.007 0.012¬±0.008 3 Ranjan <ref type="bibr" target="#b50">[51]</ref> 0.012¬±0.007 0.012¬±0.008 5 EPC++ MS <ref type="bibr" target="#b37">[38]</ref> 0.012¬±0.006 0.012¬±0.008 3 Monodepth2 M* 0.017¬±0.008 0.015¬±0.010 2 Monodepth2 MS* 0.017¬±0.008 0.015¬±0.010 2 Monodepth2 M w/o pretraining* 0.018¬±0.010 0.015¬±0.010 2 Monodepth2 MS w/o pretraining* 0.018¬±0.009 0.015¬±0.010 2 <ref type="table">Table 4</ref>.</p><p>Odometry results on the KITTI [13] odometry dataset. Results show the average absolute trajectory error, and standard deviation, in meters. ‚Ä† -newer results from the respective online implementations. * -evaluation on trajectories made from pairwise predictions -see text for details. '# frames' is the number of input frames used for pose prediction. To evaluate our method we chain integrate the poses from four pairs to make five frames for evaluation.   pose network in contrast to previous works which use three <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b61">62]</ref> or more for their depth estimation experiments. In Section H we validate the benefit of bringing additional parameters to the pose network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Ablation Experiments</head><p>In <ref type="table" target="#tab_6">Table 6</ref> we show a full ablation study on our algorithm, turning on and off different components of the system. We confirm the finding of the main paper, that all our components together gives the highest quality model, and that pretraining helps. We observe in <ref type="table" target="#tab_6">Table 6</ref> (d) that our results with ResNet 50 are even higher than our ResNet18 models. ResNet 50 is a standard encoder used by previous works e.g. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">50]</ref>. However, training with a 50-layer ResNet comes at the expense of longer training and test times. In <ref type="figure" target="#fig_4">Fig. 9</ref> we show additional qualitative results for the monocular trained variants of our model from <ref type="table" target="#tab_6">Table 6</ref>. We observe 'depth holes' in both non-pretrained and pretrained versions of the baseline model compared to ours.  <ref type="table">Table 7</ref>. KITTI improved ground truth. Comparison to existing methods on KITTI 2015 <ref type="bibr" target="#b12">[13]</ref> using 93% of the Eigen split and the improved ground truth from <ref type="bibr" target="#b58">[59]</ref>. Baseline methods were evaluated using their provided disparity files, which were either available publicly or from private communication with the authors.</p><p>Legend D* -Auxiliary depth supervision S -Self-supervised stereo supervision M -Self-supervised mono supervision ‚Ä† -Newer results from the respective online implementations. + pp -With post-processing D. Additional Evaluation D.1. Improved Ground Truth</p><p>As mentioned in the main paper, the evaluation method introduced by Eigen <ref type="bibr" target="#b7">[8]</ref> for KITTI uses the reprojected LI-DAR points but does not handle occlusions, moving objects, or the fact that the car is moving. <ref type="bibr" target="#b58">[59]</ref> introduced a set of high quality depth maps for the KITTI dataset, making use of 5 consecutive frames and handling moving objects using the stereo pair. This improved ground truth depth is provided for 652 (or 93%) of the 697 test frames contained in the Eigen test split <ref type="bibr" target="#b7">[8]</ref>. We evaluate our results on these 652 improved ground truth frames and compare to existing published methods without having to retrain each method, see <ref type="table">Table 7</ref>. We present results for all other methods for which we have obtained predictions from the authors. We use the same error metrics from the standard evaluation, and clip the predicted depths to 80 meters to match the Eigen evaluation. We evaluate on the full image and do not crop, unlike with the Eigen evaluation. We can see that our method still significantly outperforms all previously published methods on all metrics. While Superdepth <ref type="bibr" target="#b46">[47]</ref> comes a close second to our algorithm in the S category, they are run at high resolution (1024 √ó 384 vs. our 640 √ó 192), and in <ref type="table" target="#tab_2">Table 1</ref> we show that at higher resolutions our model's performance also increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Single-Scale Evaluation</head><p>Our monocular trained approach, like all self-supervised baselines, has no guarantee of producing results with a metric scale. Nonetheless, we anticipate that there could be value in estimating depth-outputs that are, without special measures, consistent with each other across all predictions. In <ref type="bibr" target="#b75">[76]</ref>, the authors independently scale each predicted depth map by the ratio of the median of the ground truth and predicted depth map -for each individual test im-age. This is in contrast to stereo based training where the scale is known and as a result no additional scaling is required during the evaluation e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. This per-image depth scaling hides unstable scale estimation in both depth and pose estimation and presents a best-case scenario for the monocular training case. If a method outputs wildly varying scales for each sequence, then this evaluation protocol will hide the issue. This gives an unfair advantage over stereo trained methods that do not perform per-image scaling.</p><p>We thus modified the original protocol to instead use a single scale for all predicted depth maps of each method. For each method, we compute this single scale by taking the median of all the individual ratios of the depth medians on the test set. While this is still not ideal as it makes use of the ground truth depth, we believe it to be fairer and representative of the performance of each method. We also calculated the standard deviation œÉ scale of the individual scales, where lower values indicate more consistent output depth map scales. As can be seen in <ref type="table">Table 8</ref>, our method outperforms previously published self-supervised monocular methods, especially in the near range depth values i.e. Œ¥ &lt; 1.25, and is more stable overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. KITTI Evaluation Server Benchmark</head><p>Here, we report the performance of our self-supervised monocular plus stereo model on the online KITTI single image depth prediction benchmark evaluation server <ref type="bibr" target="#b26">[27]</ref>. <ref type="bibr" target="#b26">[27]</ref> uses a different split of the data, which is not the same as the Eigen split. As a result, we train a new model on the provided training data. At the time of writing, there were no published self-supervised approaches among the submissions on the leaderboard. Despite not using any ground truth data during training, our monocular only predictions are competitive with fully supervised methods, see <ref type="table">Table 9</ref>. Adding stereo data and a more powerful encoder at training time results in even better performance (Monodepth2  <ref type="table">Table 9</ref>. KITTI depth prediction benchmark. Comparison of our monocular plus stereo approaches to fully supervised methods on the KITTI depth prediction benchmark <ref type="bibr" target="#b26">[27]</ref>. D indicates models that were trained with ground truth depth supervision, while M and S are monocular and stereo self-supervision respectively.</p><p>(ResNet50)). Because the evaluation server does not do median scaling (required for monocular methods), we needed a way to find the correct scaling for our mono-only model, which makes unscaled predictions. We make predictions with our monomodel on 1,000 images from the KITTI training set which have ground truth depths available, and for each of the 1,000 images we find the scale factor which best align the depth maps <ref type="bibr" target="#b75">[76]</ref>. Finally, we take the median of these 1,000 scale factors as the single factor which we use to scale all predictions from our mono model. Note that, to remain true to our 'self-supervised' philosophy, we never do any other form of validation, model selection or parameter tuning using ground truth depths. For comparison, we trained a version of the original Monodepth <ref type="bibr" target="#b14">[15]</ref> using the online code 1 on the same benchmark split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Qualitative Comparisons</head><p>We include additional qualitative results from the KITTI test set in <ref type="figure" target="#fig_0">Fig. 13</ref>. We can see that our models generate higher quality outputs and do not produce 'holes' in the depth maps or border artifacts that can be seen in many existing baselines e.g. <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b72">73]</ref>. We also show additional results from Make3D in <ref type="figure" target="#fig_0">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Results with Post-Processing</head><p>Post-processing, introduced by <ref type="bibr">[</ref>  <ref type="figure" target="#fig_0">Figure 10</ref>. Additional Make3D results. Our model (MD2 M) trained on KITTI results in plausible depths, predicting more detail than existing monocular methods. The last row is an interesting failure for all methods as it contains an image that is very different than those from the KITTI training set. estimation methods by running each test image through the network twice, once unflipped and then flipped. The two predictions are then masked and averaged. This has been shown to bring significant gains in accuracy for stereo results, at the expense of requiring two forward-passes through the network at test time <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">50]</ref>. In <ref type="table" target="#tab_2">Table 10</ref> we show, for the first time, that post-processing also improves quantitative performance in the monocular only (M) and mixed (MS) training cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Effect of Image Resolution</head><p>In the main paper, we presented results at our standard resolution (640 √ó 192). We also showed additional results at higher (1024√ó320) and lower (416√ó128) resolutions. In <ref type="table" target="#tab_2">Table 11</ref> we show a full set of results at all three resolutions. We see that higher resolution helps, confirming the finding in <ref type="bibr" target="#b46">[47]</ref>. We also include an ablation showing that, even at the highest resolution, our full-res multi-scale still provides benefit beyond just higher resolution training (vs. 'Ours w/o full-res multi-scale').</p><p>Our high resolution models were initialized using the weights from our standard resolution (640 √ó 192) model after 10 epochs of training. We then trained our high resolution models for 5 epochs with a learning rate of 10 ‚àí5 . We used a batch size of 4 to enable this higher resolution model to fit on a single 12GB Titan X GPU.</p><p>Qualitative results of the effect of resolution are illus-  <ref type="table" target="#tab_2">Table 10</ref>. Effect of post-processing. We observe that post-processing, originally motivated only for stereo training, also brings consistent benefits to all our monocular-trained models. Interestingly, for some metrics post-processing results in a larger quantitative gain than models trained at higher resolution.  <ref type="table" target="#tab_2">Table 11</ref>. Ablation study on the input/output resolutions of our model. ‚Ä†Timings for the highest resolution models comprise 10 epochs training of the 640 √ó 192 model and 5 epochs of the 1024 √ó 320 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Monodepth2 MS 128 √ó 416 Monodepth2 MS 192 √ó 640 Monodepth2 MS 320 √ó 1024 <ref type="figure" target="#fig_0">Figure 11</ref>. Effect of varying resolutions on the KITTI Eigen split. All predicted disparity maps have been resized to the same size for visualization. Our lowest resolution model (128 √ó 416) captures the broad shape of the scene successfully, but struggles with thin objects and sometimes fails to accurately capture the shape of depth discontinuities around object boundaries. trated in <ref type="figure" target="#fig_0">Fig. 11</ref>. It is clear that all resolutions accurately capture the overall shape of the scene. However, only the highest resolution model accurately represents the shape of thin objects.  <ref type="table" target="#tab_2">Table 12</ref>. Ablation of the effect of pose networks on depth prediction. Results shown are on depth prediction on the KITTI dataset, when trained from monocular sequences only. 'Input Frames' indicate how many frames are fed to the pose network. 'Shared encoder (arXiv v1)' denotes the architecture proposed in v1 of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Comparison of Pose Encoder</head><p>In <ref type="table" target="#tab_2">Table 12</ref> we evaluate different pose encoders. In an earlier version of this paper, we proposed the use of a shared pose encoder that shared features with the depth network. This resulted in fewer parameters to optimize during training, but also results in a decrease in depth prediction accuracy, see <ref type="table" target="#tab_2">Table 12</ref>. As a baseline we compare against the pose network used by <ref type="bibr" target="#b61">[62]</ref>, which builds upon <ref type="bibr" target="#b75">[76]</ref> with an additional scaling of the translation by the mean of the inverse depth. Overall, our separate encoder is superior for both pretrained and non-pretrained variants, whether we use two or three frames as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Supplementary Video Results</head><p>In the supplementary video, we show results on 'Wander', a monocular dataset collected from the 'Wind Walk Travel Videos' YouTube channel. <ref type="bibr" target="#b1">2</ref> This dataset is quite different from the car mounted videos of KITTI as it only features a monocular hand-held camera in a non-European environment. We train on four sequences and present results on a fifth unseen sequence. We use an input/output resolution of 128 √ó 224. As with our KITTI experiments we train for 20 epochs with a batch size of 12, with a learning rate of 10 ‚àí4 which is reduced by a factor of 10 for the final 5 epochs. For these handheld videos we found that the SSIM loss produced artifacts at object edges. As a result, we used a feature reconstruction loss in the appearance matching term, as in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b72">73]</ref>, by computing the L1 distance on the reprojected and normalized relu1 1 features from an ImageNet pretrained VGG16 <ref type="bibr" target="#b56">[57]</ref> as our pe function. This takes significantly longer to train, but results in qualitatively better depth maps on this dataset. Examples of predicted depths can be seen in <ref type="figure" target="#fig_0">Fig. 12.</ref> 2 https://www.youtube.com/channel/ UCPur06mx78RtwgHJzxpu2ew <ref type="figure" target="#fig_0">Figure 12</ref>. Additional Wander results. We observe that our model (Ours M) results in fewer visual artifacts when compared to the the baseline (i.e. the same model including VGG loss, but without our contributions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Garg et al. <ref type="bibr" target="#b11">[12]</ref> Monodepth <ref type="bibr" target="#b14">[15]</ref> Zhou et al. <ref type="bibr" target="#b75">[76]</ref> Zhan et al. <ref type="bibr" target="#b72">[73]</ref> DDVO <ref type="bibr" target="#b61">[62]</ref> Mahjourian et al. <ref type="bibr" target="#b39">[40]</ref> GeoNet <ref type="bibr" target="#b70">[71]</ref> Ranjan et al. <ref type="bibr" target="#b50">[51]</ref> EPC++ <ref type="bibr" target="#b37">[38]</ref> 3Net <ref type="bibr" target="#b49">[50]</ref> Baseline M Ours M Ours S Ours MS <ref type="figure" target="#fig_0">Figure 13</ref>. Additional KITTI Eigen split test results. We can see that our approaches in the last three rows produce the sharpest depth maps. 'Baseline M' is our model without our contributions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Depth from a single image. Our self-supervised model, Monodepth2, produces sharp, high quality depth maps, whether trained with monocular (M), stereo (S), or joint (MS) supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Benefit of min. reprojection loss in MS training. Pixels in the the circled region are occluded in IR so no loss is applied between (IL, IR). Instead, the pixels are matched to I‚àí1 where they are visible. Colors in the top right image indicate which of the source images on the bottom are selected for matching by Eqn. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Failure cases. Top: Our self-supervised loss fails to learn good depths for distorted, reflective and color-saturated regions. Bottom: We can fail to accurately delineate objects where boundaries are ambiguous (left) or shapes are intricate (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Qualitative ablation study. We can see that our model with all components added result in the smallest amount of depth artifacts. 'Baseline (M)' is our model without our full-resolution multi-scale appearance loss, minimum reprojection loss, or auto-masking loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Except in ablation experiments, for training which uses monocular sequences (i.e. monocular and monocular plus stereo) we follow Zhou et al.'s [76] pre-processing to remove static frames. This results in 39,810 monocular triplets for training and 4,424 for validation. We use the same intrinsics for all images, setting the principal point of the camera to the image center and the focal length to the average of all the focal lengths in KITTI. For stereo and mixed training Method Train Abs Rel Sq Rel RMSE RMSE log Œ¥ &lt; 1.25 Œ¥ &lt; 1.25 2 Œ¥ &lt; 1.25 3 Quantitative results. Comparison of our method to existing methods on KITTI 2015 [13] using the Eigen split. Best results in each category are in bold; second best are underlined.</figDesc><table><row><cell>Eigen [9]</cell><cell>D</cell><cell>0.203</cell><cell>1.548</cell><cell>6.307</cell><cell>0.282</cell><cell>0.702</cell><cell>0.890</cell><cell>0.890</cell></row><row><cell>Liu [36]</cell><cell>D</cell><cell>0.201</cell><cell>1.584</cell><cell>6.471</cell><cell>0.273</cell><cell>0.680</cell><cell>0.898</cell><cell>0.967</cell></row><row><cell>Klodt [28]</cell><cell>D*M</cell><cell>0.166</cell><cell>1.490</cell><cell>5.998</cell><cell>-</cell><cell>0.778</cell><cell>0.919</cell><cell>0.966</cell></row><row><cell>AdaDepth [45]</cell><cell>D*</cell><cell>0.167</cell><cell>1.257</cell><cell>5.578</cell><cell>0.237</cell><cell>0.771</cell><cell>0.922</cell><cell>0.971</cell></row><row><cell>Kuznietsov [30]</cell><cell>DS</cell><cell>0.113</cell><cell>0.741</cell><cell>4.621</cell><cell>0.189</cell><cell>0.862</cell><cell>0.960</cell><cell>0.986</cell></row><row><cell>DVSO [68]</cell><cell>D*S</cell><cell>0.097</cell><cell>0.734</cell><cell>4.442</cell><cell>0.187</cell><cell>0.888</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell>SVSM FT [39]</cell><cell>DS</cell><cell>0.094</cell><cell>0.626</cell><cell>4.252</cell><cell>0.177</cell><cell>0.891</cell><cell>0.965</cell><cell>0.984</cell></row><row><cell>Guo [16]</cell><cell>DS</cell><cell>0.096</cell><cell>0.641</cell><cell>4.095</cell><cell>0.168</cell><cell>0.892</cell><cell>0.967</cell><cell>0.986</cell></row><row><cell>DORN [10]</cell><cell>D</cell><cell>0.072</cell><cell>0.307</cell><cell>2.727</cell><cell>0.120</cell><cell>0.932</cell><cell>0.984</cell><cell>0.994</cell></row><row><cell>Zhou [76] ‚Ä†</cell><cell>M</cell><cell>0.183</cell><cell>1.595</cell><cell>6.709</cell><cell>0.270</cell><cell>0.734</cell><cell>0.902</cell><cell>0.959</cell></row><row><cell>Yang [70]</cell><cell>M</cell><cell>0.182</cell><cell>1.481</cell><cell>6.501</cell><cell>0.267</cell><cell>0.725</cell><cell>0.906</cell><cell>0.963</cell></row><row><cell>Mahjourian [40]</cell><cell>M</cell><cell>0.163</cell><cell>1.240</cell><cell>6.220</cell><cell>0.250</cell><cell>0.762</cell><cell>0.916</cell><cell>0.968</cell></row><row><cell>GeoNet [71] ‚Ä†</cell><cell>M</cell><cell>0.149</cell><cell>1.060</cell><cell>5.567</cell><cell>0.226</cell><cell>0.796</cell><cell>0.935</cell><cell>0.975</cell></row><row><cell>DDVO [62]</cell><cell>M</cell><cell>0.151</cell><cell>1.257</cell><cell>5.583</cell><cell>0.228</cell><cell>0.810</cell><cell>0.936</cell><cell>0.974</cell></row><row><cell>DF-Net [78]</cell><cell>M</cell><cell>0.150</cell><cell>1.124</cell><cell>5.507</cell><cell>0.223</cell><cell>0.806</cell><cell>0.933</cell><cell>0.973</cell></row><row><cell>LEGO [69]</cell><cell>M</cell><cell>0.162</cell><cell>1.352</cell><cell>6.276</cell><cell>0.252</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ranjan [51]</cell><cell>M</cell><cell>0.148</cell><cell>1.149</cell><cell>5.464</cell><cell>0.226</cell><cell>0.815</cell><cell>0.935</cell><cell>0.973</cell></row><row><cell>EPC++ [38]</cell><cell>M</cell><cell>0.141</cell><cell>1.029</cell><cell>5.350</cell><cell>0.216</cell><cell>0.816</cell><cell>0.941</cell><cell>0.976</cell></row><row><cell>Struct2depth '(M)' [5]</cell><cell>M</cell><cell>0.141</cell><cell>1.026</cell><cell>5.291</cell><cell>0.215</cell><cell>0.816</cell><cell>0.945</cell><cell>0.979</cell></row><row><cell>Monodepth2 w/o pretraining</cell><cell>M</cell><cell>0.132</cell><cell>1.044</cell><cell>5.142</cell><cell>0.210</cell><cell>0.845</cell><cell>0.948</cell><cell>0.977</cell></row><row><cell>Monodepth2</cell><cell>M</cell><cell>0.115</cell><cell>0.903</cell><cell>4.863</cell><cell>0.193</cell><cell>0.877</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell>Monodepth2 (1024 √ó 320)</cell><cell>M</cell><cell>0.115</cell><cell>0.882</cell><cell>4.701</cell><cell>0.190</cell><cell>0.879</cell><cell>0.961</cell><cell>0.982</cell></row><row><cell>Garg [12] ‚Ä†</cell><cell>S</cell><cell>0.152</cell><cell>1.226</cell><cell>5.849</cell><cell>0.246</cell><cell>0.784</cell><cell>0.921</cell><cell>0.967</cell></row><row><cell>Monodepth R50 [15] ‚Ä†</cell><cell>S</cell><cell>0.133</cell><cell>1.142</cell><cell>5.533</cell><cell>0.230</cell><cell>0.830</cell><cell>0.936</cell><cell>0.970</cell></row><row><cell>StrAT [43]</cell><cell>S</cell><cell>0.128</cell><cell>1.019</cell><cell>5.403</cell><cell>0.227</cell><cell>0.827</cell><cell>0.935</cell><cell>0.971</cell></row><row><cell>3Net (R50) [50]</cell><cell>S</cell><cell>0.129</cell><cell>0.996</cell><cell>5.281</cell><cell>0.223</cell><cell>0.831</cell><cell>0.939</cell><cell>0.974</cell></row><row><cell>3Net (VGG) [50]</cell><cell>S</cell><cell>0.119</cell><cell>1.201</cell><cell>5.888</cell><cell>0.208</cell><cell>0.844</cell><cell>0.941</cell><cell>0.978</cell></row><row><cell>SuperDepth + pp [47] (1024 √ó 382)</cell><cell>S</cell><cell>0.112</cell><cell>0.875</cell><cell>4.958</cell><cell>0.207</cell><cell>0.852</cell><cell>0.947</cell><cell>0.977</cell></row><row><cell>Monodepth2 w/o pretraining</cell><cell>S</cell><cell>0.130</cell><cell>1.144</cell><cell>5.485</cell><cell>0.232</cell><cell>0.831</cell><cell>0.932</cell><cell>0.968</cell></row><row><cell>Monodepth2</cell><cell>S</cell><cell>0.109</cell><cell>0.873</cell><cell>4.960</cell><cell>0.209</cell><cell>0.864</cell><cell>0.948</cell><cell>0.975</cell></row><row><cell>Monodepth2 (1024 √ó 320)</cell><cell>S</cell><cell>0.107</cell><cell>0.849</cell><cell>4.764</cell><cell>0.201</cell><cell>0.874</cell><cell>0.953</cell><cell>0.977</cell></row><row><cell>UnDeepVO [33]</cell><cell>MS</cell><cell>0.183</cell><cell>1.730</cell><cell>6.57</cell><cell>0.268</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Zhan FullNYU [73]</cell><cell>D*MS</cell><cell>0.135</cell><cell>1.132</cell><cell>5.585</cell><cell>0.229</cell><cell>0.820</cell><cell>0.933</cell><cell>0.971</cell></row><row><cell>EPC++ [38]</cell><cell>MS</cell><cell>0.128</cell><cell>0.935</cell><cell>5.011</cell><cell>0.209</cell><cell>0.831</cell><cell>0.945</cell><cell>0.979</cell></row><row><cell>Monodepth2 w/o pretraining</cell><cell>MS</cell><cell>0.127</cell><cell>1.031</cell><cell>5.266</cell><cell>0.221</cell><cell>0.836</cell><cell>0.943</cell><cell>0.974</cell></row><row><cell>Monodepth2</cell><cell>MS</cell><cell>0.106</cell><cell>0.818</cell><cell>4.750</cell><cell>0.196</cell><cell>0.874</cell><cell>0.957</cell><cell>0.979</cell></row><row><cell>Monodepth2 (1024 √ó 320)</cell><cell>MS</cell><cell>0.106</cell><cell>0.806</cell><cell>4.630</cell><cell>0.193</cell><cell>0.876</cell><cell>0.958</cell><cell>0.980</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 Table 2 .</head><label>22</label><figDesc>(c), i.e. without removing frames. The baseline model trained on the full KITTI split performs worse than our full model. Ablation. Results for different variants of our model (Monodepth2) with monocular training on KITTI 2015<ref type="bibr" target="#b12">[13]</ref> using the Eigen split. (a) The baseline model, with none of our contributions, performs poorly. The addition of our minimum reprojection, auto-masking and full-res multi-scale components, significantly improves performance. (b) Even without ImageNet pretrained weights, our much simpler model brings large improvements above the baseline -see alsoTable 1. (c) If we train with the full Eigen dataset (instead of the subset introduced for monocular training by<ref type="bibr" target="#b75">[76]</ref>) our improvement over the baseline increases.</figDesc><table><row><cell>Auto-masking</cell><cell>Min. reproj.</cell><cell>Full-res multi-scale</cell><cell>Pretrained</cell><cell>Full Eigen split</cell><cell cols="3">Abs Rel Sq Rel RMSE</cell><cell>RMSE log</cell><cell>Œ¥ &lt;1.25 Œ¥ &lt;1.25 2 Œ¥ &lt; 1.25 3</cell></row><row><cell>(a) Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.140</cell><cell>1.610</cell><cell>5.512</cell><cell>0.223</cell><cell>0.852</cell><cell>0.946</cell><cell>0.973</cell></row><row><cell>Baseline + min reproj.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.122</cell><cell>1.081</cell><cell>5.116</cell><cell>0.199</cell><cell>0.866</cell><cell>0.957</cell><cell>0.980</cell></row><row><cell>Baseline + automasking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.124</cell><cell>0.936</cell><cell>5.010</cell><cell>0.206</cell><cell>0.858</cell><cell>0.952</cell><cell>0.977</cell></row><row><cell>Baseline + full-res m.s.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.124</cell><cell>1.170</cell><cell>5.249</cell><cell>0.203</cell><cell>0.865</cell><cell>0.953</cell><cell>0.978</cell></row><row><cell>Monodepth2 w/o min reprojection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.117</cell><cell>0.878</cell><cell>4.846</cell><cell>0.196</cell><cell>0.870</cell><cell>0.957</cell><cell>0.980</cell></row><row><cell>Monodepth2 w/o auto-masking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.120</cell><cell>1.097</cell><cell>5.074</cell><cell>0.197</cell><cell>0.872</cell><cell>0.956</cell><cell>0.979</cell></row><row><cell>Monodepth2 w/o full-res m.s.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.117</cell><cell>0.866</cell><cell>4.864</cell><cell>0.196</cell><cell>0.871</cell><cell>0.957</cell><cell>0.981</cell></row><row><cell>Monodepth2 with [76]'s mask</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.123</cell><cell>1.177</cell><cell>5.210</cell><cell>0.200</cell><cell>0.869</cell><cell>0.955</cell><cell>0.978</cell></row><row><cell>Monodepth2 smaller (416 √ó 128)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.128</cell><cell>1.087</cell><cell>5.171</cell><cell>0.204</cell><cell>0.855</cell><cell>0.953</cell><cell>0.978</cell></row><row><cell>Monodepth2 (full)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.115</cell><cell>0.903</cell><cell>4.863</cell><cell>0.193</cell><cell>0.877</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell>(b) Baseline w/o pt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.150</cell><cell>1.585</cell><cell>5.671</cell><cell>0.234</cell><cell>0.827</cell><cell>0.938</cell><cell>0.971</cell></row><row><cell>Monodepth2 w/o pt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.132</cell><cell>1.044</cell><cell>5.142</cell><cell>0.210</cell><cell>0.845</cell><cell>0.948</cell><cell>0.977</cell></row><row><cell>(c) Baseline (full Eigen dataset)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.146</cell><cell>1.876</cell><cell>5.666</cell><cell>0.230</cell><cell>0.848</cell><cell>0.945</cell><cell>0.972</cell></row><row><cell>Monodepth2 (full Eigen dataset)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.116</cell><cell>0.918</cell><cell>4.872</cell><cell>0.193</cell><cell>0.874</cell><cell>0.959</cell><cell>0.981</cell></row></table><note>Input Zhou et al. [76] DDVO [62] Monodepth2 (M) Ground truth</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Our network architecture Here k is the kernel size, s the stride, chns the number of output channels for each layer, res is the downscaling factor for each layer relative to the input image, and input corresponds to the input of each layer where ‚Üë is a 2√ó nearest-neighbor upsampling of the layer.</figDesc><table><row><cell></cell><cell>Decoder</cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer</cell><cell cols="3">k s chns res input</cell><cell>activation</cell></row><row><cell cols="4">upconv5 3 1 256 32 econv5</cell><cell>ELU [7]</cell></row><row><cell>iconv5</cell><cell cols="4">3 1 256 16 ‚Üëupconv5, econv4 ELU</cell></row><row><cell cols="4">upconv4 3 1 128 16 iconv5</cell><cell>ELU</cell></row><row><cell>iconv4</cell><cell cols="2">3 1 128 8</cell><cell cols="2">‚Üëupconv4, econv3 ELU</cell></row><row><cell>disp4</cell><cell>3 1 1</cell><cell>1</cell><cell>iconv4</cell><cell>Sigmoid</cell></row><row><cell cols="2">upconv3 3 1 64</cell><cell>8</cell><cell>iconv4</cell><cell>ELU</cell></row><row><cell>iconv3</cell><cell>3 1 64</cell><cell>4</cell><cell cols="2">‚Üëupconv3, econv2 ELU</cell></row><row><cell>disp3</cell><cell>3 1 1</cell><cell>1</cell><cell>iconv3</cell><cell>Sigmoid</cell></row><row><cell cols="2">upconv2 3 1 32</cell><cell>4</cell><cell>iconv3</cell><cell>ELU</cell></row><row><cell>iconv2</cell><cell>3 1 32</cell><cell>2</cell><cell cols="2">‚Üëupconv2, econv1 ELU</cell></row><row><cell>disp2</cell><cell>3 1 1</cell><cell>1</cell><cell>iconv2</cell><cell>Sigmoid</cell></row><row><cell cols="2">upconv1 3 1 16</cell><cell>2</cell><cell>iconv2</cell><cell>ELU</cell></row><row><cell>iconv1</cell><cell>3 1 16</cell><cell>1</cell><cell>‚Üëupconv1</cell><cell>ELU</cell></row><row><cell>disp1</cell><cell>3 1 1</cell><cell>1</cell><cell>iconv1</cell><cell>Sigmoid</cell></row><row><cell cols="2">Pose Decoder</cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer</cell><cell cols="3">k s chns res input activation</cell><cell></cell></row><row><cell cols="4">pconv0 1 1 256 32 econv5 ReLU</cell><cell></cell></row><row><cell cols="4">pconv1 3 1 256 32 pconv0 ReLU</cell><cell></cell></row><row><cell cols="4">pconv2 3 1 256 32 pconv1 ReLU</cell><cell></cell></row><row><cell cols="2">pconv3 1 1 6</cell><cell cols="2">32 pconv3 -</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Ablation.</figDesc><table><row><cell>Results for different variants of our model (Monodepth2) with monocular training (except where specified) on KITTI</cell></row><row><cell>2015 [13].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>MethodTrain Abs Rel Sq Rel RMSE RMSE log Œ¥ &lt; 1.25 Œ¥ &lt; 1.25 2 Œ¥ &lt; 1.25<ref type="bibr" target="#b2">3</ref> </figDesc><table><row><cell>Zhou [76] ‚Ä†</cell><cell>M</cell><cell>0.176</cell><cell>1.532 6.129</cell><cell>0.244</cell><cell>0.758</cell><cell>0.921</cell><cell>0.971</cell></row><row><cell>Mahjourian [40]</cell><cell>M</cell><cell>0.134</cell><cell>0.983 5.501</cell><cell>0.203</cell><cell>0.827</cell><cell>0.944</cell><cell>0.981</cell></row><row><cell>GeoNet [71]</cell><cell>M</cell><cell>0.132</cell><cell>0.994 5.240</cell><cell>0.193</cell><cell>0.833</cell><cell>0.953</cell><cell>0.985</cell></row><row><cell>DDVO [62]</cell><cell>M</cell><cell>0.126</cell><cell>0.866 4.932</cell><cell>0.185</cell><cell>0.851</cell><cell>0.958</cell><cell>0.986</cell></row><row><cell>Ranjan [51]</cell><cell>M</cell><cell>0.123</cell><cell>0.881 4.834</cell><cell>0.181</cell><cell>0.860</cell><cell>0.959</cell><cell>0.985</cell></row><row><cell>EPC++ [38]</cell><cell>M</cell><cell>0.120</cell><cell>0.789 4.755</cell><cell>0.177</cell><cell>0.856</cell><cell>0.961</cell><cell>0.987</cell></row><row><cell>Monodepth2 w/o pretraining</cell><cell>M</cell><cell>0.112</cell><cell>0.715 4.502</cell><cell>0.167</cell><cell>0.876</cell><cell>0.967</cell><cell>0.990</cell></row><row><cell>Monodepth2</cell><cell>M</cell><cell>0.090</cell><cell>0.545 3.942</cell><cell>0.137</cell><cell>0.914</cell><cell>0.983</cell><cell>0.995</cell></row><row><cell>Monodepth [15]</cell><cell>S</cell><cell>0.109</cell><cell>0.811 4.568</cell><cell>0.166</cell><cell>0.877</cell><cell>0.967</cell><cell>0.988</cell></row><row><cell>3net [50] (VGG)</cell><cell>S</cell><cell>0.119</cell><cell>0.920 4.824</cell><cell>0.182</cell><cell>0.856</cell><cell>0.957</cell><cell>0.985</cell></row><row><cell>3net [50] (ResNet 50)</cell><cell>S</cell><cell>0.102</cell><cell>0.675 4.293</cell><cell>0.159</cell><cell>0.881</cell><cell>0.969</cell><cell>0.991</cell></row><row><cell>SuperDepth [47] + pp</cell><cell>S</cell><cell>0.090</cell><cell>0.542 3.967</cell><cell>0.144</cell><cell>0.901</cell><cell>0.976</cell><cell>0.993</cell></row><row><cell>Monodepth2 w/o pretraining</cell><cell>S</cell><cell>0.110</cell><cell>0.849 4.580</cell><cell>0.173</cell><cell>0.875</cell><cell>0.962</cell><cell>0.986</cell></row><row><cell>Monodepth2</cell><cell>S</cell><cell>0.085</cell><cell>0.537 3.868</cell><cell>0.139</cell><cell>0.912</cell><cell>0.979</cell><cell>0.993</cell></row><row><cell>Zhan FullNYU [73]</cell><cell>D*MS</cell><cell>0.130</cell><cell>1.520 5.184</cell><cell>0.205</cell><cell>0.859</cell><cell>0.955</cell><cell>0.981</cell></row><row><cell>EPC++ [38]</cell><cell>MS</cell><cell>0.123</cell><cell>0.754 4.453</cell><cell>0.172</cell><cell>0.863</cell><cell>0.964</cell><cell>0.989</cell></row><row><cell cols="2">Monodepth2 w/o pretraining MS</cell><cell>0.107</cell><cell>0.720 4.345</cell><cell>0.161</cell><cell>0.890</cell><cell>0.971</cell><cell>0.989</cell></row><row><cell>Monodepth2</cell><cell>MS</cell><cell>0.080</cell><cell>0.466 3.681</cell><cell>0.127</cell><cell>0.926</cell><cell>0.985</cell><cell>0.995</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>MethodœÉ scale Abs Rel Sq Rel RMSE RMSE log Œ¥ &lt; 1.25 Œ¥ &lt; 1.25 2 Œ¥ &lt; 1.25<ref type="bibr" target="#b2">3</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table 8. Single scale monocular</cell></row><row><cell cols="2">Zhou [76] ‚Ä† Mahjourian [40] 0.189 0.210 GeoNet [71] 0.172 Ranjan [51] 0.162</cell><cell>0.258 0.221 0.202 0.188</cell><cell cols="2">2.338 7.040 1.663 6.220 1.521 5.829 1.298 5.467</cell><cell>0.309 0.265 0.244 0.232</cell><cell>0.601 0.665 0.707 0.724</cell><cell>0.853 0.892 0.913 0.927</cell><cell>0.940 0.962 0.970 0.974</cell><cell>evaluation. ing monocular supervised methods on Comparison to exist-KITTI 2015 [13] using the Eigen split with improved ground truth from [59]</cell></row><row><cell>EPC++ [38]</cell><cell>0.123</cell><cell>0.153</cell><cell cols="2">0.998 5.080</cell><cell>0.204</cell><cell>0.805</cell><cell>0.945</cell><cell>0.982</cell><cell>using a single scale for each method.</cell></row><row><cell>DDVO [62]</cell><cell>0.108</cell><cell>0.147</cell><cell cols="2">1.014 5.183</cell><cell>0.204</cell><cell>0.808</cell><cell>0.946</cell><cell>0.983</cell><cell>‚Ä† indicates newer results from the on-</cell></row><row><cell>Monodepth2</cell><cell>0.093</cell><cell>0.109</cell><cell cols="2">0.623 4.136</cell><cell>0.154</cell><cell>0.873</cell><cell>0.977</cell><cell>0.994</cell><cell>line implementation.</cell></row><row><cell>Method</cell><cell cols="5">Train SILog sqErrorRel absErrorRel iRMSE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DORN [10]</cell><cell>D</cell><cell>11.77</cell><cell>2.23</cell><cell>8.78</cell><cell>12.98</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DABC [34]</cell><cell>D</cell><cell>14.49</cell><cell>4.08</cell><cell>12.72</cell><cell>15.53</cell><cell></cell><cell></cell><cell></cell></row><row><cell>APMoE [29]</cell><cell>D</cell><cell>14.74</cell><cell>3.88</cell><cell>11.74</cell><cell>15.63</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CSWS [32]</cell><cell>D</cell><cell>14.85</cell><cell>3.48</cell><cell>11.84</cell><cell>16.38</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DHGRL [74]</cell><cell>D</cell><cell>15.47</cell><cell>4.04</cell><cell>12.52</cell><cell>15.72</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Monodepth [15]</cell><cell>S</cell><cell>22.02</cell><cell>20.58</cell><cell>17.79</cell><cell>21.84</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Monodepth2</cell><cell>M</cell><cell>15.57</cell><cell>4.52</cell><cell>12.98</cell><cell>16.70</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Monodepth2</cell><cell cols="2">MS 15.07</cell><cell>4.16</cell><cell>11.64</cell><cell>15.27</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Monodepth2 (ResNet 50) MS 14.41</cell><cell>3.67</cell><cell>11.22</cell><cell>14.73</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc><ref type="bibr" target="#b14">15]</ref>, is a technique to improve test time results on stereo-trained monocular depth 1 https://github.com/mrharicot/monodepth</figDesc><table><row><cell>Input</cell><cell>Zhou et al. [76] DDVO [62]</cell><cell>MD2 M</cell><cell>Ground truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>MethodTrain Abs Rel Sq Rel RMSE RMSE log Œ¥ &lt; 1.25 Œ¥ &lt; 1.25 2 Œ¥ &lt; 1.25<ref type="bibr" target="#b2">3</ref> </figDesc><table><row><cell>Monodepth2 w/o pretraining</cell><cell>M</cell><cell>0.132</cell><cell>1.044</cell><cell>5.142</cell><cell>0.210</cell><cell>0.845</cell><cell>0.948</cell><cell>0.977</cell></row><row><cell>Monodepth2 w/o pretraining + pp</cell><cell>M</cell><cell>0.129</cell><cell>1.003</cell><cell>5.072</cell><cell>0.207</cell><cell>0.848</cell><cell>0.949</cell><cell>0.978</cell></row><row><cell>Monodepth2</cell><cell>M</cell><cell>0.115</cell><cell>0.903</cell><cell>4.863</cell><cell>0.193</cell><cell>0.877</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell>Monodepth2 + pp</cell><cell>M</cell><cell>0.112</cell><cell>0.851</cell><cell>4.754</cell><cell>0.190</cell><cell>0.881</cell><cell>0.960</cell><cell>0.981</cell></row><row><cell>Monodepth2 (1024 √ó 320)</cell><cell>M</cell><cell>0.115</cell><cell>0.882</cell><cell>4.701</cell><cell>0.190</cell><cell>0.879</cell><cell>0.961</cell><cell>0.982</cell></row><row><cell>Monodepth2 (1024 √ó 320) + pp</cell><cell>M</cell><cell>0.112</cell><cell>0.838</cell><cell>4.607</cell><cell>0.187</cell><cell>0.883</cell><cell>0.962</cell><cell>0.982</cell></row><row><cell>Monodepth2 w/o pretraining</cell><cell>S</cell><cell>0.130</cell><cell>1.144</cell><cell>5.485</cell><cell>0.232</cell><cell>0.831</cell><cell>0.932</cell><cell>0.968</cell></row><row><cell>Monodepth2 w/o pretraining + pp</cell><cell>S</cell><cell>0.128</cell><cell>1.089</cell><cell>5.385</cell><cell>0.229</cell><cell>0.832</cell><cell>0.934</cell><cell>0.969</cell></row><row><cell>Monodepth2</cell><cell>S</cell><cell>0.109</cell><cell>0.873</cell><cell>4.960</cell><cell>0.209</cell><cell>0.864</cell><cell>0.948</cell><cell>0.975</cell></row><row><cell>Monodepth2 + pp</cell><cell>S</cell><cell>0.108</cell><cell>0.842</cell><cell>4.891</cell><cell>0.207</cell><cell>0.866</cell><cell>0.949</cell><cell>0.976</cell></row><row><cell>Monodepth2 (1024 √ó 320)</cell><cell>S</cell><cell>0.107</cell><cell>0.849</cell><cell>4.764</cell><cell>0.201</cell><cell>0.874</cell><cell>0.953</cell><cell>0.977</cell></row><row><cell>Monodepth2 (1024 √ó 320) + pp</cell><cell>S</cell><cell>0.105</cell><cell>0.822</cell><cell>4.692</cell><cell>0.199</cell><cell>0.876</cell><cell>0.954</cell><cell>0.977</cell></row><row><cell>Monodepth2 w/o pretraining</cell><cell>MS</cell><cell>0.127</cell><cell>1.031</cell><cell>5.266</cell><cell>0.221</cell><cell>0.836</cell><cell>0.943</cell><cell>0.974</cell></row><row><cell cols="2">Monodepth2 w/o pretraining + pp MS</cell><cell>0.125</cell><cell>1.000</cell><cell>5.205</cell><cell>0.218</cell><cell>0.837</cell><cell>0.944</cell><cell>0.974</cell></row><row><cell>Monodepth2</cell><cell>MS</cell><cell>0.106</cell><cell>0.818</cell><cell>4.750</cell><cell>0.196</cell><cell>0.874</cell><cell>0.957</cell><cell>0.979</cell></row><row><cell>Monodepth2 + pp</cell><cell>MS</cell><cell>0.104</cell><cell>0.786</cell><cell>4.687</cell><cell>0.194</cell><cell>0.876</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell>Monodepth2 (1024 √ó 320)</cell><cell>MS</cell><cell>0.106</cell><cell>0.806</cell><cell>4.630</cell><cell>0.193</cell><cell>0.876</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell>Monodepth2 (1024 √ó 320) + pp</cell><cell>MS</cell><cell>0.104</cell><cell>0.775</cell><cell>4.562</cell><cell>0.191</cell><cell>0.878</cell><cell>0.959</cell><cell>0.981</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Pose network architecture Input frames Pretrained Abs Rel Sq Rel RMSE RMSE log Œ¥ &lt;1.25 Œ¥ &lt;1.25 2 Œ¥ &lt; 1.25 3</figDesc><table><row><cell>PoseCNN [62]</cell><cell>2</cell><cell>0.138</cell><cell>1.122 5.308 0.209</cell><cell>0.840</cell><cell>0.950</cell><cell>0.978</cell></row><row><cell>PoseCNN [62]</cell><cell>3</cell><cell>0.148</cell><cell>1.211 5.595 0.219</cell><cell>0.815</cell><cell>0.942</cell><cell>0.976</cell></row><row><cell>Shared encoder (arXiv v1)</cell><cell>2</cell><cell>0.125</cell><cell>0.986 5.070 0.201</cell><cell>0.857</cell><cell>0.954</cell><cell>0.979</cell></row><row><cell>Shared encoder (arXiv v1)</cell><cell>3</cell><cell>0.123</cell><cell>1.031 5.052 0.199</cell><cell>0.863</cell><cell>0.954</cell><cell>0.979</cell></row><row><cell>Monodepth2 ‚áí Separate ResNet</cell><cell>2</cell><cell>0.115</cell><cell>0.919 4.863 0.193</cell><cell>0.877</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell>Separate ResNet</cell><cell>3</cell><cell>0.115</cell><cell>0.902 4.847 0.193</cell><cell>0.877</cell><cell>0.960</cell><cell>0.981</cell></row><row><cell>PoseCNN [62]</cell><cell>2</cell><cell>0.147</cell><cell>1.164 5.445 0.221</cell><cell>0.818</cell><cell>0.940</cell><cell>0.974</cell></row><row><cell>PoseCNN [62]</cell><cell>3</cell><cell>0.147</cell><cell>1.117 5.403 0.222</cell><cell>0.815</cell><cell>0.940</cell><cell>0.976</cell></row><row><cell>Shared encoder (arXiv v1)</cell><cell>2</cell><cell>0.149</cell><cell>1.153 5.567 0.229</cell><cell>0.807</cell><cell>0.934</cell><cell>0.972</cell></row><row><cell>Shared encoder (arXiv v1)</cell><cell>3</cell><cell>0.145</cell><cell>1.159 5.482 0.224</cell><cell>0.818</cell><cell>0.937</cell><cell>0.973</cell></row><row><cell>Monodepth2 ‚áí Separate ResNet</cell><cell>2</cell><cell>0.132</cell><cell>1.044 5.142 0.210</cell><cell>0.845</cell><cell>0.948</cell><cell>0.977</cell></row><row><cell>Separate ResNet</cell><cell>3</cell><cell>0.132</cell><cell>1.017 5.169 0.211</cell><cell>0.842</cell><cell>0.947</cell><cell>0.977</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements Thanks to the authors who shared their results, and Peter Hedman, Daniyar Turmukhambetov, and Aron Monszpart for their helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Abarghouei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Undemon: Unsupervised deep network for depth and ego-motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>V Madhu Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swagat</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Se3-nets: Learning rigid body motion using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arunkumar</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Singleimage depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn√©</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-view stereo: A tutorial. Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Hern√°ndez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl√©ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Familiar size and the perception of depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Barnes Hochberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">E</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Psychology</title>
		<imprint>
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatic photo pop-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FlowNet2: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>G√ºney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised relative depth learning for urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">KITTI Single Depth Evaluation Server</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Supervising the new with the old: learning SFM from SFM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Klodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pixel-wise attentional gating for parsimonious pixel labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√∂rg</forename><surname>St√ºckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with hierarchical fusion of dilated cnns and softweighted-sum inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">UnDeepVO: Monocular visual odometry through unsupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbing</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep attention-based classification network for robust depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Hang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Megadepth: Learning singleview depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Every pixel counts++: Joint learning of geometry and motion with 3D holistic understanding. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Single view stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3D geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">What makes good synthetic training data for learning disparity and optical flow estimation? IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H√§usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Structured adversarial training for unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishit</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parikshit</forename><surname>Sakurikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ORB-SLAM: a versatile and accurate monocular SLAM system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Maria Martinez</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">AdaDepth: Unsupervised content congruent adaptation for depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phani</forename><forename type="middle">Krishna</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Superdepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unsupervised adversarial depth estimation using cycled generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Mihai Marian Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards real-time unsupervised monocular depth estimation on cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning monocular depth estimation with unsupervised trinocular assumptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">Sparsity invariant CNNs. In 3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">DeMoN: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<title level="m">SfM-Net: Learning of structure and motion from video. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Miguel</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">Rahim</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turmukhambetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Size-to-depth: A new perspective for single image depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√∂rg</forename><surname>St√ºckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">LEGO: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jure≈æbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deep hierarchical guidance and regularization learning for end-to-end depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Computational Imaging</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning ordinal relationships for mid-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
