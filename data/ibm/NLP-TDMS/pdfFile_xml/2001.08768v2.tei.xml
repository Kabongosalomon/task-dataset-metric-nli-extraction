<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cloud and Cloud Shadow Segmentation for Remote Sensing Imagery via Filtered Jaccard Loss Function and Parametric Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Sorour</forename><surname>Mohajerani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Parvaneh</forename><surname>Saeedi</surname></persName>
						</author>
						<title level="a" type="main">Cloud and Cloud Shadow Segmentation for Remote Sensing Imagery via Filtered Jaccard Loss Function and Parametric Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Cloud detection</term>
					<term>CNN</term>
					<term>image segmentation</term>
					<term>deep learning</term>
					<term>Landsat 8</term>
					<term>loss function</term>
					<term>remote sensing</term>
					<term>38-Cloud</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cloud and cloud shadow segmentation are fundamental processes in optical remote sensing image analysis. Current methods for cloud/shadow identification in geospatial imagery are not as accurate as they should, especially in the presence of snow and haze. This paper presents a deep learningbased framework for the detection of cloud/shadow in Landsat 8 images. Our method benefits from a convolutional neural network, Cloud-Net+ (a modification of our previously proposed Cloud-Net [1]) that is trained with a novel loss function (Filtered Jaccard Loss). The proposed loss function is more sensitive to the absence of foreground objects in an image and penalizes/rewards the predicted mask more accurately than other common loss functions. In addition, a sunlight direction-aware data augmentation technique is developed for the task of cloud shadow detection to extend the generalization ability of the proposed model by expanding existing training sets. The combination of Cloud-Net+, Filtered Jaccard Loss function, and the proposed augmentation algorithm delivers superior results on four public cloud/shadow detection datasets. Our experiments on Pascal VOC dataset exemplifies the applicability and quality of our proposed network and loss function in other computer vision applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C LOUD and cloud shadow detection, along with cloud coverage estimation, are among the most critical processes in the analysis of remote sensing imagery. On the one hand, transferring remotely sensed data from air/space-borne sensors to ground stations is an expensive process from time, bandwidth, storage, and computational points of view. On the other hand, no useful information about the Earth's surface can be extracted from optical images that are heavily covered by clouds and their shadows. Since, on average, 67% of the Earth surface is covered by clouds at any given time <ref type="bibr" target="#b1">[2]</ref>, it seems that a considerable amount of resources can be saved by transferring only images with no/minimum amount of cloud and shadow coverage.</p><p>Cloud coverage by itself could provide useful information about climate and atmospheric parameters <ref type="bibr" target="#b2">[3]</ref>, as well as natural disasters such as hurricanes and volcanic eruptions <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. As a result, the identification of clouds and cloud shadows in images is an essential pre-processing task for many applications. Cloud and cloud shadow detection is even more challenging when only a limited number of spectral bands are available. Many air/spaceborne systems such as ZY-3, HJ-1, S. Mohajerani and P. Saeedi are with the School of Engineering Science, Simon Fraser University, BC, Canada (e-mail: smohajer@sfu.ca, psaeedi@sfu.ca). and GF-2 are equipped only with visible and near-infrared bands <ref type="bibr" target="#b5">[6]</ref>. Therefore, algorithms that can identify clouds and their shadows from those few spectral bands become more essential.</p><p>In recent years, many cloud/shadow detection algorithms have been developed. These methods can be divided into three main categories: threshold-based <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>, handcrafted <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>, and deep learning-based methods <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b19">[20]</ref>.</p><p>Function of mask (FMask) <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref> and automated cloudcover assessment (ACCA) <ref type="bibr" target="#b23">[24]</ref> algorithms are among the most well-known threshold-based algorithms for cloud identification. They construct a decision tree to label each pixel as cloud or other non-cloud classes. In each branch of the tree, a decision is made based on a thresholding function that utilizes one or more spectral bands of the data.</p><p>A group of handcrafted methods isolated haze and thick clouds from other pixels using the relationship between spectral responses of the red and blue bands. Some examples of these algorithms include <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, which are derivates of the Haze Optimized Transformation (HOT) <ref type="bibr" target="#b10">[11]</ref>. Xu et al. <ref type="bibr" target="#b26">[27]</ref>, proposed a handcrafted approach, which involved a Bayesian probabilistic model incorporated with multiple spectral, temporal, and spatial features to separate cloud from non-cloud regions.</p><p>With recent advances in deep-learning algorithms for image segmentation, several methods have been developed to detect cloud/shadow using deep-learning. Xie et al. <ref type="bibr" target="#b18">[19]</ref> trained a convolutional neural network (CNN) from multiple small patches. Their network classified each patch into one of the three classes of thin cloud, thick cloud, and clear. A major issue in cloud/shadow detection approaches using deeplearning is the lack of accurately annotated training images since creating ground truths (GTs) for remote sensing imagery is time-consuming and tedious <ref type="bibr" target="#b27">[28]</ref>. In addition, default cloud masks provided in remote sensing products are mostly obtained through automatic/semi-automatic thresholding-based approaches, which often make them less accurate. Authors in <ref type="bibr" target="#b28">[29]</ref> have removed wrongly labeled icy/snowy regions in default cloud masks in their dataset. They showed that the performance of a well-known semantic segmentation model (U-Net <ref type="bibr" target="#b29">[30]</ref>) is considerably improved by training on those corrected GTs. Although existing methods deliver promising results, many of them do not provide robust and accurate cloud/shadow masks in scenes where bright/cold non-cloud regions exist alongside clouds <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>Here, inspired by advances made in deep learning techniques, we propose a new algorithm to identify cloud/shadow regions in Landsat 8 images. Our proposed algorithm consists of a fully convolutional neural network (FCN), which detects cloud and cloud shadow pixels in an end-to-end manner. Our network, Cloud-Net+, is a modified version of our previously open-sourced model, Cloud-Net <ref type="bibr" target="#b0">[1]</ref>. In the process of optimizing Cloud-Net+ model, a novel loss function named Filtered Jaccard Loss (FJL) is developed and used to calculate the error. As a result, Cloud-Net+ is trained more accurately. FJL makes a considerable difference in the performance of systems, especially for images with no cloud/shadow regions.</p><p>Many of the threshold-based and handcrafted algorithms for cloud shadow detection utilize geometrical relationships between the illumination source, clouds, and shadows <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. However, such relationships are not taken advantage of in deep learning-based approaches, where shadows are considered independent foreground objects. We incorporate this information through a meaningful parametric data augmentation approach to have a greater variety of shadows in each scene. As a result, newly generated images resemble original images as if they were captured at different times of the day with different sunlight directions. Our experiments show that such systematic augmentation works very well for shadow detection and simultaneous multiclass segmentation of clouds, shadows, and clear areas.</p><p>Unlike FMask and ACCA, the proposed approach is not blind to the existing global and local cloud/shadow features of an image. Also, since only four spectral bands-red, green, blue, and near-infrared (RGBNir)-are required for the system's training and prediction, the proposed method can be easily utilized for images obtained by most of the existing satellites as well as airborne systems. Unlike multitemporalbased methods such as <ref type="bibr" target="#b32">[33]</ref>, the proposed method does not require prior knowledge of the scene, e.g., cloud-free images. The only data required for the parametric augmentation are solar angles, which exist in the metadata accompanying each image. Moreover, such information is used for training purposes only and not for prediction. In addition, to being simple and straightforward, the proposed network can be used in other image segmentation applications.</p><p>In summary, the contributions of this work are as follows:</p><p>• Proposing a novel loss function (FJL), which not only penalizes a model for poor prediction of clouds and shadows but also fairly rewards the correct prediction of This new dataset-which is made public-includes 57 more Landsat 8 scenes (than 38-Cloud) along with their manually annotated GTs. It will help researchers improve their cloud detection algorithms using more training and evaluation data. The remainder of this paper is organized as follows: in Section II, a summary of related works in the cloud detection field is reviewed. In Section III, our proposed method is explained. In Section IV, experimental results are discussed. Finally, Section V summarizes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Cloud and shadow detection in remotely sensed images have been an active area of research for many years. One of the first attempts to distinguish between clouds and cloud-free areas was through a probabilistic classifier model <ref type="bibr" target="#b33">[34]</ref>. However, it was limited to scenes with clouds over open sea surfaces. One of the first successful and more general automatic cloud detection methods was Fmask (first version) <ref type="bibr" target="#b20">[21]</ref>. Fmask used seven bands of Landsat images to classify each pixel of an image into one of the five classes of land, water, cloud, shadow, and snow. Another version of it (Fmask V3) <ref type="bibr" target="#b21">[22]</ref> utilized cirrus band to distinguish cirrus clouds along with low altitude clouds. In the last version of it (Fmask V4) <ref type="bibr" target="#b22">[23]</ref>, auxiliary data such as digital elevation maps (DEM), DEM derivatives, and global surface water occurrences (GSWO) were used in addition to the other usual bands for better performance on water, high altitude regions, and Sentinel-2 images.</p><p>Several multitemporal methods were developed for cloud/shadow detection <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref>. Mateo-García et al. <ref type="bibr" target="#b32">[33]</ref> used cloud-free images of Landsat 8 scenes to identify potential clouds. A clustering and some threshold-based postprocessing steps then helped to generate final cloud masks. Zi et al. <ref type="bibr" target="#b37">[38]</ref> combined a threshold-based method with a classical machine learning approach to segment superpixels and classify them into one of three classes of cloud, potential cloud, and non-cloud.</p><p>Recently, several FCNs have been developed for cloud and cloud shadow detection tasks <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b42">[43]</ref>. Yang et al. <ref type="bibr" target="#b43">[44]</ref> proposed an FCN, which detects clouds in ZY-3 thumbnail satellite images. A built-in boundary refinement approach was incorporated into their network (CDnetV1) to avoid further post-processing. The second version of that network, CDnetV2 <ref type="bibr" target="#b44">[45]</ref>, was equipped with two new feature fusion and information guidance modules to extract cloud features more accurately. Chai et al. <ref type="bibr" target="#b45">[46]</ref> proposed Segnet Adaption (a modified version of a well-known FCN for semantic segmentation) to match with remotely sensed images. Segnet Adaption has shown promising results on Biome 8 and Landsat 7 cloud/shadow detection datasets. Recently, Jeppesen et al. <ref type="bibr" target="#b46">[47]</ref> introduced RS-Net to identify clouds in Landsat 8 images. RS-Net was an FCN inspired by U-Net and was trained with both automatically (via Fmask) and manually generated GT images of two public datasets. The authors showed that results obtained by a model trained with Fmask's outputs outperform the Fmask directly obtained results.</p><p>The authors of SPARCS CNN <ref type="bibr" target="#b47">[48]</ref> also proposed a model to distinguish cloud, shadow, water, snow, and clear regions in Landsat 8 images. They used a pretrained VGG16 as the backbone of their fully convolutional network. They succeeded in reaching human interpreter accuracy with their model. Re-fUnet v1 <ref type="bibr" target="#b48">[49]</ref> was another method which focuses on retrieving fuzzy boundaries of clouds and shadows. The authors used a UNet for extracting course/rough cloud and cloud shadow in small patches of images. Then, boundaries of clouds/shadows in complete Landsat 8 masks were refined/sharpened using a dense conditional random field (CRF). In this method, the CRF refinement was applied as a post-processing step. In the second version of RefUNet (RefUNet v2) <ref type="bibr" target="#b49">[50]</ref> a simultaneous joint pipeline for detecting and refining edges was utilized. Therefore, the UNet network, which identified course masks, was concatenated with the proposed Guided Gaussian filterbased CRF to refine boundaries in an end-to-end manner. In addition, refinements were done on small patch masks rather than large masks of the entire Landsat 8 scenes. In another work, the authors of Cloud-AttU <ref type="bibr" target="#b50">[51]</ref> employed a UNet model that is enriched with a specific attention module in its skip connections. Multiple attention modules enabled the model to learn proper features by paying attention to the most relevant locations in input training images or feature maps. CloudFCN <ref type="bibr" target="#b51">[52]</ref> was based on a UNet model with the addition of inception modules between convolution blocks in the contracting and expanding arms. In addition to pixel-level segmentation, the authors incorporated a feature into their algorithm for estimating cloud coverage in an image using a regression-friendly loss function. They demonstrated that CloudFCN could achieve high accuracy in the task of cloud screening under various conditions, such as the presence of white noise and using different quantization methods.</p><p>Image augmentation has been proven to be effective in increasing the accuracy of deep learning models. There are only a few offline augmentation algorithms reported for remote sensing imagery. Ma et al. <ref type="bibr" target="#b52">[53]</ref> proposed a generative adversarial network (GAN) to synthesize images for scene labeling. Howe et al. <ref type="bibr" target="#b53">[54]</ref> introduced another GAN-based approach for Earth's surface object detection in airborne images. Zheng et al. <ref type="bibr" target="#b54">[55]</ref> proposed a method for generating synthetic vehicles in aerial images. For the specific task of cloud/shadow segmentation, only basic geometric and color space transformations in the training phase are used for data augmentation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b55">[56]</ref>. Authors of <ref type="bibr" target="#b56">[57]</ref>, however, developed a GAN (CloudMaskGAN) to convert snowy scenes to nonsnowy and vice versa for augmentation of Landsat 8 images.</p><p>To the best of our knowledge, CloudMaskGAN is the only reported non-transformation-based augmentation approach for cloud detection purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, the proposed methodology for cloud/shadow detection is described. We focus on using Landsat 8 images. Landsat 8 is equipped with two optical sensors, which together collect eight spectral and two thermal bands. Only four spectral bands-Bands 2 to 5 (RGBNir)-are used (all with 30m spatial resolution). To keep the proposed method more general, no thermal band is utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture of the Segmentation Model</head><p>Similar to other FCNs, the proposed network is made of two main arms: a contracting arm, and an expanding arm. In the training phase, the contracting arm extracts important high-level cloud/shadow attributes. It also downsamples the input while increasing its depth. On the other hand, the expanding arm utilizes those extracted features to build an image mask-with the same size as the input. The network's input is a multi-spectral image, and its output is a grayscale cloud/shadow probability map for the input. In the case of multiclass segmentation, the output has multiple channels, one corresponding to each class.</p><p>We modified the network's architecture in our previous work <ref type="bibr" target="#b0">[1]</ref> to develop a more efficient model that is more sensitive to clouds/shadows. Cloud-Net+, consists of six contracting and five expanding blocks (see <ref type="figure" target="#fig_0">Fig.1</ref>). Successive convolutional layers are the heart of the blocks in both arms. The kernel size and the order of these layers play crucial roles in the quality of activated features, and therefore, they affect the final segmentation outcome directly. On the one hand, it seems that as the number of convolutional layers in each block increases, the distinction of the captured context improves. On the other hand, utilizing more of such layers explodes the complexity of the model. To address such trade-off, we remove the middle 3 × 3 convolution layer in the last two contracting blocks and the first expanding block of Cloud-Net. Since those layers are dense and contain thousands of parameters, such removal decreases the number of network's parameters significantly. Then, in all contracting arm blocks, a 1×1 convolution layer is added between each two adjacent 3×3 convolution layers. Since each 1×1 convolution layer contains a small number of parameters, the total number of parameters of Cloud-Net+ (32.9M) is 10% less than that of Cloud-Net (36.4M). Utilizing the 1 × 1 kernel size in convolutional layers was suggested initially in <ref type="bibr" target="#b57">[58]</ref>, and its effectiveness is proven in works such as <ref type="bibr" target="#b58">[59]</ref>. Employing such a kernel in expanding blocks does not yield a better recovery of the low-resolution feature maps. Therefore, we do not add it to the expanding blocks. Instead, an aggregation branch (AB) is added to combine all feature maps of the expanding blocks. AB consists of six up-sampling layers (bilinear interpolation) followed by a 1×1 convolution. It helps our model to retrieve cloud/shadow boundaries in the generated masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss Function</head><p>Soft Jaccard/Dice loss function has been widely used to optimize many image segmentation models <ref type="bibr" target="#b59">[60]</ref>- <ref type="bibr" target="#b61">[62]</ref>. The formulation of soft Jaccard loss for two classes of "0" and "1" is as follows:</p><formula xml:id="formula_0">JL(t, y) = 1− N i=1 tiyi + N i=1 ti + N i=1 yi − N i=1 tiyi + ,<label>(1)</label></formula><p>Here t represents the GT, and y is the output of the network. N denotes the total number of pixels in t. y i ∈ [0, 1] and t i ∈ {0, 1} are the ith pixel value of y and t, respectively, and is 10 −7 to avoid division by zero. Soft Jaccard loss function, however, has a defect: over-penalization of images with no class "1" in their GTs. It is clear that y 1 would be a better prediction than y 2 since it can be interpreted as having no class "1" in the input image. However, soft Jaccard losses obtained by y 1 and y 2 are the same: J L (t 0 , y 1 ) = J L (t 0 , y 2 ) ≈ 1. Consequently, the network penalizes y 1 as much as y 2 , even though y 1 represents a better prediction. Indeed, the major problem with soft Jaccard loss function is that whenever there is no class "1" in the GT, the numerator of Eq. (1) equals to (which is a small number) and, as a result, the value of the loss approximates 1.</p><p>Observing this behavior, we propose a modified soft Jaccard loss function, FJL, with two different versions. The main idea behind FJL is to compensate for unfair values of soft Jaccard loss and replace them with proper values whenever there is no class "1" in the GT. We can summarize the goal of the FJL as follows:</p><formula xml:id="formula_1">FJL(t, y) = GL(t, y), ti = 0, ∀i ∈ {1,2,3, ...,N } JL(t, y), Otherwise<label>(2)</label></formula><p>where G L represents a compensatory function. The condition in the first line of Eq. (2) indicates that when all pixels of GT are equal to zero, FJL is similar G L . Clearly, this condition can be rephrased as follows:</p><formula xml:id="formula_2">FJL(t, y) = GL(t, y), S = 0 JL(t, y), S &gt; 0<label>(3)</label></formula><p>where S = N i=1 t i . This formulation can be rewritten-in a more general form-as the combination of two functions of J L and G L , in which each function is multiplied by ideal highpass and lowpass filters, respectively:</p><formula xml:id="formula_3">FJL(t, y) = kGGL(t, y)LPp c (S) + kJ JL(t, y)HP p c (S) (4)</formula><p>Here, LP pc denotes a lowpass filter with the cut-off point of p c and HP p c denotes a highpass filter with the cut-off point of p c . k G and k J represent coefficients of compensatory and Jaccard losses, respectively. The magnitude of both filters is limited in the [0,1] range. In Eq. (4), the value of LP pc is 0 when S &gt; p c , so the value of G L (t, y)LP pc becomes zero, and as a result, FJL only has contributions from J L . On the other hand, when S &lt; p c , HP p c becomes 0, and FJL is only from the G L part. This behavior can be simply described as a toggle switch between J L and G L .</p><p>To satisfy Eq. <ref type="formula" target="#formula_2">(3)</ref>, the cut-off point of the two ideal filters should be equal, so they become complementary. Note that, in signal processing context, the cut-off usually refers to the "frequency" at which the magnitude of a filter changes. However, in this work, cut-off is the "value" of S at which the magnitude of a filter alters. Since this transition should occur when S = 0, the cut-off points are set to 0.</p><p>To have lowpass and highpass filters with smooth gradient characteristics, inspiring by <ref type="bibr" target="#b62">[63]</ref>, we have used the sigmoid function as follows:</p><formula xml:id="formula_4">LPp c (S) = 1 1+exp m(S −pc) , HP p c (S) = 1 1+exp m(−S +p c )<label>(5)</label></formula><p>where m denotes the steepness of the sigmoid transition.</p><p>Choosing sigmoid solves the problem of over-penalizing without adding any non-differentiable elements or piecewise conditions to the loss function. This leads to a smooth switch between soft Jaccard and compensatory loss in FJL. Since HP and LP filters are not functions of y, the gradient of FJL is still continuous (without any jumps). By substituting Eq. <ref type="bibr" target="#b4">(5)</ref> in Eq. (4), FJL is described as:</p><formula xml:id="formula_5">FJL(t, y) = kGGL(t, y) 1+exp m(S −pc) + kJ JL(t, y) 1+exp m(−S +p c )<label>(6)</label></formula><p>To keep these filters close to ideal, m is required to be a large number. We have set m to 1000 in our experiments to have fast transitions from 0 to 1 and vice versa. In addition, p c and p c are set to 0.5 to leave a safety margin and ensure that LP 0.5 (S = 0)= 1 and HP 0.5 (S = 0)= 0. Also, by setting k G = k J = 1, the magnitude of the loss remains in [0, 1] range.</p><p>We can have different versions of FJL, by selecting different functions as G L . Here, we investigate two different candidates, G L1 and G L2 , to form two corresponding FJL versions, FJL 1 and FJL 2 . Our first candidate is the inverted Jaccard function, which is calculated by the complements of GT and prediction arrays. The second one is the normalized version of a common loss function in segmentation tasks, Cross Entropy (CE). The formulation of these candidates are as follows:</p><formula xml:id="formula_6">G L1 (t, y) ≡ InvJ L (t, y) = J L (t, y),<label>(7)</label></formula><formula xml:id="formula_7">G L2 (t, y) ≡ CE norm (t, y)= CE(t, y) M ax = −1 N N i=1 t i log(y i + ) M ax ,</formula><p>where t and y denote the complements of t and y, respectively. CE norm denotes normalized CE. To make sure that the range of FJL 2 (t, y) is bounded to [0, 1], CE values are normalized by division by their maximum possible value, M ax = −N N log( ) = 16.1180. M ax is obtained when all pixels in the predicted array are complements of GT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parametric Augmentation (SDAA)</head><p>When an image is captured by Landsat 8, the solar azimuth (θ A ) and the zenith (θ Z ) angles at the time of acquisition are recorded in a metadata file. We use these angles to generate different synthetic images by hypothetically changing the sun's location in the sky and, therefore, creating shadows under different illumination directions. This could be interpreted as capturing the same scene at different times of the day. Each image can be augmented unless it is free of shadow.</p><p>The main steps of the proposed SDAA are as follows: 1) removing original shadows, 2) changing the direction of illumination, 3) generating the locations of synthetic shadows by projecting clouds using the synthetic illumination direction of step 2, 4) recoloring synthetic shadow locations to generate different levels of shades via gamma transformation. Details of these steps are explained next.</p><p>First, a Landsat 8 image (from the training set), its shadow GT, and its cloud GT are selected. To generate realistic images, original shadows should be removed from the image, otherwise, the synthetic image will have two cast shadows. For the shadow removal step, a histogram matching between shadow regions and their shadow-free neighborhood is performed. The result of this step is a shadow-free image. To create new shadows in the shadow-free image, we generate synthetic solar angles by adding offsets to the original solar angles of θ A and θ Z . Using these new angles, the direction of synthetic sunlight (in form of a 2D vector projected on the scene's image plane) is calculated. By moving cloud pixels on the image plane and in the direction of the new sunlight, synthetic shadows of those clouds are generated. We made the following assumptions to simplify this process: all clouds have the same height, and the scene is located on a flat plane. The following equations are used to obtain the new locations of the cast shadows:</p><formula xml:id="formula_8">y sh = y cl +r sin(θZ + θZ O ) cos(θA + θA O ), x sh = x cl +r sin(θZ + θZ O ) sin(θA + θA O ),<label>(8)</label></formula><p>where y cl , y sh , x cl , and x sh denote the location of a cloud pixel and its corresponding cast shadow along the y and x axes. θ Z O and θ A O represent solar angle offsets. r (in pixels) is the shifting factor, which defines the length of a cast shadow.</p><p>The greater the r is set, the more a piece of cloud is shifted, and therefore, the longer its shadow becomes. Since both r and θ Z could affect the length of the cast shadow, instead of altering both, we keep θ Z intact and only alter r. Therefore, θ Z O is set to 0.</p><p>The resultant synthetic shadow mask (SSM) will be used as the shadow GT of the augmented image in the training phase. In the next step, SSM is used to adjust the brightness of the synthetic shadow regions in the augmented image using gamma (γ) transformation (i = i γ ). Pixel values of shadowfree regions in the image-including cloudy and clear areasare not modified.</p><p>Multiple </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETTINGS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Details</head><p>The size of the input data for Cloud-Net+ is 192×192×4. Four spectral bands of each patch are stacked up in the following order: R, G, B, Nir, to create such input. Before training, input patches are normalized through division by 65535. 20% of training data is used for validation during the training.</p><p>Input patches are randomly augmented using simple online geometric translations such as horizontal flipping, rotation, and zooming. Note that to make the proposed loss functions compatible with multiclass segmentation experiments, in each iteration of the training, loss values obtained for each class are averaged (with weights proportional to the inverse number of pixels in each class). The activation function in the last convolution layer of the network is a sigmoid. In the case of multiclass segmentation, softmax function is used in the last layer. Adam method <ref type="bibr" target="#b63">[64]</ref> is utilized as the optimizer.</p><p>The initial weights of the network are obtained by a Xavier uniform random initializer <ref type="bibr" target="#b64">[65]</ref>. The initial learning rate for the model's training is set to 10 −4 . If the validation loss does not drop for more than 15 successive epochs, the learning rate is reduced by 70%. This policy is continued until the learning rate reaches 10 −8 . The batch size is set to 12. The proposed network is developed using Keras deep learning framework with a single GPU. All Landsat 8 scenes have black (empty) regions around them. These regions are created when the acquired images are aligned to have the geodetic north at their top. We eliminated training patches with more than 80% empty pixels. The number of non-empty training patches decreases to 5155, resulting in a significant decrease in training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>2) 95-Cloud Dataset: To improve the generalization ability of deep neural networks trained on 38-Cloud dataset, we have extended it by adding 57 new Landsat 8 scenes to the training scenes of 38-Cloud dataset. Therefore, in total, the new training set consists of 75 scenes. 38-Cloud test set has been kept intact in 95-Cloud dataset for evaluation consistency. The GTs for the new scenes are manually extracted. Different images in 95-Cloud are selected to include various land cover types such as soil, vegetation, urban areas, snow, ice, water, haze, and different cloud patterns. The average cloud coverage percentage in 95-Cloud dataset images is kept around 50%. Following the same pattern as in 38-Cloud dataset, the total number of patches for training is 34701 and for the test is 9201. Removing empty patches from 95-Cloud training set reduces the number of patches to 21502. We made this dataset and parts of the code publicly available to the community at https://github.com/SorourMo/95-Cloud-An-Extension-to-38-Cloud-Dataset.</p><p>3) Biome 8 Dataset: Biome 8 dataset <ref type="bibr" target="#b65">[66]</ref> is a publicly available dataset consisting of 96 Landsat 8 scenes with their manually generated GTs with five classes of cloud, thin cloud, clear, cloud shadow, and empty. We generated a binary cloud GT out of Biome 8 GTs by merging both thin cloud and cloud classes into one cloud class. We marked the rest of the classes as clear. For the shadow segmentation task, all classes, except shadow, are combined under a clear class. Following the same pattern from 38-Cloud and 95-Cloud datasets, the total number of cropped patches extracted from the entire Biome 8 dataset is 44327. Removing empty patches reduces this number to 27358. 4) SPARCS Dataset: SPARCS dataset <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b66">[67]</ref> consists of 80 patches of 1000×1000 extracted from complete Landsat 8 scenes. The GTs of these patches are manually generated. Each pixel is classified into one of the cloud, shadow, snow/ice, water, land, and flooded classes. For the binary segmentation of clouds/shadows, we have combined all non-cloud and nonshadow classes under the clear class. In multiclass segmentation, three classes of cloud, shadow, and clear have been kept. Following the same pattern as previous datasets, the number of extracted NOL patches for this dataset is 720.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Evaluation</head><p>To evaluate the performance of the proposed algorithm, various experiments are performed. For 38-Cloud and 95-Cloud datasets, our model is trained and tested on each of those datasets' explicitly defined training set and testing set, respectively. For SPARCS dataset, as suggested in <ref type="bibr" target="#b46">[47]</ref>, we randomly extract five folds of images (each fold consists of 16 complete images or 144 NOL patches). Folds 2 to 5 are considered for training and fold 1 for testing in our experiments. We have also conducted 5-fold cross-validation (5CV) over the extracted folds to compare obtained results with those from RS-Net <ref type="bibr" target="#b46">[47]</ref>.</p><p>For Biome 8 dataset, we follow the instructions described in <ref type="bibr" target="#b46">[47]</ref> and extract two folds from Biome 8 dataset. We randomly select two cloudy, two mid-cloud, and two clear scenes for each biome category. Therefore, 48 scenes are extracted for fold 1 and 48 scenes for fold 2. In our experiments, fold 2 is used for training and fold 1 for testing. To compare against RS-Net, 2-fold cross-validation (2CV) is conducted, and the obtained numerical results are averaged.</p><p>Evaluation of the SDAA method requires different arrangements. For SPARCS, multiple sets of augmented training images are generated using various combinations of hyperparameters described in section III-C. NOL patches of 384×384 pixels are extracted from those images and are added to the original training patches of SPARCS dataset one set at a time. Then, to find the best combination of hyperparameters, shadow detection training and testing are performed for each of those expanded training sets. The numerical results of these experiments are calculated and compared against those obtained from training with original patches only. Those combinations of hyperparameters that resulted in a higher Jaccard index than the training with original patches are identified <ref type="bibr">(11 combinations)</ref>. SDAA patches are generated by those combinations and added all together to the original training patches of a dataset for a final evaluation.</p><p>Considering SPARCS's folds 2 to 5 as the training set, we use the best hyperparameters confirmed in the previous steps to generate SDAA patches, leading to 6812 training patches in total. For Biome 8 dataset, we consider 70% of the patches (extracted from 32 scenes with shadow) for training and the rest for testing to have a fair comparison against Segnet Adaption <ref type="bibr" target="#b45">[46]</ref>. Generating SDAA patches for this training set and then adding them to the original training patches results in 17155 patches in total.</p><p>After training the proposed model with training patches of the above-mentioned datasets, the obtained weights are saved and used for the evaluation of the model by prediction over unseen test scenes. Test patches of each scene are resized to 384×384×4 and fed to the model. Then, the cloud/shadow probability map corresponding to each patch is generated. Next, probability maps (grayscale images) are stitched together to build a prediction map for a complete scene. A 50% threshold is used to get a final binary mask. In multiclass segmentation, an argmax operation is applied on the output probability map to produce a mask for each class. Cloud prediction by Cloud-Net+ takes about 30s for all patches of a typical Landsat 8 scene with a P100-PCIE-12GB GPU, half of which is for reading and preparing, and the other half is for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Metrics</head><p>Once cloud masks of complete Landsat 8 scenes (including empty patches) are obtained by our algorithm, they are compared against their corresponding GTs. Then, the performance is quantitatively measured by Jaccard index (or mean Intersection over Union (mIoU)), precision, recall, and accuracy. These metrics are commonly used in state-of-the-art segmentation algorithms and are defined as follows:</p><formula xml:id="formula_9">Jaccard Index = M i=1 tp i M i=1 (tp i +f p i +f n i ) , Precision = M i=1 tp i M i=1 (tp i +f p i ) , Recall = M i=1 tp i M i=1 (tp i + f n i ) , Accuracy = M i=1 (tp i + tn i ) M i=1 (tp i +tn i +f p i +f n i ) ,</formula><p>where tp, tn, f p, and f n are the numbers of true positive, true negative, false positive, and false negative pixels for each class in each test set scene. M is the total number of scenes in the test data. <ref type="table" target="#tab_1">Table I</ref> demonstrates experimental results for evaluating the proposed loss functions. From <ref type="table" target="#tab_1">Table I</ref>, we conclude that for both 38-Cloud and SPARCS datasets, Cloud-Net+ trained with both versions of FJL performs better than the original loss functions that they are derived from. A comparison of different loss functions over 38-Cloud dataset is displayed in <ref type="figure">Fig. 3</ref>. Note the better performance of both FJL loss functions on detecting thick and thin clouds (haze) over snow (two middle columns). 2) Evaluation of the Proposed SDAA Method: Numerical results for cloud shadow detection with and without SDAA using different loss functions are reported in <ref type="table" target="#tab_1">Table II</ref>. From this table, the addition of the augmented patches obtained by SDAA improves all evaluation metrics over SPARCS dataset compared to using original training patches only. Using SDAA to train over Biome 8 dataset also results in a higher Jaccard index and accuracy than without it.  with FJL 1 delivers a higher Jaccard index over 38-Cloud dataset than the U-Net, Cloud-AttU, and Fmask V3 methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Quantitative and Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Evaluation of the Proposed Loss Functions:</head><p>Four visual examples of the predicted cloud masks are displayed in <ref type="figure" target="#fig_6">Fig. 4</ref>. The second and third columns show scenes in the presence of snow. Fmask and U-Net call many f ps where land is covered with snow. The image of the third column does not include any clouds. The only method that successfully identifies all clear pixels is our method. The fourth column of this figure is captured over a region with bare soil-another difficult case for detecting clouds. The proposed method can distinguish those regions from clouds more accurate than the other methods.</p><p>Although it is not fair to compare our method (which only uses four spectral bands) with Fmask V4 (which uses seven spectral bands, one thermal band, DEM, and GSWO), we obtained the numerical results of Fmask V4 over 38-Cloud test set. Fmask V4 shows lower accuracy (96.23%) compared to Cloud-Net+ with FJL 1 . Note that Fmask V4 has limited applicability, as not many remote sensing platforms are equipped with more than four spectral bands of RGBNir.</p><p>The numerical results over 95-Cloud test set (which has the same test set as 38-Cloud) are improved since they are obtained by a network trained with more training images (95-Cloud training set is larger than 38-Cloud). <ref type="figure" target="#fig_7">Fig. 5</ref> highlights the improvement of results over this dataset.</p><p>For SPARCS dataset, Cloud-Net+ outperforms SegNet <ref type="bibr" target="#b67">[68]</ref> and PSPNet <ref type="bibr" target="#b68">[69]</ref>, two common semantic segmentation methods. In addition, Cloud-Net+ outperforms RS-Net over two datasets of SPARCS and Biome 8. On Biome 8 dataset, our method, which utilizes only four spectral bands, shows a higher recall and accuracy than that of CloudFCN that uses eleven spectral bands. Note that Jaccard index and accuracy take into account both f p and f n in a predicted mask and penalize for all of those falsely labeled pixels, as opposed to precision and recall, which only penalize for one of the f p and f n. That is why Jaccard index-which is not reported in some papers-and accuracy are the most important metrics when one judges different methods. Visual results over Biome 8 dataset are shown in <ref type="figure" target="#fig_8">Fig. 6</ref>. 4) Comparison with State-of-the-art Cloud and Shadow Detection Methods: The numerical results for simultaneous segmentation of cloud, shadow, and clear regions over two datasets are reported in <ref type="table" target="#tab_1">Table IV</ref>. The reported Average Jaccard in this table is the average of Jaccard indices over three classes. Two settings were considered for evaluation by the authors of SPARCS CNN: a) using 72 images from the SPARCS dataset for training and the rest of SPARCS images for testing, and b) utilizing the same 72 SPARCS images for training and 24 scenes from the Biome 8 dataset for testing (names of these 24 scenes are listed in <ref type="bibr" target="#b47">[48]</ref>). We used the same settings in our experiments for a fair comparison. Two pixels were considered by the authors of CNN SPARCS as a leeway buffer at boundaries of predicted clouds and shadows and clear regions, where either class was considered correct within two pixels from the predicted boundary in ground truths. Note that we used no buffer pixels for the evaluation of our method. According to <ref type="table" target="#tab_1">Table IV</ref>, when SPARCS CNN is trained on SPARCS dataset and evaluated on Biome 8 (even by considering a two-pixel buffer), it is outperformed by our method in terms of accuracy. However, when this method is trained and evaluated on SPARCS dataset, the numerical results are almost perfect. It should be noted that those perfect U-Net <ref type="bibr" target="#b28">[29]</ref> Cloud-Net+ Loss=FJL 1</p><p>Fmask <ref type="bibr" target="#b21">[22]</ref> Cloud-Net+ Loss=FJL 2  results reported for this experiment are obtained by having a two-pixel leeway buffer and using ten spectral bands, as opposed to no leeway buffer and using only four spectral bands in our method. As an example of how much this two-pixel leeway affects numerical results of the SPARCS CNN, notice the improvement in accuracy (from 91.04% to 95.4%), where SPARCS CNN method is trained on the SPARCS dataset and evaluated on the Biome 8. <ref type="figure">Fig. 7</ref> displays some visual results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>False-color</head><p>Ground truth Fmask <ref type="bibr" target="#b14">[15]</ref> Cloud-Net+, Loss=FJL 1 for simultaneous segmentation of clouds and cloud shadows over SPARCS dataset.</p><p>For comparison againt RefUNets, we downloaded the cited the training, validation, and testing Landsat 8 scenes and conducted our experiments on them. In RefUNet v1 and RefUnet v2, the input consists of four (RGBNir) and seven spectral bands, respectively, while the default cloud and shadow masks extracted from Landsat 8 products' QA bands are used for ground truths in experiments. Although sophisticated boundary refinements were applied on RefUnet methods's output masks, the obtained accuracy by either of these methods is not as high as the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>False-color</head><p>Ground truth Cloud-Net <ref type="bibr" target="#b0">[1]</ref> Proposed Cloud-Net+ +SDAA, Loss=FJL  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Recommendations on the Application of the Proposed Loss Functions</head><p>In general, the numerical results for FJL 1 and FJL 2 are not significantly different in the majority of the experiments conducted. (see <ref type="table" target="#tab_1">Tables III and IV)</ref>.</p><p>Another point is that validation loss values during training seem to be more stable in FJL 1 than FJL 2 , as shown in <ref type="figure" target="#fig_10">Fig. 8</ref>.</p><p>We believe that this is because the Inverse Jaccard Function is used as the compensatory function in FJL 1 , which is of the same nature as the main J L function. However, the cross entropy, which is used as a compensatory function in FJL 2 , is calculated very differently from J L as it is a log-based function. Although the range of both FJL 1 and FJL 2 is bound to [0,1], the similarity between the compensatory function and the main J L function in FJL 1 leads to a more stable validation loss trend and fewer abrupt spikes than FJL 2 . According to <ref type="table" target="#tab_1">Table IV</ref>, for multiclass segmentation of remote sensing imagery, FJL 1 provides a higher recall for cloud class, which means that it is more conservative in predicting non-cloud regions. As a result, for applications in which as many clear pixels as possible are required, FJL 1 is recommended. Otherwise, <ref type="table" target="#tab_1">Table IV</ref> demonstrates very similar Jaccard and accuracy results for the identification of clouds, shadows, and clear regions in images, suggesting that either of these loss functions is effective and leads to high-quality prediction masks.</p><p>We investigated how each loss function performed over different biomes/land cover types for cloud detection and reported the results in <ref type="table" target="#tab_5">Table V</ref>. We used the two-fold cross validation strategy for this investigation on Biome 8 dataset, where eight different biomes are distributed equally across the two folds. According to this table, the numerical results of FJL 1 and FJL 2 are very similar in most of the biomes. The only biome type in which both Jaccard and accuracy obtained by two proposed loss functions differ by a large gap (more than 5%), is the snow/ice biome. <ref type="table" target="#tab_5">Table V</ref> shows that FJL 1 is more effective in snowy or icy land types than FJL 2 . As a result, we recommend using FJL 1 for the cloud segmentation of regions including ice, snow, and mountains, especially during the colder seasons. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Further Experiments</head><p>To further highlight the effect of some of our choices made in this work, some experiments are conducted and summarized in <ref type="table" target="#tab_1">Table VI</ref>. All experiments in this table are conveyed using soft Jaccard loss function for cloud segmentation. First, Cloud-Net+ architecture is compared against Cloud-Net (with and without using the aggregation branch, AB). As <ref type="table" target="#tab_1">Table  VI</ref> indicates, Jaccard index and accuracy of Cloud-Net+ are higher than those of Cloud-Net, despite Cloud-Net+'s 10% less trainable parameters. Another interesting point is that by removing AB from Cloud-Net+ architecture, its performance deteriorates. This indicates that AB is capable of retrieving more details in the predicted cloud mask.</p><p>Due to computational hardware constraints, we perform our experiments with overlapping (OL) patches only on SPARCS dataset, which is the smallest of all datasets used for comparison in this work. For SPARCS, extracting and using patches with 50% overlap (as suggested in <ref type="bibr" target="#b46">[47]</ref>) improved the performance of cloud detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Experiment on Pascal VOC Dataset</head><p>To test the proposed Filtered Jaccard Loss functions beyond cloud/shadow detection applications, we have conducted experiments over Pascal VOC 2012 semantic segmentation dataset <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>. This dataset contains 10582 images for training and 1449 for testing. Each pixel in each image has been assigned to one of the 21 existing classes, including airplane, bicycle, cat, horse, and person, to name a few. To obtain a binary segmentation, we considered pixels of only one of the classes-person class-as the positive class and all other pixels in each image as the negative class. Numerical results in <ref type="table" target="#tab_1">Table VII</ref> indicate that both versions of the proposed loss function outperform soft Jaccard loss for other types of images in addition to the remote sensing ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we have developed a novel system for cloud and cloud shadow detection in Landsat 8 imagery using a deep learning-based approach. The proposed network, Cloud-Net+, benefits from multiple convolution blocks that extract global and local cloud/shadow features. This network, which outperforms its parent network, Cloud-Net, has been optimized using two new and novel loss functions that reduce the number of misclassified pixels. These loss functions can be used for other binary or multiclass segmentation tasks, where the target object exists only in some of the images. In addition, a new augmentation technique, Sunlight Direction-Aware Augmentation (SDAA), is introduced for the task of cloud shadow detection. SDAA takes into account solar angles to generate natural-looking synthetic RGBNir images. We have also released an extension to our previously introduced cloud detection dataset. It will help researchers to improve the generalization ability of their cloud segmentation algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Cloud-Net+'s architecture. Let us consider a small 2×2 input image with t 0 = [0, 0; 0, 0] and two possible predictions of y 1 = [0.01, 0.01; 0.01, 0.01] and y 2 = [0.99, 0.99; 0.99, 0.99].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Examples of SDAA obtained from one of the SPARCS images. The yellow arrow shows the sunlight direction in each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>candidates have been considered for hyperparameters of SDAA: θ A O , r, and γ. These candidates consist of: θ A O = {90, 180, 270} • , r = {20, 40, 60, 80, 100}, and γ = {0.8, 0.825, 0.85, 0.875, 0.9, 0.925, 0.95, 0.975}. Note that θ A O is selected to cover all possible ranges of the solar azimuth angle-[0, 360]. Values of r greater than 100 generate too long and unrealistic shadows. In addition, choosing γ values smaller than 0.8 and greater than 0.975 leads to too dark and unnatural-looking bright shadows. The entire process is repeated for all spectral bands (R, G, B, and Nir). Fig. 2 displays some of the augmented images generated by SDAA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1) 38 -</head><label>38</label><figDesc>Cloud Dataset: 38-Cloud dataset, which has been introduced in [1], consists of 8400 non-overlapping (NOL) patches of 384×384 pixels extracted from 18 Landsat 8 Collection 1 Level-1 scenes as the training set. 9201 patches of the same spatial size obtained from 20 Landsat 8 scenes represent the test set. Scenes are mainly from North America and their GTs are manually extracted. This dataset includes only four spectral bands of R, G, B, and Nir.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 Fig. 3 :</head><label>23</label><figDesc>Visual examples of the cloud detection results over patches of 38-Cloud test set obtained by different loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Visual examples of cloud detection over 38-Cloud dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 :</head><label>5</label><figDesc>Visual examples of cloud detection over 95-Cloud dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 :</head><label>6</label><figDesc>Visual examples of cloud detection over Biome 8 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2 Fmask [ 23 ]Fig. 7 :</head><label>2237</label><figDesc>Visual examples of cloud and shadow detection results over SPARCS dataset. White and gray pixels show cloud and shadow regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 :</head><label>8</label><figDesc>Training and validation loss trend for three experiments using two versions of the proposed loss functions. For better visualization, only the first 150 epochs are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Quantitative results for the proposed loss functions obtained by Cloud-Net+ model for cloud detection (in %).</figDesc><table><row><cell>Training/Test</cell><cell>Loss</cell><cell cols="4">Jaccard Precision Recall Accuracy</cell></row><row><cell>38-Cloud training set /test set</cell><cell cols="2">J L proposed FJL 1 88.85 88.41 CE 87.71 proposed FJL 2 88.26</cell><cell>97.44 97.39 97.25 97.70</cell><cell>90.51 91.01 89.94 90.13</cell><cell>96.21 96.35 95.97 95.91</cell></row><row><cell>SPARCS folds 2-5 /fold 1</cell><cell cols="2">J L proposed FJL 1 83.99 81.77 CE 81.55 proposed FJL 2 85.11</cell><cell>92.78 92.23 92.25 93.43</cell><cell>87.32 90.37 87.55 90.52</cell><cell>94.69 95.30 94.60 95.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Quantitative results of shadow detection using SDAA obtained with Cloud-Net+ and different loss functions (in %). Table III compares cloud detection results obtained by the proposed method with other methods on various datasets. From this table, the combination of the proposed Cloud-Net+</figDesc><table><row><cell>Training/Test</cell><cell>Loss</cell><cell cols="4">Jaccard Precision Recall Accuracy</cell></row><row><cell></cell><cell>J L</cell><cell>58.99</cell><cell>76.82</cell><cell>71.76</cell><cell>95.97</cell></row><row><cell>SPARCS folds 2-5</cell><cell>CE</cell><cell>57.55</cell><cell>79.06</cell><cell>67.90</cell><cell>95.95</cell></row><row><cell>/ fold 1</cell><cell cols="2">proposed FJL 1 60.59</cell><cell>75.09</cell><cell>75.84</cell><cell>96.02</cell></row><row><cell></cell><cell cols="2">proposed FJL 2 60.52</cell><cell>74.91</cell><cell>75.91</cell><cell>96.00</cell></row><row><cell>SPARCS folds 2-5 + SDAA patches / fold 1</cell><cell cols="2">J L CE proposed FJL 1 63.44 62.38 61.66 proposed FJL 2 63.22</cell><cell>79.05 81.09 79.20 78.41</cell><cell>74.73 72.01 76.12 76.54</cell><cell>96.36 96.38 96.45 96.40</cell></row><row><cell>Biome 8</cell><cell cols="2">proposed FJL 1 53.79</cell><cell>58.45</cell><cell>87.09</cell><cell>98.21</cell></row><row><cell>70% / 30%</cell><cell cols="2">proposed FJL 2 52.11</cell><cell>56.31</cell><cell>87.47</cell><cell>98.07</cell></row><row><cell>Biome 8</cell><cell cols="2">proposed FJL 1 54.71</cell><cell>64.05</cell><cell>78.94</cell><cell>98.43</cell></row><row><cell>70%+SDAA / 30%</cell><cell cols="2">proposed FJL 2 54.41</cell><cell>59.67</cell><cell>86.06</cell><cell>98.27</cell></row><row><cell cols="6">3) Comparison with State-of-the-art Cloud Detection Meth-</cell></row><row><cell>ods:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Comparison of the proposed cloud detection method with other methods (in %). FJL 1 (ours) 88.85 97.39 91.01 96.35 Cloud-Net+ + FJL 2 (ours) 88.26 97.70 90.13 95.91</figDesc><table><row><cell>Train/Test</cell><cell>Method</cell><cell cols="3">Jaccard Precision Recall Accuracy</cell></row><row><cell cols="2">Fmask V3 [22] U-Net [29] Cloud-Net [1] Cloud-AttU [51] Cloud-Net+ + 95-Cloud 38-Cloud training set /test set Fmask V3 [22]</cell><cell>85.91 85.03 87.32 88.72 85.91</cell><cell cols="2">88.65 96.52 94.94 96.15 88.02 95.05 97.60 89.23 95.86 97.16 91.30 97.05 88.65 96.52 94.94</cell></row><row><cell>training</cell><cell>Cloud-Net [1]</cell><cell>90.83</cell><cell cols="2">97.67 92.84 97.00</cell></row><row><cell>set /test</cell><cell cols="2">Cloud-Net+ + FJL 1 (ours) 91.57</cell><cell cols="2">96.94 94.28 97.23</cell></row><row><cell>set</cell><cell cols="2">Cloud-Net+ + FJL 2 (ours) 91.01</cell><cell cols="2">97.49 93.19 97.06</cell></row><row><cell></cell><cell>Fmask V3 [22]</cell><cell>73.01</cell><cell cols="2">78.37 91.44 90.79</cell></row><row><cell>SPARCS folds 2-5 /fold 1</cell><cell cols="2">SegNet [68] PSPNet [69] Cloud-Net [1] Cloud-Net+ + FJL 1 (ours) 84.00 75.80 78.91 81.36</cell><cell cols="2">83.35 89.33 92.23 90.11 86.39 93.71 90.81 88.66 94.46 92.23 90.37 95.30</cell></row><row><cell></cell><cell cols="2">Cloud-Net+ + FJL 2 (ours) 85.11</cell><cell cols="2">93.43 90.52 95.68</cell></row><row><cell>SPARCS 5CV</cell><cell cols="2">RS-Net [47] Cloud-Net+ + FJL 1 (ours) 83.11 -Cloud-Net+ + FJL 2 (ours) 83.14</cell><cell cols="2">88.92 83.89 94.85 90.75 90.82 96.20 90.77 90.81 96.23</cell></row><row><cell>Biome 8 fold 2 /fold 1</cell><cell cols="2">Fmask V3 [22] Cloud-Net [1] Cloud-Net+ + FJL 1 (ours) 85.44 79.81 84.84 Cloud-Net+ + FJL 2 (ours) 85.15</cell><cell cols="2">82.38 96.24 93.05 93.92 89.78 95.48 92.88 91.42 95.55 91.43 92.52 95.39</cell></row><row><cell></cell><cell>CloudFCN [52]</cell><cell>-</cell><cell>-</cell><cell>90.57 91.00</cell></row><row><cell>Biome 8</cell><cell>RS-Net [47]</cell><cell>-</cell><cell cols="2">92.15 91.31 92.10</cell></row><row><cell>2CV</cell><cell cols="2">Cloud-Net+ + FJL 1 (ours) 85.15</cell><cell cols="2">90.80 93.28 95.27</cell></row><row><cell></cell><cell cols="2">Cloud-Net+ + FJL 2 (ours) 83.71</cell><cell cols="2">88.69 93.79 94.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Quantitative results for simultaneous cloud and shadow detection (in %). In columns 2 to 4, the first row indicates metrics for the cloud class, the second row for the shadow class, and the third row for the clear class............... 32.24 .............. 88.41 78.37 .............. 52.53 .............. 95.58 91.44 .............. 45.51 .............. ............ 60.22 .............. 93.32 90.75 .............. 76.47 .............. 96.39 90.60 .............. 73.92 .............. ............ 63.42 .............. 93.99 91.93 .............. 80.49 .............. 96.43 90.63 .............. 74.94 .............. ............ 64.04 .............. 94.15 91.97 .............. 82.18 .............. 96.38 90.38 .............. 74.37 .............. ............ 87.42 .............. 97.92 96.42 .............. 94.87 .............. 98.75 95.72 .............. 91.75 .............. ............ 61.57 .............. 95.25 84.67 .............. 70.56 .............. 98.40 89.21 .............. 82.85 .............. ............ 62.32 .............. 95.29 85.33 .............. 71.47 .............. 98.19 86.83 .............. 82.97 .............. ............ 41.92 .............. 92.57 82.88 .............. 56.70 .............. 97.06 89.62 .............. 61.66 .............. ............ 41.19 .............. 92.11 81.75 .............. 53.82 .............. 96.74 86.47 .............. 63.71 .............. ............ 30.51 .............. 92.51 75.69 .............. 41.57 .............. 99.18 98.58 .............. 53.41 .............. ............ 72.20 .............. 96.88 93.71 .............. 71.48 .............. ............ 51.20 .............. 95.26 93.99 .............. 55.10 .............. 98.04 90.47 .............. 87.85 .............. ............ 51.70 .............. 95.53 94.89 .............. 56.00 .............. 97.99 89.78 .............. 87.07 .............. .............. 41.36 .............. 89.52 97.17 .............. 09.03 .............. ............ 40.64 .............. 91.99 .............. 95.83 .............. 39.00 .............. 84.51 .............. ............ 36.48 .............. 93.06 94.43 .............. 58.22 .............. 95.37 91.17 .............. 49.42 .............. ............ 36.63 .............. 92.89 94.59 .............. 57.01 .............. 95.28 90.74 .............. 50.60 ..............</figDesc><table><row><cell>Training/ Test</cell><cell>Model</cell><cell cols="3">Jaccard Precision Recall</cell><cell>Average Jaccard</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell>73.01</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fmask V3 [22]</cell><cell></cell><cell></cell><cell></cell><cell>64.56</cell><cell>92.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>92.18</cell><cell></cell><cell></cell></row><row><cell>SPARCS</cell><cell>Cloud-Net [1]</cell><cell cols="3">82.94 ..96.70</cell><cell>78.83</cell><cell>96.04</cell></row><row><cell>folds 2-5 /fold 1</cell><cell>Cloud-Net+ + SDAA + FJL 1 (proposed)</cell><cell cols="3">83.96 ..97.37</cell><cell>80.45</cell><cell>96.41</cell></row><row><cell></cell><cell>Cloud-Net+ + SDAA + FJL 2 (proposed)</cell><cell cols="3">83.77 ..97.60</cell><cell>80.65</cell><cell>96.47</cell></row><row><cell></cell><cell>SPARCS CNN [48] w/ 2-px buffer</cell><cell cols="3">92.44 ..99.14</cell><cell>92.59</cell><cell>100</cell></row><row><cell>SPARCS 72 / 8</cell><cell>Cloud-Net+ + SDAA + FJL 1 (proposed)</cell><cell cols="3">76.81 ..96.75</cell><cell>77.88</cell><cell>96.87</cell></row><row><cell></cell><cell>Cloud-Net+ + SDAA + FJL 2 (proposed)</cell><cell cols="3">75.56 ..96.99</cell><cell>77.72</cell><cell>96.87</cell></row><row><cell></cell><cell>SPARCS CNN [48] w/o 2-px buffer</cell><cell>not repo-rted</cell><cell>not repo-rted</cell><cell>not repo-rted</cell><cell>not reported</cell><cell>91.04</cell></row><row><cell>SPARCS 72 / Biome 8</cell><cell>SPARCS CNN [48] w/ 2-px buffer</cell><cell>not repo-rted</cell><cell>not repo-rted</cell><cell>not repo-rted</cell><cell>not reported</cell><cell>95.40</cell></row><row><cell>24 scenes with</cell><cell>Cloud-Net+ + SDAA + FJL 1 (proposed)</cell><cell cols="3">75.62 ..95.23</cell><cell>70.04</cell><cell>95.62</cell></row><row><cell>shadow</cell><cell>Cloud-Net+ + SDAA + FJL 2 (proposed)</cell><cell cols="3">72.48 ..95.06</cell><cell>68.59</cell><cell>95.23</cell></row><row><cell></cell><cell>Fmask V3 [22]</cell><cell cols="3">74.88 ..93.22</cell><cell>65.96</cell><cell>95.40</cell></row><row><cell>Biome 8 scenes with shadow 70% / 30%</cell><cell>Segnet Adaption [46] Cloud-Net+ + SDAA + FJL 1 (proposed) Cloud-Net+ + SDAA + FJL 2 (proposed)</cell><cell cols="3">not repo-rted 85.53 ..97.10 93.20 ..96.78 85.64 ..97.43</cell><cell>not reported 77.33 77.62</cell><cell>94.93 97.29 97.40</cell></row><row><cell></cell><cell>RefUNet v1 [49]</cell><cell>not repo-rted</cell><cell>85.49</cell><cell>84.31</cell><cell>not reported</cell><cell>93.43</cell></row><row><cell>RefUNet Landsat 8 training set / test set</cell><cell>RefUNet v2 [50] Cloud-Net+ + SDAA + FJL 1 (proposed) Cloud-Net+ + SDAA + FJL 2 (proposed)</cell><cell cols="4">not repo-rted 86.52 ..97.46 87.93 ..not reported 72.02 86.27 ..97.37 71.93</cell><cell>93.60 95.87 95.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Comparison of the cloud detection performance of the two proposed loss functions over various biome types (Jaccard/Accuracy in%). Each reported metric is the average of the metric values obtained over fold 1 and 2.</figDesc><table><row><cell>Loss</cell><cell cols="4">Snow/ Shrubland Forest Water Urban Grass/ Wetland Barren Ice Crops</cell></row><row><cell>FJL 1</cell><cell>51.8/ 86.1</cell><cell>89.7/ 96.8</cell><cell>84.8/ 91.0/ 94.4/ 94.2/ 90.0/ 94.2 97.4 98.4 98.1 96.9</cell><cell>87.3/ 96.5</cell></row><row><cell>FJL 2</cell><cell>47.4/ 81.8</cell><cell>87.9/ 96.3</cell><cell>86.1/ 90.1/ 94.6/ 92.1/ 93.0/ 94.6 97.2 98.4 97.3 97.8</cell><cell>87.0/ 96.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Quantitative results for further experiments (in %).</figDesc><table><row><cell>Training/Test</cell><cell>Model</cell><cell cols="3">Jaccard Precision Recall Accuracy</cell></row><row><cell>38-Cloud</cell><cell>Cloud-Net</cell><cell>87.32</cell><cell cols="2">97.60 89.23 95.86</cell></row><row><cell>training set</cell><cell cols="2">Cloud-Net+ w/o AB 87.84</cell><cell cols="2">97.84 89.57 96.04</cell></row><row><cell>/test set</cell><cell>Cloud-Net+</cell><cell>88.41</cell><cell cols="2">97.44 90.51 96.21</cell></row><row><cell>SPARCS folds</cell><cell cols="2">Cloud-Net+ w/o OL 81.77</cell><cell cols="2">92.78 87.32 94.69</cell></row><row><cell>2-5 /fold 1</cell><cell cols="2">Cloud-Net+ w OL 83.35</cell><cell>92.08</cell><cell>89.78 95.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Quantitative results over Pascal VOC dataset (in %).</figDesc><table><row><cell>Method</cell><cell cols="3">Loss Jaccard Accuracy</cell></row><row><cell>Cloud-Net+</cell><cell>J L</cell><cell>58.63</cell><cell>97.19</cell></row><row><cell cols="2">Cloud-Net+ FJL 1</cell><cell>63.11</cell><cell>97.59</cell></row><row><cell cols="2">Cloud-Net+ FJL 2</cell><cell>62.76</cell><cell>97.37</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the Government of Canada and MacDonald, Dettwiler and Associates Ltd. for the financial support for this research through the Technology Development Program. Also, this research was enabled in part by the technical support provided by Compute Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cloud-Net: An End-to-End Cloud Detection Algorithm for Landsat 8 Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohajerani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saeedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Geos. and Rem. Sens. Symp</title>
		<imprint>
			<biblScope unit="page" from="1029" to="1032" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spatial and Temporal Distribution of Clouds Observed by MODIS Onboard the Terra and Aqua Satellites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Platnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Menzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Hubanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geosc. and Rem. Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3826" to="3852" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Development of 1-D Ice Cloud Microphysics Data Assimilation System (IMDAS) for Cloud Parameter Retrievals by Integrating Satellite Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Geosc. and Rem. Sens. Simp</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>II-501-II-504</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Remote Sensing of Global Volcanic Eruptions Using Fengyun Series Satellites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Geosc. and Rem. Sens. Simp</title>
		<imprint>
			<biblScope unit="page" from="4797" to="4800" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simulation and Prediction of Hurricane Lili During Landfall over the Central Gulf States Using MM5 Modeling System and Satellite Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tuluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fadavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Geosc. and Rem. Sens. Simp</title>
		<imprint>
			<biblScope unit="page" from="36" to="39" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recalibration of offshore chlorophyll content based on virtual satellite constellation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Geosc. and Rem. Sens. Simp</title>
		<imprint>
			<biblScope unit="page" from="7976" to="7979" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cloud/shadow detection based on spectral indices for multi/hyperspectral optical remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. of Photogram. and Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="235" to="253" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-feature combined cloud and cloud shadow detection in GaoFen-1 wide field of view imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sens. of Env</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page" from="342" to="358" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cirrus clouds that adversely affect landsat 8 images: What are they and how to detect them?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Woodcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sens. of Env</title>
		<imprint>
			<biblScope unit="volume">246</biblScope>
			<biblScope unit="page">111884</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An automatic method for screening clouds and cloud shadows in optical satellite image time series in cloudy regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Helmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sens. of Env</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Image Transform to Characterize and Compensate for Spatial Variations in Thin Cloud Contamination of Landsat Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guindon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cihlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen. of Env</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="187" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scene learning for cloud detection on remote-sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Selec. Top. in Appl. Earth Obs. and Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4206" to="4222" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fusion of hyperspectral and lidar data for classification of cloud-shadow mixed remote sensed scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scheunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Selec. Top. in Appl. Earth Obs. and Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3768" to="3781" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Cloud and Cloud Shadow Detection Method Based on Fuzzy c-Means Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fenzhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yunshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Selec. Top. in Appl. Earth Obs. and Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1714" to="1727" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous cloud detection and removal from bitemporal remote sensing images using cascade convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geosc. and Rem. Sens</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cloud detection algorithm for multi-modal satellite imagery using convolutional neural-networks (CNN)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Segal-Rozenhaimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chirayath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sens. of Env</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page">111446</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. of Photogram. and Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="197" to="212" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transferring deep learning models for cloud detection between landsat-8 and proba-v</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mateo-García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>López-Puigdollers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gómez-Chova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. of Photogram. and Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilevel Cloud Detection in Remote Sensing Images Based on Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Selec. Top. in Appl. Earth Obs. and Rem. Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3631" to="3640" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate cloud detection in high-resolution remote sensing imagery by weakly supervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sens. of Env</title>
		<imprint>
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="page">112045</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object-Based Cloud and Cloud Shadow Detection in Landsat Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Woodcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen. of Env</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="83" to="94" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improvement and Expansion of the Fmask Algorithm: Cloud, Cloud Shadow, and Snow Detection for Landsats 4-7, 8, and Sentinel 2 Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Woodcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen. of Env</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="269" to="277" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fmask 4.0: Improved Cloud and Cloud Shadow Detection in Landsats 4-8 and Sentinel-2 Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen. of Env</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="page">111205</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Characterization of the Landsat-7 ETM+ Automated Cloud-Cover Assessment (ACCA) Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Irish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arvidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogram. Eng. and Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1179" to="1188" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Robust Approach for Object-Based Detection and Radiometric Characterization of Cloud Shadow Using Haze Optimized Transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guindon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geosc. and Rem. Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5540" to="5547" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An iterative haze optimized transformation for automatic cloud/haze detection of landsat imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geosc. and Rem. Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2682" to="2694" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A novel bayesian spatial-temporal random field model applied to cloud detection from remotely sensed imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geosc. and Rem. Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4913" to="4924" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CloudFCN: Accurate and Robust Cloud Detection for Satellite Imagery with Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">19</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Cloud Detection Algorithm for Remote Sensing Images Using Fully Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohajerani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Krammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saeedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Workshop on Multim. Sig. Proc</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Cloud Detection Method Based on Relationship Between Objects of Cloud and Cloud-Shadow for Chinese Moderate to High Resolution Satellite Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4898" to="4908" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Novel Multitemporal Cloud and Cloud Shadow Detection Method Using the Integrated Cloud Z-Scores Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Selec. Top. in Appl. Earth Obs. and Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="134" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multitemporal Cloud Masking in the Google Earth Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mateo-García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gómez-Chova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Amorós-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muñoz-Marí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Contextual Pattern Recognition Applied to Cloud Detection and Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pairman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geosc. and Rem. Sens</title>
		<imprint>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="855" to="863" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A class of cloud detection algorithms based on a map-mrf approach in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vivone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Addesso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Longo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Restaino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geosc. and Rem. Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5100" to="5115" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automated Cloud and Cloud-Shadow Masking for Landsat 8 Using Multitemporal Images in a Variety of Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Candra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Phinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scarth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">17</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simultaneous Cloud Detection and Removal From Bitemporal Remote Sensing Images Using Cascade Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geosc. and Rem. Sens</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Cloud Detection Method for Landsat 8 Images Based on PCANet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">New neural network cloud mask algorithm based on radiative transfer simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tanikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stamnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sens. of Env</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="62" to="71" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cloud Detection in Remote Sensing Images Based on Multiscale Features-Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geosc. and Rem. Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4062" to="4076" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discriminative Feature Learning Constrained Unsupervised Network for Cloud Detection in Remote Sensing Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PSegnet and NPSegnet: New Neural Network Architectures for Cloud Recognition of Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="323" to="87" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-sensor cloud and cloud shadow segmentation with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wieland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Martinis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sens. of Env</title>
		<imprint>
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="page">111203</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">CDnet: CNN-Based Cloud Detection for Remote Sensing Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geosc. and Rem. Sens</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">CDnetV2: CNN-Based Cloud Detection for Remote Sensing Imagery With Cloud-Snow Coexistence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
			<pubPlace>Rem Sens</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cloud and cloud shadow detection in landsat imagery based on deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sens. of Env</title>
		<imprint>
			<biblScope unit="volume">225</biblScope>
			<biblScope unit="page" from="307" to="316" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Cloud Detection Algorithm for Satellite Imagery Based on Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Inceoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Toftegaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen. of Env</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page" from="247" to="259" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">High-Quality Cloud Masking of Landsat 8 Imagery Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">21</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Refined UNet: UNet-Based Refinement Network for Cloud and Shadow Precise Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Refined unet v2: End-to-end patch-wise network for noise-free cloud and shadow segmentation</title>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">21</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cloud Detection for Satellite Imagery Using Attention-Based U-Net Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">CloudFCN: Accurate and Robust Cloud Detection for Satellite Imagery with Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">19</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">SiftingGAN: Generating and Sifting Labeled Samples to Improve the Remote Sensing Image Scene Classification Baseline In Vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosc. and Rem. Sens. Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1046" to="1050" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial networks for data augmentation and adaptation in remotely sensed imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Reite</surname></persName>
		</author>
		<editor>Applications of Mach. Learn., M. E. Zelinski, T. M. Taha, J. Howe, A. A. S. Awwal, and K. M. Iftekharuddin</editor>
		<imprint>
			<date type="published" when="2019" />
			<publisher>SPIE</publisher>
			<biblScope unit="volume">11139</biblScope>
			<biblScope unit="page" from="119" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Using Vehicle Synthesis Generative Adversarial Networks to Improve Vehicle Detection in Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Int. J. of Geo-Info</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Classification with an edge: Improving semantic image segmentation with boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="158" to="172" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">CloudMaskGAN: A Content-Aware Unpaired Image-toimage Translation Algorithm for Remote Sensing Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohajerani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Duynhoven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saeedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Image Proc</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1965" to="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
	</analytic>
	<monogr>
		<title level="j">Network In Network,&quot; CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Wide Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British</title>
		<editor>Mach. Vision Conf., E. R. H. Richard C. Wilson and W. A. P. Smith</editor>
		<meeting>the British</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="87" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On the Bayes-optimality of F-measure Maximizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Waegeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dembczyńki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jachnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Mach. Learn. Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3333" to="3388" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fully Convolutional Architectures for Multiclass Segmentation in Chest Radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lenis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hladůvka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bühler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1865" to="1876" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Automatic Skin Lesion Segmentation Using Deep Fully Convolutional Networks With Jaccard Distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1876" to="1886" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep Regression Tracking with Shrinkage Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Comp. Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks&quot;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on Artificial Intelligence and Statistics, ser. Proc. of</title>
		<meeting>of Int. Conf. on Artificial Intelligence and Statistics, ser. . of</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Cloud Detection Algorithm Comparison and Validation for Operational Landsat Data Products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Foga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Dilley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Laue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen. of Env</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="379" to="390" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Automated Detection of Cloud and Cloud Shadow in Single-Date Landsat Imagery Using Neural Networks and Spatial Post-Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rem. Sen</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4907" to="4926" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Comp. Vision and Patt. Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Comp. Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
