<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BERT-based Ensembles for Modeling Disclosure and Support in Conversational Social Media Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanvi</forename><surname>Dadu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Netaji Subhas Institute of Technology</orgName>
								<address>
									<settlement>Delhi</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikey</forename><surname>Pant</surname></persName>
							<email>kartikey.pant@research.iiit.ac.in</email>
							<affiliation key="aff1">
								<orgName type="department">International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Mamidi</surname></persName>
							<email>radhika.mamidi@iiit.ac.in</email>
							<affiliation key="aff1">
								<orgName type="department">International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BERT-based Ensembles for Modeling Disclosure and Support in Conversational Social Media Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>emotion recognition · sentiment analysis · natural language processing · social media analysis</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is a growing interest in understanding how humans initiate and hold conversations. The affective understanding of conversations focuses on the problem of how speakers use emotions to react to a situation and to each other. In the CL-Aff Shared Task, the organizers released Get it #OffMyChest dataset, which contains Reddit comments from casual and confessional conversations, labeled for their disclosure and supportiveness characteristics. In this paper, we introduce a predictive ensemble model exploiting the finetuned contextualized word embeddings, RoBERTa and ALBERT. We show that our model outperforms the base models in all considered metrics, achieving an improvement of 3% in the F1 score. We further conduct statistical analysis and outline deeper insights into the given dataset while providing a new characterization of impact for the dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The word Affective refers to emotions, mood, sentiment, personality, subjective evaluations, opinions, and attitude. Affect analysis refers to the techniques used to identify and measure the experience of emotion in multimodal content containing text, audio, images, and videos. <ref type="bibr" target="#b5">[6]</ref> Affect has become an essential part of the human experience, which directly influences their reaction towards a particular situation. Therefore, it has become crucial to analyze how speakers use emotions and sentiment to react to different situations and each other. This paper addresses the challenge put forward in the CL-Aff Shared Task at the AAAI-2020 Workshop on Affective Content Analysis to Model Affect in Response (AffCon 2020). The theme of this task is to the study affect in response to the interactive content which grows over time. The task offers two datasets ( The first two authors contributed equally to the work. arXiv:2006.01222v1 [cs.CL] 1 Jun 2020 a small labeled dataset and a large unlabeled dataset) sampled from casual and confessional conversations on Reddit in the subreddit /r/CasualConversations and the /r/OffMyChest. This shared task comprises two subtasks. The first subtask is a semi-supervised text classification task predicting Disclosure and Supportiveness labels based on the given two datasets. Whereas, the second subtask is an open-ended task, which requires authors to propose new characterizations and insights to capture conversation dynamics. Recent works in the task of text classification have used pre-trained contextualized word representations rather than context-independent word representations. Some of these representations include BERT <ref type="bibr" target="#b2">[3]</ref>, RoBERTa <ref type="bibr" target="#b14">[14]</ref>, and ALBERT <ref type="bibr" target="#b15">[15]</ref>. These models perform contextualized word representation and are pre-trained using bidirectional transformers <ref type="bibr" target="#b9">[10]</ref>. These BERT-based pre-trained models have outperformed many existing techniques on most NLP tasks with minimal task-specific architectural changes.</p><p>Ensemble models exploiting features learned from multiple pre-trained models are hypothesized to perform competitively. In this work, we propose an ensemble-based model exploiting pre-trained BERT-based word representations. We document the experimental results for the CL-Aff Shared Task of our proposed model in comparison to the baseline models. We further perform attributebased statistical analysis using attributes like word count, day of the week, and comment per parent post. We conclude the paper by proposing impact as a new characterization to model conversation dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Model</head><p>In this section, we introduce our predictive model that uses Transfer learning in the form of pretrained BERT-based models. We propose an ensemble of two pre-trained models: RoBERTa and ALBERT. In this section, we first outline the pre-trained models incorporated and then discuss the ensemble technique used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Transfer learning is the process of extracting knowledge from a source problem domain and applying it to a different target problem or domain. Recent works on text classification use transfer learning in the form of pre-trained embeddings. <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15]</ref> These pre-trained embeddings have outperformed many of the existing techniques with minimal architectural structure. The use of pretrained embeddings reduces the need for annotated data and allows one to perform the downstream task with minimal resources for the finetuning of the model.</p><p>Devlin et al. <ref type="bibr" target="#b2">[3]</ref> introduced BERT, a contextualized word representation, pretrained using a bi-directional Transformer-based encoder. These embeddings use a linear combination of masked language modeling and the next sentence prediction objectives. It is pre-trained on 3.3B words from various sources, including BooksCorpus <ref type="bibr" target="#b16">[16]</ref> and the English Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Liu et al. introduced</head><p>RoBERTa, a replication study of BERT, with carefully tuned hyperparameters and more extensive training data <ref type="bibr" target="#b14">[14]</ref>. It is trained with a batch size eight times larger for half as many optimization steps, thus taking significantly lesser time to train in comparison. It is trained on more than twelve times the data used to train BERT large , using data from Open-WebText <ref type="bibr" target="#b3">[4]</ref>, CC-News <ref type="bibr" target="#b4">[5]</ref>, and STORIES <ref type="bibr" target="#b8">[9]</ref> datasets. These optimizations lead the RoBERT a large pre-trained model to perform better than the BERT-large model in all benchmarking tests, including SQuAD <ref type="bibr" target="#b6">[7]</ref> and GLUE <ref type="bibr" target="#b10">[11]</ref>.</p><p>Lan et al. introduced ALBERT, a BERT-based model with two parameterreduction techniques: factorized embedding parameterization, and cross-layer parameter sharing. <ref type="bibr" target="#b15">[15]</ref> These techniques help in lowering memory consumption and increasing training speed. Moreover, this model uses a self-supervised loss that focuses on modeling inter-sentence coherence and improves on downstream tasks with multi-sentence input. ALBERT xxlarge,v2 achieves significant improvements over BERT large on multiple tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Our Approach</head><p>Ensemble methodology entails constructing a predictive model by integrating multiple models in order to improve prediction performance. They are metaalgorithms that combine several machine learning and deep learning classifiers into one predictive model to decrease variance, bias, and improve predictions. Recent works show that ensemble-based classifiers utilizing contextual embeddings outperform single-model classifiers. <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref> Hence, we use ensembling techniques to combine predictions from multiple models for the tasks for making a prediction for the given task. <ref type="figure" target="#fig_0">Figure 1</ref> depicts our proposed ensemble model. In this model, a sentence is parallelly computed by RoBERTa and ALBERT finetuned for predicting that label. The results from these base models are then combined using a weighted average based ensembling technique to predict the final label set, which includes predictions for the six labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>In this section, we outline the experimental setup, the baselines for the task, and a comparative analysis of our proposed ensemble model with the two base models finetuned for the task, RoBERT a large and ALBERT xxlarge,v2 . We further compare our ensemble model with four other ensemble models and show that our model performs the best among all the models in four out of five evaluation metrics using 10-fold cross validation.</p><p>For our baselines, we finetune RoBERT a Large and ALBERT xxlarge,v2 models for three epochs with a maximum sequence length of 50 and a batch size of 16 for predicting each label separately. We finetune the model with a learning rate of 2 * 10 −5 , a weight decay of 0.01, and 20 steps for warm-up. We evaluate  all the models on the following metrics: Accuracy, F1, Precision-1, Recall-1, and the mean of Accuracy and F1, denoted as Acc&amp;F1 from hereon. From <ref type="table" target="#tab_0">Table 1</ref>, we can discern that our ensemble-based model achieves the best results when compared with base models: RoBERTa and ALBERT. We observe a significant increase in Accuracy, Precision-1, and F1 and a slight increase in Recall-1 and Acc&amp;F1 in our best-performing ensemble model as compared to the base models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label/Metrics</head><p>Accuracy Precision-  <ref type="table">Table 2</ref> further shows the performance of our ensemble-based model on individual labels. Its performance on different labels is evaluated using the above metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Labels/Model</head><p>Model <ref type="formula">1</ref>   We further performed a comparative study on ensembling techniques by choosing different weights for RoBERTa and ALBERT, as given in <ref type="table" target="#tab_2">Table 3</ref>. It shows different combinations of weights assigned to each label for RoBERTa and ALBERT respectively. This gives rise to five different models, which are then compared using the above metrics. <ref type="table" target="#tab_3">Table 4</ref> depicts the results of the comparative study conducted on the five different ensemble models. We discern that Model 5 performs the best for Accuracy, Precision-1, F1, and Acc&amp;F1 metrics, and Model 1 performs the best for Recall-1 metric among all the compared models. Since Model 5 outperforms all other models in four out of five metrics, it is the best predictive model for the task and is referred to as Our Model in the paper.</p><p>For the shared task, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>In this section, we provide a comprehensive statistical analysis of the dataset Get it #OffMyChest, which comprises of comments and parent posts from the subreddit /r/CasualConversations, and /r/OffMyChest. We further propose new characterizations and outline semantic features for the given dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis</head><p>Statistical analysis of the labels, Emotional Disclosure, Informational Disclosure, Support, General Support, Information Support, and Emotional Support show significant variations in the number of positive and negative labels. The percentage of positive labels is maximum for Information Disclosure with 37.99% and minimum for General Support with 5.37%. Therefore, the given dataset is highly imbalanced, which makes the training of predictive models a strenuous task. Further analysis of the labeled dataset shows that there are 3, 511 parent posts for 11, 573 comments. We observe an average of 3.29 comments per parent post ranging from one comment per parent post to 52 comments per parent post. In the given dataset, there are 6, 999 unique users with an average of 1.653 comments per user and a significant variation in the number of comments per user ranging from 1 to 159 comments per user, with a standard deviation of 2.669. From this, we conclude that multiple comments within the same parent post and by the same author may be related to each other.</p><p>We also observe significant variations in the word count of the comments, with an average comment being of 14.7 words, which translates to around one sentence <ref type="bibr" target="#b1">[2]</ref>. However, the comment length varies significantly from 3 words to 151 words per comment, with the distribution having a standard deviation of 9.670. The dataset is thus, well-rounded, and represents realistic discourse setting with participants exchanging comments of varying lengths.</p><p>We intuitively proceeded to predict the effect of the day of the week in the characterized labels representing disclosure and support in a comment. It  <ref type="table">Table 5</ref>. Weekday-wise label distribution of the labelled dataset.</p><p>was expected that the users would behave differently as the week progresses. However, as illustrated in <ref type="table">Table 5</ref>, we do not see any significant variation in the existing characterizations with a change in the day of the week. Thus, we conclude, in this dataset, that the week of the day doesnt affect the users to be either more supportive or disclose more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impact Prediction</head><p>The score assigned to a comment quantifies its Impact since, on Reddit, it is the difference between the upvotes and downvotes that it obtains. We observed the posts to have a moderately positive Impact of 10.938 on average. We also see that the breadth of the spectrum in the Impact is captured well by the dataset, with a standard deviation of 57.198, and a range of −49 to 2, 637. This paves the way for a need to characterize and predict the Impact of a post. Upon performing a correlation study between Impact and the previously characterized labels using Pearsons correlation coefficient <ref type="bibr" target="#b7">[8]</ref>, we observe a very small positive correlation between the variables. As is illustrated in <ref type="table" target="#tab_5">Table 6</ref>, the maximum ρ of 0.046 between Impact and Emotional Disclosure represents that Impact is characteristically distinct from the previously predicted labels.</p><p>We further analyze the influence of Impact, characterized by the score on the semantic structure of the comments. We perform a correlation study between Impact and semantic features selected, as is explored previously in Yang et al <ref type="bibr" target="#b11">[12]</ref>. Semantic structure is captured by the following features:</p><p>1. Positive words: The number of occurrences of positive words in a comment. 2. Negative words: The number of occurrences of negative words in a comment. 3. Positive Polarity Confidence: The probability that a sentence is positive.</p><p>This metric is used to capture the polarity of comments and is calculated using Fasttext <ref type="bibr" target="#b0">[1]</ref>. 4. Subjective words: The number of occurrences of subjectivity oriented words in a comment. It is used to capture the linguistic expression of peoples opinions, beliefs, and speculations. 5. Sense Combination: It is computed as the log(Π k i=1 n wi ) where n wi is the total number of senses of word w i . 6. Sense Farmost: The largest Path Similarity of any word sense in a sentence. 7. Sense Closest: The smallest Path Similarity of any word sense in a sentence.  From <ref type="table" target="#tab_7">Table 7</ref>, we observe a minimal correlation between Impact and the selected semantic features. The maximum ρ of 0.040 between Impact and the feature Sense Closest <ref type="bibr" target="#b11">[12]</ref> depicts that the new characterization is distinct from semantic features of the comment.</p><p>Although it is essential to understand that predicting Impact is beneficial for numerous applications like finance, product marketing and provides insights on social dynamics, it is a hard problem dependent on various factors. Our attempt to capture relationships between Impact and some selected semantic features was not able to establish a strong correlation between the features. Thus, this implies that the use of sophisticated architectures in the task of Impact prediction would be valuable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents a novel BERT-based predictive ensemble model to predict given labels: Emotional Disclosure, Informational Disclosure, Support, General Support, Information Support, and Emotional Support. Our model gives competitive results for the label prediction on the given dataset Get it #OffMyChest. Analysis of dataset shows the highly imbalanced distribution of the given labels, and high variations in some features like score, word count, comments per parent post, and comments per user. We further discerned that day of the week has no significant impact on the frequency of Disclosure and Support based comments on Reddit. Future work may involve exploring more ensembling techniques and exploring sophisticated architectures to predict the impact of a comment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Architecture of our ensemble models which predict the label set denoting support and disclosure from the comment text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Label-averaged values for each metric for RoBERTa,ALBERT, and our best performing ensemble model.</figDesc><table><row><cell cols="4">Model/Metrics Accuracy Precision-1 Recall-1</cell><cell cols="2">F1 Acc&amp;F1</cell></row><row><cell>RoBERT a Large</cell><cell>84.86%</cell><cell>0.585</cell><cell cols="2">0.514 0.541</cell><cell>0.695</cell></row><row><cell>ALBERT xxlarge,v2</cell><cell>84.90%</cell><cell>0.596</cell><cell cols="2">0.472 0.524</cell><cell>0.686</cell></row><row><cell>Our Model</cell><cell>85.55%</cell><cell>0.623</cell><cell cols="2">0.515 0.558</cell><cell>0.707</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Weights assigned to each model in different Ensemble Models. Each cell contains a pair (x, y) where x denotes the weight assigned to RoBERTa and y denotes the weight assigned to ALBERT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>our System Run 1 to System Run 5 are predictions generated by the Model 1 to Model 5 respectively. System Run 6 and Sys-Label-averaged values for each metric for different ensemble models. tem Run 7 are the predictions generated by finetuned RoBERT a large and ALBERT xxlarge,v2 respectively.</figDesc><table><row><cell cols="4">Model/Metrics Accuracy Precision-1 Recall-1</cell><cell cols="2">F1 Acc&amp;F1</cell></row><row><cell>Model 1</cell><cell>85.18%</cell><cell>0.595</cell><cell cols="2">0.516 0.547</cell><cell>0.699</cell></row><row><cell>Model 2</cell><cell>85.42%</cell><cell>0.622</cell><cell cols="2">0.490 0.544</cell><cell>0.699</cell></row><row><cell>Model 3</cell><cell>85.47%</cell><cell>0.619</cell><cell cols="2">0.514 0.557</cell><cell>0.706</cell></row><row><cell>Model 4</cell><cell>85.48%</cell><cell>0.622</cell><cell cols="2">0.480 0.557</cell><cell>0.706</cell></row><row><cell>Model 5</cell><cell>85.54%</cell><cell>0.623</cell><cell cols="2">0.515 0.558</cell><cell>0.707</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>The relationship between Labels and Impact, as represented by Pearson correlation coefficient, ρ.</figDesc><table><row><cell>Labels</cell><cell>ρ with Impact</cell></row><row><cell>Emotional Disclosure</cell><cell>0.046</cell></row><row><cell>Informational Disclosure</cell><cell>0.024</cell></row><row><cell>Support</cell><cell>0.021</cell></row><row><cell>General Support</cell><cell>0.028</cell></row><row><cell>Information Support</cell><cell>0.005</cell></row><row><cell>Emotional Support</cell><cell>0.019</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>The relationship between Semantic Features and Impact, as represented by Pearson correlation coefficient, ρ.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1607.04606" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cutts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Oxford Guide to Plain English</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Openwebtext corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://Skylion007.github.io/OpenWebTextCorpus" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nagel</surname></persName>
		</author>
		<ptr target="http://web.archive.org/save/http://commoncrawl.org/2016/10/newsdataset-available/" />
		<title level="m">Cc-news</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Happy together: Learning and understanding appraisal from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdul-Mageed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AffCon@AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Thirteen ways to look at the correlation coefficient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nicewander</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2685263" />
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="66" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1806.02847</idno>
		<ptr target="http://dblp.uni-trier.de/db/journals/corr/corr1806.html#abs-1806-02847" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GLUE: A multitask benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-5446" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Humor recognition and humor anchor extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<editor>Mrquez, L., Callison-Burch, C., Su, J., Pighin, D., Marton, Y.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
				<ptr target="http://dblp.uni-trier.de/db/conf/emnlp/emnlp2015.html#YangLDH15" />
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2367" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.08237" />
		<title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
			<affiliation>
				<orgName type="collaboration">.D.C.O.L.M.L.L.Z.V.S.</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
			<affiliation>
				<orgName type="collaboration">.D.C.O.L.M.L.L.Z.V.S.</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G J D M</forename></persName>
			<affiliation>
				<orgName type="collaboration">.D.C.O.L.M.L.L.Z.V.S.</orgName>
			</affiliation>
		</author>
		<ptr target="https://openreview.net/forum?id=SyxS0T4tvS" />
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>under review</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for selfsupervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G K G P S R S</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1eA7AEtvS,underreview" />
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1506.06724" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
