<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaussian Embedding of Large-scale Attributed Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhagya</forename><surname>Hettige</surname></persName>
							<email>bhagya.hettige@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
							<email>yuanfang.li@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
							<email>wray.buntine@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gaussian Embedding of Large-scale Attributed Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>graph embedding · link prediction · node classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph embedding methods transform high-dimensional and complex graph contents into low-dimensional representations. They are useful for a wide range of graph analysis tasks including link prediction, node classification, recommendation and visualization. Most existing approaches represent graph nodes as point vectors in a low-dimensional embedding space, ignoring the uncertainty present in the real-world graphs. Furthermore, many real-world graphs are large-scale and rich in content (e.g. node attributes). In this work, we propose GLACE, a novel, scalable graph embedding method that preserves both graph structure and node attributes effectively and efficiently in an end-to-end manner. GLACE effectively models uncertainty through Gaussian embeddings, and supports inductive inference of new nodes based on their attributes. In our comprehensive experiments, we evaluate GLACE on real-world graphs, and the results demonstrate that GLACE significantly outperforms stateof-the-art embedding methods on multiple graph analysis tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Much real-world data can be expressed as graphs, e.g. citation networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>, social-media networks <ref type="bibr" target="#b7">[8]</ref>, language networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, and knowledge graphs <ref type="bibr" target="#b8">[9]</ref>. Graph embedding methods transform graph nodes with highly sparse, highdimensional content into low-dimensional representations. They are effective in capturing complex latent relationships between nodes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> and have been successfully employed in a wide array of graph analysis tasks such as link prediction, node classification, recommendation and visualization. The effective embedding of graph data faces a number of challenges.</p><p>Uncertainty modelling: Most of the previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> on graph node embedding represents nodes as point vectors in the embedding space, which fails to capture the uncertainty in node representations. Furthermore, graphs constructed from real-world data can be very complex, noisy and imbalanced. Therefore, a mere point-based representation of the nodes may not be able to capture the variability of the graph and so some hidden patterns <ref type="bibr" target="#b0">[1]</ref>. Scalability: Many real-world graphs are very large, containing millions of nodes and edges. The efficient embedding of such large graphs is thus important but challenging. arXiv:1912.00536v1 <ref type="bibr">[cs.</ref>LG] 2 Dec 2019 LINE <ref type="bibr" target="#b12">[13]</ref> is handling large-scale graphs using an optimized loss function they develop based on local and global network structure, but it does not consider node attributes. Inductiveness: Most existing graph embedding approaches are transductive and cannot infer embeddings for nodes unseen at training time. In practice, however, graphs evolve with time, and new nodes and edges can be added into the graph. There are a few recent studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref> which tried to provide a solution to this limitation. However, these methods either do not scale up to large graphs, or require additional information about the graph structure.</p><p>In this paper, we propose GLACE, Gaussian representations for Large-scale Attributed graph Content Embedding, a novel graph embedding method that addresses all of the above challenges. GLACE learns node embeddings as probability distributions from both node attributes and graph structure information in an end-to-end manner: we use node attributes to initialize the structurebased loss function, and update and transfer the learning back to the encoding function to minimize the loss. We use a proximity measure to quantify graph properties to be preserved in the embedding space, i.e. first-order proximity to learn from observed relations and second-order proximity to learn from a node's neighbourhoods. We learn from node attributes by a non-linear transformation function (encoder), and then define Gaussian embedding functions to model the uncertainty of the embedding by feeding the encoded representation. Therefore, the mean vector of the representation denotes the position of the node in the embedding space, while the covariance matrix gives the uncertainty of the node embedding. We deal with new nodes by learning from node attributes, so that a learned model can be used to infer embeddings for new nodes based on their attributes. The combination of node attributes and local sampling allows GLACE to be scalable, being able to support graphs of hundred thousand nodes with hundred thousand attributes and half a million edges on modest hardware. GLACE derives embeddings from node attributes, which allows it to converge faster during training. The main contributions of this work: (1) we propose a novel, end-to-end method to embed nodes as probability distributions to model uncertainty of the embedding, (2) our model is inductive as it can infer embedding for unseen nodes using node attributes, (3) our model is scalable and efficient, and supports graphs with hundreds of thousands of nodes on modest hardware with a fast convergence rate, while other methods require significantly more memory, more time, or both, and (4) we perform extensive experiments on real-world datasets for link prediction, node classification, induction, and visualization, and GLACE significantly outperforms the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Below we give a brief overview of recent graph embedding techniques. A more extensive introduction to the area can be found in these recent survey studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Unsupervised graph embedding approaches attempt to preserve graph properties in the embedding space. Random walk-based methods such as Deep-Walk <ref type="bibr" target="#b10">[11]</ref> and node2vec <ref type="bibr" target="#b3">[4]</ref> generate random walks for each node, and learn embeddings using these node sequences with a technique similar to Skip-Gram <ref type="bibr" target="#b9">[10]</ref>. LINE <ref type="bibr" target="#b12">[13]</ref> learns from proximity measures considering first-and second-order proximity. SDNE <ref type="bibr" target="#b15">[16]</ref> proposes a semi-supervised model, in which they learn first-order proximity in the supervised component and second-order proximity in the unsupervised component. Graph2Gauss <ref type="bibr" target="#b0">[1]</ref> proposes a personalized ranking scheme such that for a given anchor node, nodes in the immediate neighborhood are closer in the embedding space, while nodes multiple hops away are placed increasingly more distant to the node. Variational graph auto-encoders (VGAE) <ref type="bibr" target="#b6">[7]</ref> is also an unsupervised learning method for undirected graphs.</p><p>Learning uncertainty of embeddings has been shown to produce meaningful representations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1]</ref>. Word2gauss <ref type="bibr" target="#b14">[15]</ref> proposes a Gaussian embedding space to model word embeddings. Graph2Gauss <ref type="bibr" target="#b0">[1]</ref> captures uncertainty of graph nodes similarly. Both methods show that capturing embedding uncertainty learns more meaningful representations in their evaluation tasks. Another recent study <ref type="bibr" target="#b16">[17]</ref> proposes to learn node embeddings as Gaussian distributions using the Wasserstein metric rather than KL divergence, as the former preserves edge transitivity.</p><p>Graphs can vary greatly in size (i.e. number of nodes and edges). Some methods are designed to be scalable while others do not scale well due to high space and/or time complexities. LINE <ref type="bibr" target="#b12">[13]</ref> is a method designed to handle largescale graphs efficiently using negative sampling and edge sampling optimization strategies. Graph2Gauss <ref type="bibr" target="#b0">[1]</ref>, on the other hand, exhibits poor scalability as it needs to compute hops for each node up to a predefined number. This process is not only time consuming, but also consumes significant memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GLACE Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations and Problem Definition</head><p>Homogeneous Graph: Let G = (V, E, X) be an attributed homogeneous graph, where V is the set of nodes, E is the set of edges between nodes in V, where each ordered pair of nodes (i, j) ∈ E is associated with a weight w ij &gt; 0 for edge from i to j, and X ∈ R |V|×D is the attribute matrix of the nodes which represents an attribute matrix of V, where x i ∈ X is a D-dimensional attribute vector of node i. GLACE Embedding: GLACE aims to represent each node i ∈ V as a lowdimensional Gaussian distribution embedding,</p><formula xml:id="formula_0">z i = N (µ i , Σ i ), where µ i ∈ R L , Σ i ∈ R L×L where L is the embedding dimension with L</formula><p>|V|, D k , in embedding space R L , such that nodes close to each other in the original graph are also close in the embedding space. We learn Σ i as a diagonal covariance vector, Σ i ∈ R L , instead of a covariance matrix to reduce the number of parameters to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Architecture</head><p>GLACE is an end-to-end framework for learning node embeddings using both node attributes and graph structure in an efficient manner. Node attributes are first fed through a non-linear transformation function and then through two nonlinear transformation functions to obtain a mean vector and diagonal covariance vector which represent a Gaussian embedding. GLACE is flexible in handling different node attribute formats, such as text and images, since we can define the encoder architecture accordingly. Our unsupervised loss function is defined based on graph structure. We learn local and global graph structure using our proximity measure, since we can optimize the function using negative sampling <ref type="bibr" target="#b9">[10]</ref> to achieve scalability. Local structure is learnt with first-order proximity, i.e. based on edge weight between nodes <ref type="bibr" target="#b1">[2]</ref>, and global structure is learnt with second-order proximity, i.e. based on similarity between neighborhoods of a pair of nodes <ref type="bibr" target="#b1">[2]</ref>. GLACE learns in an end-to-end manner: forward learning: we use encoded node attributes as input to the optimization function of Graph Structure Encoding, and back-propagation: we minimize the optimization function of Graph Structure Encoding by updating the node embeddings, and then propagating the update back to the Node Attribute Encoding part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Node Attribute Encoding</head><p>We learn node attributes using two levels of transformations, encoding and Gaussian embedding. At the first level, we use a multi-layer perceptron (MLP) to encode the node attribute information and generate an intermediate vector from node attribute information. We use a feed-forward encoder, f : V → R m which takes an attribute vector x i ∈ X as input for node i, and outputs a m-dimensional intermediate vector.</p><formula xml:id="formula_1">ui = f (xi) = Wxi + b (1)</formula><p>The attribute encoder of the model is expressed using weight matrix W ∈ R D×m and bias vector b ∈ R m where m is the dimension of the hidden representation. Note here that, we can easily alter the encoder architecture such that it aligns and captures different types of inputs (e.g. images, text). But for efficiency purposes we have only considered an MLP architecture. This intermediate vector u i is then used as input to two encoders f µ and f Σ to learn µ and Σ in the Gaussian distributions. The final latent representation of node i of type k is</p><formula xml:id="formula_2">z i = N (µ i , Σ i ), where µ i = f µ (f (x i )) and Σ i = f Σ (f (x i )). µi = fµ(ui) = Wµui + bµ (2) Σi = fΣ(ui) = ELU (WΣui + bΣ) + 1 (3)</formula><p>The two functions defined in Eq. 2 with W µ ∈ R m×L and b µ ∈ R L , and in Eq. 3 with W Σ ∈ R m×L and b Σ ∈ R L denote the Mean Encoder and Covariance Encoder respectively. Note that, as the difference between different node types have been caught by u i generated by f k , all the node types share the same Mean Encoder and Covariance Encoder in GLACE to achieve good scalability. Here for the uncertainty representation, we constrain the covariance matrix to be diagonal to reduce the number of parameters to learn. The exponential linear unit (ELU) is used as the activation function in the Covariance Encoder. An ELU can have negative values as well, and it drives the mean of the activation outputs be closer to zero which makes learning and convergence much faster. We add 1 to obtain positive covariance.</p><p>Note that, even inside the Node Attribute Encoding component, GLACE also learns the parameters in an end-to-end manner. Through the shared parameter u i , GLACE forwards the updating inside Encoder f k to Gaussian Encoders f µ and f Σ , and propagates the updating inside Gaussian Encoders back to f k automatically during the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Graph Structure Encoding</head><p>GLACE aims at capturing both local (first-order) and global (higher-order) proximity information in graphs. But considering the scalability to large-scale graphs, for the global information, GLACE only encodes second-order proximity. For each node i, the learned Gaussian distributions, z i , in Section 3.3 are used as the input to the Graph Structure Encoding component in this section.</p><p>Dissimilarity measure: Let d(z i , z j ) be the dissimilarity measure between latent representations of two nodes i, j ∈ V. Since z i and z j are Gaussian distribution embedding, we should select a dissimilarity measure to be a function to measure the dissimilarity between two probability distributions. Therefore, the dissimilarity measure between two latent representations is calculated using asymmetric KL divergence, d(z i , z j ) = D KL (z j ||z i ). Alternatively, we could also use a Wasserstein metric instead of KL divergence as in <ref type="bibr" target="#b16">[17]</ref>. Since KL divergence is asymmetric, for undirected graphs we extend the distance to a symmetric dissimilarity measure as:</p><formula xml:id="formula_3">d(zi, zj) = 1 2 (DKL(zi||zj) + DKL(zj||zi))<label>(4)</label></formula><p>First-order proximity: We learn first-order proximity of nodes, by modelling local pairwise proximity between two connected nodes in the graph. The empirical probability for first-order proximity measure observed in the original graph between nodes i and j is defined as the ratio of the weight of the edge (i, j) to the total of the weights of all the edges in the graph. For each undirected edge (i, j) we define the joint probability as a sigmoid function between node i and j. These two functions can be defined as respectively:</p><formula xml:id="formula_4">p1(i, j) = wij Σ (î,ĵ)∈E wîĵ and p1(i, j) = 1 1 + exp (d(zi, zj))<label>(5)</label></formula><p>We preserve the first-order proximity by minimizing the distance between the two distributions, O 1 = D KL (p 1 ||p 1 ), for all edges. Motivated by this function, we use the following objective function as in LINE <ref type="bibr" target="#b12">[13]</ref> for first-order proximity:</p><formula xml:id="formula_5">O1 = − (i,j)∈E wij log p1(i, j)<label>(6)</label></formula><p>Second-order proximity: Nodes which have more similar neighbourhoods should be closer in embedding space with respect to the nodes with less similar neighbourhoods. The empirical probability of second-order proximity observed for edge (i, j) can be defined as the ratio of the weight of edge (i, j) to the total weight of edges from node i to its immediate neighborhood, N (i). Similarly to LINE, each node is represented with two complementary embeddings, the first embedding z i , is as defined previously, and the second is the context embedding, h i , defined in Eq. 10 and Eq. 11. For each directed edge (i, j) (if the edge is undirected, it can be treated as two edges with equal weights and opposite directions) we define the the probability of context j generated by node i as a softmax function. The two probability distributions are defined as follows:</p><formula xml:id="formula_6">p2(j|i) = wij Σî ∈N (i) w iî and p2(j|i) = exp (−d(zi, z j )) Σî ∈V exp (−d(zi, z î ))<label>(7)</label></formula><p>We preserve the second-order proximity by minimizing the distance between the two distributions, O 2 = i∈V λ i D KL (p 2 (.|i)||p 2 (.|i)), where λ i is the prestige of node i. Motivated by this <ref type="bibr" target="#b12">[13]</ref> we preserve the second-order proximity:</p><formula xml:id="formula_7">O2 = − (i,j)∈E wij log p2(i, j)<label>(8)</label></formula><p>When we define the second-order proximity measure, the neighbourhood nodes are considered as contexts for the anchor node. Therefore, we should define another set of node attribute encoding functions to model the context representations used for neighbourhood nodes, similarly to the Equations: 1, 2 and 3. The encoder for context nodes is f :</p><formula xml:id="formula_8">V → R m . The latent representation of context node i is z i = N (µ i , Σ i ), where µ i = f µ (f (x i )) and Σ i = f Σ (f (x i )). u i = f (xi) = W xi + b (9) µ i = f µ (ui) = W µ ui + b µ<label>(10)</label></formula><formula xml:id="formula_9">Σ i = f Σ (ui) = ELU (W Σ ui + b Σ ) + 1<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Optimization</head><p>The objective function in Eq. 8 is a bottleneck as it requires evaluation on the entire set of nodes for the optimization of one single edge as shown in Eq. 7. Based on the negative sampling approach <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref>, we sample several negative edges (i.e., defined as N ) for each edge in the training set to optimize the objective function.</p><p>With negative sampling our objective function O 2 in Eq. 8 becomes:</p><formula xml:id="formula_10">(i,j)∈E log σ(−d(zi, z j )) + N n=1 E vn∼Pn(v) log σ(d(zi, z vn ))<label>(12)</label></formula><p>Similarly, we can efficiently compute O 1 in Eg. 6 with negative sampling:</p><formula xml:id="formula_11">(i,j)∈E log σ(−d(zi, zj)) + N n=1 E vn∼Pn(v) log σ(d(zi, zv n ))<label>(13)</label></formula><p>where we draw negative edges from the noise distribution P n (v) with negative node probability distribution, P n (v) ∝ out degree(v) <ref type="bibr">3/4</ref> for v ∈ V. Similarly, we can optimize objective function O 1 in Eq. 6, replacing z j and z vn in Eq. 12 with z j and z vn respectively. We further optimize our training process by implementing early stopping for training algorithm using a validation set and assessing the performance at each iteration. Since GLACE initializes node embeddings using encoded node attribute information, it can achieve faster convergence in optimization (in practice we can see that GLACE starts to reach optimization point at T = 100. We will discuss further our method's scalability over LINE in the experiments section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Complexity Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training of GLACE takes</head><formula xml:id="formula_12">O(T × b × (dN + (N + 2) × (Dm + mL + L))) = O(T × b × N × (d + Dm + mL + L)),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our method with state-of-the-art baselines on: link prediction, node classification, inductive learning and visualization. In addition, we demonstrate the scalability and inductiveness of our model. Source code for GLACE is publicly available at https://github.com/bhagya-hettige/GLACE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use six publicly available real-world attributed graphs <ref type="table" target="#tab_0">(Table 1)</ref>. These are citation networks in which nodes denote papers and edges represent citation relations. For each paper, we have TF-IDF vectors of the paper's abstract as attributes. Cora-ML is a subset extracted from the Cora citation network. The larger ACM network is constructed using Aminer data <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Algorithms and Setup</head><p>All the experiments were performed on a MacBook Pro laptop with 16GB memory and a 2.6 GHz Intel Core i7 processor. For each of the following models, we give maximum of 5 hours as a threshold for training. Attributes: for evaluation tasks, we use raw node attributes as input features instead of node embeddings. node2vec <ref type="bibr" target="#b3">[4]</ref>: is a random walk based node embedding method that maximizes the likelihood of preserving nodes' neighbourhood using biased random walks starting from each node. Therefore, node2vec considers only second-order proximity. LINE <ref type="bibr" target="#b12">[13]</ref>: is for large-scale non-attributed graphs and uses first-order and second-order proximity information. Graph-SAGE <ref type="bibr" target="#b5">[6]</ref>: is an inductive learning approach for attributed graphs which learns an embedding function by sampling and aggregating features of local neighbourhoods of nodes. We use the unsupervised version of GraphSAGE with the pooling aggregator (which performed best for citation networks according to <ref type="bibr" target="#b5">[6]</ref>). Since we use node class labels in the node classification task, supervised version of GraphSAGE is not considered in evaluation. Graph2Gauss (G2G) <ref type="bibr" target="#b0">[1]</ref>: produces Gaussian node embeddings using node attributes and graph structure, which introduces a personalized ranking of nodes based on neighbouring hops. G2G is applicable to homogeneous graphs with plain/attributed nodes and (un)directed and unweighted edges.</p><p>We also include a non Gaussian representation model to assess the effectiveness of uncertainty modelling. LACE (without Gaussians): We use a version of our method in which we represent nodes as vectors in an embedding space using node attributes and graph structure. GLACE (with Gaussians): This is the complete version of our method which produces Gaussian distribution representations for graph nodes using node attributes and graph structure.</p><p>For LINE and GLACE, we consider first-order (1 st ), second-order (2 nd ) and a concatenated representation of first-and second-order proximities (1 st + 2 nd ). Accordingly, the concatenated representation would have both local and global information. For all the models, we use 128 as the dimension of the embedding. Since GLACE learns two vectors for mean and variance respectively, we set L = 64 to conduct a fair comparison with other methods, so the number of dimensions learned for each node still remains the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Link Prediction</head><p>For all the methods we extract a test set containing 20% randomly selected edges from the graph and an equal number of non-edges which are not present in the graph. For all datasets we use the same splits for all the methods. The remaining 80% of the edges are used for training the embedding models. In probability distribution based embedding methods (G2G and GLACE) we use negative KL divergence to rank the Gaussian embeddings. For other embedding methods (attributes, node2vec, LINE and LACE), we use dot product similarity of node embedding to ranking. We consider both 1 st -order and 2 nd -order proximity. We also consider joint embedding performance by concatenating the resulting embedding from the two proximity. For LINE, we record the concatenated embedding of the two proximities, which is identified as the best-performing setting for LINE <ref type="bibr" target="#b12">[13]</ref>. AUC and AP scores of link prediction task are shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>A number of important observations can be made from the tables. (1) GLACE clearly outperforms the state-of-art embedding methods by a significant margin in both homogeneous and bipartite graphs. The introduction of uncertainty modelling in GLACE improves performance considerably when compared to models without Gaussian embedding, i.e. node2vec, LINE and LACE. (2) In homogeneous graphs, GLACE (1 st +2 nd ) , which learns from both the explicit edges in the graph and neighbourhood similarities, is the best performing model. (3) G2G shows a very competitive performance to GLACE in smaller graphs (Cora, DBLP and Pubmed) due to its hop-based node ranking scheme, but it does not scale up for large-scale graphs, ACM and Stackoverflow. (4) GLACE's better scalability is also shown, as it is the only attributed graph embedding model that completes the largest dataset, ACM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-class Node Classification</head><p>The node embeddings are obtained using the complete node set from the evaluated models. Similarly to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1]</ref>, we randomly sample different percentages of labeled nodes from the graph for training a logistic regression classifier to predict class label, and use the rest of the nodes for evaluation. The percentages of nodes used for training the classifier for node classification task are 10%, 20%, . . . , 90%. The evaluation metric we report is F1-score, and the results are averaged over 10 trials. We performed this experiment on all the evaluated graphs, and we report the results for Cora-ML, Citeseer, and DBLP datasets in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Based on the results, it can be seen that GLACE again consistently outperforms the baseline methods. This is clearly due to uncertainty modelling of the representations. As can be seen in the figures, there is a clear separation of node classification performance between the methods that consider node  attributes and the methods that do not. An exception to this observation is GraphSAGE, which considers attributes but has a considerably poorer performance than GLACE, G2G and LR. This can be due to its aggregation process which magnifies any error. LACE (without Gaussians) is able to outperform some of the baseline methods, and this is due to the incorporation of node attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Inductive Learning</head><p>We have evaluated the inductive property by training the models with 10% and 50% nodes hidden from the original graph. Then we evaluate how well the models can infer embeddings for unseen nodes on the link prediction task, comparing our model against G2G <ref type="bibr" target="#b0">[1]</ref>, which also takes attributes into account. Although GraphSAGE <ref type="bibr" target="#b5">[6]</ref> is also an inductive node embedding method, it is not applicable in this scenario as it requires unseen nodes to be connected to existing nodes. We perform this task on Cora-ML, Citeseer, Pubmed and ACM graphs. <ref type="table" target="#tab_2">Table 3</ref> summarizes the results.</p><p>As can be seen from the table, GLACE outperforms G2G across all the datasets over the two hidden percentage values. It can also be observed that, GLACE suffers considerably less performance degradation than G2G when more nodes are hidden (i.e. from 10% to 50%). Since G2G requires constructing hops and keeping them in memory, we could not run experiments for G2G (with maximum number of hops to consider &gt; 1) on the ACM dataset, which also demonstrates the scalability advantage of GLACE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Scalability</head><p>LINE is a scalable embedding method for plain graphs. In this study we introduced GLACE as an improved scalable embedding method for attributed graphs.</p><p>In this section we evaluate the efficiency of our method against the large-scale embedding method, LINE, and see how the introduction of attributes and uncertainty modelling assist GLACE in converging faster for optimization. We report the validation AUC for link prediction task in ACM dataset against time. The trend is similar in other datasets. It is worth noting that even though LINE is designed for large-scale graphs, it takes a much longer time to converge ( <ref type="figure" target="#fig_2">Figure  2</ref>). This is due to the fact that the number of iterations required by LINE for convergence is proportional to the number of edges <ref type="bibr" target="#b12">[13]</ref>. On the other hand, taking advantage of node attributes and uncertainty modelling, GLACE achieves convergence substantially faster. GLACE achieves a significant performance boost even after 1 minute of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Visualization</head><p>We evaluate the ability to visualize the Cora-ML citation network. First, each model learns 128-dimensional node embeddings (L = 64 for Gaussians). Then, the dimensions are reduced to 2 dimensions using t-SNE. <ref type="figure" target="#fig_3">Figure 3</ref> shows the visualizations from the models which produced the best layouts. The color of a node (i.e. a paper) represents one of the seven research areas. G2G produces moderately good clustering, but papers belonging to different areas are still not clearly separated. LACE and GLACE learn node embeddings that can clearly separate different classes. GLACE produces the best result in terms of tightly clustered papers of the same area with clearly visible boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present GLACE, an unsupervised learning approach to efficiently learn node embeddings as probability distributions to capture uncertainty of the representations. GLACE learns from both node attributes and graph structural information, and is efficient, scalable and easily generalizable to different types of graphs. GLACE has been evaluated with respect to several state-of-the-art embedding methods in different graph analysis tasks, and the results demonstrate that our method significantly outperforms all the evaluated baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where T is the maximum number of iterations, b is the batch size, d is the maximum node degree, N is the number of negative samples, D is the attribute vector dimension, m is the intermediate vector dimension (hidden layer of Node Attribute Encoder), and L is the embedding dimension. For each edge in the batch, fetching N negative samples takes O(dN ) time. For each of the (N + 2) nodes, i.e., i, j and {v n } vn∈N eg(i) , we compute and update parameters in the Node Attribute Encoder with two levels of transformations (i.e., f enc , f µ and f Σ ) in O(Dm)+O(2mL)+O(2L) = O(Dm+mL+L) time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Node classification performance. Improvements of GLACE are statistically significant for p &lt; 0.01 estimated by a paired t-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>GLACE's faster convergence. Link prediction performance in ACM training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of Cora-ML graph (L = 64).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the real-world graphs.</figDesc><table><row><cell cols="2">Dataset |V1|</cell><cell>|E|</cell><cell>D1 #Labels</cell></row><row><cell cols="4">Cora-ML 2,995 8,416 2,879</cell><cell>7</cell></row><row><cell>Cora</cell><cell cols="3">19,793 65,311 8,710</cell><cell>70</cell></row><row><cell cols="4">Citeseer 4,230 5,358 2,701</cell><cell>6</cell></row><row><cell>DBLP</cell><cell cols="3">17,716 105,734 1,639</cell><cell>4</cell></row><row><cell cols="4">Pubmed 18,230 79,612 500</cell><cell>3</cell></row><row><cell>ACM</cell><cell cols="3">115,772 539,910 124,856</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Link prediction performance. Experiments not completed within threshold settings are marked with "-". st +2 nd ) 97.51 97.40 95.35 95.76 93.82 94.14 89.53 89.85 96.01 95.79 GLACE (1 st ) 98.54 98.46 96.41 96.40 98.48 98.33 97.69 97.42 98.00 97.94 GLACE (2 nd ) 98.43 98.31 97.22 97.20 98.16 97.95 97.02 96.56 97.94 97.79 GLACE (1 st +2 nd ) 98.60 98.52 98.43 98.37 98.55 98.40 97.82 97.49 98.34 98.24</figDesc><table><row><cell>Algorithm</cell><cell>Cora</cell><cell>Citeseer</cell><cell>DBLP</cell><cell>Pubmed</cell><cell cols="2">ACM</cell></row><row><cell></cell><cell cols="6">AUC AP AUC AP AUC AP AUC AP AUC AP</cell></row><row><cell>Attributes</cell><cell cols="4">82.98 77.71 81.53 75.60 75.89 69.56 82.98 77.71</cell><cell>-</cell><cell>-</cell></row><row><cell>node2vec</cell><cell cols="6">87.86 87.19 79.91 82.08 87.03 84.36 88.74 86.58 91.18 91.49</cell></row><row><cell>LINE</cell><cell cols="6">75.23 77.96 71.20 72.11 80.01 83.09 79.97 82.86 75.32 76.81</cell></row><row><cell>GraphSAGE</cell><cell cols="4">85.30 84.72 83.33 85.38 89.63 90.12 89.43 90.90</cell><cell>-</cell><cell>-</cell></row><row><cell>G2G</cell><cell cols="4">97.87 98.03 96.28 96.54 96.35 96.79 95.75 95.65</cell><cell>-</cell><cell>-</cell></row><row><cell>LACE (1 st )</cell><cell cols="6">96.59 96.66 94.21 94.95 91.91 92.68 83.89 84.26 95.14 95.07</cell></row><row><cell>LACE (2 nd )</cell><cell cols="6">96.83 96.67 94.29 94.61 93.30 93.37 93.72 92.80 94.37 93.91</cell></row><row><cell>LACE (1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Link prediction performance with inductive learning. 50%] 87.62 74.64 83.69 70.74 92.18 79.99 93.96 85.33</figDesc><table><row><cell>Algorithm</cell><cell>Cora-ML Citeseer Pubmed</cell><cell cols="2">ACM</cell></row><row><cell cols="4">[hidden %] AUC AP AUC AP AUC AP AUC AP</cell></row><row><cell>G2G [10%]</cell><cell cols="2">88.83 79.34 87.96 80.39 88.96 77.08 -</cell><cell>-</cell></row><row><cell cols="4">GLACE [10%] 93.07 86.72 90.76 85.03 93.00 84.19 95.05 89.09</cell></row><row><cell>G2G [50%]</cell><cell cols="2">57.26 34.70 61.71 43.87 51.22 27.39 -</cell><cell>-</cell></row><row><cell>GLACE [</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep gaussian embedding of attributed graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: Problems, techniques, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph embedding techniques, applications, and performance: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>KBS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attributed social network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepwalk: online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">PTE: predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM SIGKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">LINE: large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Arnetminer: Extraction and mining of academic social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACM KDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Word Representations via Gaussian Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep variational network embedding in wasserstein space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
