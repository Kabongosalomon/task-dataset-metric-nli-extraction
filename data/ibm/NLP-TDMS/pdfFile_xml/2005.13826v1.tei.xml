<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting Few-Shot Learning With Adaptive Margin Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
							<email>weiran.huang@outlook.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Lan</surname></persName>
							<email>x.lan@qmul.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
							<email>li.zhenguo@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<email>wanglw@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting Few-Shot Learning With Adaptive Margin Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot learning (FSL) has attracted increasing attention in recent years but remains challenging, due to the intrinsic difficulty in learning to generalize from a few examples. This paper proposes an adaptive margin principle to improve the generalization ability of metric-based meta-learning approaches for few-shot learning problems. Specifically, we first develop a class-relevant additive margin loss, where semantic similarity between each pair of classes is considered to separate samples in the feature embedding space from similar classes. Further, we incorporate the semantic context among all classes in a sampled training task and develop a task-relevant additive margin loss to better distinguish samples from different classes. Our adaptive margin method can be easily extended to a more realistic generalized FSL setting. Extensive experiments demonstrate that the proposed method can boost the performance of current metric-based meta-learning approaches, under both the standard FSL and generalized FSL settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has achieved great success in various computer vision tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>. However, with a large number of parameters, deep neural networks require large amounts of labeled data for model training. This severely limits their scalability -for many rare classes, it is infeasible to collect a large number of labeled samples. In contrast, humans can recognize an object after seeing it once. Inspired by the few-shot learning ability of humans, there has been an increasing interest in the few-shot learning (FSL) prob- * This work was done when the first author was an intern at Huawei Noah's Ark Lab.  <ref type="figure">Figure 1</ref>. The illustration of the key insight of our adaptive margin loss. In our approach, semantic similarities between different classes (measured in the semantic space of classes) are leveraged to generate adaptive margin between classes. Then, the margin is integrated into the classification loss to make similar classes more separable in the embedding space, which benefits FSL. lem <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>. Given a set of base classes with sufficient labeled samples, and a set of novel classes with only a few labeled samples, FSL aims to learn a classifier for the novel classes by learning a generic knowledge from the base classes.</p><p>Recently, metric-based meta-learning approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref> have shown the superior performance in solving the FSL problem, with attractive simplicity. These approaches usually learn a good embedding space, where samples from the same class are clustered together while samples from different classes are far away from each other. In this way, a new sample from the novel class can be rec-ognized directly through a simple distance metric within the learned embedding space. The success of these metricbased approaches relies on learning a discriminative embedding space.</p><p>To further improve the performance, we introduce the adaptive margin in the embedding space, which helps to separate samples from different classes, especially for similar classes. The key insight of our approach is that the semantic similarity between different classes can be leveraged to generate adaptive margin between classes, i.e., the margin between similar classes should be larger than the one between dissimilar classes (as illustrated in <ref type="figure">Figure 1</ref>). By integrating the adaptive margin into the classification loss, our method learns a more discriminative embedding space with better generalization ability.</p><p>Specifically, we first propose a class-relevant margin generator which produces an adaptive margin for each pair of classes based on their semantic similarity in the semantic space. By combining the margin generated by classrelevant margin generator and the classification loss of FSL approaches, our class-relevant additive margin loss can effectively pull each class away from other classes. Considering the semantic context among a sampled training task in the FSL, we further develop a task-relevant margin generator. By comparing each class with the rest classes among the task in the semantic space, our task-relevant margin generator produces more suitable margin for each pair of classes. By involving these margin penalty, our task-relevant margin loss learns more discriminative embedding space, thus leads to stronger generalization ability to recognize novel class samples. Moreover, our approach can be easily extended to a more realistic yet more challenging FSL setting (i.e., the generalized FSL) where the label space of test data covers both base and novel classes. This is as opposed to the standard FSL setting where the test data contain novel class samples only. Experimental results on the two FSL benchmarks show that our approach significantly improves the performance of current metric-learning-based approaches on both of the two FSL settings.</p><p>In summary, our contributions are three folds: (1) To the best of our knowledge, this is the first work to propose an adaptive margin principle to improve the performance of current metric-based meta-learning approaches for FSL.</p><p>(2) We propose a task-relevant adaptive margin loss to well distinguish samples from different classes in the embedding space according to their semantic similarity, and experimental results demonstrate that our method achieves the stateof-the-art results on the benchmark dataset. (3) Our approach can be easily extended to a more realistic yet more challenging generalized FSL setting, with superior performance obtained. This further validates the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Few-shot Learning</head><p>In recent years, few-shot object recognition has become topical. With the success of deep convolutional neural network (DCNN) based approaches in the data-rich setting <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>, there has been a great of interest in generalizing such deep learning approaches to the few-shot setting. Most of the recent approaches use a meta-learning strategy. With the meta-learning, these models extract transferable knowledge from a set of auxiliary tasks via episodic training. The knowledge then helps to learn the few-shot classifier trained for the novel classes.</p><p>Existing meta-learning based FSL approaches usually learn a model that, given a task (a set of few-shot labeled data and some test query data), produces a classifier that generalizes across all tasks <ref type="bibr" target="#b7">[8]</ref>. A main group of gradientbased meta-learning models attempt to modify the classical gradient-based optimization to adapt to a new episodic task by producing efficient parameter updates <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref>. Recently, many meta-learning approaches attempt to learn an effective metric on the feature space. The intuition is that if a model can determine the similarity of two images, it can classify an unseen test image with a few labeled examples <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>. To learn an effective metric, these methods make their prediction conditioned on distances to a few labeled examples during the training stage <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref>. These examples are sampled from base classes designed to simulate the few-shot scenario. In this paper, we propose a novel generic adaptive margin strategy which can be integrated in existing metric-based meta-learning approaches. Our method can force different classes far from each other in the embedding space. This makes it much easier to recognize novel class samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Margin Loss in Visual Recognition</head><p>Softmax loss has been widely used in training DC-NNs for extracting discriminative visual features for object recognition tasks. By observing that the weights from the last fully connected layer of a classification DCNN trained on the softmax loss bear conceptual similarities with the centers of each class, the works in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref> proposed several margin losses to improve the discriminative power of the trained model. Liu et al. <ref type="bibr" target="#b17">[18]</ref> introduced the important idea of angular margin. However, their loss function required a series of approximations in order to be computed, which resulted in an unstable training of the network. Wang et al. <ref type="bibr" target="#b31">[32]</ref> and Wang et al. <ref type="bibr" target="#b32">[33]</ref> directly add cosine margin to the target logits and achieve better results than <ref type="bibr" target="#b17">[18]</ref>. Deng et al. <ref type="bibr" target="#b3">[4]</ref> proposed an additive angular margin loss to further improve the discriminative power of feature embedding space. Although the aforementioned margin losses have achieved promising results on visual recog-nition tasks, they are not designed for FSL, where limited samples are provided for novel classes. To learn more suitable margin for FSL, we thus propose an adaptive margin principle, where the semantic context among a sampled training task is considered. By training the FSL approach with our adaptive margin loss, the learned model generalizes better across all tasks and thus achieves better recognition results on novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary: Metric-Based Meta-Learning</head><p>In the few-shot learning (FSL), we are given a base class set C base consisting of n base base classes, and for each base class, we have sufficient labeled samples. Meanwhile, we also have a novel class set C novel with n novel novel classes, each of which has only a few labeled samples (e.g., less than 5 samples). The goal of FSL is to obtain a good classifier for the novel classes.</p><p>Meta-learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> is a common approach for the FSL. A standard meta-learning procedure involves two stages: meta-training and meta-testing. In the metatraining stage, we train the model in an episodic manner. In each episode, a small classification task is constructed by sampling a small training set and a small test set from the whole base class dataset, and then it is used to update the model. In the meta-testing stage, the learned model is used to recognize samples from novel classes. Recently, metricbased meta-learning approaches become popular <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b34">35]</ref>. Most metric-based meta-learning approaches generally assume that there exists an embedding space in which samples cluster around a single representation for each class, and then these class representations are used as references to infer labels of test samples. In the following, we introduce the framework of metric-based meta-learning approaches. Meta-Training. In each episode of meta-training, we sample a n t -way n s -shot classification task from the base class dataset. Specifically, we randomly choose n t classes from base class set C base for the episodic training, denoted as C t . We randomly select n s samples from each episodic training class and combine them to form a small training set, which is called support set S. Moreover, we also randomly select some other samples from each episodic training class and combine them to form a small test set, which is called query set Q.</p><p>In the current episode, all samples from both query set and support set are embedded into the embedding space by using an embedding module F. Then, the meta-learner generates class representations r 1 , r 2 , · · · , r nt by using the samples from support set S. For example, Prototypical Networks <ref type="bibr" target="#b26">[27]</ref> generates class representations by averaging the embeddings of support samples by class. After that, the meta-learner uses a metric module D (e.g., cosine similar-ity) to measure the similarity between every query point (x, y) ∈ Q and the current class representations in the embedding space. Based on these similarities, the metalearner incurs a classification loss for each point in the current query set. The meta-learner then back-propagates the gradient of the total loss of all query samples. The classification loss can be formulated as:</p><formula xml:id="formula_0">L cls = − 1 |Q| (x,y)∈Q log e D(F (x),ry) k∈Ct e D(F (x),r k ) ,<label>(1)</label></formula><p>where D(F(x), r k ) denotes the similarity between sample x and the k-th class representation r k predicted by the metalearner. Meta-Testing. In an episode of meta-testing, a novel classification task is similar to a training base classification task. Specifically, the labeled few-shot sample set and unlabeled test examples are used to form the support set and query set, respectively. Then they are fed into the learned model with predicted classification results of query samples as outputs. Different metric-based meta-learning approaches differ in the form of the class representation generation module and metric module, our work introduces different margin loss to improve current metric-based meta-learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Naive Additive Margin Loss</head><p>An intuitive idea to learn a discriminative embedding space is to add a margin between the predicted results of different classes. This helps to increase the inter-class distance in the embedding space and make it easier to recognize test novel samples. To achieve this, we propose a naive additive margin loss (NAML), which can be formulated as:</p><formula xml:id="formula_1">L na = − 1 |Q| (x,y)∈Q log p na (y|x, S),<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">p na (y|x, S) = e D(F (x),ry) e D(F (x),ry) + k∈Ct\{y} e D(F (x),r k )+m .</formula><p>The above naive additive margin loss assumes all classes should be equally far away from each other and thus add a fixed margin among all classes. In this way, this loss forces the embedding module F to extract more separable visual features for samples from different classes, which benefits the FSL. However, the fixed additive margin may lead to mistakes on test samples of similar classes, especially for the FSL where very limited number of labelled samples are provided in the novel classes.  <ref type="figure">Figure 2</ref>. The overview of the proposed approach. Our approach consists of two stages: 1) In each episode of the meta-training stage, we first sample a meta-training task from the base class dataset. Then, the names of classes in the meta-training task are fed into a word embedding model to extract semantic vectors for classes. After that, we propose an adaptive margin generator to produce margin penalty for each pair of classes (e.g., the class relevant margin generator proposed in Section 3.3 or the task relevant margin generator proposed in Section 3.4). Finally, we integrate the margin penalty into the classification loss and thus obtain an adaptive margin loss. A meta-learner consisting of an embedding module and a metric module is trained by minimizing the adaptive margin loss. 2) In the meta-testing stage, with the embedding module and metric module learned in the meta-training stage, we use a simple softmax (without any margin) to predict the labels of test novel samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Class-Relevant Additive Margin Loss</head><p>To better separate similar classes in the feature embedding space, the margin between two classes should be adaptive, i.e., the margin should be larger for similar classes than dissimilar classes. To achieve such adaptive margin in a principled manner, we design a class-relevant additive margin loss (CRAML), where semantic similarities between classes are introduced to adjust the margin.</p><p>Before introducing the class-relevant additive margin loss, we first describe how to measure the semantic similarity between classes in a semantic space. Specifically, we represent each class name using a semantic vector extracted by a word embedding model (e.g., Glove <ref type="bibr" target="#b20">[21]</ref>). As illustrated in <ref type="figure">Figure 2</ref>, we feed a class name, such as wolf or dog, into the word embedding model, and it will embed the class name into the semantic space and return a semantic word vector. Then, we construct a class-relevant margin generator M. For each pair of classes, class i and class j, M uses their semantic word vectors e i and e j as inputs and generates their margin m cr i,j as follows:</p><formula xml:id="formula_3">m cr i,j := M(ei, ej) = α · sim(ei, ej) + β,<label>(3)</label></formula><p>where sim denotes a metric (e.g., cosine similarity) to measure the semantic similarity between classes. We use α and β to denote the scale and bias parameters for the classrelevant margin generator, respectively.</p><p>By introducing the class-relevant margin generator into the classification loss, we obtain a class-relevant additive margin loss as follows. By exploiting the semantic similarity between classes properly, our class-relevant margin loss makes the samples from similar classes to be more separable in the embedding space. The more discriminative embedding space will help better recognize test novel class samples.</p><formula xml:id="formula_4">L cr = − 1 |Q| (x,y)∈Q log p cr (y|x, S),<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Task-Relevant Additive Margin Loss</head><p>So far, we assume that the margin is task-irrelevant. A dynamic task-relevant margin generator, which considers the semantic context among all classes in a meta-training task, should generate more suitable margin between different classes. By comparing each class with other classes among a meta-training task, our task-relevant margin generator can measure the relatively semantic similarity between classes. Thus, the generator will add larger margin for relatively similar classes and smaller margin for relatively dissimilar classes. Therefore, we incorporate the generator into  the classification loss and obtain the task-relevant additive margin loss (TRAML). Specifically, given a class y ∈ C t in a meta-training task, the generator will produce the margins between class y and the other classes C t \ {y} in the task according to their semantic similarities, namely,</p><formula xml:id="formula_5">{m tr y,k } k∈C t \{y} = G {sim(ey, e k )} k∈C t \{y} ,<label>(5)</label></formula><p>where m tr y,k denotes the task-relevant margin between class y and class k, and G denotes the task-relevant margin generator, whose architecture is illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. As shown in this figure, for a query sample (e.g., a dog image) with label y ∈ C t , we first compute the similarities between its semantic vector e y and the semantic vectors of the other classes in the task (e.g., class wolf, sofa and cabinet), respectively. Then, these semantic similarities 1 are fed into the a fully-connected network to generate task-relevant margin for each class pair. By considering the context among all the classes in a meta-training task, our task-relevant margin generator can better measure the similarity among classes, thus generate more suitable margin for each class pair.</p><p>By integrating our task-relevant margin generator into the classification loss, we can obtain a task-relevant additive margin loss given in Equation 6 and the outline of computing task-relevant additive margin loss for a training episode is given in Algorithm 1.</p><formula xml:id="formula_6">L tr = − 1 |Q| (x,y)∈Q log p tr (y|x, S),<label>(6)</label></formula><p>where p tr (y|x, S) = e D(F(x),ry) In a test episode, with the learned embedding module and metric module, we use the simple softmax function (without any margin) to predict the label of unlabeled data, i.e., we don't need to use semantic vectors of novel classes during the test stage, which makes our model flexible for any novel class. <ref type="bibr" target="#b0">1</ref> The order of input similarities has little impact on the performance. Algorithm 1 Task-relevant additive margin loss computation for a training episode in few-shot learning Input: Base class set C base , task-relevant generator G. Output: Task-relevant additive margin loss L tr . 1: Randomly sample n t classes from base class set C base to form an episodic training class set C t ; 2: Randomly sample n s images per class in C t to form a support set S; 3: Randomly sample n q images per class in C t to form a query set Q; 4: Obtain the semantic vector for each class in C t by feeding its class name into a word embedding model; <ref type="bibr">5:</ref> For each query sample, compute the task-relevant margins between its class y and the classes in C t \ {y} by using task-relevant margin generator G according to Equation 5; 6: Compute the task-relevant additive margin loss L tr according to Equation 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Extension to Generalized Few-Shot Learning</head><p>Although the proposed approach is originally designed for the standard FSL, it can be easily extended to the generalized FSL: simply including test data from both base and novel classes, and their labels are predicted from all classes in both base and novel class set in the test stage. This setting is much more challenging and realistic than the standard FSL, where test data are from only novel classes. Note that, our adaptive margin loss is flexible for the generalized FSL: the embedding module and the metric module trained by the adaptive loss with all training samples from base classes can be directly used for label inference of test samples from the disjoint space of both base and novel classes. Experimental results show that our method can improve the state-of-theart alternative and create a new state-of-the-art for metricbased meta-learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Discussions</head><p>In this section, we evaluate our approach by conducting three groups of experiments: 1) standard FSL setting where  <ref type="bibr" target="#b26">[27]</ref>, i.e., four stacked convolutions layers of 64 filters; 'ResNet12' -the feature embedding module as in <ref type="bibr" target="#b19">[20]</ref>, i.e., ResNet12 architecture containing four residual blocks of three stacked 3 × 3 convolutional layers; 'Metric' -metric-based meta-learning approaches for FSL; 'Gradient' -gradient-based meta-learning approaches for FSL.</p><p>the label space of test data is restricted to a few novel classes at each test episode, 2) generalized FSL setting where the label space of test data is extended to both base classes and novel classes, and 3) further evaluation including ablation study and comparison with other margin losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Standard Few-Shot Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets and Settings</head><p>Under the standard FSL setting <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>, we evaluate our approach on the most popular benchmark, i.e., miniImageNet. It contains 100 classes randomly selected from ImageNet <ref type="bibr" target="#b25">[26]</ref> and each class contains 600 images with resolution of 84 × 84. Following the widely used setting in prior works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>, we take 64 classes for training, 16 for validation and 20 for testing. In the training stage, the 64 training classes and 16 validation classes are respectively regarded as base classes and novel classes to decide the model hyperparameters. Following the standard setting adopted by most existing few-shot learning works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>, we conduct 5-way 1-shot/5-shot classification on the miniIma-geNet dataset. In 1-shot and 5-shot scenarios, each query set has 15 images per class, while each support set contains 1 and 5 image(s) per class, respectively. For a training episode, images in the support sets and query sets are randomly selected from the base class set. In a test episode, images in the support sets and the query sets are randomly selected from the novel class set. The evaluation metric for the miniImageNet dataset is defined as the top-1 classification accuracy on randomly selected 600 test episodes. We test our task-relevant additive margin loss with two backbone metric-based meta learning approaches: Prototypical Networks <ref type="bibr" target="#b26">[27]</ref> and its most recent improvement AM3 (Prototypical Networks) <ref type="bibr" target="#b34">[35]</ref> which are the state-of-the-art metricbased meta learning methods for FSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Implementation Details</head><p>Our feature embedding module mirrors the ResNet12 architecture used by <ref type="bibr" target="#b19">[20]</ref>, which consists of four residual blocks. Each block comprises three stacked 3 × 3 convolutional layers. Each block is followed by max pooling. We use the same feature extractor on images in both the support set and query set. The fully-connected network in the relation module consists of two fully-connected layers, each followed by a batch normalization layer and a ReLU nonlinearity layer. The word embedding model we used in this paper is Glove <ref type="bibr" target="#b20">[21]</ref>.  <ref type="table">Table 1</ref> provides comparative results for FSL on the mini-ImageNet dataset. We can observe that: 1) our approach significantly improve the performance of baseline models (i.e., Prototypical Network <ref type="bibr" target="#b26">[27]</ref> and AM3 (Prototypical Networks <ref type="bibr" target="#b34">[35]</ref>). This indicates that the proposed taskrelevant additive margin loss can boost performance of metric-based meta-learning approaches very effectively. 2) Our approach clearly outperforms the state-of-the-art FSL model on both 5-way 1-shot and 5-way 5-shot settings, thanks to the discriminative feature embedding learned by the proposed task-relevant additive margin loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generalized Few-Shot Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Dataset and Settings</head><p>To further evaluate the effectiveness of our approach, we test our approach in a more challenging yet practical generalized FSL setting, where the label space of test data is extended to both base and novel classes. Following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>, we conduct experiment on the large-scale ImageNet2012 dataset. This benchmark splits the 1000 ImageNet classes into 389 base classes and 611 novel classes; 193 of the base classes and 300 of the novel classes are used for cross validation and the remaining 196 base classes and 311 novel classes are used for the final evaluation (for more details we refer to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref>). As in <ref type="bibr" target="#b7">[8]</ref>, the embedding module we used is ResNet10 network that gets as input images of 224 × 224 resolution. We compare our model with several generalized FSL alternatives: Matching Networks <ref type="bibr" target="#b30">[31]</ref>, Prototypical Networks <ref type="bibr" target="#b26">[27]</ref>, Logistic Regression <ref type="bibr" target="#b33">[34]</ref>, Batch Squared Gradient Magnitude <ref type="bibr" target="#b8">[9]</ref>, Squared Gradient Magnitude With Hallucination <ref type="bibr" target="#b8">[9]</ref>, Prototype Matching Nets <ref type="bibr" target="#b33">[34]</ref>, and Dynamic FSL <ref type="bibr" target="#b7">[8]</ref>.</p><p>We implement our task-relevant additive margin loss on the state-of-the-art model (i.e., Dynamic FSL <ref type="bibr" target="#b7">[8]</ref>). Following <ref type="bibr" target="#b33">[34]</ref>, we first train the embedding module (i.e., ResNet10) by using our task-relevant additive margin loss with all base classes. Then we extract features for all training samples with the learned embedding module and save them to disk. The weight generator in Dynamic FSL <ref type="bibr" target="#b7">[8]</ref> will use these pre-computed features as inputs. Finally, we train the weight generator by replacing the original classification loss with our task-relevant additive margin loss. The evaluation metric is the top-5 accuracy on the novel classes and on all classes. We repeat the above experiment 5 times (sampling each time a different set of training images for the novel classes) and report the mean accuracy. <ref type="table" target="#tab_2">Table 2</ref> provides the comparative results of generalized FSL on the large-scale ImageNet2012 dataset. We can observe that: 1) our approach achieves the best results on all evaluation metrics. This indicates that, with the discriminative embedding space learned by our task-relevant additive margin loss, our approach has the strongest generalization ability under this more challenging setting. 2) Our approach yields consist performance improvement over the state-of-the-art generalized FSL model (i.e., Dynamic FSL [8]) on the 1shot, 2-shot, 5-shot, 10-shot, and 20-shot settings. This further validates the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Ablation Study on Key Components</head><p>We compare our full model with a number of stripped down versions to evaluate the effectiveness of the key components of our approach. Specifically, three of such loss are compared, each of which uses the AM3 (Prototypical Networks) <ref type="bibr" target="#b34">[35]</ref>   <ref type="table">Table 3</ref>. Ablation study for FSL on the miniImageNet dataset under the standard FSL setting. The evaluation metric is the same as in <ref type="table">Table 1</ref>.</p><p>-model training using the softmax loss provided in <ref type="bibr" target="#b34">[35]</ref>; 'Naive Additive Margin Loss' -model training by the loss proposed in Section 3.2; 'Class-Relevant Additive Margin Loss' -model training by the loss proposed in Section 3.3. <ref type="table">Table 3</ref> presents the comparative results of the above losses on the miniImageNet dataset under the standard FSL setting. It can be observed that: 1) Training metric-based meta-learning approaches with our adaptive margin loss leads to significant improvements (see Our Full Model vs. Original Classification Loss). This provides strong supports for our main contribution on embedding learning for FSL.</p><p>2) The model trained by the proposed naive additive margin loss shows slight performance improvement over the model trained by original classification loss. This means that simply adding a fixed margin into the classification loss has limited effectiveness in FSL. 3) Thanks to the adaptive margin produced by the class-relevant margin generator, our classrelevant margin additive loss is shown to benefit the embedding learning for FSL (see Class-Relevant Additive Margin Loss vs. Naive Additive Margin Loss). 4) By considering the semantic context among classes in a meta-training task, our task-relevant additive margin loss yields better results than the class-relevant margin loss. Moreover, we observe that the learned coefficient α in Eq. (3) is positive, which verifies our intuition that the margin between similar classes should be larger than the one between dissimilar classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with Other Margin Losses</head><p>To validate the effectiveness of the proposed adaptive margin loss, we compare our approach with two margin losses which are widely used in face recognition. Each of them uses the AM3 (Prototypical Networks) <ref type="bibr" target="#b34">[35]</ref> as the baseline model and differs only in which loss is used to train the model. The two margin losses are: 1) Additive angular margin loss <ref type="bibr" target="#b3">[4]</ref>, which add an additive angular margin to the angle between the weight vector and feature embeddings. 2) Additive cosine margin loss <ref type="bibr" target="#b32">[33]</ref>, which directly adds a cosine margin to the target logits. Note that, both of these two methods add margin penalty to the target logits computed by the dot product between feature embeddings and weight vectors. This is different from Prototypical Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Test Accuracy 5-way 1-shot 5-way 5-shot Additive angular margin loss <ref type="bibr" target="#b3">[4]</ref>  67.10 ± 0.52 79.54 ± 0.60 <ref type="table">Table 4</ref>. Comparative classification accuracies (%) of two other margin losses on the miniImageNet dataset under the standard FSL setting. Notations: 'Our Full Model (cosine)' -implementing our task-relevant additive margin loss on AM3 (Prototypical Network) <ref type="bibr" target="#b34">[35]</ref> with cosine distance as metric in the embedding space; 'Our Full Model (euclidean)' -implementing our task-relevant additive margin loss on AM3 (Prototypical Network) <ref type="bibr" target="#b34">[35]</ref> with euclidean distance as metric in the embedding space. and its variants, which use the opposite of the euclidean distances between class representations and feature embedding as the logits. For fair comparison, we replace the opposite of euclidean metric used in AM3 (Prototypical Network) <ref type="bibr" target="#b34">[35]</ref> with the cosine distance, and train the AM3 model with our task-relevant margin loss (the model is denoted by 'Our Full Model (cosine)' in <ref type="table">Table 4</ref>) . <ref type="table">Table 4</ref> presents the comparative results of the two margin losses and our losses on the miniImageNet dataset under the standard FSL setting. We can observe that our method is shown to be more effective than the two competitors. It can be expected that, our method is designed for the FSL problem. That is, our method involves semantic similarity among classes in meta-training task to learn a more suitable margin penalty, compared with a fixed one generated by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref>. The suitable margin of each pair of classes helps to learn more discriminative embedding space and thus better distinguish samples from different novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an adaptive margin principle, which can effectively enhance the discriminative power of embedding space for few-shot image recognition. We first develop a class-relevant additive margin loss which combines the standard classification loss with an adaptive margin generator based semantic similarity between classes. Then, by considering the semantic context among classes in a meta-training task, a task-relevant additive margin loss is further proposed to learn more discriminiative embbeding space for FSL. Furthermore, we also extend the proposed model to the more realistic generalized FSL setting. Experimental results demonstrate that our method is effective under both of the two FSL settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where p cr (y|x, S) = e D(F (x),ry) e D(F (x),ry) + k∈Ct\{y} e D(F (x),r k ))+m cr y,k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The illustration of the architecture of our task-relevant margin generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>e</head><label></label><figDesc>D(F (x),ry) + k∈Ct\{y} e D(F (x),r k )+m tr y,k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparative results for generalized FSL on the ImageNet2012 dataset. The top-5 accuracies (%) on the novel classes and on all classes are used as the evaluation metrics for this dataset. Methods with w/ H use mechanisms that hallucinate extra training examples for the novel classes.</figDesc><table><row><cell>Model</cell><cell>ns=1</cell><cell>2</cell><cell>Novel 5</cell><cell>10</cell><cell>20</cell><cell>ns=1</cell><cell>2</cell><cell>All 5</cell><cell>10</cell><cell>20</cell></row><row><cell>Logistic regression (from [34])</cell><cell>38.4</cell><cell>51.1</cell><cell>64.8</cell><cell>71.6</cell><cell>76.6</cell><cell>40.8</cell><cell>49.9</cell><cell>64.2</cell><cell>71.9</cell><cell>76.9</cell></row><row><cell>Logistic regression w/H (from [9])</cell><cell>40.7</cell><cell>50.8</cell><cell>62.0</cell><cell>69.3</cell><cell>76.5</cell><cell>52.2</cell><cell>59.4</cell><cell>67.6</cell><cell>72.8</cell><cell>76.9</cell></row><row><cell>Prototypical Network [27] (from [34])</cell><cell>39.3</cell><cell>54.4</cell><cell>66.3</cell><cell>71.2</cell><cell>73.9</cell><cell>49.5</cell><cell>61.0</cell><cell>69.7</cell><cell>72.9</cell><cell>74.6</cell></row><row><cell>Matching Networks [31] (from [34])</cell><cell>43.6</cell><cell>54.0</cell><cell>66.0</cell><cell>72.5</cell><cell>76.9</cell><cell>54.4</cell><cell>61.0</cell><cell>69.0</cell><cell>73.7</cell><cell>76.5</cell></row><row><cell>Squared Gradient Magnitude w/H [9]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.3</cell><cell>62.1</cell><cell>71.3</cell><cell>75.8</cell><cell>78.1</cell></row><row><cell>Batch Squared Gradient Magnitude [9]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.3</cell><cell>60.5</cell><cell>71.4</cell><cell>75.8</cell><cell>78.5</cell></row><row><cell>Prototype Matching Nets [34]</cell><cell>43.3</cell><cell>55.7</cell><cell>68.4</cell><cell>74.0</cell><cell>77.0</cell><cell>55.8</cell><cell>63.1</cell><cell>71.1</cell><cell>75.0</cell><cell>77.1</cell></row><row><cell>Prototype Matching Nets w/H [34]</cell><cell>45.8</cell><cell>57.8</cell><cell>69.0</cell><cell>74.3</cell><cell>77.4</cell><cell>57.6</cell><cell>64.7</cell><cell>71.9</cell><cell>75.2</cell><cell>77.5</cell></row><row><cell>Dynamic FSL [8]</cell><cell>46.0</cell><cell>57.5</cell><cell>69.2</cell><cell>74.8</cell><cell>78.1</cell><cell>58.2</cell><cell>65.2</cell><cell>72.2</cell><cell>76.5</cell><cell>78.7</cell></row><row><cell>Dynamic FSL + TRAML (OURS)</cell><cell>48.1</cell><cell>59.2</cell><cell>70.3</cell><cell>76.4</cell><cell>79.4</cell><cell>59.2</cell><cell>66.2</cell><cell>73.6</cell><cell>77.3</cell><cell>80.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>as the baseline model and differs only in which loss is used to train the model: 'Original Classification Loss' ± 0.49 75.20 ± 0.36 Naive Additive Margin Loss 65.42 ± 0.25 75.48 ± 0.34 Class-Relevant Additive Margin Loss 66.36 ± 0.57 77.21 ± 0.48 Our Full Model 67.10 ± 0.52 79.54 ± 0.60</figDesc><table><row><cell>Model</cell><cell>Test Accuracy 5-way 1-shot 5-way 5-shot</cell></row><row><cell>Original Classification Loss</cell><cell>65.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>66.21±0.46 77.30 ±0.71 Additive cosine margin loss [33] 65.96 ±0.56 76.93 ±0.49 Our Full Model (cosine) 66.92 ± 0.43 79.08 ± 0.52 Our Full Model (euclidean)</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work is supported by National Key R&amp;D Program of China (2018YFB1402600), BJNSF (L172037) and Beijing Acedemy of Artificial Intelligence.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to train your maml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Memory matching networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4080" to="4088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Low-shot learning with large-scale diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3037" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to learn with conditional class dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farshid</forename><surname>Varno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale few-shot learning: Knowledge transferwith class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7212" to="7220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9715" to="9724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9715" to="9724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Metasgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09833</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dense classification and implanting for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lifchitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvaine</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9258" to="9267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6738" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08479</idno>
		<title level="m">Lcc: Learning to customize and combine neural networks for few-shot learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodrguez Lpez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-learning with implicit gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Few-shot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin Ravi Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr Tao Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Martial Hebert, and Bharath Hariharan. Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive cross-modal few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sung Whan Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moon</surname></persName>
		</author>
		<title level="m">Tapnet: Neural network augmented with task-adaptive projection for few-shot learning. In ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7115" to="7123" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
