<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
							<email>chunliang@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Cloud AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
							<email>kihyuks@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Cloud AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
							<email>jinsungyoon@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Cloud AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
							<email>tpfister@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Cloud AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim at constructing a high performance model for defect detection that detects unknown anomalous patterns of an image without anomalous data. To this end, we propose a two-stage framework for building anomaly detectors using normal training data only. We first learn self-supervised deep representations and then build a generative one-class classifier on learned representations. We learn representations by classifying normal data from the CutPaste, a simple data augmentation strategy that cuts an image patch and pastes at a random location of a large image. Our empirical study on MVTec anomaly detection dataset demonstrates the proposed algorithm is general to be able to detect various types of real-world defects. We bring the improvement upon previous arts by 3.1 AUCs when learning representations from scratch. By transfer learning on pretrained representations on ImageNet, we achieve a new state-of-theart 96.6 AUC. Lastly, we extend the framework to learn and extract representations from patches to allow localizing defective areas without annotations during training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anomaly detection aims to detect an instance containing anomalous and defective patterns that are different from those seen in normal instances. Many problems from different vision applications are anomaly detection, including manufacturing defect detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref>, medical image analysis <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b47">48]</ref>, and video surveillance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b52">53]</ref>. Unlike a typical supervised classification problem, anomaly detection faces unique challenges. First, due to the nature of the problem, it is difficult to obtain a large amount of anomalous data, either labeled or unlabeled. Second, the difference between normal and anomalous patterns are often finegrained as defective areas might be small and subtle in highresolution images.</p><p>Due to limited access to anomalous data, constructing an anomaly detector is often conducted under semi-supervised or one-class classification settings using normal data only. * Equal contributions.</p><p>Since the distribution of anomaly patterns is unknown in advance, we train models to learn patterns of normal instances and determine anomaly if the test example is not represented well by these models. For example, an autoencoder that is trained to reconstruct normal data is used to declare anomalies when the data reconstruction error is high. Generative models declare anomalies when the probability density is below a certain threshold. However, the anomaly score defined as an aggregation of pixel-wise reconstruction error or probability densities lacks to capture a high-level semantic information <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b36">37]</ref>. Alternative methods using high-level learned representations have shown more effective for anomaly detection. For example, deep one-class classifier <ref type="bibr" target="#b45">[46]</ref> demonstrates an effective end-to-end trained one-class classifiers parameterized by deep neural networks. It outperforms its shallow counterparts, such as one-class SVMs <ref type="bibr" target="#b48">[49]</ref> and reconstruction-based approaches such as autoencoders <ref type="bibr" target="#b33">[34]</ref>. In self-supervised representation learning, predicting geometric transformations of an image <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4]</ref>, such as rotation or translation, and contrastive learning <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b51">52]</ref> have shown to be successful in distinguishing normal data from outliers. However, most existing works focus on detecting semantic outliers (e.g., visual objects from different classes) from object-centric natural images. In Section 4.1, we show these methods do not generalize well in detecting fine-grained anomalous patterns as in defect detection.</p><p>In this work, we tackle a one-class defect detection problem, a special case of image anomaly detection, where various forms of unknown anomalous patterns present locally in the high-resolution images. We follow the two-stage framework <ref type="bibr" target="#b51">[52]</ref>, where we first learn self-supervised representations by solving a proxy task, then build a generative oneclass classifier on learned representations to distinguish data with anomalous patterns from normal ones. Our innovation is at designing a novel proxy task for self-supervised learning of representations. Specifically, we formulate a proxy classification task between normal training data and the ones augmented by the CutPaste, the proposed data augmentation strategy that cuts an image patch and pastes at a random location of an image. CutPaste augmentation is mo- <ref type="figure">Figure 1</ref>: An overview of our method for anomaly detection and localization. (a) A deep network (CNN) is trained to distinguish images from normal (blue) and augmented (green) data distributions by CutPaste (orange dotted box), which cuts a small rectangular region (yellow dotted box) from normal data and pastes it at random location. Representations are trained either from the whole image or local patches. (b, top) An image-level representation makes a holistic decision for anomaly detection and is used to localize defect via GradCAM <ref type="bibr" target="#b50">[51]</ref>. (b, bottom) A patch-level representation extracts dense features from local patches to produce anomaly score map, which is then max-pooled for detection or upsampled for localization <ref type="bibr" target="#b31">[32]</ref>. tivated to produce a spatial irregularity to serve as a coarse approximation of real defects, which we have no access at training. Rectangular patches of different sizes, aspect ratios, and rotation angles are pasted to generate diverse augmentations. Although CutPaste augmented samples (Figure 2(e)) are easily distinguishable from real defects and thus might be a crude approximation of a real anomaly distribution, we show that representations learned by detecting irregularity introduced by CutPaste augmentations generalize well on detecting real defects.</p><p>We evaluate our methods on MVTec anomaly detection dataset <ref type="bibr" target="#b4">[5]</ref>, a real-world industrial visual inspection benchmark. By learning deep representations from scratch, we achieve 95.2 AUC on image-level anomaly detection, which outperforms existing works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b60">61]</ref> by at least 3.1 AUC. Moreover, we report state-of-the-art 96.6 image-level AUC by transfer learning from an ImageNet pretrained model. Moreover, we explain how learned representations could be used to localize the defective areas in high-resolution images. Without using any anomaly data, a simple patch model extension can achieve 96.0 pixel-level localization AUC, which improves upon previous state-of-the-art <ref type="bibr" target="#b60">[61]</ref> (95.7 AUC). We conduct an extensive study using different types of augmentation and proxy tasks to show the effectiveness of CutPaste augmentations for self-supervised representation learning on unknown defect detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A Framework for Anomaly Detection</head><p>In this section, we present our anomaly detection framework for high-resolution image with defects in local regions. Following <ref type="bibr" target="#b53">[54]</ref>, we adopt a two-stage framework for building an anomaly detector, where in the first stage we learn deep representations from normal data and then construct an one-class classifier using learned representations. Subsequently, in Section 2.1, we present a novel method for learning self-supervised representations by predicting Cut-Paste augmentation, and extend to learning and extracting representations from local patches in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Self-Supervised Learning with CutPaste</head><p>Defining good pretext tasks is essential for selfsupervised representation learning. While popular methods including rotation prediction <ref type="bibr" target="#b18">[19]</ref> and contrastive learning <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b11">12]</ref> have been studied in the context of semantic one-class classification <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b51">52]</ref>, our study in Section 4.1 shows that naively applying existing methods, such as rotation prediction or contrastive learning, is sub-optimal for detecting local defects as we will show in Section 4.1.</p><p>We conjecture that geometric transformations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4]</ref>, such as rotations and translations, are effective in learning representation of semantic concepts (e.g., objectness), but less of regularity (e.g., continuity, repetition). As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), anomalous patterns of defect detection typically include irregularities such as cracks (bottle, wood) or twists (toothbrush, grid). Our aim is to design an augmentation strategy creating local irregular patterns. Then we train the model to identify these local irregularity with the hope that it can generalize to unseen real defects at test time.</p><p>A popular augmentation method that could create a local irregularity in image is Cutout <ref type="bibr" target="#b17">[18]</ref>  <ref type="figure" target="#fig_1">(Figure 2(c)</ref>), which wipes out a randomly selected small rectangular area of an image. Cutout is found to be a useful data augmenta-  <ref type="figure" target="#fig_1">Figure 2</ref>: Visualization of (a, green) normal, (b, red) anomaly, and (c-h, blue) augmented normal samples from bottle, toothbrush, screw, grid, and wood classes of MVTec anomaly detection dataset <ref type="bibr" target="#b4">[5]</ref>. Augmented normal samples are generated by baseline augmentations including (c) Cutout and (d) Scar, and our proposed (e) CutPaste and (f) CutPaste (Scar). We use red arrows in (f) to highlight the pasted patch of scar shape, a thin rectangle with rotation.</p><p>tion that enforces invariance, leading to improved accuracy on multi-class classification tasks. In contrast, we start by discriminating Cutout images from the normal ones. At first glance, the task seems easy to solve by well-crafted low level image filters. Surprisingly, as we will show in Section 4, without the hindsight of knowing this, a deep convolution network does not learn these shortcuts. Using Cutout in the algorithm design for defect detection can also be found in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b56">57]</ref>. We can make the task harder by randomly choosing colors and the scale as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(d) to avoid naive shortcut solutions. To further prevent learning naive decision rules for discriminating augmented images and encouraging the model to learn to detect irregularity, we propose the CutPaste augmentation as follows:</p><p>1. Cut a small rectangular area of variable sizes and aspect ratios from a normal training image.</p><p>2. Optionally, we rotate or jitter pixel values in the patch.</p><p>3. Paste a patch back to an image at a random location.</p><p>We show the CutPaste augmentation process in the orange dotted box of <ref type="figure">Figure 1</ref> and more examples in <ref type="figure" target="#fig_1">Figure 2</ref>(e). Following the idea of rotation prediction <ref type="bibr" target="#b18">[19]</ref>, we define the training objective of the proposed self-supervised representation learning as follows:</p><formula xml:id="formula_0">L CP = E x∈X CE(g(x), 0) + CE(g(CP(x)), 1)<label>(1)</label></formula><p>where X is the set of normal data, CP(·) is a CutPaste augmentation and g is a binary classifier parameterized by deep networks. CE(·, ·) refers to a cross-entropy loss. In practice, data augmentations, such as translation or color jitter, are applied before feeding x into g or CP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">CutPaste Variants</head><p>CutPaste-Scar. A special case of Cutout called "scar" using a long-thin rectangular box of random color, as in <ref type="figure">Fig</ref> Multi-Class Classification. While CutPaste (large patch) and CutPaste-Scar share a similarity, the shapes of an image patch of two augmentations are very different. Empirically, they have their own advantages on different types of defects. To leverage the strength of both scales in the training, we formulate a finer-grained 3-way classification task among normal, CutPaste and CutPaste-Scar by treating Cut-Paste variants as two separate classes. Detailed study will be presented in Section 5.2.</p><p>Similarity between CutPaste and real defects. The success of CutPaste may be understood from outlier exposure <ref type="bibr" target="#b22">[23]</ref>, where we generate the pseudo anomalies (Cut-Paste) during the training. Apart from using natural images as in <ref type="bibr" target="#b22">[23]</ref>  <ref type="figure">Figure 3</ref>: t-SNE visualization of representations of models trained with 3-way CutPaste prediction task. We plot embeddings of normal (blue), anomaly (red), and augmented normal by CutPaste ("Patch", green) and CutPaste-scar ("Scar", yellow).</p><p>from the same domain), which is more challenging for the model to learn to find this irregularity.</p><p>On the other hand, CutPaste does look similar to some real defects. A natural question is if the success of Cut-Paste is from a good mimic of real defects. In <ref type="figure">Figure 3</ref>, we show the t-SNE plots of the representations from the trained model. Clearly, the CutPaste examples are almost not overlapped with real defect examples (anomaly), but the learned representation is able to distinguish between normal example, different CutPaste augmented samples and real defects. It suggests (1) CutPaste is still not a perfect simulation of real defects and (2) learning on it to find irregularity generalizes well on unseen anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Computing Anomaly Score</head><p>There exist various ways to compute anomaly scores via one-class classifiers. In this work, we build generative classifiers like kernel density estimator <ref type="bibr" target="#b51">[52]</ref> or Gaussian density estimator <ref type="bibr" target="#b42">[43]</ref>, on representations f . Below, we explain how to compute anomaly scores and the trade-offs.</p><p>Although nonparametric KDE is free from distribution assumptions, it requires many examples for accurate estimation <ref type="bibr" target="#b57">[58]</ref> and could be computationally expensive. With limited normal training examples for defect detection, we consider a simple parametric Gaussian density estimator (GDE) whose log-density is computed as follows:</p><formula xml:id="formula_1">log p gde (x) ∝ − 1 2 (f (x) − µ) Σ −1 (f (x) − µ)<label>(2)</label></formula><p>where µ and Σ are learned from normal training data. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Localization with Patch Representation</head><p>While we present a method for learning a holistic representation of an image, learning a representation of an image patch would be preferred if we want to localize defective regions <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b60">61]</ref> in addition to image-level detection. By learning and extracting representations from an image patch, we can build an anomaly detector that is able to compute the score of an image patch, which then can be used to localize the defective area.</p><p>CutPaste prediction is readily applicable to learn a patch representation -all we need to do at training is to crop a patch before applying CutPaste augmentation. Similar to Equation <ref type="formula" target="#formula_0">(1)</ref>, the training objective can be written as:</p><formula xml:id="formula_2">E x∈X CE(g(c(x)), 0) + CE(g(CP(c(x))), 1)<label>(3)</label></formula><p>where c(x) crops a patch at random location of x. At test time, we extract embeddings from all patches with a given stride. For each patch, we evaluate its anomaly score and use a Gaussian smoothing to propagate the score to every pixel <ref type="bibr" target="#b31">[32]</ref>. In Section 4.2, we visualize a heatmap using patch-level detector for defect localization, along with that of an image-level detector using visual explanation techniques such as GradCAM <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Anomaly detection under one-class classification setting, where we assume only the normal data is given during the training, has been widely studied <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b26">27]</ref>. Recent success of self-supervised learning in computer vision <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref> has also been demonstrated effective for one-class classification and anomaly detection. One major family is by predicting geometric transformations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4]</ref>, such as rotation, translation or flips. The other family includes variants of contrastive learning with geometric augmentations <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b51">52]</ref>. However, the success has been limited to semantic anomaly detection benchmarks, such as CIFAR-10 <ref type="bibr" target="#b27">[28]</ref> or ImageNet <ref type="bibr" target="#b16">[17]</ref>, and as we show in Section 4.1, methods relying on geometric transformations perform poorly on defect detection benchmarks.</p><p>Because of practical applications, such as industrial inspection or medical diagnosis, defect detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref> has received lots of attention. The initial steps have been taken with methods including autoencoding <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b58">59]</ref>, generative adversarial networks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b2">3]</ref>, using pretrained models on ImageNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, and self-supervised learning by solving different proxy tasks with augmentations <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b14">15]</ref>. The proposed CutPaste prediction task is not only shown to have strong performance on defect detection, but also amenable to combine with existing methods, such as transfer learning from pretrained models for better performance or patch-based models for more accurate localization, which we demonstrate in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Relation to Other Augmentations</head><p>Although Cutout <ref type="bibr" target="#b17">[18]</ref> and RandomErasing <ref type="bibr" target="#b64">[65]</ref> are similar to CutPaste, they create irregularities by a small rectangular region filled with either zero or uniformly sampled pixel values instead of a structural image patch as CutPaste. Moreover, unlike typical use of augmentations for learning invariant representations, we learn a representation that is discriminative to these augmentations.</p><p>Scar augmentation <ref type="bibr" target="#b15">[16]</ref>  <ref type="figure" target="#fig_1">(Figure 2(d)</ref>) is a special case of Cutout, which uses a long-thin rectangle with random colors. While it demonstrates strong performance, we show that CutPaste with the same scale <ref type="figure" target="#fig_1">(Figure 2</ref>(f)), which fills a long-thin rectangle by a patch from the same image, improves upon representations trained by predicting Cutout.</p><p>CutMix <ref type="bibr" target="#b61">[62]</ref>, which extracts a rectangular image patch from an image and pastes at random location of another image, is related to CutPaste in terms of pasting operations. One main difference is CutMix leverages existing image labels with MixUp <ref type="bibr" target="#b63">[64]</ref> in the objective while CutPaste prediction is a self-supervised learning without the need of image labels. The other difference is CutMix studies standard supervised tasks, while we aim for one-class classification.</p><p>[11] presents a denoising autoencoder with patch-swap augmentation as noise process. <ref type="bibr" target="#b25">[26]</ref> proposes to learn representations by predicting local augmentations using GAN. Our method is simpler (e.g., no need to train decoder or GAN) while highly performant, thus more practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct most experiments on MVTec Anomaly Detection dataset <ref type="bibr" target="#b4">[5]</ref> that contains 10 object and 5 texture categories for anomaly detection. The dataset is composed of normal images for training and both normal and anomaly images with various types of defect for testing. It also provides pixel-level annotations for defective test images. The dataset is relatively small scale in number of images, where the number of training images varies from 60 to 391, posing a unique challenge for learning deep representations.</p><p>We follow one-class classification protocol, also known as semi-supervised anomaly detection <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b1">2</ref> where we train a one-class classifier for each category on its respective normal training examples. Following <ref type="bibr" target="#b51">[52]</ref>, we learn the representations by augmentation prediction from scratch with ResNet-18 <ref type="bibr" target="#b21">[22]</ref> plus an MLP projection head on top of average pooling layer followed by the last linear layer. We construct a Gaussian density estimation (GDE) as Equation <ref type="formula" target="#formula_1">(2)</ref> for anomaly detector based on the top pooled features.</p><p>We train a model on 256×256 image. We note that the same training strategy, such as the selection of hyperparameters or data augmentations, is applied to all categories. Detailed settings of training can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Main Results</head><p>We report the anomaly detection performance in <ref type="table">Table 1</ref>.</p><p>We run experiments 5 times with different random seeds and report the mean AUC and standard error for each category. We also report the average of mean and standard errors for texture, object, and all categories.</p><p>We test representations trained with different proxy tasks of self-supervised learning, including baselines such as rotation <ref type="bibr" target="#b19">[20]</ref>, Cutout or scar predictions, the proposed Cut-Paste, CutPaste-Scar predictions, and using both with 3-way classification. We also compare with previous works, including deep one-class classifier (DOCC) <ref type="bibr" target="#b44">[45]</ref>, uninformed student <ref type="bibr" target="#b4">[5]</ref>, and patch SVDD <ref type="bibr" target="#b60">[61]</ref>. We note that some of these methods use ImageNet pretrained model for transfer learning, either by fine-tuning (DOCC) or distillation (uninformed student). The results are in <ref type="table">Table 1</ref>.</p><p>Rotation prediction is demonstrated to be powerful in semantic anomaly detection <ref type="bibr" target="#b51">[52]</ref>. However, it results in unsatisfactory 73.1 AUC in defect detection compared with the Scar prediction (85.0), a Cutout variant. Some failure of rotation prediction is due to the unaligned objects, such as screw shown in <ref type="figure" target="#fig_1">Figure 2</ref>. For aligned objects, although it performs well on toothbrush, it is sub-optimal on capsule. Detailed ablation study of Cutout variants can be found in Section 5.</p><p>CutPaste and CutPaste-Scar, which improve Cutout and Scar prediction by avoiding potential naive solutions, outperform other augmentation predictions with 90.9 and 93.5 AUCs, respectively. With a finer-grained 3-way classification to leverage different scale of CutPaste, we achieve the best 95.2 AUC, which surpasses existing works on learning from scratch, such as P-SVDD <ref type="bibr" target="#b60">[61]</ref> (92.1 AUC). The proposed data-driven approach via CutPaste is also better than existing works leveraging pretrained networks, including DOCC <ref type="bibr" target="#b44">[45]</ref> (87.9 AUC) with pretrained VGG16 and Uninformed Student <ref type="bibr" target="#b5">[6]</ref> (92.5 AUC) with pretrained ResNet18. Last, we further improve the AUC to 96.1 by ensembling anomaly scores from 5 CutPaste (3-way) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Defect Localization</head><p>We conduct anomaly localization experiments using our representations trained with 3-way classification task. One challenge to accurate localization of defect is that it is difficult to use a heatmap-style approach for localization as our <ref type="table">Table 1</ref>: Anomaly detection performance on MVTec AD dataset <ref type="bibr" target="#b4">[5]</ref>. We report AUCs of representations trained to classify CutPaste, CutPaste (scar), both (3-way), and baseline augmentations such as rotation, Cutout, or scar. For comparison, we report those of deep one-class classifier <ref type="bibr" target="#b44">[45]</ref>, uninformed student <ref type="bibr" target="#b5">[6]</ref> and patch-SVDD <ref type="bibr" target="#b60">[61]</ref>. We report mean and standard error tested with 5 random seeds. Lastly, we report the AUC using ensemble of 5 CutPaste (3-way) models. The best performing model and those within standard error are bold-faced.  Instead, we learn a representation of an image patch using CutPaste prediction, as in Section 2.4. We train models of 64×64 patches from 256×256 image. At test time, we densely extract anomaly scores with a stride of 4 and propagate the anomaly scores via receptive field upsampling with Gaussian smoothing <ref type="bibr" target="#b31">[32]</ref>. We report a localization AUC in <ref type="table" target="#tab_2">Table 2</ref>. Our patch-based model achieves 96.0 AUC. Specifically, our model shows strong performance on texture categories over previous state-of-the-art (96.3 AUC compared to 93.7). We also outperforms the DistAug contrastive learning <ref type="bibr" target="#b51">[52]</ref>, which only results in 90.4 localization AUC. Finally, we visualize representative samples for localization in <ref type="figure" target="#fig_2">Figure 4</ref>, showing accurate localization even when defects are tiny. More comprehensive results on defect localization are given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer Learning with Pretrained Models</head><p>In Section 4.1, we have shown the proposed data-driven approach is better than leveraging pretrained networks, such as DOCC <ref type="bibr" target="#b44">[45]</ref> and Uninformed Student <ref type="bibr" target="#b5">[6]</ref>. It is consistent  with the prior study on semantic anomaly detection <ref type="bibr" target="#b51">[52]</ref>. On the other hand, pretrained EfficientNet <ref type="bibr" target="#b54">[55]</ref> is found useful for defect detection <ref type="bibr" target="#b42">[43]</ref>. As shown in <ref type="table" target="#tab_3">Table 3</ref>, without fine-tuning, the representation from the pretrained Efficient-Net (B4) results in 94.5 AUC, which is competitive with the proposed CutPaste prediction (95.2 from <ref type="table">Table 1</ref>).</p><p>Here we demonstrate that the proposed self-supervised learning via CutPaste is versatile, which can also be used to improve the pretrained networks to better adapt to the data. We use pretrained EfficientNet (B4) as a backbone, and follow the standard fine-tuning steps to train with the same  CutPaste prediction (3-way) task. Detailed settings can be found in Appendix A. We show the results in <ref type="table" target="#tab_3">Table 3</ref>. After fine-tuning via CutPaste, we achieve the new state-of-theart 96.6 AUC. Furthermore, CutPaste prediction is a general and useful strategy to adapt to the data for most of the situations. For example, CutPaste improves by a large margin on class pill (81.9 → 91.3). For many nearly perfect situations, such as bottle, CutPaste is still able to improve by a small margin. Last, as suggested by <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43]</ref>, we investigate the performance of various deep features. We find that level-7 feature shows the best performance, and we further improve the level-7 feature of EfficientNet from 96.8 (pretrained) to 97.1±0.0 with CutPaste.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study</head><p>We conduct various additional studies to provide deeper insights of the proposed CutPaste. We first compare Cut-Paste with different Cutout variants in addition to the standard ones reported in Section 4.1. Second, we showcase the representation learned via predicting CutPaste generalizes well to more crafted unseen defects. Last, we compare with the semantic anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">From Cutout to CutPaste</head><p>We evaluate the performance of representations trained to predict variants of Cutout augmentations whose areas are filled by grey color (standard), mean pixel values, random color, or image patch from different location, i.e., CutPaste. We also test Confetti noise <ref type="bibr" target="#b31">[32]</ref> that jitters a color of a local patch. We show samples from considered augmentations in <ref type="figure" target="#fig_3">Figure 5</ref> and report the detection AUCs in <ref type="table" target="#tab_4">Table 4</ref>. While achieving 71.3 AUC that already is significantly better than random guessing, predicting a standard Cutout augmentation is still a simple task and the network may have learned a naive solution from easy proxy task, as discussed in Section 2. By gradually increasing the difficulty of proxy task to avoid known trivial solutions with random color to the patch, or with structures similar to local patterns of the normal data (Confetti noise, CutPaste), the network learns to find irregularity and generalizes better to detect real defects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Binary v.s. Finer-Grained Classification</head><p>In <ref type="table">Table 1</ref>, although CutPaste-scar shows better performance on average than CutPaste, there is no clear winner that works the best for all. As there are diverse types of defect in practice, we leverage the strength of both augmentations for representation learning. In Section 2.2, we train a model by solving a 3-way classification task between normal, CutPaste and CutPaste-scar. Alternatively, we train to solve a binary classification task by discriminating normal examples and the union of two augmentations.</p><p>The results, along with those of representations trained with CutPaste and CutPaste-scar, are in <ref type="table" target="#tab_5">Table 5</ref>. It is clear that using both augmentations improve the performance. Between binary with union of augmentations and 3-way, we observe better detection performance with representations trained by 3-way classification task. A plausible hypothesis on the superiority of 3-way formulation in our case is that it is more natural to model CutPaste and CutPastescar augmentations separately than together as there exists a systematic difference between them in the size, shape, and rotation angle of patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">CutPaste on Synthetic Anomaly Detection</head><p>We further study the generalization of our models to unseen anomalies. Specifically, we test on synthetic anomaly datasets created by patching diverse shape masks to normal data, such as digits <ref type="bibr" target="#b28">[29]</ref>, square, ellipse, or heart <ref type="bibr" target="#b34">[35]</ref>, filled with random color or natural images. Samples of synthetic anomalies are shown in <ref type="figure">Figure 6</ref> and detection results are in <ref type="table" target="#tab_6">Table 6</ref>. We first note that these datasets are not trivial -a model trained by predicting Cutout augmentations achieves only 81.5. Our proposed CutPaste (3-way) model performs well on synthetic dataset, achieving 98.3 AUC on average. We highlight that some shapes (e.g., ellipse, heart) or color <ref type="figure">Figure 6</ref>: Synthetic defects on pill class. From left to right, we use MNIST <ref type="bibr" target="#b28">[29]</ref>, square, ellipse, heart <ref type="bibr" target="#b34">[35]</ref> with random color, and those filled with natural image patches. statistics inside the patch (e.g., constant color, natural images) are not seen at training, but we can still generalize to these unseen cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Application to Semantic Outlier Detection</head><p>We also conduct the semantic anomaly detection experiment on CIFAR-10 <ref type="bibr" target="#b27">[28]</ref> following the protocol in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b51">52]</ref>, where a single class is treated as normal and remaining 9 classes are anomalies. We make a comparison of Cutout, CutPaste and rotation prediction <ref type="bibr" target="#b51">[52]</ref>. Cutout results in 60.2 AUC, and CutPaste achieves 69.4 AUC, which significantly improves upon Cutout (60.2). However, these are still far behind that of rotation prediction (91.3 AUC) on CIFAR-10 semantic anomaly detection. On the other hand, in Section 4.1, we have discussed the reversed situation that rotation prediction is much worse than 3-way CutPaste prediction. The results suggest the difference between semantic anomaly detection and defect detection, which needs different algorithm and augmentation designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a data-driven approach for defect detection and localization. The key to our success is self-supervised learning of representations with CutPaste, a simple yet effective augmentation that encourages the model to find local irregularities. We show superior image-level anomaly detection performance on the real-world dataset. Furthermore, by learning and extracting patch-level representations, we demonstrate state-of-the-art pixel-wise anomaly localization performance. We envision the CutPaste augmentation could be a cornerstone for building a powerful model for semi-supervised and unsupervised defect detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details on Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Experiment with ResNet-18</head><p>We train a model on 256×256 image. Model parameters are updated for 65k steps using momentum SGD with the learning rate of 0.03, momentum of 0.9, and the batch size of 64 (or 96 for 3-way). A single cycle of cosine learning rate decay schedule <ref type="bibr" target="#b32">[33]</ref> and L2 weight regularization with a coefficient of 0.00003 are used. We apply random translation and color jitters for data augmentation to enhance invariance of representations. We note that the same training strategy, such as the selection of hyperparameters or data augmentations, is applied to different categories. Full range of hyperparameters we studied are provided in below. We use Tensorflow <ref type="bibr" target="#b0">[1]</ref>, and scikit-learn <ref type="bibr" target="#b40">[41]</ref> for GDE implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation details on CutPaste</head><p>Our implementation of the CutPaste augmentation closely follows that of RandomErasing 3 <ref type="bibr" target="#b64">[65]</ref>. First, we determine the size of the patch by sampling the area ratio between the patch and the full image from (0.02, 0.15). We then determine the aspect ratio by sampling from (0.3, 1) ∪ <ref type="figure">(1, 3.3)</ref>. The locations of the patch where we cut from and paste to are randomly selected in a way that the entire patch appears in the full image. To construct CutPaste-Scar, we directly sample the width between <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> and the length between <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref> in terms of number of pixels. CutPaste-Scar is randomly rotated between (−45, 45) degrees. Before pasting, we apply color jitter, <ref type="bibr" target="#b3">4</ref> which applies brightness, contrast, saturation, and hue transformations in sequence with random order, with the maximum intensity of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Ablation Study on Hyperparameters</head><p>We study the impact of different values for optimization hyperparameters. Furthermore, we study the impact of CutPaste hyperparameters, such as jitter intensity or the size of the patch. All experiments are done on CutPaste 3-way setting. Below, we enumerate the ranges for hyperparameters we studied. The bold-faced items are default values used for experiments in the main text as well as in this section.</p><p>1. Learning rates ∈{0.1, 0.03, 0.01, 0.003}. We report the mean and standard error of detection AUCs in <ref type="table">Table 7</ref> and 8. We observe that our method is fairly robust across different learning rates and number of epochs. When the learning rate becomes too small (≤ 0.003) we observe slow convergence, so it suggests to train longer. The number of training epochs is also an important hyperparameter for semi-supervised anomaly detection as early stopping is particularly difficult. We observe that our method provides a reliable solution when trained for different number of epochs.</p><p>For jitter intensity on patch of the CutPaste augmentation, we find it more important for texture categories. Part of the reasons is that the CutPaste augmentation, when applied without jitter augmentation, makes it too difficult to distinguish from the original images, as texture categories contain repetitive patterns. By adding jitter on the patch, the contrast between pasted patch and the surrounding area becomes more apparent and this makes the CutPaste prediction task a bit easier. Similarly, the size of the patch mostly affects the performance on texture categories and we observe the method prefers generally smaller patch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Experiment with EfficientNet</head><p>We follow the Keras guide 6 of fine-tuning EfficientNet. We use EfficientNet B4 with batch size 24, which is the limit we tried to fit into single GPU training. We first train only the MLP head for 10 epochs with learning rate of 0.03 while freezing the pretrained backbone. We then fine-tune all layers for 64 epochs with learning rate of 0.0001. We note that the batchnorm layers are kept frozen as suggested by the Keras guide. The other unstated hyperparameters are the same as in Section A.1. <ref type="table">Table 7</ref>: Detection AUCs using (1) different learning rates and (2) the number of epochs. We report the mean and standard error of AUCs for 5 runs with different random seeds.  + 1)×512 = 57×57×512 dimensional tensor of embedding vectors. Then we compute the anomaly score of 512 dimensional embedding vector at each location to obtain 57×57 anomaly score map. Finally, to obtain an anomaly score map of full resolution (256×256), we apply receptive field upsampling via Gaussian smoothing following <ref type="bibr" target="#b31">[32]</ref>, which essentially applies the transposed convolution with the stride of 4, the same stride that we used for dense feature extraction, using a single convolution kernel of size 32×32 whose weights are determined by a Gaussian distribution.</p><p>Depending on the category, we find different strategies for computing score improves the performance. For those categories with "aligned objects" (bottle, cable, capsule, metal nut, pill, toothbrush, transistor, zipper), we find it useful to construct one-class classifiers at each location separately. This is particularly useful for detecting missing or dislocated components (e.g., examples in the second row of <ref type="figure" target="#fig_3">Figure 15</ref>) as global context is captured by classifiers at each location. For some object categories, such as hazelnut or screw whose objects are randomly rotated, and texture categories, we use a single one-class classifier applied to all locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Localization Visualizations</head><p>From <ref type="figure" target="#fig_6">Figure 7</ref> to <ref type="figure" target="#fig_1">Figure 21</ref>, we show localization visualizations of 24 examples for each of 10 object and 5 texture categories via GradCAM for image-level CutPaste models and patch heatmap for patch-based models. We not only show successful cases, but also some failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Failure Case Analysis</head><p>Here we enumerate some failure cases. Note that this is not the comprehensive list of all failure cases, but a few representative failure cases we find via visually inspecting localization visualizations.</p><p>• Cable <ref type="figure" target="#fig_7">(Figure 8</ref>     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>- ure 2</head><label>2</label><figDesc>(d), is proposed in<ref type="bibr" target="#b15">[16]</ref> for defect detection. Similarly, in addition to original CutPaste using a large rectangular patch, we propose a CutPaste-Scar using a scar-like (long-thin) rectangular box filled with an image patch (Figure 2(f)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Defect localization on bottle, hazelnut, metal nut, screw, wood and grid classes of MVTec datasets. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. We provide more examples in Appendix B.model learns a holistic representation of an image. Instead, we use visual explanation techniques, GradCAM<ref type="bibr" target="#b50">[51]</ref>, to highlight the area affecting the decision of anomaly detector. We show qualitative results in the second row ofFigure 4, which are visually pleasing. We further evaluate the pixel-wise localization AUC, achieving 88.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visual comparison between the proposed Cut-Paste and Cutout variants, including filling with grey color, mean pixel values, random colors and Confetti noise<ref type="bibr" target="#b31">[32]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 .</head><label>2</label><figDesc>Number of training epochs ∈{128, 192, 256, 320, 384}. 5 3. Maximum jitter intensity on patch ∈{0, 0.1, 0.2, 0.3}. 4. Maximum size of patch ∈{0.1, 0.15, 0.2, 0.3}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>): Missing components (row 2 column 6-8).• Metal Nut (Figure 11): Flipped components (row 1 column 1-2).• Screw (Figure 13): Speckle noise in the background (row 3 column 1, 6).• Transistor(Figure 15): Dislocated or missing components (row 2).• Tile(Figure 20): dyed tiles (row 2 column 7-8, row 3 column 1-2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Defect localization on bottle class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Defect localization on cable class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Defect localization on capsule class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :Figure 11 :Figure 12 :Figure 13 :Figure 14 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :Figure 19 :Figure 20 :Figure 21 :</head><label>101112131415161718192021</label><figDesc>Defect localization on hazelnut class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. Defect localization on metal nut class of MVTec dataset. From top to bottom, input images, those with groundtruth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. Defect localization on pill class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. Defect localization on screw class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. Defect localization on toothbrush class of MVTec dataset. From top to bottom, input images, those with groundtruth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. Defect localization on transistor class of MVTec dataset. From top to bottom, input images, those with groundtruth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. Defect localization on zipper class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. Defect localization on carpet class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. Defect localization on grid class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. Defect localization on leather class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. Defect localization on tile class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector. Defect localization on wood class of MVTec dataset. From top to bottom, input images, those with ground-truth localization mask in red, GradCAM results using image-level detector, and heatmaps using patch-level detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2±1.4 67.7±1.5 99.9±0.1 99.7±0.1 100.0 ±0.0 100.0 ±0.0</figDesc><table><row><cell cols="2">Category</cell><cell cols="4">DOCC U-Student P-SVDD Rotation [45] [6] [61]</cell><cell>Cutout</cell><cell>Scar</cell><cell>CutPaste</cell><cell>CutPaste (scar)</cell><cell>CutPaste (3-way)</cell><cell>Ensemble</cell></row><row><cell></cell><cell>carpet</cell><cell>90.6</cell><cell>95.3</cell><cell>92.9</cell><cell cols="4">29.7±1.4 35.3±2.3 92.7±0.4 67.9±1.8</cell><cell>94.6±0.6</cell><cell>93.1±1.1</cell><cell>93.9</cell></row><row><cell></cell><cell>grid</cell><cell>52.4</cell><cell>98.7</cell><cell>94.6</cell><cell cols="4">60.5±7.0 57.5±3.0 74.4±2.5 99.9±0.1</cell><cell>95.5±0.3</cell><cell>99.9±0.1</cell><cell>100.0</cell></row><row><cell>texture</cell><cell>leather tile</cell><cell>78.3 96.5</cell><cell>93.4 95.8</cell><cell>90.9 97.8</cell><cell cols="5">55.100.0 70.1±1.9 71.8±4.0 96.7±0.9 95.9±1.0 89.4±2.8 93.4±1.0 94.6</cell></row><row><cell></cell><cell>wood</cell><cell>91.6</cell><cell>95.5</cell><cell>96.5</cell><cell cols="4">95.8±1.1 92.0±0.8 98.9±0.2 94.9±0.5</cell><cell>98.7±0.3</cell><cell>98.6±0.5</cell><cell>99.1</cell></row><row><cell></cell><cell>average</cell><cell>81.9</cell><cell>95.7</cell><cell>94.5</cell><cell cols="4">62.3±2.6 64.9±2.3 92.5±0.8 91.7±0.7</cell><cell>95.7±0.8</cell><cell>97.0±0.5</cell><cell>97.5</cell></row><row><cell></cell><cell>bottle</cell><cell>99.6</cell><cell>96.7</cell><cell>98.6</cell><cell cols="4">95.0±0.7 88.7±0.8 98.5±0.2 99.2±0.2</cell><cell>98.0±0.5</cell><cell>98.3±0.5</cell><cell>98.2</cell></row><row><cell></cell><cell>cable</cell><cell>90.9</cell><cell>82.3</cell><cell>90.3</cell><cell cols="4">85.3±0.8 80.2±1.4 78.3±1.7 87.1±0.8</cell><cell>78.8±2.9</cell><cell>80.6±0.5</cell><cell>81.2</cell></row><row><cell></cell><cell>capsule</cell><cell>91.0</cell><cell>92.8</cell><cell>76.7</cell><cell cols="4">71.8±1.4 69.5±1.1 82.9±0.7 87.9±0.7</cell><cell>95.3±0.8</cell><cell>96.2±0.5</cell><cell>98.2</cell></row><row><cell></cell><cell>hazelnut</cell><cell>95.0</cell><cell>91.4</cell><cell>92.0</cell><cell cols="4">83.6±0.8 69.7±1.3 98.9±0.2 91.3±0.6</cell><cell>96.7±0.4</cell><cell>97.3±0.3</cell><cell>98.3</cell></row><row><cell></cell><cell>metal nut</cell><cell>85.2</cell><cell>94.0</cell><cell>94.0</cell><cell cols="4">72.7±0.5 84.6±0.7 86.9±1.5 96.8±0.5</cell><cell>97.9±0.2</cell><cell>99.3±0.2</cell><cell>99.9</cell></row><row><cell>object</cell><cell>pill</cell><cell>80.4</cell><cell>86.7</cell><cell>86.1</cell><cell cols="4">79.2±1.4 78.7±0.7 82.2±1.4 93.4±0.9</cell><cell>85.8±1.3</cell><cell>92.4±1.3</cell><cell>94.9</cell></row><row><cell></cell><cell>screw</cell><cell>86.9</cell><cell>87.4</cell><cell>81.3</cell><cell cols="4">35.8±2.9 17.6±4.4 11.3±2.2 54.4±1.7</cell><cell>83.7±0.7</cell><cell>86.3±1.0</cell><cell>88.7</cell></row><row><cell></cell><cell>toothbrush</cell><cell>96.4</cell><cell>98.6</cell><cell>100.0</cell><cell cols="4">99.1±0.2 98.1±0.6 94.8±1.0 99.2±0.2</cell><cell>96.7±0.4</cell><cell>98.3±0.9</cell><cell>99.4</cell></row><row><cell></cell><cell>transistor</cell><cell>90.8</cell><cell>83.6</cell><cell>91.5</cell><cell cols="4">88.9±0.4 82.5±1.2 92.0±0.7 96.4±0.7</cell><cell>91.1±0.6</cell><cell>95.5±0.5</cell><cell>96.1</cell></row><row><cell></cell><cell>zipper</cell><cell>92.4</cell><cell>95.8</cell><cell>97.9</cell><cell cols="4">74.3±1.6 75.7±1.0 86.8±0.9 99.4±0.1</cell><cell>99.5±0.1</cell><cell>99.4±0.2</cell><cell>99.9</cell></row><row><cell></cell><cell>average</cell><cell>90.9</cell><cell>90.9</cell><cell>90.8</cell><cell cols="4">78.6±1.1 74.5±1.3 81.3±1.1 90.5±0.6</cell><cell>92.4±0.8</cell><cell>94.3±0.6</cell><cell>95.5</cell></row><row><cell cols="2">average</cell><cell>87.9</cell><cell>92.5</cell><cell>92.1</cell><cell cols="4">73.1±1.6 71.3±1.6 85.0±1.0 90.9±0.7</cell><cell>93.5±0.8</cell><cell>95.2±0.6</cell><cell>96.1</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Grad</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CAM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Patch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Heatmap</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Pixel-wise localization AUC on MVTec dataset. The best and models within standard error are bold-faced.</figDesc><table><row><cell cols="2">Category</cell><cell cols="3">FCDD [32] P-SVDD [61] CutPaste (3-way)</cell></row><row><cell></cell><cell>carpet</cell><cell>96</cell><cell>92.6</cell><cell>98.3±0.0</cell></row><row><cell></cell><cell>grid</cell><cell>91</cell><cell>96.2</cell><cell>97.5±0.1</cell></row><row><cell>texture</cell><cell>leather tile</cell><cell>98 91</cell><cell>97.4 91.4</cell><cell>99.5±0.0 90.5±0.2</cell></row><row><cell></cell><cell>wood</cell><cell>88</cell><cell>90.8</cell><cell>95.5±0.1</cell></row><row><cell></cell><cell>average</cell><cell>93</cell><cell>93.7</cell><cell>96.3±0.1</cell></row><row><cell></cell><cell>bottle</cell><cell>97</cell><cell>98.1</cell><cell>97.6±0.1</cell></row><row><cell></cell><cell>cable</cell><cell>90</cell><cell>96.8</cell><cell>90.0±0.2</cell></row><row><cell></cell><cell>capsule</cell><cell>93</cell><cell>95.8</cell><cell>97.4±0.1</cell></row><row><cell></cell><cell>hazelnut</cell><cell>95</cell><cell>97.5</cell><cell>97.3±0.1</cell></row><row><cell></cell><cell>metal nut</cell><cell>94</cell><cell>98.0</cell><cell>93.1±0.4</cell></row><row><cell>object</cell><cell>pill</cell><cell>81</cell><cell>95.1</cell><cell>95.7±0.1</cell></row><row><cell></cell><cell>screw</cell><cell>86</cell><cell>95.7</cell><cell>96.7±0.1</cell></row><row><cell></cell><cell>toothbrush</cell><cell>94</cell><cell>98.1</cell><cell>98.1±0.0</cell></row><row><cell></cell><cell>transistor</cell><cell>88</cell><cell>97.0</cell><cell>93.0±0.2</cell></row><row><cell></cell><cell>zipper</cell><cell>92</cell><cell>95.1</cell><cell>99.3±0.0</cell></row><row><cell></cell><cell>average</cell><cell>91</cell><cell>96.7</cell><cell>95.8±0.1</cell></row><row><cell cols="2">average</cell><cell>92</cell><cell>95.7</cell><cell>96.0±0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Category</cell><cell></cell><cell>Pool</cell><cell cols="2">Level-7</cell></row><row><cell></cell><cell></cell><cell cols="2">Pretrain Finetune</cell><cell cols="2">Pretrain Finetune</cell></row><row><cell></cell><cell>carpet</cell><cell>98.3</cell><cell>100.0±0.0</cell><cell>97.6</cell><cell>100.0±0.0</cell></row><row><cell></cell><cell>grid</cell><cell>96.4</cell><cell>98.8±0.1</cell><cell>98.2</cell><cell>99.1±0.0</cell></row><row><cell>texture</cell><cell>leather tile</cell><cell>100.0 99.9</cell><cell>100.0±0.0 98.9±0.2</cell><cell>100.0 99.9</cell><cell>100.0±0.0 99.8±0.2</cell></row><row><cell></cell><cell>wood</cell><cell>99.7</cell><cell>99.8±0.0</cell><cell>99.6</cell><cell>99.8±0.0</cell></row><row><cell></cell><cell>average</cell><cell>98.9</cell><cell>99.5±0.0</cell><cell>99</cell><cell>99.7±0.0</cell></row><row><cell></cell><cell>bottle</cell><cell>99.8</cell><cell>100.0±0.0</cell><cell>100.0</cell><cell>100.0±0.0</cell></row><row><cell></cell><cell>cable</cell><cell>91.2</cell><cell>93.9±0.1</cell><cell>96.5</cell><cell>96.2±0.3</cell></row><row><cell></cell><cell>capsule</cell><cell>93</cell><cell>94.3±0.3</cell><cell>94.7</cell><cell>95.4±0.1</cell></row><row><cell></cell><cell>hazelnut</cell><cell>96.6</cell><cell>99.7±0.0</cell><cell>100.0</cell><cell>99.9±0.0</cell></row><row><cell></cell><cell>metalnut</cell><cell>94.3</cell><cell>98.7±0.1</cell><cell>97.7</cell><cell>98.6±0.0</cell></row><row><cell>object</cell><cell>pill</cell><cell>81.9</cell><cell>91.3±0.2</cell><cell>91</cell><cell>93.3±0.2</cell></row><row><cell></cell><cell>screw</cell><cell>86.3</cell><cell>86±0.1</cell><cell>92.0</cell><cell>86.6±0.2</cell></row><row><cell></cell><cell>toothbrush</cell><cell>89.3</cell><cell>92.8±0.2</cell><cell>90.3</cell><cell>90.7±0.1</cell></row><row><cell></cell><cell>transistor</cell><cell>94.6</cell><cell>95.6±0.2</cell><cell>97.5</cell><cell>97.5±0.2</cell></row><row><cell></cell><cell>zipper</cell><cell>95.6</cell><cell>99.9±0.0</cell><cell>97</cell><cell>99.9±0.1</cell></row><row><cell></cell><cell>average</cell><cell>92.3</cell><cell>95.2±0.1</cell><cell>95.7</cell><cell>95.8±0.1</cell></row><row><cell cols="2">average</cell><cell>94.5</cell><cell>96.6±0.1</cell><cell>96.8</cell><cell>97.1±0.0</cell></row></table><note>Detection performance on MVTec dataset using representations of EfficientNet (B4) [55] pretrained on Im- ageNet [17] and fine-tuned by the CutPaste (3-way). The number is bold when it is better than its pretrained or fine- tuned counterpart under the same feature (pool v.s. level-7).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Detection AUCs of representations trained to predict Cutout, with mean pixel value, with random color, Confetti noise<ref type="bibr" target="#b31">[32]</ref>, or the proposed CutPaste.</figDesc><table><row><cell>Category</cell><cell>Cutout</cell><cell>Cutout (Mean)</cell><cell>Cutout (Color)</cell><cell></cell><cell>Confetti</cell><cell>CutPaste</cell></row><row><cell>texture</cell><cell>64.9±2.3</cell><cell>65.5±1.8</cell><cell cols="2">70.5±2.2</cell><cell>80.1±2.3</cell><cell>91.7±0.7</cell></row><row><cell>object</cell><cell>74.5±1.3</cell><cell>78.1±1.1</cell><cell cols="2">78.9±1.0</cell><cell>76.7±0.3</cell><cell>90.5±0.6</cell></row><row><cell>all</cell><cell>71.3±1.7</cell><cell>73.9±1.3</cell><cell cols="2">76.1±1.4</cell><cell>77.8±1.5</cell><cell>90.9±0.7</cell></row><row><cell>(a) Cutout (Standard)</cell><cell>(b) Cutout (Mean)</cell><cell cols="2">(c) Cutout (Random Color)</cell><cell cols="2">(d) Confetti</cell><cell>(e) CutPaste</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Detection AUCs of representations trained with binary classification between normal and the union of Cut-Paste and CutPaste-Scar examples and 3-way classification among normal, CutPaste and CutPaste-scar examples.</figDesc><table><row><cell cols="4">Category CutPaste CutPaste (scar) Binary (Union)</cell><cell>3-Way</cell></row><row><cell>texture</cell><cell>91.7±0.7</cell><cell>95.7±0.8</cell><cell>97.3±0.3</cell><cell>97.0±0.5</cell></row><row><cell>object</cell><cell>90.5±0.6</cell><cell>92.4±0.8</cell><cell>92.8±0.5</cell><cell>94.3±0.6</cell></row><row><cell>all</cell><cell>90.9±0.7</cell><cell>93.5±0.8</cell><cell>94.3±0.5</cell><cell>95.2±0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Detection AUCs on synthetic data. Various shapes, such as digit, square, ellipse, or heart, are patched to normal images with random color or natural images ( † ).</figDesc><table><row><cell>Dataset</cell><cell cols="7">MNIST Square Ellipse Heart Square  † Ellipse  † Heart  †</cell><cell>Avg</cell></row><row><cell>Cutout</cell><cell>52.3</cell><cell>90.6</cell><cell>89.3</cell><cell>87.5</cell><cell>86.4</cell><cell>84.0</cell><cell>80.7</cell><cell>81.5</cell></row><row><cell>CutPaste</cell><cell>96.1</cell><cell>98.4</cell><cell>98.2</cell><cell>97.9</cell><cell>99.3</cell><cell>99.2</cell><cell>99.0</cell><cell>98.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Detection AUCs using (1) different jitter intensity and (2) the size of patch of the CutPaste augmentation. We report the mean and standard error of AUCs for 5 runs with different random seeds.The training procedure of patch-based model with CutPaste should be straightforward from Section 2.4. Once patch-based models are trained, we densely extract representations of 32×32 patches with the stride of 4, which results in( 256−32</figDesc><table><row><cell>Category</cell><cell></cell><cell cols="2">Jitter intensity</cell><cell></cell><cell></cell><cell cols="2">Size of patch</cell></row><row><cell></cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell>texture</cell><cell cols="8">96.2±0.6 97.0±0.5 97.4±0.3 97.5±0.2 97.1±0.4 97.0±0.5 95.8±0.9 96.6±0.4</cell></row><row><cell>object</cell><cell cols="8">94.3±0.6 94.3±0.6 94.5±0.5 94.5±0.5 94.5±0.5 94.3±0.6 94.4±0.5 94.5±0.5</cell></row><row><cell>all</cell><cell cols="8">94.9±0.6 95.2±0.6 95.5±0.4 95.5±0.4 95.3±0.5 95.2±0.6 94.8±0.6 95.2±0.5</cell></row><row><cell cols="5">A.5. Details on Localization with Patch-based model</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1)×( 256−32 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>+</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We note that a mixture of Gaussian, which is a middle ground between KDE and GDE, can also be used for more expressive density modeling. We do not observe significant performance gain empirically.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">While previous works<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> have used unsupervised to describe their settings, it could be misleading as training data is curated to include normal data only.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://pytorch.org/docs/stable/_modules/torchvision/transforms/transforms.html#RandomErasing 4 https://pytorch.org/docs/stable/_modules/torchvision/transforms/transforms.html#ColorJitter 5 Note that, unlike conventional definition for an epoch, we define 256 parameter update steps as one epoch. 6 https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We thank Yang Feng for sharing the implementation of uninformed student and Sercan Arik for the proofread of our manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">{USENIX}</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixed-location monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilan</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daviv</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02011</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting anomalous structures by convolutional sparse models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendt</forename><surname>Wohlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-supervised learning for medical image analysis using image context restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensaku</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunari</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michitaka</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01392</idno>
		<title level="m">Waic, but why? generative ensembles for robust anomaly detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improved anomaly detection by training an autoencoder with skip connections on images corrupted with stain-shaped noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne-Sophie</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12977,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spotting defects! -deep metric learning solution for mvtec anomaly detection dataset. https : / / medium . com / analyticsvidhya / spotting -defects -deep -metriclearning -solution -for -mvtec -anomalydetection -dataset -c77691beb1eb</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04606</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Inverse-transform autoencoder for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10676</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Steering selfsupervised feature learning beyond local pixel statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hailin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Why normalizing flows fail to detect out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08545,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NIPS</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Classifier two sample test for video anomaly detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusha</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Liznerski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><forename type="middle">Joe</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Müller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01760</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Explainable deep one-class classification. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement testing sprites dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<idno>on: 2018-05-08</idno>
		<ptr target="https://github.com/deepmind/dsprites-dataset/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Density of states estimation for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cusuh</forename><surname>Warren R Morningstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09273,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Detecting out-of-distribution inputs to deep generative models using a test for typicality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02994</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by cnn-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Modeling the distribution of normal data in pre-trained deep features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><surname>Merhof</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14140</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Same same but differnet: Semi-supervised defect detection with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12577,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jacob R Kauffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Müller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11732</idno>
		<title level="m">A unifying review of deep and shallow anomaly detection</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainaz</forename><surname>Eftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niousha</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad Hossein Rohban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamid R Rabiee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12959,2020.4</idno>
		<title level="m">Puzzleae: Novelty detection in images through solving puzzles</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPMI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Klimscha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bianca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Gerendas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schlegl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00686</idno>
		<title level="m">and Georg Langs. Identifying and categorizing anomalies in retinal imaging data</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning and evaluating representations for deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minho</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08176</idno>
		<title level="m">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Support vector data description. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Distance-based anomaly detection for industrial surfaces using triplet networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tareq</forename><surname>Tayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sulaiman</forename><surname>Aburakhia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Shami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04121</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Introduction to nonparametric estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexandre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08616</idno>
		<title level="m">Rajat Vikram Singh, and Abhijit Mahalanobis. Attention guided anomaly localization in images</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Patch svdd: Patch-level svdd for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16067</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deep structured energy based models for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07717</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">MixUp: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeki</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
