<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Symbiotic Graph Neural Networks for 3D Skeleton-based Human Action Recognition and Motion Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Maosen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Siheng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Qi</forename><surname>Tian</surname></persName>
						</author>
						<title level="a" type="main">Symbiotic Graph Neural Networks for 3D Skeleton-based Human Action Recognition and Motion Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D skeleton-based action recognition</term>
					<term>motion prediction</term>
					<term>multi-scale graph convolution networks</term>
					<term>graph inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D skeleton-based action recognition and motion prediction are two essential problems of human activity understanding. In many previous works: 1) they studied two tasks separately, neglecting internal correlations; 2) they did not capture sufficient relations inside the body. To address these issues, we propose a symbiotic model to handle two tasks jointly; and we propose two scales of graphs to explicitly capture relations among body-joints and body-parts. Together, we propose symbiotic graph neural networks, which contains a backbone, an action-recognition head, and a motion-prediction head. Two heads are trained jointly and enhance each other. For the backbone, we propose multi-branch multi-scale graph convolution networks to extract spatial and temporal features. The multi-scale graph convolution networks are based on joint-scale and part-scale graphs. The joint-scale graphs contain actional graphs, capturing action-based relations, and structural graphs, capturing physical constraints. The part-scale graphs integrate body-joints to form specific parts, representing high-level relations. Moreover, dual bone-based graphs and networks are proposed to learn complementary features. We conduct extensive experiments for skeleton-based action recognition and motion prediction with four datasets, NTU-RGB+D, Kinetics, Human3.6M, and CMU Mocap. Experiments show that our symbiotic graph neural networks achieve better performances on both tasks compared to the state-of-the-art methods. The code is relased at github.com/limaosen0/Sym-GNN</p><p>Input Observed ActionX</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>3D skeleton-based action recognition. Numerous methods are proposed for 3D skeleton-based action recognition. Conventionally, some models learned semantics based on handcrafted features and physical intuitions [10], [19], [20]. In the deep learning era, models automatically learn features from data. Some recurrent-neural-network-based (RNN-based) models captured the temporal dependencies between consecutive frames [12], [34]. Moreover, convolutional neural networks (CNN) also achieve remarkable results [13], [24]. Recently, the graph-based approaches drew many attentions [1], [14], [28], [29], [35], [36], [37], [38]. In this work, we adopt the graph-based approach. We construct multiscale graphs adaptively from data, capturing useful and comprehensive information about actions. 3D skeleton-based motion prediction. In earlier studies, state models were considered to predict future motions [15], [21], [39]. Recently, deep learning technique plays increasingly important roles. Some RNN-based methods learned the dynamics from sequences [16], [17], [27], [40], [41], [42]. Moreover, adversarial mechanics and geodesic loss could further improve predictions [18]. As for our method, we use graph structures to explicitly model the relations between body-joints and body-parts, guiding the networks to learn local and non-local motion patterns. Graph deep learning. Graphs, focused on by many recent studies, are effective to express data associated with non-grid structures [14], [28], [43], [44], [45], [46], [47], [48]. Given the fixed topologies, previous works explored to propagate node features based on the spectral domain [46], [47] or the vertex domain [48]. [1], [14], [29], [35], [36] leveraged graph convolution for 3D skeleton-based action recognition.</p><p>[16] also considered the skeleton-based relations for motion prediction. In this paper, we propose multi-scale graphs to represent multiple relations: joint-scale and partscale relations. Then, we propose novel graph convolution operators to extract deep features for action recognition and motion prediction. Different from [1] obtaining multiple actional graphs with complicated inference processes, our method employs more efficient graph learning operations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>H UMAN action recognition and motion prediction are crucial problems in computer vision, being widely applicable to surveillance <ref type="bibr">[2]</ref>, pedestrian tracking <ref type="bibr">[3]</ref>, and human-machine interaction <ref type="bibr" target="#b6">[4]</ref>. Respectively, action recognition aims to accurately classify the categories of query actions <ref type="bibr" target="#b7">[5]</ref>; and motion prediction forecasts the future movements based on observations <ref type="bibr" target="#b8">[6]</ref>.</p><p>The data of actions can be represented with various formats, including RGB videos <ref type="bibr" target="#b9">[7]</ref> and 3D skeleton data <ref type="bibr" target="#b10">[8]</ref>. Notably, 3D skeleton data, locating 3D body-joints, is shown to be effective in action representation, efficient in computation, as well as robust against environmental noise <ref type="bibr" target="#b11">[9]</ref>. In this work, we focus on action recognition and motion prediction based on the 3D skeleton data.</p><p>In most previous studies, 3D skeleton-based action recognition and motion prediction are treated separately as the former needs to discriminate the classes; while the latter • M. <ref type="bibr">Li</ref>  Parts of this paper appear in <ref type="bibr">[1]</ref>.</p><p>generates the future poses. For action recognition, methods employed full action sequences for pattern learning <ref type="bibr" target="#b12">[10]</ref>, <ref type="bibr" target="#b13">[11]</ref>, <ref type="bibr" target="#b14">[12]</ref>, <ref type="bibr" target="#b15">[13]</ref>, <ref type="bibr" target="#b16">[14]</ref>; however, with the long-term inputs, these methods failed in some real-time applications due to the hysteretic discrimination, while the model should response as early as possible. As for motion prediction, previous works built generative models <ref type="bibr" target="#b17">[15]</ref>, <ref type="bibr" target="#b18">[16]</ref>, <ref type="bibr" target="#b19">[17]</ref>, <ref type="bibr" target="#b20">[18]</ref>; they learned motion dynamics, but often ignored semantics. Actually, there are mutual promotions between the tasks of action recognition and motion prediction, while previous works rarely explored them. For example, the classifier provides the action categories as the auxiliary information to guide prediction, as well as the predictor preserves more detailed information for accurate recognition via selfsupervision. Considering to exploit mutual promotions, we aim to develop a symbiotic method to enable action recognition and motion prediction simultaneously. For both 3D skeleton-based action recognition and motion prediction, the key is to effectively capture the motion patterns of various actions. A lot of efforts have been made to push towards this direction. Concretely, some traditional attempts often vectorized all the joints to a pose vector and built hand-crafted models for feature learning <ref type="bibr" target="#b12">[10]</ref>, <ref type="bibr" target="#b17">[15]</ref>, <ref type="bibr" target="#b21">[19]</ref>, <ref type="bibr" target="#b22">[20]</ref>, <ref type="bibr" target="#b23">[21]</ref>, <ref type="bibr" target="#b24">[22]</ref>, <ref type="bibr" target="#b25">[23]</ref>. Recently, some deep models based on either convolutional neural networks (CNN) or recurrent neural networks (RNN) learned high-level features from data <ref type="bibr" target="#b13">[11]</ref>, <ref type="bibr" target="#b14">[12]</ref>, <ref type="bibr" target="#b15">[13]</ref>, <ref type="bibr" target="#b18">[16]</ref>, <ref type="bibr" target="#b19">[17]</ref>, <ref type="bibr" target="#b20">[18]</ref>, <ref type="bibr" target="#b26">[24]</ref>, <ref type="bibr" target="#b27">[25]</ref>, <ref type="bibr" target="#b28">[26]</ref>, <ref type="bibr" target="#b29">[27]</ref>; however, these methods rarely investigated the joint relations, missing crucial activity dynamics. To capture richer features, several works exploited joint relations from various aspects. <ref type="bibr" target="#b16">[14]</ref> proposed skeleton-graphs with nodes as <ref type="figure">Fig. 1</ref>. Symbiotic Graph Neural Networks (Sym-GNN) contains a prime joint-based network to learn body-joint-based features, and a dual bonebased network to learn body-bone-based features. Each network has three main modules: a backbone, an action-recognition head, and a motionprediction head. The backbone is essentially multi-branch multi-scale graph convolution networks (multi-branch multi-scale GCN). The actionrecognition head and the motion-prediction head predict the action category and future poses, respectively. The predicted action category is further used in the motion-prediction head. This symbiotic design allows the two heads to enhance each other.</p><p>joints and edges as bones. <ref type="bibr" target="#b18">[16]</ref>, <ref type="bibr" target="#b30">[28]</ref>, <ref type="bibr" target="#b31">[29]</ref> built the relations between body-parts, such as limbs. <ref type="bibr" target="#b32">[30]</ref> merged individual part features. <ref type="bibr" target="#b33">[31]</ref> leveraged spatial convolutions on pose vectors, but it varied on joint permutation. These works aggregated information from local or coarse neighborhoods. Notably, some relations may exist among action-related joints, such as hands and feet moving collaboratively during walking. Moreover, some methods of motion prediction fed ground-truth action categories to enhance performance in both training and testing phases, but the true labels are hard to obtain during the real-world scenarios. To solve those issues, we construct graphs to model both local and longrange body relations and use graph convolutions to capture informative spatial features.</p><p>In this paper, we propose a novel model called symbiotic graph neural network (Sym-GNN), which handles 3D skeleton-based action recognition and motion prediction simultaneously and uses graph-based operations to capture spatial features. As basic operators of Sym-GNN, we propose the joint-scale graph convolution (JGC) and partscale graph convolution (PGC) operators to extract multi-scale spatial information. JGC is based on two types of graphs: actional graphs and structural graphs. The actional graphs are learned from 3D skeleton data by an actional graph inference module (AGIM), capturing action-based relations; the structural graphs are built by extending the skeleton graphs, capturing physical constraints. PGC is based on a part-scale graph, whose nodes are integrated body-part features and edges are based on body-part connections. We also propose a difference operator to extract multiple orders of motion differences, reflecting positions, velocities, and accelerations of body-joints.</p><p>The proposed Sym-GNN consists of a backbone, called multi-branch multi-scale graph convolutional network (multibranch multi-scale GCN), an action-recognition head and a motion-prediction head; see <ref type="figure">Fig. 1</ref>. The backbone uses jointscale and part-scale graphs for spatial relations presentation and high-level feature extraction; two heads work on two separated tasks. Moreover, there are task promotions, i.e. the action-recognition head determines action categories, which is used to enhance prediction performance; meanwhile, the motion-prediction head predicts poses and improves recognition by promoting self-supervision and preserving detailed features. The model is trained through a multitask-ing paradigm. Additionally, we build a dual bone-based network which treats bones as graph nodes and learns bone features to obtain complementary semantics for more effective classification and prediction.</p><p>To validate the Sym-GNN, we conduct extensive experiments on four large-scale datasets: NTU-RGB+D <ref type="bibr" target="#b34">[32]</ref>, Kinetics <ref type="bibr" target="#b16">[14]</ref>, Human 3.6M <ref type="bibr" target="#b35">[33]</ref>, and CMU Mocap 1 . The results show that 1) Sym-GNN outperforms the state-ofthe-art methods in both action recognition and motion prediction; 2) Using the symbiotic model to train the two tasks simultaneously produces better performance than using individual models; and 3) the multi-scale graphs model complicated relations between body-joints and body-parts, and the proposed JGC extract informative spatial features.</p><p>Overall, the main contributions in this paper are summarized as follows:</p><p>• Multitasking framework. We propose novel symbiotic graph neural networks (Sym-GNN) to achieve 3D skeleton-based action recognition and motion prediction in a multitasking framework. Sym-GNN contains a backbone, an action-recognition head, and a motion-prediction head. We exploit the mutual promotion between two heads, leading to improvements in both tasks; see Section 5</p><p>• Basic operators. We propose novel operators to extract information from 3D skeleton data: 1) a jointscale graph convolution operator is proposed to extract joint-level spatial features based on both actional and structral graphs; see Section 4.1.4; 2) a part-scale graph convolution operator is proposed to extract partlevel spatial features based on part-scale graphs; see Section 4.2.1; 3) a pair of bidirectional fusion operators is proposed to fuse information across two scales; see Section 5.1.2 and 4) a difference operator is proposed to extract temporal features; see Section 4.3; and • Experimental findings. We conduct extensive experiments for both tasks of 3D skeleton-based action recognition and motion prediction. The results show that Sym-GNN outperforms the state-of-the-art methods in both tasks; see Section 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>In this paper, we study 3D skeleton-based action recognition and motion prediction jointly. We here use the 3D joint positions along the time to represent the action sequences. Mathematically, let the action pose at time stamp t be X (t) ∈ R M ×Dx , where t &gt; 0 indicates the future frames, otherwise the observed frames; notably, t = 0 denotes the current frame. M is the number of joints and D x = 3 reflects the 3D joint positions. The action pose is essentially associated with a skeleton graph, which represents the pairwise bone connectivity. We can represent a skeleton graph by a binary adjacent matrix; that is, A ∈ {0, 1} M ×M , where the (i, j)th elements (A) ij = 1 when the ith and the jth bodyjoints are connected with bones, and (A) ij = 0, otherwise. Note that A includes self-loops.</p><p>For an action sequence belonging to one class, we have {X prev , X pred , y}, where X prev = [X (−Tprev) , . . . , X (0) ] ∈ R Tprev×M ×Dx denotes the previous motion tensor; X prev = [X <ref type="bibr">(1)</ref> , . . . , X (−T pred ) ] ∈ R T pred ×M ×Dx denotes the future (c) structural graph (b) actional graph (a) skeleton graph <ref type="figure">Fig. 2</ref>. Examples of joint-scale graphs for walking. In the joint scale, we consider an actional graph (Plot (b)) and a structural graph (Plot (c)), which is an extension of a skeleton graph (Plot (a)). In each graph, the edges from "Left Hand" to its neighbors are shown in solid lines and other links in the skeleton are shown in dashed lines. motion tensor; T prev and T pred are the frame numbers of previous and future motions, respectively; and one-hot vector y ∈ {0, 1} C denotes the class-label in C possible classes. Let F(·) be the overall model. The discriminated class categorŷ y and the predicted motionX pred are formulated aŝ</p><formula xml:id="formula_0">y,X pred = F(X prev ; θ bk , θ recg , θ pred ),</formula><p>where θ bk , θ recg and θ pred denote trainable parameters of the backbone, the action-recognition head and the motionprediction head, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BASIC COMPONENTS</head><p>In this section, we propose some novel components in our model. We first propose some joint-scale graph operators, extracting features among body-joints; we next propose the part-scale graph operators, extracting features among bodyparts; finally, we propose a difference operator to provide richer motion priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Joint-Scale Graph Operators</head><p>To model the joint relations, we build joint-scale graphs including actional graphs, capturing moving interactions between joints even without bone-connection, and structural graphs, extending the skeleton structures to represent physical constraints. <ref type="figure">Fig. 2</ref> sketches some examples. Plot (a) shows a skeleton graph with local neighborhood; plot (b) shows an actional graph, which captures action-based dependencies, e.g. 'Left Hand' is linked with 'Right Hand' and feet during walking; plot (c) shows a structural graph, which allows 'Left Hand' to link with entire arm.</p><p>As follows, we propose the construction of joint-scale actional and structural graphs. And, we present the jointscale graph and temporal convolution (J-GTC) block to learn spatial and temporal features of sequential actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Actional Graph Convolution</head><p>For different movements, some structurally distant joints may interact, leading to action-based relations. For example, a walking person moves hands and feet collaboratively. To represent actional relations, we employ an actional graph: G act (V, A act ), where V = {v 1 , . . . , v M } is the joint set and A act ∈ R M ×M is the adjacency matrix that reveals the pairwise joint-scale actional relations. To obtain this topology, we propose a data-adaptive module, called actional graphs inference module (AGIM), to learn A act purely from observations without knowing action categories. </p><formula xml:id="formula_1">… … X prev fv k (·) fe k (·) p k i p k i p k q k i,j A act femb(·) gemb(·) p K i j j j ×K v i p K f f i g i g j</formula><p>Joint-edge feature propagation <ref type="figure">Fig. 3</ref>. Actional graphs inference module (AGIM) propagates features between joints and edges for K iterations and uses correlations between joint features to obtain actional graphs.</p><p>To utilize the body dynamics, we let the vector representation of the ith joint positions across all observed frames be x i = vec (X prev [:, i, :]) ∈ R DxTprev , which includes the previous positions. To learn all relations, we propagate pose information between body-joints and possible edges. We first initialize p</p><formula xml:id="formula_2">0 i = f 0 v (x i ) ∈ R Dv , where f 0 v (·)</formula><p>is a multilayer perceptron (MLP) that maps the raw joint moving data x i to joint features p 0 i . In the kth iteration, the features are propagated as follows:</p><formula xml:id="formula_3">q k i,j = f k e p k−1 i , p k−1 j ∈ R De ,<label>(1a)</label></formula><formula xml:id="formula_4">p k i = f k v   1 M − 1 vj ∈V,j =i q k i,j   ∈ R Dv ,<label>(1b)</label></formula><p>where p k i , q k i,j are the feature vectors of the ith joints and the edge connecting the ith and jth joints at the kth iteration; f k e (·) and f k v (·) are two MLP-formed feature extractors; [·, ·] is the concatenation; D e and D v denote the dimensions of edge and joint features, respectively. (1a) maps a pair of joint features to the in-between edge features; (1b) aggregates all edge features associated with the same joint and maps to the corresponding joint features. After K iterations, information are fully propagated between joints and edges; in other words, each joint feature obtained has aggregated the integrated information in a long-range.</p><p>Given any joint feature after K iterations, p K , we compute the relation strength between each pair of joints, leading to an actional graph. We build two individual embedding networks, f emb (·) and g emb (·), to further learn the high-level representations of joints. The (i, j)th element of the adjacent matrix of actional graph is formulated as</p><formula xml:id="formula_5">(A act ) i,j = exp (f T i g j ) M k=1 exp (f T i g k ) ∈ [0, 1]<label>(2)</label></formula><p>where f i = f emb (p K i ) and g i = g emb (p K i ) ∈ R D emb are the two different embeddings of joint v i . Notably, (A act ) i,j = (A act ) j,i , indicating incoming and outgoing relations between joints. (2) uses the softmax to normalize the edge weights and promote a few large ones.</p><p>The structure of AGIM is illustrated in <ref type="figure">Fig. 3</ref>, where the information is propagated for K times between joints and any possible edges and two embedded joint features are used to calculate actional graphs in the end.</p><p>Given the joint-scale actional graphs with adjacent matrix A act , we aggregate the joint information along the action-based relations. We design an actional graph convolution (AGC) to capture the actional features. Mathemati-cally, let the input features at frame t be X (t) ∈ R M ×Dx , the output features be Y (t) AGC ∈ R M ×Dx , the AGC works as</p><formula xml:id="formula_6">Y (t) AGC = AGC(X (t) ) = A act X (t) W act<label>(3)</label></formula><p>where W act ∈ R Dy×Dx is a trainable weight. Therefore, the model aggregates the action-based information from collaboratively moving joints even in the distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Structural Graph Convolution</head><p>Intuitively, the joint dynamics is limited due to physical constraints, namely bone connections. To capture these relations, we develop a structural graph, G str (V, A str ). Let A is the adjacency matrix of the skeleton graph (see Section 3), the normalized adjacency matrix bẽ</p><formula xml:id="formula_7">A = D −1 A, where D ∈ N M ×M is a diagonal degree matrix with (D) ii = j (A) ij</formula><p>.Ã provides nice initialization to learn the edge weights and avoids multiplication explosion <ref type="bibr" target="#b51">[49]</ref>, <ref type="bibr" target="#b52">[50]</ref>.</p><p>We note thatÃ only describes the 1-hop neighborhood on body; that is, the bone-connected joints. To represent long-range relations, we use the high-order polynomial of A. Let the γ-order polynomial ofÃ beÃ γ , which could be directly computed from the skeleton structure;Ã γ indicates the relations between each joint and its γ-hop neighbors on skeleton. Given the high-order topologies, we introduce several individual edge-weight matrices M (γ) ∈ R M ×M corresponding toÃ γ , where each element is trainable to reflect the relation strength. We finally obtain the γ-order structural graph, whose weighted adjacency matrices is</p><formula xml:id="formula_8">A (γ) str =Ã γ M (γ) ∈ R M ×M , where</formula><p>denotes the element-wise multiplication. In this way, we are able to model the structure-based relations between one joint and others in relatively longer range. Practically, we consider the order γ = 1, . . . , Γ, thus we have multiple structural graphs for one body. See plot (c) in <ref type="figure">Fig. 2</ref>, the hand is correlated with the entire arm.</p><p>Given the structural graphs A (γ) str , we propose the structural graph convolution (SGC) operator. Let the input feature at frame t be X (t) ∈ R M ×Dx , the output feature be Y (t) SGC ∈ R M ×Dy , the SGC operator is formulated as</p><formula xml:id="formula_9">Y (t) SGC = SGC(X (t) ) = Γ γ=1 A (γ) str X (t) W (γ) str (4) where W (γ) str ∈ R Dy×Dx is the trainable model parameters.</formula><p>Notably, the multiple structural graphs have different corresponding weights, which help to extract richer features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Joint-Scale Graph Convolution</head><p>To extract both spatial and temporal features of actions, we now propose the joint-scale graph and temporal convolution block. Based on AGC (3) and SGC (4), we present the jointscale graph convolution (JGC) to capture comprehensive joint-scale spatial features. Mathematically, let the input joint features at frame t be X (t) ∈ R M ×Dx , the output features be Y (t) JGC ∈ R M ×Dy , the JGC is formulated as</p><formula xml:id="formula_10">Y (t) JGC = JGC(X (t) ) = λ act AGC(X (t) ) + SGC(X (t) ), (5)</formula><p>where λ act is a hyper-parameter to trade off the contribution between actional and structural features. Some non-linear activation functions can be applied to it. In this way, the joint features are effectively aggregated to update each center joint according to the joint-scale graphs.</p><p>We further show the stability of the proposed activated joint-scale graph convolution layer; that is, when input 3D skeleton data is disturbed, the distortion of the output features is upper bounded. Theorem 1 (Stability) Let two joint-scale feature matrices be X and X * ∈ R M ×Dx associated with a skeleton graph A ∈ {0, 1} M ×M , where D x = 3 and X * − X F ≤ ( ≥ 0). Let Y = ρ (JGC (X)) and Y * = ρ (JGC (X * )) ∈ R M ×Dy . Let A * act and A act ∈ [0, 1] M ×M be the joint-scale actional graph inferred from X * and X, respectively, where A * act X * − A act X F ≤ C q , with q the amplify factor and C some constant. Let µ act = W act max , η (γ) = M (γ) max and µ</p><formula xml:id="formula_11">(γ) str = W (γ) str max , where W act , W (γ) str ∈ R Dy×Dx and M (γ) ∈ R M ×M . Then, Y * − Y F ≤ 3D y q λ act µ act C + Γ γ=1 A γ 0 η (γ) µ (γ) str = O (max ( q , )) .</formula><p>Note that · F denotes Frobenius norm and · 0 is zero norm. ρ(·) denotes ReLU-activation on each element of the data. O(·) denotes the effects that rely on 'max ( q , )'.</p><p>See the proof in Appendix. Theorem 1 only shows the jointscale graph convolution at the first layer, but the bound can be extended to the subsequent layers. We make this distinction because the actional graph only depends on the input data. Theorem 1 shows that 1) the outputs of JGC followed by the activation function can be upper bounded, reflecting its robustness against perturbation of inputs; and 2) given a fixed model, the bound is mainly related to the amplify factor q, reflecting how much the actional graph would amplify the perturbation. In the experiments, we show that q is around 1. We also test JGC's robustness against the input perturbation, ensuring that Sym-GNN has stable performance given small noises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Joint-Scale Graph and Temporal Convolution Block</head><p>While the JGC operator leverages the joint spatial relations and extracts rich features, we should consider modeling the temporal dependencies among consecutive frames. We develop the temporal convolution operator (TC); that is, a convolution along time to learn the movements. Stacking JGC and TC, we build the joint-scale graph and temporal convolution block (J-GTC block), which learn the spatial and temporal features in tandem. Mathematically, let X in ∈ R T ×M ×Dx be an input tensor, each J-GTC block works as</p><formula xml:id="formula_12">(X ) t = ρ (JGC((X in ) t )) ∈ R M ×D x ,<label>(6a)</label></formula><formula xml:id="formula_13">X out = ρ (TC(X )) ∈ R T /s×M ×(sD x ) ,<label>(6b)</label></formula><p>where ρ(·) represents a nonlinear ReLU function, TC(·) is a standard 1D convolution along the time axis, whose  temporal kernel size is τ ; s is the convolution stride along time to shrink the temporal dimension; t is the time stamp.</p><p>In each J-GTC block, (6a) extracts spatial features using the multiple spatial relations between joints; and (6b) extracts temporal features by aggregating the information in several consecutive frames. Our J-GTC also includes batch normalizations and dropout operations. Moreover, there is a residual connection preserving the input features. The architecture of one J-GTC block is illustrated in <ref type="figure" target="#fig_1">Fig. 4</ref>. By stacking several J-GTC blocks in a hierarchy, we could gradually convert the motion dynamics from the sample space to the feature space; that is we capture the high-level semantic information of input sequence for action recognition and motion prediction downstream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Part-Scale Graph Operators</head><p>The joint-scale graphs treat body-joints as nodes and model their relations, but some action patterns depend on more abstract movements of body-parts. For example, 'hand waving' shows a rising arm, but the finger and wrist are less important. To model the part dynamics, we propose a part-scale graph and the part-scale graph and temporal convolution (P-GTC) block to extract part-scale features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Part-Scale Graph Convolution</head><p>For a part-scale graph, we define M p = 10 body-parts as graph nodes: 'head', 'torso', pairs of 'upper arms', 'forearms', 'thighs' and 'crura', which integrates the covered joints on joint-scale body. And we build the edges according to body nature. The right plot of <ref type="figure">Fig. 5</ref> shows an example of part-scale graph, whose vertices are 10 parts and edges are based on nature. The self-looped binary adjacent matrix of part-scale graph is A p ∈ {0, 1} Mp×Mp , where (A p ) ij = 1 if the ith and jth parts are connected. We normalize A p by</p><formula xml:id="formula_14">A part = (D −1 p A p ) M p ∈ R Mp×Mp where D p ∈ N Mp×Mp is the diagonal degree matrix of A p ; M p ∈ R Mp×Mp is a trainable weight matrix and is the element-wise multiplication.</formula><p>Similarly to the JGC operator (5), we propose the partscale graph convolution (PGC) for spatial feature learning. Let the part features at time t be X</p><formula xml:id="formula_15">(t) p ∈ R Mp×Dx , the output features be Y (t) PGC ∈ R M ×Dy , the PGC works as Y (t) p = PGC(X (t) p ) = A part X (t) p W part ,<label>(7)</label></formula><p>where W part is the trainable parameters. With <ref type="bibr" target="#b9">(7)</ref>, we propagate information between body-parts on the part-scale graph, leading to abstract spatial patterns. Notably, We do not need a part-scale actional graph, because the part-scale graph includes some integrated relations internally, as well as it has a shorter distance to build long-range links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint-scale Graph Part-scale Graph</head><p>Avg Copy joint2part pooling part2joint matching <ref type="figure">Fig. 5</ref>. A joint-scale graph consists of body-joints represented as blue nodes and a part-scale graph consists of body-parts represented as orange nodes. The bidirectional fusion converts features across two scales through the operations of joint2part pooling and part2joint matching. We only plot the 1-hop structural graph for the joint-scale graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Part-Scale Graph and Temporal Convolution Block</head><p>Considering the temporal evolution, we use the same temporal convolution as in J-GTC block to form the part-scale graph and temporal convolution block (P-GTC block). Let the input part feature tensor be X p,in ∈ R T ×Mp×Dx , we have</p><formula xml:id="formula_16">(X p ) t = ρ (PGC((X p,in ) t )) ∈ R Mp×D x ,<label>(8a)</label></formula><formula xml:id="formula_17">X p,out = ρ TC(X p ) ∈ R T /s×Mp×(sD x ) ,<label>(8b)</label></formula><p>where t denotes the time stamp and s is the temporal convolution stride. Comparing to the J-GTC block, the P-GTC block extracts the spatial and temporal features of actions in a higher scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Difference Operator</head><p>Intuitively, the states of motion, such as velocity and acceleration, carry important dynamics information and make it easier to extract spatial-temporal features. To achieve this, we employ a difference operator to preprocess the input sequences. The idea is to compute high-order differences of the pose sequences, guiding the model to learn motion information more easily. The zero-order difference is</p><formula xml:id="formula_18">∆ 0 X (t) = X (t) ∈ R M ×Dx , where X (t)</formula><p>is the pose at the time t, and the β-order difference (β &gt; 0) of the pose is</p><formula xml:id="formula_19">∆ β+1 X (t) = ∆ β X (t) − ∆ β X (t−1) ∈ R M ×Dx ,<label>(9)</label></formula><p>where ∆ β denotes the βth-order difference operator. We use zero paddings to handle boundary conditions. We take the first three orders (β = 0, 1, 2) to our model, reflecting positions, velocities, and accelerations. In the model, the three differences can be efficiently computed in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SYMBIOTIC GRAPH NEURAL NETWORKS</head><p>To construct the multitasking model, we need a deep backbone for high-level action pattern extraction as well as two task-specific modules. In this section, we present the architecture of our Symbiotic Graph Neural Networks (Sym-GNN). First, we present the deep backbone network, which uses multi-scale graphs for feature learning; We then present the action-recognition head and the motion-prediction head with an effective multitasking scheme. Finally, we present a dual network, which learns features from body bones, instead of body-joints, and provides complementary information for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Backbone: Multi-Branch Multi-Scale Graph Convolution Networks</head><p>To learn the high-level action pattern, the proposed Sym-GNN consists of a deep backbone called multi-branch multiscale graph convolution network (multi-branch multi-scale GCN). It employs parallel multi-scale GCN branches to treat high-order action differences for rich dynamics learning and also considers multi-scale graphs for spatial feature extraction. <ref type="figure" target="#fig_3">Fig. 6</ref> shows the backbone, where the left plot is the backbone framework including three branches of multiscale GCNs; the right plot is the structure of each branch of multi-scale GCN. As follows, we propose the backbone architecture in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Multiple Branches</head><p>The backbone has three branches of multi-scale GCN. Each branch uses a distinct order of action differences as input, treating the motion states for dynamics learning (see <ref type="figure" target="#fig_3">Fig. 6</ref>). Calculating the differences by difference operators, we first obtain the proxies of 'positions', 'velocities' and 'accelerations' as the input of the network; see <ref type="bibr" target="#b11">(9)</ref>. The three branches have identical network architectures. We obtain semantics of high-order differences and concatenate them together for action recognition and motion prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Multi-Scale GCN</head><p>To learn the detailed and general action features comprehensively, each branch of the backbone is a multi-scale GCN based on two scales of graphs: joint-scale and part-scale graphs. For each scale, we use the corresponding operators, i.e. J-GTC blocks (see section 4.1.4) and P-GTC block (see section 4.2.1) to extract spatial and temporal features. Concretely, in the joint scale, the body is modeled by joint-scale graphs, where both the actional graph and structural graphs are used to capture body-joint correlations. This joint scale uses a cascade of J-GTC blocks based on the learned actional-structural graphs. In the part scale, we use part-scale graphs whose nodes are body-parts to represent high-level body instances, and we stack multiple P-GTC blocks for feature capturing. To be aware of the multi-scale immediate representations and learn rich and consistent patterns, we introduce a fusion mechanism between the hidden layers of two scales; called bidirectional fusion.</p><p>Bidirectional Fusion. The bidirectional fusion exchanges features from both the joint scale and the part scale; see illustrations in <ref type="figure">Fig. 5</ref> and <ref type="figure" target="#fig_3">Fig. 6</ref>. It contains two operations:</p><p>• Joint2part pooling. For the joint scale, we use pooling to average the joint features on the same part to represent a super node. Then, we concatenate the pooling result to the corresponding part feature in the part scale. As shown in <ref type="figure">Fig. 5</ref>, we average torso joints to obtain a node in the part-scale graph and concatenate it to the original part-scale features.</p><p>• Part2joint matching. For the part scale, the part features are copied for several times to match the number of corresponding joints, as well as we concatenate the copied parts to the joints. As shown in <ref type="figure">Fig. 5</ref>, we copy the thigh twice and concatenate them to the hip and knee in the joint scale.   <ref type="figure" target="#fig_3">Fig. 6</ref>. Backbone is essentially multi-branch multi-scale graph convolution networks. It uses three individual multi-scale GCNs to extract spatial and temporal features. A difference operator ('Diff') calculate three orders of differences, which represent joint positions ('pos.'), velocities ('vel.') and accelerations ('acc.'). Each multi-scale GCN takes one order as input and uses multiple J-GTC, P-GTC blocks and bidirectional fusion to learn spatial and temporal features from two scales. Given the jointscale input attributes, we first use a J-GTC block to extract the initial joint-scale features, and a joint2part pooling is applied on the joint-scale features to compute the initial part-scale features. We next feed them into two parallel J-GTC and P-GTC blocks. Then we concatenate the responses mapped by joint2part pooling and part2joint matching to the features in opposite scales. Therefore, both scales have good adaptability to multi-scale information. After multiple interactive J-GTC and P-GTC blocks in the multi-scale GCN, we fuse the outputs of two scales through summation, followed by the average pooling to remove the temporal dimension, and obtain the high-level features. Finally, we concatenate the outputs from three branches together and use them as the comprehensive semantics for action recognition and motion prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale GCN</head><formula xml:id="formula_20">H 0 H 1 H 2 Tprev × M × Dx M × 3Dh Tprev × M × Dx Tprev × M × 2Dx Tprev × Mp × Dx Tprev × Mp × 2Dx T × M × Dh Output</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multitasking I: Action Recognition</head><p>For action recognition, Sym-GNN can be represented aŝ y = F recg (X prev ; θ bk , θ recg ), where F recg (·) is the recognition sub-model of entire Sym-GNN, F(·); that is, the recognition performance depends on the backbone network and a recognition module constructed following the backbone. Given the high-level features extracted by three branches of backbone, H 0 , H 1 and H 2 ∈ R M ×D h , we concatenate them and employ an MLP to produce the fused feature:</p><formula xml:id="formula_21">H recg = MLP recg ([H 0 , H 1 , H 2 ]) ∈ R M ×D h ,</formula><p>where MLP recg (·) denotes the fusing network of recognition task and [·, ·, ·] is the concatenation operator of three matrices along feature dimension. To integrate the joint dynamics, we apply the global averaging pooling on the M joints of H recg and obtain a feature vector h recg ∈ R D h that represents the whole body. To generate the recognition results, we finally feed the vector into a 1-layer network with a softmax classifier, obtainingŷ ∈ [0, 1] C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multitasking II: Motion Prediction</head><p>For motion preidction, our Sym-GNN works asX pred = F pred (X prev ; θ bk , θ pred ), using the backbone and a prediction module, where F pred (·) is the prediction sub-model of Sym-GNN. Therefore, we additionally build a motionprediction head, whose functionality is to sequentially predict the future poses. <ref type="figure" target="#fig_4">Fig. 7</ref> shows the overall structure. We adopt the self-regressive mechanics and identical connection in the motion-prediction head, which utilizes gated recurrent unit (GRU) to model the temporal evolution.</p><p>Concretely, an MLP is first employed to embed the features of three action differences:</p><formula xml:id="formula_22">H pred = MLP pred ([H 0 , H 1 , H 2 ]) ∈ R M ×D h ,</formula><p>where MLP pred (·) denotes the fusing network of prediction task. Let H (0) pred = H pred be the initial states of GRU-based predictor andX (0) = X (0) be the pose in the current time stamp. To produce the (t + 1)th pose (t ≥ 0), the motionprediction head works as</p><formula xml:id="formula_23">H (t) pred = JGC(H (t) pred ),<label>(10a)</label></formula><formula xml:id="formula_24">H (t+1) pred = GRU([X (t) , ∆ 1X(t) , ∆ 2X(t) ,ŷ], H (t) pred ), (10b) X (t+1) =X (t) + f pred (H (t+1) pred ),<label>(10c)</label></formula><p>where JGC(·), GRU(·) and f pred (·) represent JGC operator <ref type="formula">(5)</ref>, GRU cell and output MLP, respectively. The follow-ingX (t) s are the predictions obtained sequentially and used recylingly. In Step (10a), we apply the JGC to update the hidden states; in Step (10b), we feed the updated hidden states, current pose and classified labels into the GRU cell to produce the features that reflect future displacement; In Step (10c), we add the predicted displacement to the previous pose to predict the next frame.</p><p>The motion-prediction head has three advantages: (i) we use JGC to update hidden features, capturing more complicated motion patterns; (ii) we input multiple orders of poses differences and classified labels to the GRU, providing explicit motion priors; and (iii) Connected by the residual, the GRU and MLP predict the displacement for each frame; this makes predictions precise and robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multi-Objective Optimization</head><p>To train action recognition and motion prediction simultaneously, we consider a multi-objective scheme.</p><p>To recognize actions, we minimize the cross entropy between the ground-truth categorical labels and the inferred ones. Let the true label of the nth sample be (y) n ∈ {0, 1} C and the corresponding classification results be (ŷ) n ∈ {0, 1} C . For N training samples in one mini-batch, the action recognition loss is formulated as</p><formula xml:id="formula_25">L recg = − 1 N N n=1 (y) n log(ŷ) n ,<label>(11)</label></formula><p>where denotes the transpose operation.</p><p>For motion prediction, we minimize the 1 distance between the target motions and the predicted clips. Let the nth target and predictions be (X pred ) n and (X pred ) n , for N samples in one mini-batch, the prediction loss is</p><formula xml:id="formula_26">L pred = 1 N N n=1 (X pred ) n − (X pred ) n 1 ,<label>(12)</label></formula><p>where · 1 denotes the 1 norm. According to our experiments, the 1 norm leads to more precise predictions compared to the common 2 norm. To integrate two losses for training, we propose a convex combination that weighted sums (11) and <ref type="bibr" target="#b14">(12)</ref>; that is</p><formula xml:id="formula_27">L = λL recg + (1 − λ)L pred ,</formula><p>where λ trade-offs the importances of two tasks. To balance action recognition and motion prediction, instead of using a fixed λ selected by hand, we employ the 'multiple-gradient descent algorithm' (MGDA) to obtain the proper coefficient λ for multitasking loss terms. Following <ref type="bibr" target="#b53">[51]</ref>, the parameter λ is adaptively adjusted during training by matching one of Karush-Kuhn-Tucker (KKT) conditions of the optimization problem. Therefore, the optimized λ allocates weights for the two tasks adaptively, leading to better performances for both tasks. In our training scheme, all the model parameters are trained end-to-end with the stochastic gradient descent algorithm <ref type="bibr" target="#b54">[52]</ref>; see more details in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Bone-based Dual Graph Neural Networks</head><p>While the joints contain some information of action representation from the joint aspect, the attributes of bones, such as lengths and orientations, are crucial to provide some complementary information. In this section, we construct a bone-based dual graph against original joint-scale graph, whose vertices are bones and edges link bones.</p><p>To represent the feature of each bone, we compute the subtraction of two endpoint joints coordinates, which includes information of bone lengths and orientations. The subtraction order is from the centrifugal joint v j to the centripetal v i . Let the joint locations along time be</p><formula xml:id="formula_28">x i , x j ∈ R DxTprev , the bone attribute is b i,j = x j − x i ∈ R DxTprev .</formula><p>Then, we construct the bone-based dual actional and structural graphs to model the bone relations; and we also build the part-scale dual graph. The dual actional graph is learned from bone features by AGIM (see section 4.1.1); for dual structural graph, the 1-hop edges are linked when two bones with articulated joints and the high-hop edges are extended from the 1-hop edges; the part-scale attributes are obtained by integrating bone attributes and the part-scale graph is built according to body nature; The bone-based graphs are dual of joint-scale graphs, which are employed to extract complementary bone features.</p><p>Given the bone-based graphs, we train a bone-based graph neural network. We input bone attributes; then everything else follows the joint-based network. Finally, we fuse the joint-based and bone-based recognition outputs before softmax functions by a weighted summation to calculate the classification results, which tend to be more accurate and improve motion prediction effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS AND ANALYSIS</head><p>In this section, we evaluate the proposed Sym-GNN. First, we introduce the datasets and model settings in detail; then, the performance comparisons between Sym-GNN and other state-of-the-art methods are presented; and we finally show the ablation studies of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets and Model Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Dataset</head><p>We conduct extensive experiments on four large-scale datasets: NTU-RGB+D <ref type="bibr" target="#b34">[32]</ref>, Kinetics <ref type="bibr" target="#b16">[14]</ref>, Human 3.6M <ref type="bibr" target="#b35">[33]</ref> and CMU Mocap. The details is shown as follow.</p><p>NTU-RGB+D: NTU-RGB+D, containing 56, 880 skeleton action sequences completed by one or two performers and categorized into 60 classes, is one of the largest datasets for 3D skeleton-based action recognition. It provides the 3D spatial coordinates of 25 joints for each subject in an action. For method evaluation, two protocols are recommended: 'Cross-Subject' (CS) and 'Cross-View' (CV). In CS, 40, 320 samples performed by 20 subjects are separated into the training set, and the rest belong to the test set. CV assigns data according to camera views, where training and test set have 37, 920 and 18, 960 samples, respectively.</p><p>Kinetics: Kinetics is a large dataset for human action analysis, containing over 240, 000 video clips. There are 400 classes of actions. Due to only RGB videos, we obtain skeleton data by estimating joint locations on pixels with OpenPose toolbox <ref type="bibr" target="#b55">[53]</ref>. The toolbox generates 2D pixel coordinates (x, y) and confidence score c for totally 18 joints. We represent each joint as a three-element feature vector: [x, y, c] . For the multiple-person cases, we select the body with the highest average joint confidence in each sequence. Therefore, one clip with T frames is transformed into a skeleton sequence with the dimension of 18 × 3 × T . Human 3.6M: Human 3.6M (H3.6M) is a large motion capture dataset and also receives increasing popularity. Seven subjects are performing 15 classes of actions, where each subject has 32 joints. We downsample all sequences by two. The models are trained on six subjects and tested on the specific clips of the 5th subject. Notably, the dataset provides the joint locations in angle space, and we transform them into exponential maps and only use the joints with non-zero values (actually 21 joints).</p><p>CMU Mocap: CMU Mocap includes five major action categories, and each subject in CMU Mocap has 38 joints, which are presented by angle positions. We use the same strategy presented in <ref type="bibr" target="#b33">[31]</ref> to select the actions. Thus we choose eight actions: 'Basketball', 'Basketball Signal', 'Directing Traffic', 'Jumping', 'Running', 'Soccer', 'Walking' and 'Washing Window'. We preprocess the data and compute the corresponding exponential maps with the same approach as we do for Human 3.6M dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Model Setting and Implementation Details</head><p>The models are implemented with PyTorch 0.4.1. Since different datasets have distinctive patterns and complexities, we employ specific configurations of Sym-GNN networks on corresponding datasets for features learning.</p><p>For NTU-RGB+D and Kinetics, the backbone network of Sym-GNN contains 9 J-GTC blocks and 8 P-GTC blocks. In each three J-GTC and P-GTC blocks, the feature dimensions are respectively 64, 128 and 256. The kernel size of TC is 9 and it shrinks the temporal dimension with stride 2 after the 3rd and 6th blocks, where we use bidirectional fusion mechanisms. λ act = 0.5. The action-recognition head is a 2-layer MLP, whose hidden dimension is 256. For the motion-prediction head, the hidden dimensions of GRU and output MLP are 256. For the actional graph inference module (AGIM), we use 2-layer 128-D MLPs with ReLU, batch normalization and dropout in each iteration. We use SGD algorithm to train Sym-GNN, where the learning rate is initially 0.1 and decays by 10 every 30 epochs. The model is trained with batch size 48 for 100 epochs on 8 GTX-1080Ti GPUs. For both NTU-RGB+D and Kinetics, the last 10 frames are used for motion prediction and other previous frames are fed into Sym-GNN for action recognition.</p><p>As for Human 3.6M and CMU Mocap, due to the simpler dynamics and fewer categories, we propose a light version of Sym-GNN, which extracts meaningful features with more shallow networks, improving efficiency for motion prediction. In the backbone, we use 4 J-GTC blocks and 3 P-GTC blocks, whose feature dimensions are 32, 64, 128 and 256; the temporal convolution strides in 4 blocks are: 1, 2, 2, 2, respectively. We apply bidirectional fusions at the last 3 layers. λ act = 1.0. The recognition and motion-prediction heads, as well as AGIM, leverage the same architecture as we set for NTU-RGB+D. We train the model using Adam optimizer with the learning rate 1 × 10 4 and batch size 64 for 10 5 iterations on one GTX-1080Ti GPU. All the hyperparameters are selected using a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison with State-of-the-Arts</head><p>On the three large-scale skeleton-formed datasets, we compare the proposed Sym-GNN with state-of-the-art methods for human action recognition and motion prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">3D Skeleton-based Action Recognition</head><p>For action recognition, we first show the classification accuracies of Sym-GNN and baselines on two recommended benchmarks of NTU-RGB+D, i.e. Cross-Subject and Cross-View <ref type="bibr" target="#b34">[32]</ref>. The state-of-the-art models are based on manifold analysis <ref type="bibr" target="#b12">[10]</ref>, recurrent neural networks <ref type="bibr" target="#b14">[12]</ref>, <ref type="bibr" target="#b30">[28]</ref>, <ref type="bibr" target="#b34">[32]</ref>, convolution networks <ref type="bibr" target="#b11">[9]</ref>, <ref type="bibr" target="#b15">[13]</ref>, <ref type="bibr" target="#b26">[24]</ref>, and graph networks <ref type="bibr">[1]</ref>, <ref type="bibr" target="#b16">[14]</ref>, <ref type="bibr" target="#b30">[28]</ref>, <ref type="bibr" target="#b31">[29]</ref>, <ref type="bibr" target="#b37">[35]</ref>, <ref type="bibr" target="#b38">[36]</ref>, <ref type="bibr" target="#b39">[37]</ref>, <ref type="bibr" target="#b40">[38]</ref>, <ref type="bibr" target="#b56">[54]</ref>. Moreover, to investigate different components of Sym-GNN, such as multiple graphs and multitasking, we test several model variants, <ref type="bibr">TABLE 1</ref> Comparison of action recognition on NTU-RGB+D. The accuracies on both Cross-Subject (CS) and Cross-View (CV) benchmarks.</p><p>Methods CS CV Lie Group <ref type="bibr" target="#b12">[10]</ref> 50.1% 52.8% H-RNN <ref type="bibr" target="#b14">[12]</ref> 59.1% 64.0% Deep LSTM <ref type="bibr" target="#b34">[32]</ref> 60.7% 67.3% PA-LSTM <ref type="bibr" target="#b34">[32]</ref> 62.9% 70.3% ST-LSTM+TS <ref type="bibr" target="#b36">[34]</ref> 69.2% 77.7% Temporal Conv <ref type="bibr" target="#b26">[24]</ref> 74.3% 83.1% Visualize CNN <ref type="bibr" target="#b15">[13]</ref> 76.0% 82.6% C-CNN+MTLN 79.6% 84.8% ST-GCN <ref type="bibr" target="#b16">[14]</ref> 81.5% 88.3% DPRL <ref type="bibr" target="#b56">[54]</ref> 83.5% 89.8% SR-TSL <ref type="bibr" target="#b30">[28]</ref> 84.8% 92.4% HCN <ref type="bibr" target="#b11">[9]</ref> 86.5% 91.1% STGR-GCN <ref type="bibr" target="#b39">[37]</ref> 86.9% 92.3% motif-GCN <ref type="bibr" target="#b40">[38]</ref> 84.2% 90.2% AS-GCN <ref type="bibr">[1]</ref> 86.8% 94.2% 2s-AGCN <ref type="bibr" target="#b37">[35]</ref> 88.5% 95.1% AGC-LSTM <ref type="bibr" target="#b31">[29]</ref> 89.2% 95.0% DGNN <ref type="bibr" target="#b38">[36]</ref> 89 including Sym-GNN using only joint-scale structural graphs (Only J-S), only joint-scale actional graphs (Only J-A), onlypart scale graph (Only P), no bone-based dual graphs (No bone), no prediction for multitasking (No pred) and complete model. <ref type="table">Table 1</ref> presents recognition accuracies of methods. We see that the complete Sym-GNN, which utilizes joint-scale and part-scale graphs, motion motionprediction head and dual bone-based network, outperforms the baselines on both benchmarks. The results reveal that richer joint relations promote to capture more useful patterns, and additional motion prediction and complementary bone-based features improve the discrimination. Then, we evaluate our model for action recognition on Kinetics and compare it with six previous models, includ-  .05 (%) between Sym-GNN and state-of-the-art methods for short-term motion prediction on NTU-RGB+D. The variant of Sym-GNN (No recg) denotes our model without using the recognition task to enhance motion prediction. <ref type="table" target="#tab_4">Future frames  1  2  3  4  5  6  7  8  9</ref> 10 Average</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Subject</head><p>ZeroV <ref type="bibr" target="#b19">[17]</ref> 70. <ref type="bibr" target="#b34">32</ref>    ing a hand-crafted based method, Feature Encoding <ref type="bibr" target="#b13">[11]</ref>, two deep models, Deep LTSM <ref type="bibr" target="#b34">[32]</ref> and Temporal Con-vNet <ref type="bibr" target="#b26">[24]</ref>, and three graph-based methods, ST-GCN <ref type="bibr" target="#b16">[14]</ref>, 2s-AGCN <ref type="bibr" target="#b37">[35]</ref>, and DGNN <ref type="bibr" target="#b38">[36]</ref>. <ref type="table">Table 2</ref> shows the top-1 and top-5 classification results, and 'no pred' denotes the Sym-GNN variant without motion-prediction head and multitasking framework. We see that Sym-GNN outperforms other methods on top-1 recognition accuracy and achieves competitive results on top-5 recognition accuracy.</p><p>Additionally, we evaluate our model for action recognition on Human 3.6M and CMU Mocap. <ref type="table" target="#tab_4">Table 3</ref> presents the top-1 and top-5 classification accuracies for both two datasets. Here we compare Sym-GNN with a few recently proposed methods: ST-GCN <ref type="bibr" target="#b16">[14]</ref>, HCN <ref type="bibr" target="#b11">[9]</ref>, and 2s-AGCN <ref type="bibr" target="#b37">[35]</ref>. We also show the effectiveness of our model. Notably, for Human 3.6M, there is a relatively large gap between the top-1 and top-5 accuracies, because the input motions are some fragmentary clips of long sequences with incomplete semantics and activities have subtle differences (e.g. 'Eating' and 'Smoking' are similar). In other words, Sym-GNN learns the common features and provides reasonable discrimination, resulting in high top-5 accuracy; but it confuses in non-semantic variances, causing not high top-1 accuracy. However, CMU Mocap has more distinctive actions, where we obtain high classification accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">3D Skeleton-based Motion Prediction</head><p>To validate the model for predicting future motions, we train the Sym-GNN on NTU-RGB+D, Human 3.6M, and CMU Mocap. There are two specific tasks: short-term and long-term motion prediction. Concretely, the target of shortterm prediction is commonly to predict poses within 400 milliseconds, while the long-term prediction aims to predict poses in 1000 ms or longer. To reveal the effectiveness of Sym-GNN, we introduce many state-of-the-art methods, which learned dynamics from pose vectors <ref type="bibr" target="#b19">[17]</ref>, <ref type="bibr" target="#b20">[18]</ref>, <ref type="bibr" target="#b29">[27]</ref>, <ref type="bibr" target="#b44">[42]</ref>, <ref type="bibr" target="#b58">[56]</ref> or separate body-parts <ref type="bibr" target="#b18">[16]</ref>, <ref type="bibr" target="#b32">[30]</ref>, <ref type="bibr" target="#b33">[31]</ref>. We also introduce a naive baseline, named ZeroV <ref type="bibr" target="#b19">[17]</ref>, which sets all predictions to be the last observed frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short-term motion prediction:</head><p>We validate Sym-GNN on two datasets: NTU-RGB+D and Human 3.6M. For NTU-RGB+D, we train Sym-GNN to generate future 10 frames for each input sequence. We compare our Sym-GNN with several previous methods and the Sym-GNN variant which abandons auxiliary action-recognition head (No recg). As the metric, We use the percentage of correct points within a normalized region 0.05 (PCK@0.05), that is, a joint is counted as correctly predicted if the normalized distance between the predicted location and ground-truth is less than 0.05. The PCK@0.05 of different models are presented in <ref type="table" target="#tab_5">Table 4</ref>. We see: 1) our model extremely outperforms the baselines with a large margin especially for the longer term;  <ref type="figure" target="#fig_1">29 1.43 0.18 0.44 0.99 1.22 0.40 0.62 1.00 1.08 0.23 0.41 0.80 0.97</ref> Sym <ref type="figure" target="#fig_1">-GNN  0.23 0.42 0.57 0.65 0.35 0.60 0.95 1.15 0.48 0.80 1.28 1.41 0.18 0.45 0.97 1.20 0.40 0.60 0.97 1.04 0.24 0.41 0.77 0</ref>  <ref type="figure" target="#fig_1">28 0.60 0.89 0.99 0.14 0.32 0.53 0.64 0.22 0.48 0.87 1.06 0.42 0.73 1.08 1.22 0.16 0.33 0.50 0.56 0.26 0.49 0.79 0</ref>.92  <ref type="figure" target="#fig_1">.48 0.91 1.06 1.47 0.12 0.21 0.38 0.49 0.94 0.20 0.41 0.75 0.87 1.84 0</ref>  <ref type="bibr" target="#b19">[17]</ref> 0 2) Using action recognition and motion prediction together obtains the highest PCK@0.05 along time, demonstrating the enhancements from recognition task for dynamics learning.</p><p>Then, we compare Sym-GNN to baselines for short-term prediction on Human 3.6M, where the models generate poses up to the future 400 ms. We analyze several variants of Sym-GNN with different components, including using only joint-scale actional graphs (Only J-A) or joint-scale structural graphs (Only J-S), as well as no recognition task (No recg). As another metric, the mean angle errors (MAE) between the predictions and the ground truths are computed, representing the errors from predicted poses to targets in angle space. We first test 4 representative actions: 'Walking', 'Eating', 'Smoking' and 'Discussion'. <ref type="table" target="#tab_7">Table 5</ref> shows MAEs of different methods that predict motions up to 400 ms. As we see, when Sym-GNN simultaneously employs multiple graphs and multitasking, our method outperforms all the baselines and its own ablations.</p><p>We also test Sym-GNN on the remaining 11 actions in Human 3.6M, where the MAEs of some recent methods are shown in <ref type="table" target="#tab_8">Table 6</ref>. Sym-GNN also achieves the best performance on most actions and the lowest average MAE on 15 motions. Although the mentioned top-1 classification accuracy on this dataset is not very high (see <ref type="table" target="#tab_4">Table 3</ref>), we note that the estimated soft labels cover the common motion factors, resulting in high top-5 recognition accuracy. For example, people walk in 'Walking', 'Walking Dog' and 'Walking Together', and we need the walking factors instead of the specific labels for motion generation. Given the soft labels, the model tends to obtain precise predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long-term motion prediction: For long-term prediction,</head><p>the Sym-GNN is tested on Human 3.6M and CMU Mocap. We predict the future poses up to 1000 millisecond. It is challenging due to action variation and non-linearity <ref type="bibr" target="#b19">[17]</ref>. <ref type="table" target="#tab_12">Table 8</ref> presents the MAEs of various models for predicting the 4 motions in Human 3.6M at the future 560 ms and 1000 ms. We see that Sym-GNN outperforms the competitors on 'Eating', 'Smoking' and 'Discussion', and obtain competitive results on 'Walking'.</p><p>To further evaluate Sym-GNN, we conduct long-term prediction on eight classes of actions in CMU Mocap. We present the MAEs of Sym-GNN with or without using the action-recognition head. <ref type="table" target="#tab_10">Table 7</ref> shows the predicting MAEs ranging from future 80 ms to 1000 ms. We note that we train the model for long-term prediction, where the 'short-term' MAEs are the intermediate results during predicting up to  1000 ms. We see that Sym-GNN significantly outperforms the state-of-the-art methods on actions 'Basketball', 'Basketball Signal' and 'Washing Window', and obtains competitive performance on 'Jumping' and 'Running'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness-efficiency tradeoff:</head><p>We also compare the prediction errors and efficiency of various models, because the high response speed and precise generation are both essential for real-time motion prediction. Notably, the AGIM propagates the features between joints and edges iteratively. The iteration times K trades off between effectiveness and speed; i.e. larger K leads to a lower MAE but slower speed. To represent the running speed, we use the generated frame numbers in each 20 ms (frame period) when we predict up to 400 ms. We tune K and compare Sym-GNN to other methods on Human 3.6M and show the running speeds and MAEs for prediction in 400 ms. <ref type="figure" target="#fig_7">Fig. 8</ref> shows the effectiveness-efficiency tradeoff, where the x-axis is the generated frame numbers in 20 ms (reflecting prediction speed) and the y axis is the MAE. Different red circles denote different numbers of iterations K in AGIM, i.e. from the rightmost circle to the leftmost one, K = 0, 1, 2, 3, 4. We see that the proposed Sym-GNN is both faster and more precise compared to its competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Symbiosis of Recognition and Prediction</head><p>To analyze the mutual effects of action recognition and motion prediction, we conduct several experiments.  We first study the effects on action recognition from motion prediction. We use accurate class labels but noisy future poses to train the multitasking Sym-GNN for action recognition. To represent noisy supervisions, we randomly shuffle a percentage of targets motions among training data. <ref type="table" target="#tab_13">Table 9</ref> presents the recognition accuracies with various ratios of noisy prediction targets on two benchmarks of NTU-RGB+D. We also show the recognition results of the model without motion-prediction head. We see that 1) the predicted head benefits the action-recognition head. Introducing a motion-prediction head is beneficial even when the noise ratio is around 50%; 2) when the noise ratio exceeds 50%, the recognition performance tends to be slightly worse than that of the model without the motion-prediction head, reflecting that the action recognition is robust against the deflected motion prediction. Consequently, we show that motion prediction strengthens action recognition.</p><p>On the other hand, we test how confused recognition results affect motion prediction by using noisy action categories. Following <ref type="table" target="#tab_13">Table 9</ref>, we shuffle training labels to represent categorical noise. <ref type="table" target="#tab_14">Table 10</ref> presents the average MAEs for short-term prediction with noisy action labels on Human 3.6M. We demonstrate that an accurate actionrecognition head helps effective motion prediction.</p><p>We finally test the promotion on recognition when the observed data is limited, where we intercept the early motions by a ratio (e.g. 10%) for action recognition. There are three models with various prediction strategies: 1) predicting the future 10 frames ('Pred 10 frames'); 2) predicting all future frames ('Pred all frames'); 3) no prediction ('No pred'). <ref type="figure">Fig. 9</ref> illustrates the recognition accuracies of three models on different observation ratios. As we see, when the observation ratio is low, 'Pred all frames' can be aware of the entire action sequences and capture richer dynamics, showing the best performance; when the observation ratio is high, predicting 10 or all frames are similar because the inputs carry sufficient patterns, but they outperform 'No pred' as they preserve information, showing enhancement. <ref type="figure">Fig. 9</ref>. Given the same input, predicting more future poses leads to a better performance of action recognition. We see that across all the observation ratios, predicting all future poses is better than predicting 10 future poses; and both are better than no prediction. By introducing the motion-prediction head, our Sym-GNN has the potential for action classification in the early period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Effects of Graphs</head><p>In this section, we study the abilities of various graphs, namely, only joint-scale structural graphs (Only J-S), only joint-scale actional graph (Only J-A), only part-scale graph (Only P), and combining them (full). For action recognition, we train Sym-GNN on NTU-RGB+D, Cross-Subject and investigate different graph configurations. While involving joint-scale structural graph, we respectively set the number of hop in the joint-scale structural graphs (JS-Hop) to be Γ = 1, 2, 3, 4. Note that when we use only joint-scale structural graph with Γ = 1, the corresponding graph is exactly the skeleton itself. <ref type="table" target="#tab_15">Table 11</ref> presents the results of Sym-GNN with different graph components for action recognition. We see that 1) representing long-range structural relations, higher Γ leads to more effective action recognition; 2) combining the multiple graphs introduced from different perspectives improves the action recognition performance significantly.</p><p>For motion prediction, we study graphs components using similar setting of <ref type="table" target="#tab_15">Table 11</ref>. We validate Sym-GNN on Human 3.6M, the average short-term prediction MAEs are presented in <ref type="table" target="#tab_16">Table 12</ref>. We see that the effects multiple relations for promoting motion prediction are demonstrated. We note that too large Γ introduces redundancy and confusing relations, enlarging the prediction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Balance Joint-Scale Actional and Structural Graphs</head><p>In our model, we present that the power of joint-scale actional and structural graphs in JGC operator are traded off by a hyper-parameter λ act (see <ref type="bibr" target="#b7">(5)</ref>). Here we analyze how λ act affects the model performances. For action recognition, we test our model on NTU-RGB+D, Cross-Subject, and present the classification accuracies with different λ act ; for motion prediction, we show the average MAEs for short-term prediction. <ref type="figure" target="#fig_8">Fig. 10</ref> illustrates the model performances for both tasks. We see: 1) when λ act = 0.5, we obtain the highest recognition accuracies, showing large improvements than cases with other λ act ; 2) for motion prediction, the performance is robust against different λ act , where the MAEs fluctuate around 0.615, but λ act = 0.9 and 1.0 lead to the lowest errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">High-order Difference</head><p>Here we study the high-order differences of the input actions for action recognition and motion prediction, which help to capture richer motion dynamics. In our model, the difference orders are considered to be β = 0, 1, 2, reflecting positions, velocities, and accelerations. We present the results of action recognition on NTU-RGB+D and the results of motion prediction on Human 3.6M.</p><p>For action recognition, we test Sym-GNN with β = 0, β = 0, 1 and β = 0, 1, 2 on the two benchmarks (CS &amp; CV) of NTU-RGB+D dataset. <ref type="table" target="#tab_4">Table 13</ref> presents the average recognition accuracies of Sym-GNN with three difference configurations. We see that β = 0, 1 leads to the highest classification accuracies on both benchmarks, indicating that positions and velocities capture comprehensive movement patterns to provide rich semantics, meanwhile the model complexity is not very high.</p><p>For motion prediction, we feed the model with various action differences of Human 3.6M to generate future poses in within 400 ms. We obtain the predicting average MAEs on the timestamps of 80, 160, 320 and 400 ms. The results are presented in <ref type="table" target="#tab_5">Table 14</ref>. We see that Sym-GNN forecasts the future poses with the lowest prediction errors when we feed the action differences with β = 0, 1, 2, showing the effectiveness of combining high-order motion states.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.5">Bone-based Dual Graph Neural Networks</head><p>We validate the effectiveness of using dual networks which take joint and bone features as inputs for action recognition, respectively. <ref type="table" target="#tab_7">Table 15</ref> presents the recognition accuracies for different combinations of joint-based and bone-based dual networks on two benchmarks of NTU-RGB+D dataset. We see that only using joint features or bone features for action recognition cannot obtain the most accurate recognition, but combining joint and bone features could improve the classification performances with a large margin, indicating the complementary information carried by the two networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Visualization</head><p>In this section, we visualize some representations of Sym-GNN, including the learned joint-scale actional graphs and their low dimensional manifolds. Moreover, we show some predicted motions to evaluate model qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Joint-Scale Actional Graphs</head><p>We first show the learned joint-scale actional graphs on four motions in Human 3.6M. <ref type="figure">Fig. 11</ref> illustrates the edges which have the top-15 largest weights in each graph, indicating the 15 strongest action-based relations associated with different motions. We see: 1) The joint-scale actional graphs capture some action-based long-range relations beyond direct boneconnections; 2) Some reasonable relations are captured, e.g. for 'Directions', the stretched arms are correlated to other joints; 3) for motions with the same category, we tend to obtain the similar graphs; see two plots of 'Walking', while different classes of motions have distinct actional graphs; see 'Walking' and the other motions, where the model learns the discriminative patterns from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Manifolds of Joint-Scale Actional Graphs</head><p>To verify how discriminative the patterns embedded in the joint-scale actional graphs, we visualize the low-dimension manifolds of different joint-scale actional graphs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">Predicted Sequences</head><p>Finally, we compare the generated samples of Sym-GNN to those of Res-sup <ref type="bibr" target="#b19">[17]</ref>, AGED <ref type="bibr" target="#b20">[18]</ref>, and CSM <ref type="bibr" target="#b33">[31]</ref> on Human 3.6M and CMU Mocap. <ref type="figure">Fig. 13</ref> illustrates the future poses of 'Eating' and 'Running' in 1000 ms with the frame interval of 80 ms, where plot (a) shows the predictions of 'Eating' in Human 3.6M and plot (b) visualize 'Running' in CMU Mocap. Comparing to baselines, we see that Sym-GNN provides significantly better predictions. The poses generated by Res-sup has large errors after the 600th ms (two orange boxes); AGED produces over movements for the downward hand in long-term (red box in plot (a)); CSM gives tortile poses in long-term (red box in plot (b)). But Sym-GNN completes the action accurately and reasonably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Stability Analysis: Robustness against Input Perturbation</head><p>According to Theorem 1, we present that Sym-GNN is robust against perturbation on inputs, where we calculate an upper bound of output deviation. To verify the stability, we add Gaussian noises sampled from N (0, σ 2 ) on input actions. We show the recognition accuracies on NTU-RGB+D (Cross-Subject) and short-term prediction MAEs on Human 3.6M with standard deviation σ varied from 0.01 to 0.1. The recognition/prediction performances with different σ are illustrated in <ref type="figure" target="#fig_1">Fig. 14.</ref> We see: 1) for action recognition, Sym-GNN stays a high accuracy when the noise has σ ≤ 0.04, but it tends to deteriorate due to severe perturbation when σ &gt; 0.04; 2) for motion prediction, Sym-GNN produces precise poses when the noise has σ &lt; 0.03, but the prediction performance is degraded for larger σ. In all, Sym-GNN is robust against small perturbation. Given two inputs X and X * , which satisfy X * −X ≤ , we validate the assumption of A * act X * − A act X F ≤ C q claimed in Theorem 1. We calculate the ratio between the perturbations of responses and inputs; that is,</p><formula xml:id="formula_29">Ratio = A * act X * − A act X F X * − X F</formula><p>Similar to <ref type="figure" target="#fig_1">Fig. 14,</ref> we tune the standard deviations of input noises, obtaining the corresponding A * act and calculate the perturbation ratios. <ref type="figure">Fig. 15</ref> illustrated the Ratio with different noise standard deviations. We see that the Ratio is steady at around 0.33 for σ adjusted from 0.01 to 0.1, which indicates the amplify factor q ≈ 1 and the responses and inputs are mostly linearly correlated. In other words, the actional graph inference module does not amplify the perturbation and the stability of JGC is still preserved. −AactX F X * −X F <ref type="figure">Fig. 15</ref>. The ratio between the perturbations of responses and inputs with different noise standard deviations. The amplify factor q ≈ 1, indicating that AGIM is stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we propose a novel symbiotic graph neural network (Sym-GNN), which handles action recognition and motion prediction jointly and use graph-based operations to capture action patterns. Our model consists of a backbone, an action-recognition head, and a motion-prediction head, where the two heads enhance each other. As building components in the backbone and the motion-prediction head, graph convolution operators based on learnable joint-scale and part-scale graphs are used to extract spatial information. We conduct extensive experiments for action recognition and motion prediction with four datasets, NTU-RGB+D, Kinetics, Human 3.6M, and CMU Mocap. Experiments show that our model achieves consistently improvements compared to the previous methods. Ya Zhang received the B.S. degree from Tsinghua University and the Ph.D. degree in information sciences and technology from the Pennsylvania State University. Since March 2010, she has been a professor with Cooperative Medianet Innovation Center, Shanghai Jiao Tong University. Prior to that, she worked with Lawrence Berkeley National Laboratory, University of Kansas, and Yahoo! Labs. Her research interest is mainly on data mining and machine learning, with applications to information retrieval, web mining, and multimedia analysis. She is a member of the IEEE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yanfeng</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOF OF THEOREM 1</head><p>Here, we prove the proposed theorem 1; that is the activated JGC operator is robust against the input perturbation.</p><p>Proof We first bound the discrepancy after joint-scale graph convolution. We have JGC (X * ) − JGC (X) F = λ act (A * act X * − A act X) W act</p><formula xml:id="formula_30">+ Γ γ=1 A (γ) str (X * − X) W (γ) str F (a) ≤ λ act (A * act X * − A act X) W act F + Γ γ=1 D −1 A γ M (γ) (X * − X) W (γ) str F (b) ≤ λ act A * act X * − A act X F W act F + Γ γ=1 D −1 A γ M (γ) F X * − X F W (γ) str F (c) ≤ λ act D x D y µ act C q + Γ γ=1 A γ 0 η (γ) D x D y µ (γ) str ,<label>(13)</label></formula><p>where (a) follows from the norm triangle inequality; (b) follows from the norm sub-multiplicativity and (c) follows from the assumptions. We next show that ReLU(x) = max(0, x) is contractive; that is,</p><formula xml:id="formula_31">|ReLU(x * ) − ReLU(x)| =        |0 − x| (≤ |x * − x|)</formula><p>x ≥ 0, x * ≤ 0 |0 − 0| (≤ |x * − x|)</p><p>x ≥ 0, x * ≥ 0 |x * − 0| (≤ |x * − x|)</p><p>x ≤ 0, x * ≥ 0 |x * − x| (= |x * − x|)</p><p>x ≤ 0, x * ≤ 0 ≤|x * − x|. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B TRAINING ALGORITHM</head><p>To optimize the multi-tasking model, the KKT conditions for both backbone network and task-specific heads are stated as • The convex sum, λ∇ θ bk L recg +(1−λ)∇ θ bk L pred = 0, where 0 ≤ λ ≤ 1;</p><p>• For the task of action recognition and motion prediction, there is ∇ θrecg L recg = 0 and ∇ θ pred L pred = 0.</p><p>We see that the KKT conditions contain two gradient constraints model parameters. We note that the KKT condition is the necessity of the optimal solutions for our method; that is, any solution which satisifies the KKT conditions is a possible optimal solution. To find the appropriate λ, we attempt to match the first condition, leading to a stationary point. Here we compute,</p><formula xml:id="formula_32">λ = arg min λ { λ∇ θ bk L recg + (1 − λ)∇ θ bk L pred 2 2 },<label>(15)</label></formula><p>where θ bk denotes the trainable parameters of backbone, including AGIM and J-GTC blocks. To calculate λ from <ref type="formula" target="#formula_32">(15)</ref>, we employ the mechanism of multi-objective optimization from <ref type="bibr" target="#b60">[58]</ref> to adjust λ adaptatively during training. The overall loss of Sym-GNN is</p><formula xml:id="formula_33">L = λ L recg + (1 − λ )L pred .</formula><p>Note that λ in <ref type="formula" target="#formula_32">(15)</ref> is optimized in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C NETWORK ARCHITECTURE</head><p>Here we present the network structure with more details. We list the sizes of parameters and corresponding operations of the actional graph inference module (AGIM), backbone networks of the proposed Sym-GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Actional Graph Inference Module</head><p>As an important component in the Sym-GNN, the actional graph inference module (AGIM) is employed to learn the action-based correlations among different moving joints. We propagate the features of joints and arbitrary links to aggregate long-range joint feature for relation capturing and action graph estimation. The structure of AGLM is presented in <ref type="table" target="#tab_8">Table 16</ref>, where 'bn' denotes the batch normalization. We list all the detailed architectures and operations, and Step 2 in the table indicates the iterative feature propagations. At the end of AGLM, we use two individual embedding networks to extract the joint embeddings from two aspects, and we use (2) in the submitted paper to model the incoming and outgoing relations between joints. We note that the edge weights in the action graph are normalized by a softmax operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Backbone (9 layers)</head><p>The architecture of backbone network of Sym-GNN on NTU-RGB+D and Kinetics dataset is presented in <ref type="table" target="#tab_10">Table 17</ref>.</p><p>There are 9 layers of J-GTC blocks and 8 layers of P-GTC blocks. For each block, we show the spatial and temporal convolution operator, where the kernel sizes, batch normalization and dropout operations, activation functions and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Backbone (4 layers light version)</head><p>For Sym-GNN on Human 3.6M and CMU Mocap datasets, we use a light version to extract the action features. The backbone architecture is shown in <ref type="table" target="#tab_12">Table 18</ref>. Similar to <ref type="table" target="#tab_10">Table 17</ref>, we show the spatial and temporal convolution operations with the corresponding kernel sizes, batch normalization and dropout operations, activation functions and feature shapes. The joint2part pooling and part2joint matching are presented.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>J-GTC block consists of JGC (5) and temporal convolution (TC). The triples below the blocks denote the tensor shapes. The quaternions are the shapes of parameters in JGC and TC operators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 (</head><label>6</label><figDesc>right plot) shows the internal operations and one bidirectional fusion in multi-scale GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>The motion-prediction head of Sym-GNN uses JGC (5), the difference operator (9) and GRU to predict the future poses sequentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>47 0.64 0.72 0.27 0.40 0.64 0.79 0.36 0.61 0.85 0.92 0.46 0.82 0.95 1.21 Sym-GNN (Only J-A) 0.19 0.35 0.54 0.63 0.18 0.34 0.54 0.66 0.23 0.43 0.84 0.82 0.26 0.62 0.81 0.87 Sym-GNN (Only J-S) 0.19 0.33 0.54 0.69 0.17 0.32 0.52 0.66 0.21 0.41 0.83 0.82 0.24 0.64 0.93 1.01 Sym-GNN (No recg) 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>.35 0.50 0.69 0.76 1.04 0.31 0.51 0.90 1.00 1.77 0.36 0.47 0.62 0.65 0.93 0.33 0.47 0.75 0.95 1.40 CSM [31] 0.28 0.41 0.52 0.57 0.67 0.26 0.44 0.75 0.87 1.56 0.35 0.44 0.45 0.50 0.78 0.30 0.47 0.80 1.01 1.39 BiHMP-GAN [27] 0.28 0.40 0.50 0.53 0.62 0.26 0.44 0.72 0.82 1.51 0.35 0.45 0.44 0.46 0.72 0.31 0.46 0.77 0.92 1.31 Skel-TNet [30] 0.38 0.48 0.57 0.62 0.71 0.24 0.41 0.69 0.79 1.44 0.33 0.41 0.45 0.48 0.73 0.31 0.46 0.79 0.96 1.37 Sym-GNN (No recg) 0.21 0.33 0.53 0.56 0.66 0.22 0.38 0.72 0.83 1.38 0.26 0.32 0.38 0.41 0.54 0.22 0.33 0.62 0.83 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Sym-GNN is both faster and more precise compared to others. Various red circles denote different iteration numbers K in AGIM, where K = 0, 1, 2, 3, 4. The bottom right corner (highlighted by a trophy cup) indicates higher speed and lower error, showing an ideal target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Average action recognition accuracies and motion prediction MAEs of models with different λact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>Visualization of motion prediction on Human 3.6M and CMU Mocap. Plot (a) shows the predictions of 'Eating' in Human 3.6M and plot (b) shows the predictions of 'Running' in CMU Mocap. We compare the predictions of Sym-GNN, Res-sup, AGED, and CSM with the ground truth (GT). The recognition accuracy and prediction MAE perturbed Gaussian noises with different standard deviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Wang received the B.E. degree in information engineering from the University of PLA, Beijing, China, and the M.S. and Ph.D. degrees in business management from the Antai College of Economics and Management, Shanghai Jiao Tong University, Shanghai, China. He is currently the Vice Director of Cooperative Medianet Innovation Center and also the Vice Dean of the School of Electrical and Information Engineering with Shanghai Jiao Tong University. His research interest mainly include media big data and emerging commercial applications of information technology. Qi Tian is currently the Chief Scientist of Computer Vision in Huawei Noah's Ark Laboratory, a Full Professor with the Department of Computer Science, University of Texas at San Antonio (UTSA). He was a tenured Associate Professor from 2008-2012 and a tenure-track Assistant Professor from 2002-2008. During 2008-2009, he took one-year Faculty Leave at Microsoft Research Asia (MSRA) as Lead Researcher in the Media Computing Group. Dr. Tian received his Ph.D. in ECE from University of Illinois at Urbana-Champaign (UIUC) in 2002 and received his B.E. in Electronic Engineering from Tsinghua University in 1992 and M.S. in ECE from Drexel University in 1996. Dr. Tian's research interests include multimedia information retrieval, computer vision, pattern recognition and published over 360 refereed journal and conference papers. He was the co-author of a Best Paper in ACM ICMR 2015, a Best Paper in PCM 2013, a Best Paper in MMM 2013, a Best Paper in ACM ICIMCS 2012, a Top 10% Paper Award in MMSP 2011, a Best Student Paper in ICASSP 2006, and co-author of a Best Student Paper Candidate in ICME 2015, and a Best Paper Candidate in PCM 2007. Dr. Tian received 2017 UTSA President's Distinguished Award for research Achievement, 2016 UTSA Innovation Award, 2014 Research Achievement Awards from College of Science, UTSA, 2010 Google Faculty Award, and 2010 ACM Service Award. He is the associate editor of many journals and in the Editorial Board of Journal of Multimedia (JMM) and Journal of Machine Vision and Applications (MVA). He is a fellow of the IEEE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>( 14 )</head><label>14</label><figDesc>Therefore, we obtainY * − Y F = ReLU (JGC (X * )) − ReLU (JGC (X)) F (a) ≤ JGC (X * ) − JGC (X) F (b) ≤ λ act D x D y µ act C q + Γ γ=1 A γ 0 η (γ) D x D y µ (γ) str (c) = 3D y q λ act µ act C + Γ γ=1 A γ 0 η (γ) µ (γ) str = O (max ( q , )) ,where (a) follows from(14); (b) follows from (13); and (c) follows the input feature dimension D x = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is with the Cooperative Medianet Innovation Center and the Shanghai Key Laboratory of Multimedia Processing and Transmissions, Shanghai Jiao Tong University, Shanghai 200240, China. E-mail: maosen li@sjtu.edu.cn • S. Chen is with Mitsubishi Electric Research Laboratories, Cambridge, MA 02139, USA.</figDesc><table /><note>E-mail: schen@merl.com.• X. Chen, Y. Zhang and Y. Wang are with the Cooperative Medianet Innovation Center and the Shanghai Key Laboratory of Multimedia Processing and Transmissions, Shanghai Jiao Tong University, Shanghai 200240, China.• Q. Tian is with the Huawei Noah's Ark Lab, Shenzhen, Guangdong 518129, China.E-mail: tianqi1@huawei.com</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Comparison of action recognition on Human 3.6M and CMU Mocap dataset. The top-1 and top-5 classification accuracies are listed.</figDesc><table><row><cell></cell><cell cols="2">Human 3.6M</cell><cell cols="2">CMU Mocap</cell></row><row><cell>Methods</cell><cell>Top-1</cell><cell>Top-5</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>ST-GCN [14]</cell><cell>40.2%</cell><cell>78.4%</cell><cell>87.5%</cell><cell>96.9%</cell></row><row><cell>HCN [9]</cell><cell>47.6%</cell><cell>88.8%</cell><cell>95.4%</cell><cell>99.2%</cell></row><row><cell>2s-AGCN [35]</cell><cell>55.4%</cell><cell>94.1%</cell><cell>97.1%</cell><cell>99.8%</cell></row><row><cell>Sym-GNN (Only J)</cell><cell>55.6%</cell><cell>93.9%</cell><cell>96.5%</cell><cell>99.4%</cell></row><row><cell>Sym-GNN (Only P)</cell><cell>54.3%</cell><cell>93.1%</cell><cell>94.9%</cell><cell>98.0%</cell></row><row><cell>Sym-GNN (No bone)</cell><cell>53.5%</cell><cell>93.2%</cell><cell>93.5%</cell><cell>95.8%</cell></row><row><cell>Sym-GNN (No pred)</cell><cell>55.2%</cell><cell>94.1%</cell><cell>96.6%</cell><cell>99.4%</cell></row><row><cell>Sym-GNN</cell><cell cols="3">56.5% 95.3% 98.8%</cell><cell>100%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 Comparison</head><label>4</label><figDesc></figDesc><table /><note>of PCK@0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>52.77 32.64 23.19 18.05 14.11 11.92 9.84 8.35 6.04 24.72 Res-sup [17] 76.25 55.96 40.31 29.47 22.81 16.96 13.65 11.57 10.13 8.87 28.60 CSM [31] 82.38 68.11 56.84 45.26 38.65 30.41 26.15 22.74 17.52 15.96 40.40 Skel-TNet [30] 93.62 86.44 81.03 75.85 70.81 66.57 59.60 54.45 46.92 40.18 67.55 Sym-GNN (No recg) 98.74 97.07 94.95 93.94 93.48 91.79 90.69 89.27 87.87 86.01</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>92.34</cell></row><row><cell></cell><cell>Sym-GNN</cell><cell cols="2">99.00 97.65 95.89 95.10 93.44 92.70 91.75 90.65 89.54 89.10</cell><cell>93.48</cell></row><row><cell></cell><cell>ZeroV [17]</cell><cell>75.69 55.72 39.88 29.60 21.91 15.23 12.06 10.18 8.70</cell><cell>7.33</cell><cell>27.63</cell></row><row><cell></cell><cell>Res-sup [17]</cell><cell cols="2">78.85 59.91 43.82 32.37 24.32 18.51 14.86 12.29 10.38 8.85</cell><cell>30.42</cell></row><row><cell>Cross-View</cell><cell>CSM [31] Skel-TNet [30]</cell><cell cols="2">85.41 71.75 58.20 46.69 39.07 31.85 28.43 24.17 19.66 18.93 94.81 89.12 83.85 79.00 72.74 69.11 62.39 66.97 48.88 42.70</cell><cell>42.62 70.66</cell></row><row><cell></cell><cell cols="3">Sym-GNN (No recg) 98.99 96.79 95.55 94.68 93.03 92.13 90.88 89.69 88.70 87.57</cell><cell>92.79</cell></row><row><cell></cell><cell>Sym-GNN</cell><cell cols="2">99.25 97.87 96.38 95.21 94.06 92.95 91.94 91.01 90.18 89.27</cell><cell>93.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Comparisons of MAEs between Sym-GNN and state-of-the-art methods for short-term motion prediction on the 4 representative actions of H3.6M.</figDesc><table><row><cell cols="17">Sym-GNN (J-A) and Sym-GNN (J-S) are Sym-GNN with joint-scale actional graphs only and with joint-scale structural graph only, respectively. Sym-GNN (No recg) represents the model trained without action classification.</cell></row><row><cell>Motion</cell><cell></cell><cell cols="2">Walking</cell><cell></cell><cell></cell><cell cols="2">Eating</cell><cell></cell><cell></cell><cell cols="2">Smoking</cell><cell></cell><cell></cell><cell cols="2">Discussion</cell><cell></cell></row><row><cell>milliseconds</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell></row><row><cell>ZeroV [17]</cell><cell cols="16">0.39 0.68 0.99 1.15 0.27 0.48 0.73 0.86 0.26 0.48 0.97 0.95 0.31 0.67 0.94 1.04</cell></row><row><cell>ERD [42]</cell><cell cols="16">0.93 1.18 1.59 1.78 1.27 1.45 1.66 1.80 1.66 1.95 2.35 2.42 2.27 2.47 2.68 2.76</cell></row><row><cell>Lstm3LR [42]</cell><cell cols="16">0.77 1.00 1.29 1.47 0.89 1.09 1.35 1.46 1.34 1.65 2.04 2.16 1.88 2.12 2.25 2.23</cell></row><row><cell>SRNN [16]</cell><cell cols="16">0.81 0.94 1.16 1.30 0.97 1.14 1.35 1.46 1.45 1.68 1.94 2.08 1.22 1.49 1.83 1.93</cell></row><row><cell>DropAE [55]</cell><cell cols="3">1.00 1.11 1.39</cell><cell>/</cell><cell cols="3">1.31 1.49 1.86</cell><cell>/</cell><cell cols="3">0.92 1.03 1.15</cell><cell>/</cell><cell cols="3">1.11 1.20 1.38</cell><cell>/</cell></row><row><cell>Samp-loss [17]</cell><cell cols="16">0.92 0.98 1.02 1.20 0.98 0.99 1.18 1.31 1.38 1.39 1.56 1.65 1.78 1.80 1.83 1.90</cell></row><row><cell>Res-sup [17]</cell><cell cols="16">0.27 0.46 0.67 0.75 0.23 0.37 0.59 0.73 0.32 0.59 1.01 1.10 0.30 0.67 0.98 1.06</cell></row><row><cell>CSM [31]</cell><cell cols="16">0.33 0.54 0.68 0.73 0.22 0.36 0.58 0.71 0.26 0.49 0.96 0.92 0.32 0.67 0.94 1.01</cell></row><row><cell>TP-RNN [56]</cell><cell cols="16">0.25 0.41 0.58 0.65 0.20 0.33 0.53 0.67 0.26 0.47 0.88 0.90 0.30 0.66 0.96 1.04</cell></row><row><cell>QuaterNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc>Comparisons of MAEs between Sym-GNN and previous methods for short-term motion prediction on other 11 actions of H3.6M dataset. .59 0.79 0.89 0.54 0.89 1.30 1.49 0.64 1.21 1.65 1.83 0.28 0.57 1.13 1.37 0.62 0.88 1.19 1.27 0.40 1.63 1.02 1.18 Res-sup [17] 0.41 0.64 0.80 0.92 0.57 0.83 1.45 1.60 0.59 1.06 1.45 1.60 0.45 0.85 1.34 1.56 0.58 0.79 1.08 1.15 0.41 0.68 1.12 1.33 CSM [31] 0.39 0.60 0.80 0.91 0.51 0.82 1.21 1.38 0.59 1.13 1.51 1.65 0.29 0.60 1.12 1.37 0.63 0.91 1.19 1.29 0.39 0.61 1.02 1.18 TP-RNN [56] 0.38 0.59 0.75 0.83 0.51 0.86 1.27 1.44 0.57 1.08 1.44 1.59 0.42 0.76 1.29 1.54 0.59 0.82 1.12 1.18 0.41 0.66 1.07 1.22 AGED [18] 0.23 0.39 0.62 0.69 0.54 0.80 1.29 1.45 0.52 0.96 1.22 1.43 0.30 0.58 1.12 1.33 0.46 0.78 1.00 1.07 0.41 0.75 1.04 1.19 Skel-TNet [30] 0.36 0.58 0.77 0.86 0.50 0.84 1.28 1.45 0.58 1.12 1.52 1.64 0.29 0.62 1.19 1.44 0.58 0.84 1.17 1.24 0.40 0.61 1.01 1.15 Sym-GNN (No recg) 0.24 0.45 0.61 0.67 0.36 0.61 0.98 1.17 0.50 0.86 1.</figDesc><table><row><cell>Motion</cell><cell>Directions</cell><cell>Greeting</cell><cell>Phoning</cell><cell>Posing</cell><cell>Purchases</cell><cell>Sitting</cell></row><row><cell>millisecond</cell><cell cols="6">80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400</cell></row><row><cell>ZeroV [17]</cell><cell>0.39 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7</head><label>7</label><figDesc>Comparisons of MAEs between our model and the state-of-the-art methods on the 8 actions of CMU Mocap dataset. We evaluate the model for long-term prediction and present the MAEs at both short and long-term prediction time stamps.</figDesc><table><row><cell>Motion</cell><cell>Basketball</cell><cell>Basketball Signal</cell><cell>Directing Traffic</cell><cell>Jumping</cell></row><row><cell>milliseconds</cell><cell cols="4">80 160 320 400 1000 80 160 320 400 1000 80 160 320 400 1000 80 160 320 400 1000</cell></row><row><cell>Res-sup [17]</cell><cell cols="4">0.49 0.77 1.26 1.45 1.77 0.42 0.76 1.33 1.54 2.17 0.31 0.58 0.94 1.10 2.06 0.57 0.86 1.76 2.03 2.42</cell></row><row><cell>Res-uns [17]</cell><cell cols="4">0.53 0.82 1.30 1.47 1.81 0.44 0.80 1.35 1.55 2.17 0.35 0.62 0.95 1.14 2.08 0.59 0.90 1.82 2.05 2.46</cell></row><row><cell>CSM [31]</cell><cell cols="4">0.37 0.62 1.07 1.18 1.95 0.32 0.59 1.04 1.24 1.96 0.25 0.56 0.89 1.00 2.04 0.39 0.60 1.36 1.56 2.01</cell></row><row><cell>BiHMP-GAN [27]</cell><cell cols="4">0.37 0.62 1.02 1.11 1.83 0.32 0.56 1.01 1.18 1.89 0.25 0.51 0.85 0.96 1.95 0.39 0.57 1.32 1.51 1.94</cell></row><row><cell>Skel-TNet [30]</cell><cell cols="4">0.35 0.63 1.04 1.14 1.78 0.24 0.40 0.69 0.80 1.07 0.22 0.44 0.78 0.90 1.88 0.35 0.53 1.28 1.49 1.85</cell></row><row><cell cols="5">Sym-GNN (No recg) 0.33 0.48 0.95 1.09 1.47 0.15 0.26 0.47 0.56 1.04 0.20 0.41 0.77 0.89 1.95 0.32 0.55 1.40 1.60 1.87</cell></row><row><cell>Sym-GNN</cell><cell>0.32 0</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8</head><label>8</label><figDesc>Comparisons of MAEs between our model and other methods for long-term motion prediction on 4 actions of H3.6M. .32 1.04 1.38 1.02 1.69 1.41 1.96 ERD [42] 2.00 2.38 2.36 2.41 3.68 3.82 3.47 2.92 Lstm3LR [42] 1.81 2.20 2.49 2.82 3.24 3.42 2.48 2.93 SRNN [16] 1.90 2.13 2.28 2.58 3.21 3.23 2.39 2.43 DropAE [55] 1.55 1.39 1.76 2.01 1.38 1.77 1.53 1.73</figDesc><table><row><cell cols="2">Motion</cell><cell></cell><cell>Walking</cell><cell>Eating</cell><cell>Smoking Discussion</cell></row><row><cell cols="2">milliseconds</cell><cell></cell><cell cols="3">560 1k 560 1k 560 1k 560</cell><cell>1k</cell></row><row><cell cols="6">ZeroV [17] 1.35 1Res-sup. [17] 0.93 1.03 0.95 1.08 1.25 1.50 1.43 1.69</cell></row><row><cell cols="2">CSM [31]</cell><cell></cell><cell cols="3">0.86 0.92 0.89 1.24 0.97 1.62 1.44 1.86</cell></row><row><cell cols="2">TP-RNN [56]</cell><cell></cell><cell cols="3">0.74 0.77 0.84 1.14 0.98 1.66 1.39 1.74</cell></row><row><cell cols="2">AGED [18]</cell><cell></cell><cell cols="3">0.78 0.91 0.86 0.93 1.06 1.21 1.25 1.30</cell></row><row><cell cols="6">BiHMP-GAN [27] / 0.85 / 1.20 / 1.11 /</cell><cell>1.77</cell></row><row><cell cols="3">Skel-TNet [30]</cell><cell cols="3">0.79 0.83 0.84 1.06 0.98 1.21 1.19 1.75</cell></row><row><cell cols="2">Sym-GNN</cell><cell></cell><cell cols="3">0.75 0.78 0.77 0.88 0.92 1.18 1.17 1.28</cell></row><row><cell>Mean Angle Error (MAE)</cell><cell>1.00 1.05 1.10 1.15 1.20</cell><cell cols="2">Res-sup CSM TP-RNN AGED Skel-TNet Sym-GNN</cell><cell></cell></row><row><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>2</cell><cell cols="3">4 Number of predicting frames in 20ms 6 8 10 12 14</cell><cell>16</cell><cell>18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 9</head><label>9</label><figDesc>Action recognition accuracies with noisy motion prediction targets in varying degrees on NTU-RGB+D dataset. 'No pred' denotes model without motion prediction task.</figDesc><table><row><cell>Noise ratio</cell><cell>CS</cell><cell>CV</cell></row><row><cell>0%</cell><cell>90.1%</cell><cell>96.4%</cell></row><row><cell>10%</cell><cell>89.8%</cell><cell>96.1%</cell></row><row><cell>20%</cell><cell>89.5%</cell><cell>96.1%</cell></row><row><cell>50%</cell><cell>89.1%</cell><cell>95.5%</cell></row><row><cell>70%</cell><cell>88.5%</cell><cell>94.9%</cell></row><row><cell>100%</cell><cell>87.7%</cell><cell>93.9%</cell></row><row><cell>No pred</cell><cell>89.0%</cell><cell>95.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 10</head><label>10</label><figDesc>Motion prediction MAEs with noisy action labels in varying degrees on Human 3.6M dataset. 'No recg' denotes model without recognition task.</figDesc><table><row><cell>Motion</cell><cell></cell><cell cols="2">Average</cell><cell></cell></row><row><cell>Noise ratio</cell><cell>80 ms</cell><cell>160 ms</cell><cell>320 ms</cell><cell>400 ms</cell></row><row><cell>0%</cell><cell>0.26</cell><cell>0.49</cell><cell>0.79</cell><cell>0.92</cell></row><row><cell>10%</cell><cell>0.26</cell><cell>0.48</cell><cell>0.79</cell><cell>0.93</cell></row><row><cell>20%</cell><cell>0.26</cell><cell>0.49</cell><cell>0.82</cell><cell>0.94</cell></row><row><cell>30%</cell><cell>0.26</cell><cell>0.50</cell><cell>0.82</cell><cell>0.93</cell></row><row><cell>50%</cell><cell>0.26</cell><cell>0.51</cell><cell>0.83</cell><cell>0.97</cell></row><row><cell>100%</cell><cell>0.27</cell><cell>0.53</cell><cell>0.85</cell><cell>1.03</cell></row><row><cell>No recg</cell><cell>0.26</cell><cell>0.50</cell><cell>0.82</cell><cell>0.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 11</head><label>11</label><figDesc></figDesc><table><row><cell cols="5">Recognition accuracies on NTU-RGB+D, CS with various graphs: only joint-scale structural graphs (Only J-S), only joint-scale actional graph (Only J-A), only part-scale graph (Only P) and all graphs (full).</cell></row><row><cell>JS-Hop (Γ)</cell><cell>Only J-S</cell><cell>Only J-A</cell><cell>Only P</cell><cell>full</cell></row><row><cell>1</cell><cell>85.9%</cell><cell></cell><cell></cell><cell>86.1%</cell></row><row><cell>2 3</cell><cell>86.2% 87.5%</cell><cell>85.7%</cell><cell>87.3%</cell><cell>86.9% 88.3%</cell></row><row><cell>4</cell><cell>88.3%</cell><cell></cell><cell></cell><cell>90.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 12 Average</head><label>12</label><figDesc>MAEs for short-term prediction on H3.6M with various graphs: only joint-scale structural graphs (Only J-S), only joint-scale actional graph (Only J-A), only part-scale graph (Only P) and all graphs (full).</figDesc><table><row><cell>JS-Hop (Γ)</cell><cell>Only J-S</cell><cell>Only J-A</cell><cell>Only P</cell><cell>full</cell></row><row><cell>1</cell><cell>0.622</cell><cell></cell><cell></cell><cell>0.616</cell></row><row><cell>2 3</cell><cell>0.619 0.613</cell><cell>0.618</cell><cell>0.615</cell><cell>0.615 0.611</cell></row><row><cell>4</cell><cell>0.618</cell><cell></cell><cell></cell><cell>0.614</cell></row><row><cell cols="5">TABLE 13 The recognition accuracies of the model with various input difference orders on NTU-RGB+D.</cell></row><row><cell cols="2">Difference Order</cell><cell>CS</cell><cell>CV</cell><cell></cell></row><row><cell>β = 0</cell><cell></cell><cell>88.2%</cell><cell cols="2">95.0%</cell></row><row><cell>β = 0, 1</cell><cell></cell><cell>90.1%</cell><cell cols="2">96.4%</cell></row><row><cell cols="2">β = 0, 1, 2</cell><cell>89.8%</cell><cell cols="2">96.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 14</head><label>14</label><figDesc>The average MAEs of short-term motion prediction with various input difference orders on Human 3.6M.</figDesc><table><row><cell>Motion</cell><cell></cell><cell cols="2">Average</cell><cell></cell></row><row><cell>Milliseconds</cell><cell>80</cell><cell>160</cell><cell>320</cell><cell>400</cell></row><row><cell>β = 0</cell><cell>0.33</cell><cell>0.59</cell><cell>0.85</cell><cell>0.91</cell></row><row><cell>β = 0, 1</cell><cell>0.28</cell><cell>0.52</cell><cell>0.81</cell><cell>0.85</cell></row><row><cell>β = 0, 1, 2</cell><cell>0.26</cell><cell>0.49</cell><cell>0.79</cell><cell>0.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 15 The</head><label>15</label><figDesc></figDesc><table><row><cell cols="3">recognition accuracies of model with different parallel networks on NTU-RGB+D.</cell></row><row><cell>Parallel Network</cell><cell>CS</cell><cell>CV</cell></row><row><cell>Only Joint</cell><cell>87.1%</cell><cell>93.8%</cell></row><row><cell>Only Bone</cell><cell>87.4%</cell><cell>93.5%</cell></row><row><cell>Joint &amp; Bone</cell><cell>90.1%</cell><cell>96.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Fig. 11. Joint-scale actional graphs on motions in H3.6M. Yellow lines indicate the connections in actional graphs and blue lines indicate the connections in skeleton graphs. The two plots on the left show the graphs of 'Walking', the three plots on the right show the graphs of 'Directions', 'Greeting' and 'Phoning'.Fig. 12. 2D T-SNE map of learned actional graphs corresponding to 8 activities in H3.6M. Walking-related graphs are separated from sittingrelated graphs with a large margin.</figDesc><table><row><cell>Walking</cell><cell>Walking</cell><cell>Directions</cell><cell>Greeting</cell><cell>Phoning</cell></row><row><cell cols="5">more clips from long test motion sequences. Here we treat</cell></row><row><cell cols="5">all the joint-scale actional graphs as vectors and obtain their</cell></row><row><cell cols="5">2D T-SNE map; see Fig. 12. We see that 'Walking', 'Walking [4] J. Martinez, M Dog' and 'Walking Together', which have the common prediction using walking dynamics, are distributed closely, as well as 'Sitting' ference on Com and 'Sitting Down' are clustered; however, walking-related</cell></row><row><cell cols="5">pages 4674-468 actions and sitting-related actions are separated with a large</cell></row><row><cell cols="5">margin; as for 'Eating', 'Smoking' and 'Taking Photo', they</cell></row><row><cell cols="5">have similar movements on arms, showing a new cluster.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell></row><row><cell>We select 8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>representative classes of actions in Human 3.6M and sample</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>Maosen Li recieved the B.E. degree in optical engineering from University of Electronic Science and Technology of China (UESTC), Chengdu, China, in 2017. He is working toward the Ph.D. degree at Cooperative Meidianet Innovation Center in Shanghai Jiao Tong University since 2017. His research interests include computer vision, machine learning, graph representation learning, and video analysis. He is a student member of the IEEE. Chen is a research scientist at Mitsubishi Electric Research Laboratories (MERL). Before that, he was an autonomy engineer at Uber Advanced Technologies Group, working on the perception and prediction systems of selfdriving cars. Before joining Uber, he was a postdoctoral research associate at Carnegie Mellon University. Chen received the doctorate in Electrical and Computer Engineering from Carnegie Mellon University in 2016, where he also received two masters degrees in Electrical and Computer Engineering and Machine Learning, respectively. He received his bachelor's degree in Electronics Engineering in 2011 from Beijing Institute of Technology, China. Chen was the recipient of the 2018 IEEE Signal Processing Society Young Author Best Paper Award. His coauthored paper received the Best Student Paper Award at IEEE GlobalSIP 2018. His research interests include graph signal processing, graph neural networks and 3D computer vision. Chen received the B.E. degree in electronics engineering from Xidian University in 2016. He is working toward the Ph.D. degree at Cooperative Meidianet Innovation Center in Shanghai Jiao Tong University since 2016. He is now a dual Ph.D. student of Shanghai Jiao Tong University and University of Technology, Sydney. His research interests include machine learning, graph representation learning, recommendation systems, and computer vision.</figDesc><table><row><cell>Siheng Xu</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 16</head><label>16</label><figDesc>The structure of the AGIM in Sym-GNN model.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://mocap.cs.cmu.edu/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Step</p><p>Shape &amp; Operations Implementation Computing (2) in paper action graph data dimensions are presented. Moreover, for joint2part pooling and part2joint matching operators are annotated. We note that these two operators carry feature concatenations to increase the feature dimensions.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adversarial geometry-aware human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="786" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence model for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Sun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="5226" to="5234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Actionalstructural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A string of feature graphs model for recognition of complex activities in natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="2595" to="2602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Action-reaction: Forecasting the dynamics of human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014-07" />
			<biblScope unit="page" from="489" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Teaching robots to predict human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1408" to="1424" />
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action anticipation with rbf kernelized feature mapping rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="301" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dividing and aggregating network for multi-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="451" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look into person: Joint body parsing pose estimation network and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="871" to="885" />
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="786" to="792" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018-02" />
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient nonlinear markov models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1314" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="4674" to="4683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial geometryaware human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="786" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2466" to="2472" />
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2006-12" />
			<biblScope unit="page" from="1441" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting><address><addrLine>Decemeber</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The recurrent temporal restricted boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Quaternet: A quaternionbased recurrent model for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Converence (BMVC)</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Few-shot human motion prediction via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="432" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bihmp-gan: Bidirectional 3d human motion prediction gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human motion prediction via learning local structure representations and temporal dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence model for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Sun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="5226" to="5234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Papaca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph routing for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019-02" />
			<biblScope unit="page" from="8561" to="8568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph cnns with motif and variable temporal block for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019-02" />
			<biblScope unit="page" from="8989" to="8996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning switching linear models of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="942" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="3332" to="3341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Feastnet: Feature-steered graph convolutions for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="2598" to="2606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning localized generative models for 3d point clouds via graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World-Wide Web Conference (WWW)</title>
		<imprint>
			<date type="published" when="1998-05" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast resampling of three-dimensional point clouds via graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vetro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kovačević</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing (TSP)</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="666" to="681" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multiple-gradient descent algorithm (mgda) for multiobjective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Desideri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes Rendus Mathematique</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="2012-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Statistics (COMPSTAT)</title>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning human motion models for long-term predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<idno>abs/1704.02827</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Actionagnostic human pose forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niebles</surname></persName>
		</author>
		<idno>abs/1810.09676</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A neural temporal model for human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ororbia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018-12" />
			<biblScope unit="page" from="527" to="538" />
		</imprint>
	</monogr>
	<note>Multi-task learning as multi-objective optimization. 64, 1, 1, 3] × 2 -bn-relu -[M, 300, 3] → [M, 300, 64. 64, 1, 9, 64. stride=1 bn-dropout-relu -[M, 300, 64] → [M, 300, 64] joint2part pooling 2-3 [64, 1, 1, 64] × 2 [64, 1, 1, 64] -bn-relu -bn-relu [M, 300, 64] → [M, 300, 64. Mp, 300, 64] → [Mp, 300, 64. 64(128), 1, 9, 64], stride=1, 2 [64(128), 1, 9, 64], stride=1, 2 bn-dropout-relu bn-dropout-relu [M, 300, 64] → [M, 150, 128. M, 300, 64] → [M, 120, 128</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<title level="m">bn-relu -bn-relu</title>
		<imprint/>
	</monogr>
	<note>256, 1, 1, 512(256)] × 2 [256, 1, 1, 512(256). M, 75, 512] → [M, 75, 256. Mp, 75, 512] → [Mp, 75, 256. 256, 1, 9, 256], stride=1,1,1 [256, 1, 9, 256], stride=1,1,1 bn-dropout-relu bn-dropout-relu [M, 75, 256] → [M, 75, 256. Mp, 75, 256] → [Mp, 75, 256] joint2part summation temporal average pooling: [256, 75, M ] → [256, M</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
