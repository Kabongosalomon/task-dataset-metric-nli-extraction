<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Occupancy Anticipation for Efficient Exploration and Navigation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santhosh</forename><forename type="middle">K</forename><surname>Ramakrishnan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<postCode>78701</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziad</forename><surname>Al-Halah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
							<email>grauman@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<postCode>78701</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Occupancy Anticipation for Efficient Exploration and Navigation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>page: http: // vision. cs. utexas. edu/ projects/ occupancy_ anticipation/</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art navigation methods leverage a spatial memory to generalize to new environments, but their occupancy maps are limited to capturing the geometric structures directly observed by the agent. We propose occupancy anticipation, where the agent uses its egocentric RGB-D observations to infer the occupancy state beyond the visible regions. In doing so, the agent builds its spatial awareness more rapidly, which facilitates efficient exploration and navigation in 3D environments. By exploiting context in both the egocentric views and top-down maps our model successfully anticipates a broader map of the environment, with performance significantly better than strong baselines. Furthermore, when deployed for the sequential decision-making tasks of exploration and navigation, our model outperforms state-of-the-art methods on the Gibson and Matterport3D datasets. Our approach is the winning entry in the 2020 Habitat PointNav Challenge. Project page: http: // vision. cs. utexas. edu/ projects/ occupancy_ anticipation/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In visual navigation, an agent must move intelligently through a 3D environment in order to reach a goal. Visual navigation has seen substantial progress in the past few years, fueled by large-scale datasets and photo-realistic 3D environments <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">74,</ref><ref type="bibr" target="#b35">69]</ref>, simulators <ref type="bibr" target="#b40">[74,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">38]</ref>, and public benchmarks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">38]</ref>. Whereas traditionally navigation was attempted using purely geometric representations (i.e., SLAM), recent work shows the power of learned approaches to navigation that integrate both geometry and semantics <ref type="bibr" target="#b45">[79,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b22">56,</ref><ref type="bibr">41,</ref><ref type="bibr" target="#b43">77,</ref><ref type="bibr" target="#b10">11]</ref>. Learned approaches operating directly on pixels and/or depth as input can be robust to noise <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref> and can generalize well on unseen environments [20,38,77,10] -even outperforming pure SLAM given sufficient experience <ref type="bibr">[38]</ref>.</p><p>One of the key factors for success in navigation has been the movement towards complex map-based architectures <ref type="bibr">[20,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref> that capture both geometry [20, <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref> and semantics <ref type="bibr">[20,</ref><ref type="bibr">46,</ref><ref type="bibr">19,</ref><ref type="bibr">24]</ref>, thereby facilitating efficient policy learning and planning. These learned maps allow an agent to exploit prior knowledge from training scenes when navigating in novel test environments.</p><p>Despite such progress, state-of-the-art approaches to navigation are limited to encoding what the agent actually sees in front of it. In particular, they build maps arXiv:2008.09285v2 [cs.CV] 25 Aug 2020 by its field-of-view and obstacles (the visible map). We propose to anticipate occupancy for unseen regions (anticipated map) by exploiting the context from egocentric views. We then train a deep reinforcement learning agent to move intelligently in a 3D environment, rewarding movements that improve the anticipated map.</p><p>of the environment using only the observed regions, whether via geometry <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">24]</ref> or learning <ref type="bibr">[20,</ref><ref type="bibr">46,</ref><ref type="bibr">19,</ref><ref type="bibr" target="#b9">10]</ref>. Thus, while promising, today's models suffer from an important inefficiency: to map a space in the 3D environment as free or occupied, the agent must directly see evidence thereof in its egocentric camera.</p><p>Our key idea is to anticipate occupancy. Rather than wait to directly observe a more distant or occluded region of the 3D environment to declare its occupancy status, the proposed agent infers occupancy for unseen regions based on the visual context in its egocentric views. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref>, with only the partial observation of the scene, the agent could infer that it is quite likely that the wall extends to its right, a corridor is present on its left, and the region immediately in front of it is free space. Such intelligent extrapolation beyond the observed space would lead to more efficient exploration and navigation. To achieve this advantage, we introduce a model that anticipates occupancy maps from normal field-of-view RGB(D) observations, while aggregating its predictions over time in tight connection with learning a navigation policy. Furthermore, we incorporate the anticipation objective directly into the agent's exploration policy, encouraging movements in the 3D space that will efficiently yield broader and more accurate inferred occupancy maps.</p><p>We validate our approach on Gibson <ref type="bibr" target="#b40">[74]</ref> and Matterport3D <ref type="bibr" target="#b8">[9]</ref>, two 3D environment datasets spanning over 170 real-world spaces with a variety of obstacles and floor plans. Using only RGB(D) inputs to anticipate occupancy, the proposed agent learns to explore intelligently, achieving faster and more accurate maps compared to a state-of-the-art approach for neural SLAM <ref type="bibr" target="#b9">[10]</ref>, and navigating more efficiently than strong baselines. Furthermore, for navigation under noisy actuation and sensing, our agent improves the state of the art, winning the 2020 Habitat PointNav Challenge <ref type="bibr" target="#b0">[1]</ref> by a margin of 6.3 SPL points.</p><p>Our main contributions are: (1) a novel occupancy anticipation framework that leverages visual context from egocentric RGB(D) views; (2) a novel exploration approach that incorporates intelligent anticipation for efficient environment mapping, providing better maps in less time; and (3) successful navigation results that improve the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Navigation Classical approaches to visual navigation perform passive or active SLAM to reconstruct geometric point-clouds <ref type="bibr" target="#b37">[71,</ref><ref type="bibr">23]</ref> or semantic maps <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">55]</ref>, facilitated by loop closures or learned odometry <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b7">8]</ref>. More recent work uses deep learning to learn navigation <ref type="bibr" target="#b45">[79,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b22">56,</ref><ref type="bibr">41,</ref><ref type="bibr" target="#b43">77,</ref><ref type="bibr" target="#b41">75,</ref><ref type="bibr" target="#b25">59,</ref><ref type="bibr" target="#b30">64]</ref> or exploration <ref type="bibr">[48,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">57,</ref><ref type="bibr">28,</ref><ref type="bibr" target="#b17">51]</ref> policies in an end-to-end fashion. Explicit map-based navigation models <ref type="bibr">[21,</ref><ref type="bibr">46,</ref><ref type="bibr">19,</ref><ref type="bibr" target="#b10">11]</ref> usually outperform their implicit counterparts by being more sample-efficient, generalizing well to unseen environments, and even transferring from simulation to real robots <ref type="bibr">[20,</ref><ref type="bibr" target="#b9">10]</ref>. However, existing approaches only encode visible regions for mapping (i.e., the ground plane projection of the observed or inferred depth). In contrast, our model goes beyond the visible cues and anticipates maps for unseen regions to accelerate navigation.</p><p>Layout estimation Recent work predicts 3D Manhattan layouts of indoor scenes given 360 panoramas <ref type="bibr" target="#b46">[80,</ref><ref type="bibr" target="#b42">76,</ref><ref type="bibr" target="#b36">70,</ref><ref type="bibr" target="#b39">73,</ref><ref type="bibr" target="#b14">15]</ref>. These methods predict structured outputs such as layout boundaries <ref type="bibr" target="#b46">[80,</ref><ref type="bibr" target="#b36">70]</ref>, corners <ref type="bibr" target="#b46">[80]</ref>, and floor/ceiling probability maps <ref type="bibr" target="#b42">[76]</ref>. However, they do not extrapolate to unseen regions. FloorNet <ref type="bibr">[36]</ref> and Floor-SP [29] use walkthroughs of previously scanned buildings to reconstruct detailed floorplans that may include predictions for the room type, doors, objects, etc. However, they assume that the layouts are polygonal, the scene is fully explored, and that detailed human annotations are available. Our occupancy map representation can be seen as a new way for the agent to infer the layout of its surroundings. Unlike any of the above approaches, our model does not make strict assumptions on the scene structure, nor does it require detailed semantic annotations. Furthermore, the proposed anticipation model is learned jointly with the exploration policy and without human guidance. Finally, unlike prior work, our goal is to accelerate navigation and map creation.</p><p>Scene completion Past work in scene completion focuses on pixelwise reconstruction of 360 panoramas with limited glimpses <ref type="bibr">[28,</ref><ref type="bibr">50,</ref><ref type="bibr" target="#b17">51,</ref><ref type="bibr" target="#b27">61]</ref>, inpainting <ref type="bibr">[49,</ref><ref type="bibr">26,</ref><ref type="bibr">35]</ref>, and inferring unseen 3D structure and semantics <ref type="bibr" target="#b34">[68,</ref><ref type="bibr" target="#b44">78]</ref>. While some methods allow pixelwise extrapolation outside the current field of view (FoV) <ref type="bibr" target="#b17">[51,</ref><ref type="bibr" target="#b34">68,</ref><ref type="bibr" target="#b44">78,</ref><ref type="bibr">27]</ref>, they do not permit inferences about occluded regions in the scene. Our results show that this limitation is detrimental to successful occupancy estimation (cf. our view extrapolation baseline). SSCNet <ref type="bibr" target="#b33">[67]</ref> performs voxelwise geometric and semantic predictions for unseen 3D structures; however, it is computationally expensive, requires voxelwise semantic labels, limits predictions to the agent's FoV, and needs carefully curated viewpoints for training. In contrast, our approach predicts 2D occupancy from egocentric RGB(D) views, and it learns to do so in an active perception setting. Since the agent controls its own camera, the viewpoints tend to be more challenging than those in curated datasets of human-taken photos used in the scene completion literature <ref type="bibr" target="#b33">[67,</ref><ref type="bibr" target="#b34">68,</ref><ref type="bibr">28,</ref><ref type="bibr">50,</ref><ref type="bibr" target="#b44">78]</ref>.</p><p>Occupancy maps In robotics, methods for occupancy focus on building continuous representations of the world [45, <ref type="bibr" target="#b19">53,</ref><ref type="bibr" target="#b28">62]</ref>, mapping for autonomous driving <ref type="bibr">[25,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b32">66,</ref><ref type="bibr">37,</ref><ref type="bibr">42]</ref>, and indoor robot navigation [31, <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">65]</ref>. Prior extrapolation methods assume wide FoV LIDAR inputs, only exploit geometric cues from topdown views, and demonstrate results in relatively simple 2D floorplans devoid of non-wall obstacles <ref type="bibr">[32,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">65]</ref>. In contrast, our approach does not require expensive LIDAR sensors. It operates with standard RGB(D) camera inputs, and it exploits both semantic and geometric context from those egocentric views to perform accurate occupancy anticipation. Furthermore, we demonstrate efficient navigation in visually rich 3D environments with challenging obstacles other than walls. Finally, unlike prior work, our anticipation models are learned jointly with a navigation policy that rewards accurate anticipatory mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We propose an occupancy anticipation approach for efficient exploration and navigation. Our model anticipates areas not directly visible to the agent because of occlusion (e.g., behind a table, around a corner) or due to being outside its FoV. The agent's first-person view is provided in the form of RGB-D images (see <ref type="figure" target="#fig_1">Fig. 2</ref> left). The goal is to anticipate the occupancy for a fixed region in front of the agent, and integrate those predictions over time as the agent moves about.</p><p>Next, we define the task setup and notation, followed by our approach for occupancy anticipation (Sec. 3.1) and a new formulation for exploration that rewards correctly anticipated regions (Sec. 3.2). Then, we explain how our occupancy anticipation model can be integrated into a state-of-the-art approach <ref type="bibr" target="#b9">[10]</ref> for autonomous exploration and navigation in 3D environments (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Occupancy anticipation model</head><p>We formulate occupancy anticipation as a pixelwise classification task. The egocentric occupancy is represented as a two-channel top-down map p ∈ [0, 1] 2×V ×V which comprises a local area of V × V cells in front of the camera. Each cell in the map represents a 25mm × 25mm region. The two channels contain the probabilities (confidence values) of the cell being occupied and explored, respectively. A cell is considered to be occupied if there is an obstacle, and it is explored if we know whether it is occupied or free. For training, we use the 3D meshes of indoor environments (Sec. 4.1) to obtain the ground-truth local occupancy of a V × V region in front of the camera, which includes parts that may be occluded or outside the field of view <ref type="figure" target="#fig_1">(Fig. 2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, bottom right).</head><p>Our occupancy anticipation model consists of three main components ( <ref type="figure" target="#fig_1">Fig. 2</ref>): (1) Feature extraction: Given egocentric RGB-D inputs, we compute: RGB CNN features: We encode the RGB images using blocks 1 and 2 of a ResNet-18 that is pre-trained on ImageNet, followed by three additional convolution layers that prepare these features to be passed forward with the visible occupancy map. This step extracts a mixture of textural and semantic features. Depth projection: We estimate a map of occupied, free, and unknown space by setting height thresholds on the point cloud obtained from depth and camera intrinsics <ref type="bibr" target="#b10">[11]</ref>. Consistent with past work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>, we restrict the projection-based  estimates to points within ∼ 3m, the range at which modern depth sensors would provide reliable results. This yields the initial visible occupancy map.</p><p>(2) Feature encoding: Given the RGB-D features, we independently encode them using UNet <ref type="bibr" target="#b20">[54]</ref> encoders and project them to a common feature space. We encode the depth projection features using a stack of five convolutional blocks which results in features f d = f d 1:5 . Since the RGB features are already at a lower resolution, we use only three convolutional blocks to encode them, which results in features f r = f r 3:5 . We then combine these features using the Merge module which contains layer-specific convolution blocks to merge each [f r i , f d i ]:</p><formula xml:id="formula_0">f = merge(f d , f r ).<label>(1)</label></formula><p>For experiments with only the depth modality, we skip the RGB feature extractor and Merge layer and directly use the occupancy features obtained from the depth image. For experiments with only the RGB modality, we learn a model to infer the visible occupancy features from RGB (to be defined at the end of Sec. 4.1) and use that instead of the features computed from the depth image.</p><p>(3) Anticipation decoding: Given the encoded features f , we use a UNet decoder that outputs a 2 × V × V tensor of probabilities:</p><formula xml:id="formula_1">p = σ(Decode(f )),<label>(2)</label></formula><p>wherep ∈ [0, 1] 2×V ×V is the estimated egocentric occupancy and σ is the sigmoid activation function. For training the occupancy anticipation model, we use binary cross entropy loss per pixel and per channel:</p><formula xml:id="formula_2">L = V 2 i=1 2 j=1 − p ij logp ij + (1 − p ij )log(1 −p ij ) ,<label>(3)</label></formula><p>where p is the ground-truth (GT) occupancy map that is derived from the 3D mesh of training environments (see Sec. S5 in Supp. for details). So far, we have presented our occupancy anticipation approach supposing a single RGB-D observation as input. However, our model is ultimately used in the context of an embodied agent that moves in the environment and actively collects a sequence of RGB-D views to build a complete map of the environment. Next, we introduce a new reward function that utilizes the agent's anticipation performance to guide its exploration during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Anticipation reward for exploration policy learning</head><p>In visual exploration, an agent must quickly map a new environment without having a specified target. Prior work on exploration <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">52]</ref> often uses area-coverage-the area seen in the environment during navigation-as a reward function to guide exploration. However, the traditional area-coverage approach is limited to rewarding the agent only for directly seeing areas. Arguably, an ideal exploration agent would obtain an accurate and complete map of the environment without necessarily directly observing all areas.</p><p>Thus, we propose to encourage exploratory behaviors that yield a correctly anticipated map. In this case, the occupancy entries in the map need not be obtained via direct agent observations to register a reward; it is sufficient to correctly infer them. In particular, we reward agent actions that yield accurate occupancy predictions for the global environment map, i.e., the number of grid cells where the predicted occupancy matches the layout of the environment.</p><p>More concretely, letm t ∈ [0, 1] 2×G×G be the global environment map obtained by anticipating occupancy for the RGB-D observations {x r 1:t , x d 1:t } from time 1 to t, and then geometrically registering the predictions to a single global map based on the agent's pose estimates at each time step (see <ref type="figure" target="#fig_3">Fig. 3</ref>). Note G &gt; V . Let m be the ground-truth layout of the environment. Then, the unnormalized accuracy of a map predictionm is measured as follows:</p><formula xml:id="formula_3">Accuracy(m, m) = G 2 i=1 2 j=1 1[m ij = m ij ],<label>(4)</label></formula><p>where 1[m ij = m ij ] is an indicator function that returns one ifm ij = m ij and zero otherwise. We reward the increase in map accuracy from time t − 1 to t:</p><formula xml:id="formula_4">R anticp t = Accuracy(m t , m) − Accuracy(m t−1 , m).<label>(5)</label></formula><p>This function rewards actions leading to correct global map predictions, irrespective of whether the agent actually observed those locations. For example, if the agent correctly anticipates free space behind a table and is rewarded for that, it then learns to avoid spending additional time around tables in the future to observe that space directly. Resources can be instead allocated to visiting more interesting regions that are harder to anticipate. Additionally, this reward  to the original Active Neural SLAM (ANS) model <ref type="bibr" target="#b9">[10]</ref> (see text): <ref type="bibr" target="#b0">(1)</ref> We replace the projection unit in the mapper with our occupancy anticipation model (see <ref type="figure" target="#fig_1">Fig. 2</ref>). <ref type="bibr" target="#b1">(2)</ref> We replace the area-coverage reward function with the proposed reward (Eqn. 5), which encourages the agent to efficiently explore and build accurate maps through occupancy anticipation. Note that the reward signals (in red) are provided only during training.</p><p>provides a better learning signal while training under noisy conditions by accounting for mapping errors arising from noisy pose and map predictions. Thus, our approach encourages more intelligent exploration behavior by injecting our anticipated occupancy idea directly into the agent's sequential decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Exploration and navigation with occupancy anticipation</head><p>Having defined the core occupancy anticipation components, we now demonstrate how our model can be used to benefit embodied navigation in 3D environments. We consider both exploration (discussed above) and PointGoal navigation <ref type="bibr" target="#b24">[58,</ref><ref type="bibr" target="#b1">2]</ref>, a.k.a PointNav, where the agent must navigate efficiently to a target specified by a displacement vector from the agent's starting position.</p><p>For both tasks, we adapt the state-of-the-art Active Neural SLAM (ANS) architecture <ref type="bibr" target="#b9">[10]</ref> that previously achieved the best exploration results in the literature and was the winner of the 2019 Habitat PointNav challenge. However, our anticipation model is generic and can be easily integrated with most mapbased embodied navigation models [20, <ref type="bibr" target="#b10">11,</ref><ref type="bibr">18</ref>].</p><p>The ANS model is a hierarchical, modular policy for exploration that consists of a mapper, a planner, a local policy, and a global policy (shown in <ref type="figure" target="#fig_3">Fig. 3</ref>). Given RGB images, the mapper estimates the egocentric occupancy and agent pose, and then temporally aggregates the maps into a global top-down map using the pose estimates. At regular time intervals ∆, the global policy picks a location on the global map to explore. A shortest-path planner decides what trajectory to take from the current position to the target and picks an intermediate goal (within 1.25m) to navigate to. The local policy then selects actions that lead to the intermediate goal; it gets another intermediate goal upon reaching the current goal. See <ref type="bibr" target="#b9">[10]</ref> for details. Critically, and like other prior work, the model of <ref type="bibr" target="#b9">[10]</ref> is supervised to generate occupancy estimates based solely on the visible occupancy obtained from the egocentric views.</p><p>We adapt ANS by modifying the mapper and the reward function. For the mapper, we replace the projection unit from ANS with our anticipation model (see <ref type="figure" target="#fig_3">Fig. 3</ref>). Additionally, we account for incorrect occupancy estimates in two ways: (1) we filter out high entropy predictions and (2) we maintain a moving average estimate of occupancy at each location in the global map (see Sec. S7 in Supp.). For the reward function, we use the anticipation-based reward presented in Sec. 3.2.</p><p>We train the exploration policy with our anticipation model end-to-end, as this allows adapting to the changing distribution of the agent's inputs. Both the local and the global reinforcement learning policies are trained with Proximal Policy Optimization (PPO) <ref type="bibr" target="#b26">[60]</ref>. In our model, the reward of the global policy is our anticipation-based reward defined in Eqn. 5. This replaces the traditional area-coverage reward used in ANS and other current models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">52]</ref>, which rewards the increment in the actual area seen, not the correctly registered area in the map. The reward for the local policy is simply based on the reduction in the distance to the local goal:</p><formula xml:id="formula_5">R local t = d t−1 − d t ,</formula><p>where d is the Euclidean distance between the current position and the local goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the following experiments we demonstrate that 1) our occupancy anticipation module can successfully infer unseen parts of the map (Sec. 4.2) and 2) trained together with an exploration and navigation policy, it accelerates active mapping and navigation in new environments (Sec. 4.3 and Sec. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>We use the Habitat [38] simulator along with Gibson <ref type="bibr" target="#b40">[74]</ref> and Matterport3D <ref type="bibr" target="#b8">[9]</ref> environments. Each dataset contains around 90 challenging large-scale photorealistic 3D indoor environments such as houses and office buildings. On average, the Matterport3D environments are larger. Our observation space consists of 128 × 128 RGB-D observations and odometry sensor readings that denote the change in the agent's pose x, y, θ. Our action space consists of three actions: move-forward by 25cm, turn-left by 10 • , turn-right by 10 • . For navigation, we add a stop action, which the agent emits when it believes it has reached the goal. We simulate noisy actuation and odometer readings for realistic evaluation (see Sec. S6 in Supp.).</p><p>We train our exploration models on Gibson, and then transfer them to Point-Goal navigation on Gibson and exploration on Matterport3D. We use the default train/val/test splits provided for both datasets [38] with disjoint environments across the splits. For evaluation on Gibson, we divide the validation environments into small (area less than 36m 2 ) and large (area greater than 36m 2 ) to observe the influence of environment size on results. For policy learning, we use  the Adam optimizer and train on episodes of length 1000 for 1.5 − 2 million frames of experience. Please see Sec. S8 in Supp. for more details.</p><p>Baselines: We define baselines based on prior work:</p><p>-ANS(rgb) <ref type="bibr" target="#b9">[10]</ref>: This is the state-of-the-art Active Neural SLAM approach for exploration and navigation. We use the original mapper architecture <ref type="bibr" target="#b9">[10]</ref>, which infers the visible occupancy from RGB. 3 -ANS(depth): We use depth projection to infer the visible occupancy (similar to <ref type="bibr" target="#b10">[11]</ref>) instead of predicting it from RGB. -View-extrap.: We extrapolate an 180 • FoV depth map from 90 • FoV RGB-D and project it to the top-down view. This is representative of scene completion approaches <ref type="bibr" target="#b34">[68,</ref><ref type="bibr" target="#b44">78]</ref>. See Sec. S11 in Supp. for network details. -OccAnt(GT): This is an upper bound that cheats by using the groundtruth anticipation maps for exploration and navigation.</p><p>We implement all baselines on top of the ANS framework. Our goal is to show the impact of our occupancy model, while fixing the backbone navigation architecture and policy learning approach across methods for a fair comparison. We consider three versions of our models based on the input modality:</p><p>-OccAnt(depth): anticipate occupancy given the visible occupancy map.</p><p>-OccAnt(rgb): anticipate occupancy given only the RGB image. We replace the depth projections in <ref type="figure" target="#fig_1">Fig. 2</ref> with the pre-trained ANS(rgb) estimates (kept frozen throughout training). -OccAnt(rgbd): anticipate occupancy given the full RGB-D inputs.</p><p>By default, our methods use the proposed anticipation reward from Sec. 3.2. We denote ablations without this reward as "w/o AR".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Occupancy anticipation results</head><p>First we evaluate the per-frame prediction accuracy of the mapping models trained during exploration. We evaluate on a separate dataset of images sampled from validation environments in Gibson at uniform viewpoints from discrete locations on a 1m grid, a total of 1, 034 (input, output) samples. This allows standardized evaluation of the mapper, independent of the exploration policy.</p><p>To quantify the local occupancy maps' accuracy, we compare the predicted maps to the ground truth. We report the Intersection over Union (IoU) and F1 scores for the "free" and "occupied" classes independently. In addition to the baselines from Sec. 4.1, we add two naive baselines that classify all locations as free (all-free), or occupied (all-occupied). <ref type="table" target="#tab_2">Table 1</ref> shows the results. Our anticipation models OccAnt are substantially better than all the baselines. Comparing different modalities, OccAnt(depth) is much better than OccAnt(rgb) under all the metrics. This makes sense, as visible occupancy is directly computable from the depth input, but must be inferred for RGB (see <ref type="figure">Fig. 4</ref>). Interestingly, the rgbd models are not better than the depth-only models, likely because (1) geometric cues are more easily learned from depth than RGB, and (2) the RGB encoder contains significantly more parameters and could lead to overfitting. See <ref type="table" target="#tab_17">Table S5</ref> in Supp. for network sizes. Overall, <ref type="table" target="#tab_2">Table 1</ref> demonstrates our occupancy anticipation models successfully broaden the coverage of the map beyond the visible regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Exploration results</head><p>Next we deploy our models for visual exploration. The agent is given a limited time budget (T =1000) to intelligently explore and build a 2D top-down occupancy map of a previously unseen environment.</p><p>To quantify exploration, we measure both map quality and speed (number of agent actions): (1) Map accuracy (m 2 ): the area in the global map built during exploration (both free and occupied) that matches with the ground-truth layout of the environment. The map is built using predicted occupancy maps which are registered using estimated pose (may be noisy). Note that this is an unnormalized accuracy measure (see Eqn. 4). (2) IoU: the intersection over union between that same global map and the ground-truth layout of the environment. (3) Area seen (m 2 ): the amount of free and occupied regions directly seen during exploration. The map for this metric is built using ground-truth pose and depth-projections (similar to <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>). (4) Episode steps: the number of actions taken by the agent. While the first two metrics measure the quality of the created map, the latter two are a function of how (and how long) the agent moved to get that map. Higher accuracy in fewer steps or lower area-seen is better.</p><p>All agents are trained on 72 scenes from Gibson under noisy odometry and actuation (see Sec. 4.1), and evaluated on Gibson and Matterport3D under both noisy and noise-free conditions.</p><formula xml:id="formula_6">RGB-D ANS(rgb) ANS(depth) OccAnt(rgb) View-extrap.</formula><p>Ground truth OccAnt(depth) Occupied Free Unknown <ref type="figure">Fig. 4</ref>: Per-frame local occupancy predictions: First and last columns show the RGB-D input and anticipation ground-truth, respectively. ANS(*) are restricted to only predicting occupancy for visible regions. View-extrap. extrapolates, but is unable to predict occupancy for occluded regions (first row) and struggles to make correct predictions in cluttered scenes (second row). Our model successfully anticipates with either RGB or depth. For example, in the first row, we successfully predict the presence of a corridor and another room on the left. In the second row, we successfully predict the presence of navigable space behind the table. In the third row, we are able to correctly anticipate the free space behind the chair and the corridor to the right.   <ref type="figure">Fig. 6</ref> shows the exploration results. Our approach generally outperforms the baselines, improving the map quality more rapidly, whether in terms of time (top row) or area seen (bottom row). When compared on a same-modality basis, we see that OccAnt(rgb) converges much faster than ANS(rgb). Similarly, OccAnt(depth) is able to rapidly improve the map quality and outperforms ANS(depth) on all cases. This apples-to-apples comparison shows that anticipating occupancy leads to much more efficient mapping in unseen environments. Again, using depth generally provides more reliable mapping than pure RGB.</p><p>Furthermore, the proposed anticipation reward generally provides significant benefits to map accuracy in the noisy setting (compare our full model to the "w/o AR" models in <ref type="figure">Fig. 6</ref>). While map accuracy generally increases over time for noise-free conditions (see Sec. S1 in Supp.), it sometimes saturates early or even declines slightly over time in the noisy setting as noisy pose estimates accumulate and hurt map registration accuracy. This is most visible in Gibson small (top left plot). However, our anticipatory reward alleviates this decline. <ref type="table" target="#tab_4">Table 2</ref> summarizes the map accuracy and IoU for all methods at T =500. Our method obtains significant improvements, supporting our claim that occupancy anticipation accelerates exploration and mapping. Additionally, perfect anticipation with the OccAnt(GT) model gives comparably good noisy exploration, and good gains in noise-free exploration (+10-20% IoU). This shows that there is indeed a lot of mileage in anticipating occupancy; our model moves the stateof-the-art towards this ceiling. <ref type="figure">Fig. 5</ref> shows example exploration trajectories and the final global map predictions on Gibson.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Navigation results</head><p>Next we evaluate the utility of occupancy anticipation for quickly reaching a target. In PointNav <ref type="bibr" target="#b24">[58,</ref><ref type="bibr" target="#b1">2]</ref>, the agent is given a 2D coordinate (relative to its position) and needs to reach that target as quickly as possible. Following <ref type="bibr" target="#b9">[10]</ref>, we use noise-free evaluation and directly transfer the mapper, planner, and local policy learned during exploration to this task. In this way, instead of navigating to a point specified by the global policy, the agent has to navigate to a fixed goal location. To evaluate navigation, we use the standard metrics-success rate, success rate normalized by inverse path length (SPL) <ref type="bibr" target="#b1">[2]</ref>, and time taken. The agent succeeds if it stops within 0.2m of the target under a time budget of T = 1000. <ref type="table" target="#tab_6">Table 3</ref> shows the navigation results on the Gibson validation set. Our approach outperforms the baselines. Thus, not only does occupancy anticipation successfully map the environment, but it also allows the agent to move to a specified goal more quickly by modeling the navigable spaces. This apples-to-apples comparison shows that our idea improves the state of the art for PointNav. As with exploration, using ground truth (GT) anticipation leads to good gains in the navigation performance, and our methods bridge the gap between the prior state of the art and perfect anticipation.</p><p>In concurrent work, the DD-PPO approach <ref type="bibr" target="#b38">[72]</ref> obtains 0.96 SPL for Point-Nav, but it requires 2.5 billion frames of experience to do so (and it fails for noisy conditions; see below). To achieve the performance of our method (0.8 SPL in 2M frames), DD-PPO requires more than 50× the experience. Our sample efficiency can be attributed to explicit mapping along with occupancy anticipation.   Finally, we validate our approach on the 2020 Habitat PointNav Challenge <ref type="bibr" target="#b0">[1]</ref>, which requires the agent to adapt to noisy RGB-D sensors and noisy actuators, and to operate without an odometer. This presents a much more difficult evaluation setup than past work which assumes perfect odometry as well as noise-free sensing and actuation [38, <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">72]</ref>. See Sec. S13 in Supp. for more details. <ref type="table" target="#tab_7">Table 4</ref> shows the results. Our method won the challenge, outperforming the competing approaches by large margins. While our approach generalizes well to this setting, DD-PPO <ref type="bibr" target="#b38">[72]</ref> fails (0 SPL) due to its reliance on perfect odometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced the idea of occupancy anticipation from egocentric views in 3D environments. By learning to anticipate the navigable areas beyond the agent's actual field of view, we obtain more accurate maps more efficiently in novel environments. We demonstrate our idea both for individual local maps, as well as integrated within sequential models for exploration and navigation, where the agent continually refines its (anticipated) map of the world. Our results clearly demonstrate the advantages on multiple datasets, including improvements to the state-of-the-art embodied AI model for exploration and navigation. correctly, AR is likely to be low even if the per-frame map estimates are very good. Therefore, in addition to covering more area, the agent also has to better train the pose estimator which would then lead to higher AR over time. Since noise correction is not needed under noise-free conditions, using AR has limited impact on the final performance. Matterport3D under noise-free conditions. Top: Our OccAnt approach (solid lines) rapidly attains higher map accuracy than the baselines (dotted lines). Using anticipation reward (AR) largely retains the original performance in the noise-free conditions (but improves significantly in the noisy conditions, see <ref type="figure">Fig. 5</ref> main paper). Bottom: OccAnt achieves higher map accuracy for the same area covered (we show best variants here to avoid clutter). These results show the agent actively moves better to explore the environment with our occupancy anticipation idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Occupancy anticipation ablation study</head><p>As discussed in the main paper, our key contributions are a novel framework for occupancy anticipation and a novel anticipation reward which encourages the agent to build more accurate maps (as opposed to covering more area). To isolate the gains achieved by these individual contributions, we view the results from the main paper <ref type="table" target="#tab_2">(Tables 1, 2</ref>, and 3 in main paper) in a different way. We first group the results based on the modality (rgb/depth/rgbd), and further sort the methods based on whether they use occupancy anticipation (OccAnt) or the anticipation reward (AR). We present these ablations for the per-frame map evaluation <ref type="table" target="#tab_2">(Table S1)</ref>, the exploration evaluation <ref type="table" target="#tab_4">(Table S2)</ref>, and the navigation evaluation <ref type="table" target="#tab_6">(Table S3</ref>). By default, the ANS baselines do not use occupancy anticipation or the anticipation reward and our methods always use occupancy anticipation.</p><p>For per-frame maps, in <ref type="table" target="#tab_2">Table S1</ref> we see that adding occupancy anticipation to the base model significantly improves the IoU and F1 scores as expected. Adding the anticipation reward leads to comparable or better results, showing that it leads to better training of the mapper during the exploration training.</p><p>For exploration, in <ref type="table" target="#tab_4">Table S2</ref> we see that adding occupancy anticipation generally leads to better map quality than ANS across different modalities and testing conditions. Adding the anticipation reward (AR) leads to significant improvements in the map quality under noisy conditions for both depth and rgbd modalities (rgb slightly underperforms). This is primarily due to improved training of the mapper module which leads to better map registration (see Sec. S4). As we also noted in Sec. S1, using AR in noise-free conditions has limited impact on the performance as the pose-estimation is assumed to be perfect in these cases. It mainly benefits exploration in the more real-world testing scenarios with noisy actuation and sensing.</p><p>For navigation, in <ref type="table" target="#tab_6">Table S3</ref> we see that adding occupancy anticipation leads to significant improvements in all three metrics. The impact of using AR here is limited because we assume noise-free test conditions for PointNav (following <ref type="bibr">[38,</ref><ref type="bibr" target="#b9">10]</ref>). However, the challenge results reported in the main paper remove this assumption to test PointNav with noisy odometry and actuation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3 Occupancy anticipation qualitative examples</head><p>See Figs. S2 and S3 for some successful cases and failure cases for our best method from Table S1 when compared with the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4 Exploration with occupancy anticipation examples</head><p>In <ref type="table" target="#tab_4">Table 2</ref> and <ref type="figure">Fig. 6</ref> from the main paper, and <ref type="table" target="#tab_4">Table S2</ref> in this supplementary, we see that adding occupancy anticipation on top of the ANS baseline leads to better performance, and adding anticipation reward (AR) leads to better mapping in the noisy cases.</p><p>Here, we highlight some example episodes to show that (1) using occupancy anticipation avoids local navigation difficulties and obtains higher map qualities for lower area coverage <ref type="figure">(Fig. S4)</ref>, while sometimes being susceptible to inaccuracies in map predictions <ref type="figure">(Fig. S5)</ref>, and (2) the anticipation reward leads to better map registration (i.e., good pose estimates) which results in higher map quality <ref type="figure">(Fig. S6)</ref>. The color scheme for the trajectories (from [38]) and the predicted maps in the center (from <ref type="bibr" target="#b9">[10]</ref>) are indicated below each plot. the visible occupancy (2nd column) and ANS(depth) (3rd column) directly uses the visible occupancy (within a 3m range). Both these methods are unable to account for regions that are not visible or outside the sensing range. While View-extrap (4th column) is able to expand beyond a 90 • FoV, its predictions are often noisy and do not include occluded regions. Also, the predictions are not guaranteed to be smooth in the top-down projection as smoothness in the depth-image prediction space does not necessarily lead to smoothness in the top-down maps, resulting in speckled outputs. Our method OccAnt(rgbd) (5th column) is able to successfully anticipate occupancy for regions that are occluded and outside the field-of-view with high accuracy (see ground-truth in column 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB ANS(rgb) ANS(depth) View-extrap. OccAnt(rgbd) GT</head><p>Occupied Free Unknown <ref type="figure" target="#fig_3">Fig. S3</ref>: Occupancy anticipation failure cases: Our approach OccAnt(rgbd) incorrectly predicts narrow corridors as occupied, and is unable to handle cases where multiple solutions may exist. For example, in rows 2, 5 and 8, it predicts that the corridors in the center of the map are blocked. In row 1, it predicts that the two doors correspond to the same room, even though the wall colors are different and it is unlikely that a small room would have two doors. In row 3, 4 and 6, it predicts entrances to spaces that do not exist. Such predictions are generally difficult to make given only the context of the current first-person view, and therefore our model tends to fail at these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OccAnt(depth) w/o AR ANS(depth)</head><p>Occupied Free</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unknown</head><p>Correct predictions Incorrect predictions Occupied Free t = 0 t = 1000</p><p>Map prediction color scheme Map prediction color scheme <ref type="figure">Fig. S6</ref>: The impact of using anticipation reward: In <ref type="table" target="#tab_4">Table 6 from the main paper   and Table S2</ref> in Supp., we could see that models using anticipation reward generally leads to higher map qualities in noisy test conditions. Here, we show that, when the model that uses the anticipation reward (OccAnt(depth) on the left) accounts much better for the noise in map registration when compared to a vanilla anticipation model that does not use it (OccAnt(depth) w/o AR on the right).     Using 3D meshes of indoor environments from Gibson and Matterport3D, we obtain the ground-truth local occupancy of a V ×V region in front of the camera which includes parts that may be occluded or outside the field-of-view (see <ref type="figure" target="#fig_1">Fig.2</ref> from main paper). However, this may include regions in the environment that are outside the bounds of the environment's mesh. To alleviate this problem, we devise a simple heuristic that generates the ground truth by masking out regions in the occupancy map that are outside the bounds of the environment (highlighted in <ref type="figure" target="#fig_8">Fig. S7</ref>).</p><p>We first obtain the visible occupancy via a geometric projection of the depth inputs (2nd column). We then selectively sample the ground-truth layout (last column) around the visible regions by growing a mask around the visible occupancy by sequential hole-filling and morphological dilation operations. We perform two iterations of this region growing to obtain the final ground-truth used to train our model (3rd &amp; 4th columns). This heuristic captures the occupied regions that are closer to navigable space in the environment (likely to be objects, walls, etc), while ignoring regions outside the bounds of the environment. This is necessary since the occupancy map from the simulator does not distinguish between obstacles and regions outside the bounds of the environment mesh. Note that these steps apply only in training; during inference the occupancy anticipation proceeds solely in the end-to-end model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6 Noise models for actuation and odometry</head><p>Following <ref type="bibr" target="#b9">[10]</ref>, we simulate realistic actuation and odometry to train and evaluate our exploration agents. For this purpose, we use the PyRobot actuation model provided by Habitat which consists of truncated Gaussians for both the rotation and translation motions. <ref type="bibr" target="#b2">3</ref> Specifically, we use the default LoCoBot noise-model with the ILQR controller. For simulating noise in the odometry, we similarly use truncated Gaussians for both rotation and translation measurements. For the translation measurement, we use a mean of 0.025m and a standard deviation of 0.001 For the rotation measurement, we use a mean of 0.9 • and standard deviation of 0.057 • . The distributions are truncated at 2 standard deviations. These are based on approximate values provided by the authors of ANS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7 Differences in ANS implementation</head><p>We implemented the ANS approach using the published details in <ref type="bibr" target="#b9">[10]</ref> and instructions obtained directly from the authors via private communication as code was not publicly available at the time of our research. Our implementation has a few differences from that in <ref type="bibr" target="#b9">[10]</ref>, which we discuss in the following. For shortest path planning, we use an A* planner instead of fast-marching <ref type="bibr" target="#b29">[63]</ref> used in <ref type="bibr" target="#b9">[10]</ref> since we were able to find a fast A* implementation that was publicly available. <ref type="bibr" target="#b3">4</ref> For aggregating the local occupancy mapsp t from each observation 5 into the global mapm t−1 from the previous time-step , the authors in <ref type="bibr" target="#b9">[10]</ref> use channelwise max-pooling of the local and global maps to obtain the updated global mapm t .</p><formula xml:id="formula_7">m t = ChannelwiseMax(m t−1 ,p t )<label>(1)</label></formula><p>Instead, we opt to perform a moving-average over time to allow the agent to account for errors in the map prediction by averaging predictions from multiple views over time.m</p><formula xml:id="formula_8">t = α emt−1 + (1 − α e )p t<label>(2)</label></formula><p>We found that this provided robustness to false positives in the map predictions and registration errors due to odometry noise. Additionally, since our proposed model anticipates occupancy beyond the visible regions, we found that it is helpful to filter out low-confidence predictions of occupancy on a per-frame basis using the EntropyFilter() operation. Given predictionp t , EntropyFilter() masks out the predictions for locations i, j in p t where the binary-entropy of the probabilities across the map channels are larger than a threshold τ ent before performing the moving-average aggregation. These low-confidence predictions generally correspond to regions that are hard to anticipate or may have multiple solutions. Hence, our global map update formula is:m</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S8 Implementation details</head><p>The key hyperparameters for learning the policy and mapper are specified in <ref type="table" target="#tab_7">Table S4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S10 ANS projection unit architecture</head><p>The projection unit architecture for the ANS(rgb) baseline is shown in <ref type="figure" target="#fig_0">Fig. S12</ref>. This is based on the architecture in <ref type="bibr" target="#b9">[10]</ref> with some minor differences. It uses nn.BatchNorm + nn.ReLU blocks instead of nn.Dropout in the fully connected layers, it has a larger convolutional decoder to account for our larger map outputs, and it consists of nn.Conv2d + nn.Upsample layers instead of than nn.ConvTranspose2D layers as this has been shown to reduce checkerboard artifacts [44].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11 View extrapolation baseline</head><p>We now provide more details on the task-defintion and architecture for the  not natively support panoramic rendering, we use a simpler solution to account for this. We place two cameras with ±45 • heading offsets and aim to regress those from the egocentric view (see <ref type="figure" target="#fig_0">Fig. S13</ref>). Since each camera has a 90 • FoV, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional decoder</head><p>Fig. S12: ANS projection unit: ResNet-18 features are extracted, followed by two fully connected layers (represented by 1 × 1 convolutions) and a convolutional decoder that uses "Upsample" blocks to increase the output resolution and predict the occupancy estimatesp. Note that this is supervised to predict the visible occupancy map, not the anticipated occupancy map (see <ref type="figure" target="#fig_0">Fig. 1</ref> in main paper).</p><p>this leads to an effective coverage of 180 • once the agent anticipates the unobserved portions. We base our architecture for view extrapolation on the model from <ref type="bibr" target="#b44">[78]</ref> with a capacity similar to our model to permit online training during policy learning (see <ref type="figure" target="#fig_0">Fig. S14</ref>). It takes as input the 90 • FoV RGB-D images and regresses the left and right cameras. It is trained to minimize the pixelwise 1 loss between the prediction and the ground-truth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S12 Comparing the model capacities of different methods</head><p>We compare the overall model capacity of our approaches with the baselines in <ref type="table" target="#tab_17">Table S5</ref>. The depth-only models (bottom 2 rows) tend to have fewer parameters than the rgb-only models as they rely on geometric projection for processing depth (no ResNet backbone). Our depth model has comparable number of parameters with the depth-only baselines. Our rgb model has slightly more parameters than the rgb baseline. However, this is due to the fact that OccAnt(rgb) takes the output of ANS(rgb) as an additional input. However, since ANS(rgb) is kept frozen throughout the training of OccAnt(rgb), this effectively gives us 5.7M trainable parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S13 Habitat Challenge 2020</head><p>We detail the key issues we had to address for the PointNav track of Habitat Challenge 2020 <ref type="bibr" target="#b0">[1]</ref> and the changes to our system required to achieve our stateof-the-art results. Compared to the 2019 Habitat Challenge, there were two key changes that increased the task difficulty:</p><p>Lack of GPS+Compass sensor: The presence of the GPS+Compass sensor used in earlier challenges continually provides the agent with a perfect estimate of the position and heading angle of the goal relative to its current position. Such perfect localization has been exploited in the past by purely geometric [20] and learned <ref type="bibr" target="#b38">[72]</ref> approaches to achieve high-quality PointNav performance. However, such high precision localization is hard to achieve in practice. The 2020 challenge instead requires navigation in the absence of the GPS+Compass sensor. Instead, the goal location is only specified initially at the start of the episode, requiring the agent to accurately keep track of its position in the environment to successfully reach the goal.</p><p>Noisy actuation and sensing: In the 2020 challenge, RGB-D sensing noise is simulated artificially by using a Gaussian noise-model for the RGB sensor and the Redwood noise model <ref type="bibr" target="#b11">[12]</ref> for the depth sensor. Additionally, actuation noise in the robot motions is simulated by using a noise model obtained from the LoCoBot <ref type="bibr">[43]</ref>. We adapted our model in several ways to address these challenges. To address the lack of GPS+Compass sensor, we used an online pose estimator that uses RGB-D inputs x t and x t+1 to estimate the relative change in the pose ∆p t+1 . These pose changes are summed up over time to track the agent's pose p t+1 . When compared to the original ANS model, we found that using RGB-D inputs gave slightly better estimates and was more computationally efficient than using top-down maps. The pose estimator consists of a 6 convolutional layers followed by 3 fully-connected layers to predict the pose for each modality (RGB, depth) independently. The predictions are combined by using input-conditioned weighting factors that are estimated using a learned MLP with 4 fully-connected layers.</p><p>To handle noisy sensing, we train our occupancy anticipation model endto-end on the noisy inputs, which gave accurate predictions (see <ref type="figure" target="#fig_0">Fig. S15</ref>). We found that OccAnt(depth) gave the best performance, and that adding RGB information to occupancy anticipation did not lead to significant changes in performance.</p><p>To deal with noisy actuation, we found that the learned pose estimator gave robust estimates of the agent position. Despite having this pose estimator, we experienced large drifts in the estimate over time due to high variance in the actuation noise. To partially mitigate this issue, we focused on efficient navigation with safe planning that maintains sufficient distance from obstacles while planning shortest paths. In practice, we found that reducing the number of collisions leads to faster navigation and lower drift in the pose estimates. We achieve this by using a weighted variant of the classic A-star search algorithm <ref type="bibr">[22]</ref>. <ref type="bibr" target="#b5">6</ref> Additionally, we incorporated some simple heuristics from the original Active Neural SLAM implementation to update the occupancy maps based on collisions, and used an analytical local policy for navigation instead of a learned policy. <ref type="figure" target="#fig_0">Fig. S15</ref>: Qualitative results from the 2020 Habitat Challenge: On the left, we show the noisy RGB and depth inputs. On the right, we show the corresponding anticipated and ground-truth occupancy. Our model learns to anticipate accurately in the presence of noise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Occupancy anticipation: A robot's perception of the 3D world is limited</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Our occupancy anticipation model uses RGB(D) inputs to extract features, and processes them using a UNet to anticipate the occupancy. The depth map is projected to the ground plane to obtain the preliminary visible occupancy map. See text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Exploration with occupancy anticipation: We introduce two key upgrades</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>18. Gan, C., Zhang, Y., Wu, J., Gong, B., Tenenbaum, J.B.: Look, listen, and act: Towards audio-visual embodied navigation. arXiv preprint arXiv:1912.11684 (2019) 19. Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., Farhadi, A.: Iqa:Visual question answering in interactive environments. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4089-4098 (2018) 20. Gupta, S., Davidson, J., Levine, S., Sukthankar, R.,Malik, J.: Cognitive mapping and planning for visual navigation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2616-2625 (2017) 21. Gupta, S., Fouhey, D., Levine, S., Malik, J.: Unifying map and landmark based representations for visual navigation. arXiv preprint arXiv:1712.08125 (2017) 22. Hart, P., Nilsson, N., Raphael, B.: A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics 4(2), 100-107 (1968). https://doi.org/10.1109/tssc.1968.300136, https://doi.org/10. 1109/tssc.1968.300136 23. Hartley, R., Zisserman, A.: Multiple view geometry in computer vision. Cambridge university press (2003) 24. Henriques, J.F., Vedaldi, A.: Mapnet: An allocentric spatial memory for mapping environments. In: proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8476-8484 (2018) 25. Hoermann, S., Bach, M., Dietmayer, K.: Dynamic occupancy grid prediction for urban autonomous driving: A deep learning approach with fully automatic labeling. In: 2018 IEEE International Conference on Robotics and Automation (ICRA). pp. 2056-2063. IEEE (2018) 26. Iizuka, S., Simo-Serra, E., Ishikawa, H.: Globally and locally consistent image completion. ACM Transactions on Graphics (ToG) 36(4), 1-14 (2017) 27. Jayaraman, D., Gao, R., Grauman, K.: Shapecodes: self-supervised feature learning by lifting views to viewgrids. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 120-136 (2018) 28. Jayaraman, D., Grauman, K.: Learning to look around: Intelligently exploring unseen environments for unknown tasks. In: Computer Vision and Pattern Recognition, 2018 IEEE Conference on (2018) 29. Jiacheng Chen, Chen Liu, J.W., Furukawa, Y.: Floor-sp: Inverse cad for floorplans by sequential room-wise shortest path. In: The IEEE International Conference on Computer Vision (ICCV) (2019) 30. Karkus, P., Ma, X., Hsu, D., Kaelbling, L.P., Lee, W.S., Lozano-Pérez, T.: Differentiable algorithm networks for composable robot learning. arXiv preprint arXiv:1905.11602 (2019) 31. Katyal, K., Popek, K., Paxton, C., Burlina, P., Hager, G.D.: Uncertainty-aware occupancy map prediction using generative networks for robot navigation. In: 2019 International Conference on Robotics and Automation (ICRA). pp. 5453-5459. IEEE (2019) 32. Katyal, K., Popek, K., Paxton, C., Moore, J., Wolfe, K., Burlina, P., Hager, G.D.: Occupancy map prediction using generative and fully convolutional networks for vehicle navigation. arXiv preprint arXiv:1803.02007 (2018) 33. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 34. Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Gordon, D., Zhu, Y., Gupta, A., Farhadi, A.: AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv (2017) 35. Li, Y., Liu, S., Yang, J., Yang, M.H.: Generative face completion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3911-3919 (2017) 36. Liu, C., Wu, J., Furukawa, Y.: Floornet: A unified framework for floorplan reconstruction from 3d scans. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 201-217 (2018) 37. Lu, C., Dubbelman, G.: Hallucinating beyond observation: Learning to complete with partial observation and unpaired prior knowledge (2019) 38. Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., Parikh, D., Batra, D.: Habitat: A Platform for Embodied AI Research. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 39. Martinez-Cantin, R., De Freitas, N., Brochu, E., Castellanos, J., Doucet, A.: A bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot. Autonomous Robots 27(2), 93-103 (2009) 40. Mohajerin, N., Rohani, M.: Multi-step prediction of occupancy grid maps with recurrent neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 10600-10608 (2019) 41. Mousavian, A., Toshev, A., Fišer, M., Košecká, J., Wahid, A., Davidson, J.: Visual representations for semantic target driven navigation. In: 2019 International Conference on Robotics and Automation (ICRA). pp. 8846-8852. IEEE (2019) 42. Müller, M., Dosovitskiy, A., Ghanem, B., Koltun, V.: Driving policy transfer via modularity and abstraction. arXiv preprint arXiv:1804.09364 (2018) 43. Murali, A., Chen, T., Alwala, K.V., Gandhi, D., Pinto, L., Gupta, S., Gupta, A.: Pyrobot: An open-source robotics framework for research and benchmarking. arXiv preprint arXiv:1906.08236 (2019) 44. Odena, A., Dumoulin, V., Olah, C.: Deconvolution and checkerboard artifacts. Distill 1(10), e3 (2016) 45. OCallaghan, S.T., Ramos, F.T.: Gaussian process occupancy maps. The International Journal of Robotics Research 31(1), 42-62 (2012) 46. Parisotto, E., Salakhutdinov, R.: Neural map: Structured memory for deep reinforcement learning. arXiv preprint arXiv:1702.08360 (2017) 47. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning library. In: Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran Associates, Inc. (2019), http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf 48. Pathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration by self-supervised prediction. In: International Conference on Machine Learning (2017) 49. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context encoders: Feature learning by inpainting. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016) 50. Ramakrishnan, S.K., Grauman, K.: Sidekick policy learning for active visual exploration. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 413-430 (2018)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 )Fig. S1 :</head><label>2S1</label><figDesc>Noise-free exploration results: Map accuracy (m 2 ) as a function of episode duration (top row) and area seen (bottom row) for Gibson (small and large splits) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. S2 :</head><label>S2</label><figDesc>Occupancy anticipation successful cases: ANS(rgb) is trained to predict</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. S4 :Fig. S5 :</head><label>S4S5</label><figDesc>We enumerate some of the key advantages of exploration using occupancy anticipation by comparing OccAnt(depth) w/o AR with ANS(depth) in Gibson under noise-free conditions. The exploration trajectories and the map created during exploration are shown at the extremes and the center, respectively. 'ANS(depth) tends to achieve worse exploration in some cases where the visible occupancy is incorrectly estimated (top 3 rows), causing the agent to get stuck in local regions. In other cases, the map accuracy is generally higher for OccAnt(depth) w/o AR for similar amounts of area seen (bottom 3 rows) as it is better at filling up the occupancy for unvisited regions. We highlight one key weakness of exploration using occupancy anticipation, which is the impact of classification errors in occupancy estimates. We compare Oc-cAnt(depth) w/o AR with ANS(depth) in Gibson. In some cases, OccAnt(depth) w/o AR tends to generate false negatives for occupied regions, classifying some of the explored obstacles as free-space (gray regions in the first 3 rows, 2nd column). While this does not impact the area seen, it does reduce the map quality. On the flip side, OccAnt(depth) w/o AR may prematurely classify some narrow corridors as blocked (similar toFig. S3) causing the agent to stop exploring beyond that corridor (light green regions in last two rows, 2nd column).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. S7 :</head><label>S7</label><figDesc>Pipeline for generating anticipation ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. S10 :</head><label>S10</label><figDesc>MERGE: combines the RGB (f r 3:5 ) and visible occupancy (f 1 : 5 d ) features obtained from Feature encoding layers in a layerwise fashion to obtain a set of merged features f = f1:5. Since the RGB features are not available at levels 1 and 2, it simply uses the visible occupancy features for those levels. The expanded view of the "Merge i (F)" block is shown on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. S13 :</head><label>S13</label><figDesc>View extrapolation task: Given the agent's egocentric RGB-D input, we predict the depth-map for two additional depth-sensors placed at 45 • angles to the left (purple) and right (green) of the central input (orange). These are geometrically projected to the top-down view to obtain the occupancy estimates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Occupancy anticipation results on the Gibson validation set. Our models, OccAnt(·), substantially improve the map quality and extent, showing the advantage of learning to anticipate 3D structures beyond those directly observed.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Exploration examples: We compare OccAnt with ANS<ref type="bibr" target="#b9">[10]</ref> in Gibson under noisy actuation and odometry. The exploration trajectories and the corresponding maps are shown at the extremes and center, respectively. Row 1: Both methods cover similar area, but our method better anticipates the unseen parts with fewer registration errors. Row 2: Our method achieves better area coverage and mapping quality whereas the baseline gets stuck in a small room for extended periods of time. Row 3: A failure case for our method, where it gets stuck in one part of the house after anticipating that a narrow corridor leading to a different room was occupied.</figDesc><table><row><cell>OccAnt(depth) Exploration trajectory Map created t = 0 Occupied Free Correct: Incorrect: Map prediction color scheme Map created Occupied Free ANS(depth) Exploration trajectory t = 1000 Unknown Gibson large Matterport3D # Episode steps Area seen (m 2 ) IoU ANS(rgb) [10] Map accuracy (m 2 18.5 55 35.0 47 44.7 18 22.4 76 43.4 64 53.4 23 ANS(depth) 18.5 56 39.4 53 72.5 26 21.4 74 48.0 72 85.9 34 View-extrap. 12.0 26 28.1 27 39.4 14 12.1 27 26.5 27 33.9 13 OccAnt(rgb) w/o AR 21.8 66 44.2 57 65.8 23 22.6 71 45.2 60 64.4 24 OccAnt(depth) w/o AR 20.2 58 44.2 54 92.7 29 24.9 84 54.1 75 . 104.7 38. OccAnt(rgbd) w/o AR 16.9 45 35.6 40 76.3 23 24.8 84 52.0 71 98.7 34 OccAnt(rgb) 20.9 62 42.1 54 66.2 22 22.3 70 43.5 58 64.4 22 OccAnt(depth) 22.7 71 50.3 67 94.1 33 24.8 83 53.1 74 96.5 35 OccAnt(rgbd) 22.7 71 48.4 62 99.9 32 24.5 82 51.0 69 100.3 34 Fig. 5: Gibson small OccAnt(GT) 21.7 67 51.9 63 --26.1 93 65.4 91 --</cell></row></table><note>) Fig. 6: Exploration results: Map accuracy (m 2 ) as a function of episode duration (top row) and area seen (bottom row) for Gibson (small and large splits) and Matterport3D under noisy conditions (see Sec. S1 in Supp. for noise-free). Higher and steeper curves are better. Top: Our OccAnt approach rapidly attains higher map accuracy than the baselines (dotted lines). Bottom: OccAnt achieves higher map accuracy for the same area seen (we show the best variants here to avoid clutter). These results show the agent actively moves better to explore the environment with occupancy anticipation.Noisy test conditions Noise-free test conditions Gibson small Gibson large Matterport3D Gibson small Gibson large Matterport3D Method Map acc. IoU Map acc. IoU Map acc. IoU Map acc. IoU Map acc. IoU Map acc.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Timed exploration results: Map quality at T =500 for all models and datasets. See text for details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>PointNav results: Our approach provides more efficient navigation.</figDesc><table><row><cell></cell><cell cols="2">Test standard</cell><cell></cell><cell cols="2">Test challenge</cell><cell></cell></row><row><cell cols="2">Rank Team</cell><cell>SPL %</cell><cell cols="2">Success % Team</cell><cell>SPL %</cell><cell>Success %</cell></row><row><cell>1</cell><cell>OccupancyAnticipation</cell><cell>19.2</cell><cell>24.8</cell><cell cols="2">OccupancyAnticipation 20.9</cell><cell>27.5</cell></row><row><cell>2</cell><cell>ego-localization [14]</cell><cell>10.4</cell><cell>13.6</cell><cell>ego-localization [14]</cell><cell>14.6</cell><cell>19.2</cell></row><row><cell>3</cell><cell>Information Bottleneck</cell><cell>5.0</cell><cell>7.5</cell><cell>DAN [30]</cell><cell>13.2</cell><cell>25.3</cell></row><row><cell>4</cell><cell>cogmodel team</cell><cell>0.8</cell><cell>1.3</cell><cell>Information Bottleneck</cell><cell>6.0</cell><cell>8.8</cell></row><row><cell>5</cell><cell>UCULab</cell><cell>0.5</cell><cell>0.8</cell><cell>cogmodel team</cell><cell>0.7</cell><cell>1.2</cell></row><row><cell>6</cell><cell cols="2">Habitat Team (DD-PPO) [72] 0.0</cell><cell>0.2</cell><cell>UCULab</cell><cell>0.1</cell><cell>0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Habitat Challenge 2020 results: Our approach is the winning entry.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S1 :</head><label>S1</label><figDesc>Per-frame occupancy anticipation ablation study</figDesc><table><row><cell></cell><cell cols="2">Noisy test conditions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Gibson small</cell><cell cols="2">Gibson large</cell><cell cols="2">Matterport3D</cell></row><row><cell>Method</cell><cell cols="6">OccAnt AR Map acc. IoU % Map acc. IoU % Map acc. IoU %</cell></row><row><cell>ANS(rgb) [10]</cell><cell>18.46</cell><cell>55</cell><cell>34.95</cell><cell>47</cell><cell>44.70</cell><cell>18</cell></row><row><cell>OccAnt(rgb) w/o AR</cell><cell>21.77</cell><cell>66</cell><cell>44.15</cell><cell>57</cell><cell>65.76</cell><cell>23</cell></row><row><cell>OccAnt(rgb)</cell><cell>20.87</cell><cell>62</cell><cell>42.08</cell><cell>54</cell><cell>66.15</cell><cell>22</cell></row><row><cell>ANS(depth)</cell><cell>18.54</cell><cell>56</cell><cell>39.35</cell><cell>53</cell><cell>72.48</cell><cell>26</cell></row><row><cell>OccAnt(depth) w/o AR</cell><cell>20.22</cell><cell>58</cell><cell>44.18</cell><cell>54</cell><cell>92.70</cell><cell>29</cell></row><row><cell>OccAnt(depth)</cell><cell>22.74</cell><cell>71</cell><cell>50.30</cell><cell>67</cell><cell>94.12</cell><cell>33</cell></row><row><cell>OccAnt(rgbd) w/o AR</cell><cell>16.92</cell><cell>45</cell><cell>35.60</cell><cell>40</cell><cell>76.32</cell><cell>23</cell></row><row><cell>OccAnt(rgbd)</cell><cell>22.70</cell><cell>71</cell><cell>48.42</cell><cell>62</cell><cell>99.92</cell><cell>32</cell></row><row><cell></cell><cell cols="2">Noise-free test conditions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Gibson small</cell><cell cols="2">Gibson large</cell><cell cols="2">Matterport3D</cell></row><row><cell>Method</cell><cell cols="6">OccAnt AR Map acc. IoU % Map acc. IoU % Map acc. IoU %</cell></row><row><cell>ANS(rgb) [10]</cell><cell>22.43</cell><cell>76</cell><cell>43.41</cell><cell>64</cell><cell>53.40</cell><cell>23</cell></row><row><cell>OccAnt(rgb) w/o AR</cell><cell>22.60</cell><cell>71</cell><cell>45.19</cell><cell>60</cell><cell>64.44</cell><cell>24</cell></row><row><cell>OccAnt(rgb)</cell><cell>22.32</cell><cell>70</cell><cell>43.52</cell><cell>58</cell><cell>64.35</cell><cell>22</cell></row><row><cell>ANS(depth)</cell><cell>21.39</cell><cell>74</cell><cell>48.01</cell><cell>72</cell><cell>85.91</cell><cell>34</cell></row><row><cell>OccAnt(depth) w/o AR</cell><cell>24.91</cell><cell>84</cell><cell>54.05</cell><cell>75</cell><cell>104.68</cell><cell>38</cell></row><row><cell>OccAnt(depth)</cell><cell>24.80</cell><cell>83</cell><cell>53.08</cell><cell>74</cell><cell>96.45</cell><cell>35</cell></row><row><cell>OccAnt(rgbd) w/o AR</cell><cell>24.80</cell><cell>84</cell><cell>51.99</cell><cell>71</cell><cell>98.70</cell><cell>34</cell></row><row><cell>OccAnt(rgbd)</cell><cell>24.51</cell><cell>82</cell><cell>50.97</cell><cell>69</cell><cell>100.25</cell><cell>34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S2 :</head><label>S2</label><figDesc>Timed exploration ablation: Map quality at T =500 for all models and datasets.</figDesc><table><row><cell>Method</cell><cell>OccAnt</cell><cell>AR SPL %</cell><cell cols="2">Success Rate % Time taken</cell></row><row><cell>ANS(rgb) [10]</cell><cell></cell><cell>66.8</cell><cell>87.9</cell><cell>254.109</cell></row><row><cell>OccAnt(rgb) w/o AR</cell><cell></cell><cell>71.2</cell><cell>88.2</cell><cell>223.411</cell></row><row><cell>OccAnt(rgb)</cell><cell></cell><cell>66.1</cell><cell>81.3</cell><cell>293.321</cell></row><row><cell>ANS(depth)</cell><cell></cell><cell>76.8</cell><cell>86.6</cell><cell>226.161</cell></row><row><cell>OccAnt(depth) w/o AR</cell><cell></cell><cell>78.6</cell><cell>92.2</cell><cell>187.358</cell></row><row><cell>OccAnt(depth)</cell><cell></cell><cell>77.8</cell><cell>91.3</cell><cell>194.751</cell></row><row><cell>OccAnt(rgbd) w/o AR</cell><cell></cell><cell>77.9</cell><cell>92.9</cell><cell>174.105</cell></row><row><cell>OccAnt(rgbd)</cell><cell></cell><cell>80.0</cell><cell>93.0</cell><cell>171.874</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S3 :</head><label>S3</label><figDesc>PointGoal navigation ablation: Time taken refers to the average number of agent actions required; the maximum time budget is T =1000.</figDesc><table><row><cell cols="4">S5 Generating ground-truth for occupancy anticipation</cell></row><row><cell>Visible occupancy</cell><cell>Region growing iteration # 1</cell><cell>Region growing iteration # 2</cell><cell>Full occupancy</cell></row><row><cell></cell><cell></cell><cell>Ground-truth</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S4 :</head><label>S4</label><figDesc>Policy and mapper hyperparameters used to train our models S9 Occupancy anticipation architecture The architecture diagrams for the individual components of our occupancy anticipation model (Fig. 2 in main paper) are shown in Figs. S8, S9, S10 and S11 with a brief description of the role of each module. We follow the PyTorch [47] conventions to describe individual layers, with the tensor shapes represented in (C, H, W) notations. The descriptions for individual layers are: -ConvBR: a combination of nn.Conv2d, nn.BatchNorm2d and nn.ReLU layers with the arguments representing the input channels, output channels, kernel size, stride and padding. -MaxPool: an instantiation of the nn.MaxPool2d layer with the arguments representing the kernel size, stride and padding. -Conv: a nn.Conv2d layer with the arguments representing the input channels, output channels, kernel size, stride and padding. -AvgPool: an instantiation of the nn.AvgPool2d layer with the arguments representing the kernel size, stride and padding. -2x Upsample: an instantiation of the nn.Upsample layer with a scaling factor of 2.Fig. S8: RGB CNN features: extracts features from RGB images using ResNet18 blocks, and further processes the features to obtain compatible RGB features in a top-down view.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">RGB CNN features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet18 features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ego2TopDown</cell><cell></cell><cell></cell></row><row><cell>RGB</cell><cell>(3, H, W)</cell><cell>ConvBR(3, 64, 6, 2, 3)</cell><cell>MaxPool(3, 2, 1)</cell><cell>ResNet18 -block 1</cell><cell>AvgPool(3, 2, 1) (64, H/4, W/4)</cell><cell>ResNet18 -block 2</cell><cell>(128, H/8, W/8)</cell><cell>Concatenate</cell><cell>(192, H/8, W/8)</cell><cell>ConvBR(192, 192, 3, 1, 1)</cell><cell>ConvBR(192, 192, 5, 1, 2)</cell><cell>2x Upsample</cell><cell>Conv(192, 192, 5, 1, 2)</cell><cell>(192, H/4, W/4) RGB features</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(64, H/8, W/8)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Viewextrap. baseline introduced in Sec. 4.1 in the main paper. The goal is to extrapolate 180 • FoV depth from 90 • FoV RGB-D inputs in order to evaluate the performance of scene completion approaches [68,78]. Since Habitat [38] does Feature encoding: The RGB features and visible occupancy are encoded using independent UNet encoding layers. The expanded view of the "InConv" and "Down" blocks are shown on the right. The encoded RGB and visible occupancy features at different levels are f r 3:5 and f d 1:5 , respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Feature Encoding</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>InConv(in, out)</cell></row><row><cell>RGB features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(192, H/4, W/4)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>InConv(192, 64)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Down(64, 128)</cell><cell></cell><cell>Down(128, 128)</cell><cell></cell><cell></cell><cell>(128, H/16, W/16) 5</cell><cell>ConvBR(in, out, 3, 1, 1)</cell><cell>ConvBR(out, out, 3, 1, 1)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Down(in, out)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3</cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(2, H, W) Visible occupancy</cell><cell>InConv(2, 16)</cell><cell></cell><cell></cell><cell></cell><cell>Down(16, 32)</cell><cell cols="5">(64, H/4, W/4) Down(32, 64)</cell><cell>Down(64, 128)</cell><cell>(128, H/8, W/8)</cell><cell cols="2">Down(128, 128)</cell><cell></cell><cell>(128, H/16, W/16) 5</cell><cell>MaxPool(2, 2, 0)</cell><cell>ConvBR(in, out, 3, 1, 1)</cell><cell>ConvBR(out, out, 3, 1, 1)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(64, H/4, W/4)</cell><cell></cell><cell>(128, H/8, W/8)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">(16, H, W)</cell><cell>(32, H/2, W/2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="18">Fig. S9: Merge i (F) MERGE</cell></row><row><cell>1</cell><cell>2</cell><cell>[</cell><cell>3</cell><cell>,</cell><cell>3</cell><cell>]</cell><cell>[</cell><cell>4</cell><cell>,</cell><cell>4</cell><cell>]</cell><cell></cell><cell>[</cell><cell>5</cell><cell>,</cell><cell>5</cell><cell>]</cell></row><row><cell></cell><cell></cell><cell cols="5">Merge 3 (64)</cell><cell cols="5">Merge 4 (128)</cell><cell></cell><cell cols="5">Merge 5 (128)</cell><cell>ConvBR(2F, F, 3, 1, 1)</cell><cell>ConvBR(F, F, 3, 1, 1)</cell><cell>Conv(F, F, 3, 1, 1)</cell></row><row><cell>1</cell><cell>2</cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table S5 :</head><label>S5</label><figDesc>Comparing model capacity of different approaches</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use our own implementation of ANS since authors' code was unavailable at the time of our experiments. See Sec. S7 in Supp. for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The University of Texas at Austin, Austin TX 78712, USA 2 Facebook AI Research, Austin TX 78701, USA srama@cs.utexas.edu, ziadlhlh@gmail.com, grauman@cs.utexas.edu</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebookresearch/habitat-sim/habitat_sim/agent/controls/pyrobot_noisy_ controls.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = α emt−1 + (1 − α e )EntropyFilter(p t ).(3)4 A* implementation: https://github.com/hjweide/a-star 5p t is the local map at t registered to the global coordinates using the agent's pose estimate.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Weighted A-star implementation: https://github.com/srama2512/astar_pycpp</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>UT Austin is supported in part by DARPA Lifelong Learning Machines and the GCP Research Credits Program. We thank Devendra Singh Chaplot for clarifying the implementation details for ANS.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Occupancy Anticipation for Efficient Exploration and Navigation Supplementary Materials</head><p>Santhosh K. <ref type="bibr">Ramakrishnan 1,</ref><ref type="bibr" target="#b1">2</ref> , Ziad Al-Halah 1 , and Kristen Grauman <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2</ref> This document provides additional information about the experimental settings as well as qualitative and quantitative results to support the experiments from the main paper. Below is a summary of the sections in the supplementary file:</p><p>-( §S1) Noise-free exploration results -( §S2) Occupancy anticipation ablation study -( §S3) Occupancy anticipation qualitative examples -( §S4) Exploration with occupancy anticipation examples -( §S5) Generating ground-truth for occupancy anticipation -( §S6) Noise models for actuation and odometry -( §S7) Differences in ANS implementation -( §S8) Implementation details -( §S9) Occupancy anticipation architecture -( §S11) View extrapolation baseline -( §S12) Comparing the model capacities of different methods -( §S13) Habitat challenge 2020 S1 Noise-free exploration results</p><p>As noted in the main paper, we evaluate on both noisy and noise-free conditions. We showed the change in map accuracy as a function of episode steps and area seen under noisy conditions in <ref type="figure">Fig. 6</ref> in the main paper. We show the same results on noise-free conditions here in <ref type="figure">Fig. S1</ref>. Similar to the noisy case, OccAnt approach (solid lines) rapidly leads to higher map accuracy when compared to the baselines (dotted lines). However, we can see that adding the anticipation reward (AR) in this noise-free setting does not lead to improvements in performance in contrast to what was observed for the more realistic noisy setup ( <ref type="figure">Fig.  5 in main)</ref>.</p><p>As we will qualitatively demonstrate in Sec. S4, the main benefit of using the anticipation reward is that it leads to better noise correction in the pose estimates under noisy test conditions, resulting in more effective map registration. This is due to the fact that achieving high AR (i.e., the map accuracy) inherently depends on better map registration. If the per-frame maps are not registered</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Habitat Challenge</title>
		<ptr target="https://aihabitat.org/challenge/2020/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06757</idno>
		<title level="m">On evaluation of embodied navigation agents</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic structure from motion with points, regions, and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2703" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Largescale study of curiosity-driven learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04355</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1309" to="1332" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the comparison of uncertainty criteria for active slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Castellanos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2080" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV) (2017), matter-Port3D dataset license available at</title>
		<meeting>the International Conference on 3D Vision (3DV) (2017), matter-Port3D dataset license available at</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to explore using active neural mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning exploration policies for navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust reconstruction of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Embodied question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2054" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Integrating egocentric localization for more realistic pointgoal navigation agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2020 Embodied AI Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object-driven multi-layer scene decomposition from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elhafsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Janson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08184</idno>
		<title level="m">Map-predictive motion planning in unknown environments</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scene memory transformer for embodied agents in long-horizon tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="538" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Emergence of exploratory look-around behaviors through active observation completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="DOI">10.1126/scirobotics.aaw6326</idno>
		<ptr target="https://robotics.sciencemag.org/content/4/30/eaaw6326" />
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">30</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An exploration of embodied visual exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02192</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hilbert maps: scalable continuous occupancy mapping with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1717" to="1730" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Slam++: Simultaneous localisation and mapping at the level of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1352" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00653</idno>
		<title level="m">Semi-parametric topological memory for navigation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raichuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02274</idno>
		<title level="m">Episodic curiosity through reachability</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Minos: Multimodal indoor simulator for navigation in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03931</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Mid-level visual representations improve generalization and sample efficiency for learning visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Emi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11971</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Where to look next: Unsupervised active visual exploration on 360 {\deg} input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10304</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep occupancy maps: a continuous mapping technique for dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Senanayake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ganegedara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A fast marching level set method for monotonically advancing fronts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Sethian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1591" to="1595" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Situational fusion of visual representation for visual navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learned map prediction for enhanced mobile robot exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1197" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Self supervised occupancy grid learning from sparse radar for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shlomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00415</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Im2pano3d: Extrapolating 360 structure and semantics beyond the field of view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3847" to="3856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Budge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Briales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gillingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pesqueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05797</idno>
		<title level="m">The Replica dataset: A digital replica of indoor spaces</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Horizonnet: Learning room layout with 1d representation and pano stretch data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Probabilistic robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="57" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Data-driven interior plan generation for residential buildings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3355089.3356556</idno>
		<ptr target="https://doi.org/10.1145/3355089.3356556" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gibson env: Realworld perception for embodied agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/gibson_material/Agreement%20GDS%2006-04-18.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9068" to="9079" />
		</imprint>
	</monogr>
	<note>gibson dataset license agreement available at</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Embodied amodal recognition: Learning to move to perceive objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dula-net: A dual-projection network for estimating room layouts from a single rgb panorama</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3363" to="3372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06543</idno>
		<title level="m">Visual semantic navigation using scene priors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extreme relative pose estimation for rgb-d scans via scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visual Semantic Planning using Deep Successor Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Layoutnet: Reconstructing the 3d room layout from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2051" to="2059" />
		</imprint>
	</monogr>
	<note>RGB ConvBLr(3, 8, 3, 1, 1) ConvBLr(8, 16, 4, 2, 1) ConvBLr(16, 32, 4, 2, 1) Depth ConvBLr(1, 8, 3, 1, 1) ConvBLr(8, 16, 4, 2, 1) ConvBLr(16. 3, H, W) (1, H, W) Concatenate (64, H/4, W/4</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Down(16, 32) Down(32, 64) Down(64, 128) Down(128, 128) Up(64, 64) Up(128, 32) Up(64, 16) Up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Inconv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
	<note>Conv(16, 64, 1, 1, 1)</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">UNet Fig. S14: View extrapolation architecture [78]: The 90 • FoV RGB and depth inputs are independently encoded using Convolutional layers, concatenated and processed using a UNet model. The decoded features from UNet are used to extrapolate the final depth predictions</title>
		<imprint>
			<pubPlace>1, H, 3W</pubPlace>
		</imprint>
	</monogr>
	<note>Conv(1, 1, 1, 1, 0) Extrapolated Depth. DeconvBLr&quot; uses nn.ConvTranspose2D to perform the upsampling. Note that &quot;ConvBLr&quot; and &quot;DeconvBLr&quot; use nn.LeakyReLU(0.1) instead of &quot;nn.ReLU(</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
