<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Derivative Manipulation for General Example Weighting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford Queen&apos;s University</orgName>
								<address>
									<settlement>Belfast</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Anyvision Research Team</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Queen&apos;s University Belfast</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Anyvision Research Team Queen&apos;s University Belfast</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Derivative Manipulation for General Example Weighting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world large-scale datasets usually contain noisy labels and are imbalanced. Therefore, we propose derivative manipulation (DM), a novel and general example weighting approach for training robust deep models under these adverse conditions. DM has two main merits. First, loss function and example weighting are two common techniques in robust learning. In gradient-based optimisation, the role of a loss function is to provide the gradient for back-propagation to update a model, so that the derivative magnitude of an example defines how much impact it has, namely its weight. By DM, we connect the design of loss function and example weighting together. Second, although designing a loss function sometimes has the same effect, we need to care whether a loss is differentiable, and derive its derivative to understand its example weighting scheme. They make the design complicated. Instead, DM is more flexible and straightforward by directly modifying the derivative. Concretely, DM modifies a derivative magnitude function, including transformation and normalisation, after which we term it an emphasis density function, which expresses a weighting scheme. Accordingly, diverse weighting schemes are derived from common probability density functions, including those of well-known robust losses, e.g., MAE and GCE. We conduct extensive experiments demonstrating the effectiveness of DM on both vision and language tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In large-scale machine learning tasks, addressing label noise and sample imbalance is fundamental and has been widely studied <ref type="bibr" target="#b7">[1]</ref><ref type="bibr" target="#b8">[2]</ref><ref type="bibr">[3]</ref>. There are two popular approaches, i.e., robust loss design <ref type="bibr">[4,</ref><ref type="bibr" target="#b11">5,</ref><ref type="bibr">3]</ref> and example weighting design <ref type="bibr" target="#b8">[2,</ref><ref type="bibr" target="#b12">[6]</ref><ref type="bibr" target="#b13">[7]</ref><ref type="bibr" target="#b14">[8]</ref>, due to their easy implementation and widely demonstrated effectiveness. To improve them further, we reveal their connection and unify the design of them via derivative manipulation (DM). On the one hand, the derivative magnitude of an example decides how much impact it has on updating a model <ref type="bibr" target="#b15">[9,</ref><ref type="bibr" target="#b16">10]</ref>. As a result, a derivative magnitude function defines a weighting scheme over training examples. On the other hand, in gradient-based optimisation, the role of a loss function is to provide gradient used for back-propagation. DM designs the derivative directly so that we do not need to derive it from a loss function. * For source codes, we are happy to offer when there is a request conditioned on pure academic use and kindness to cite this work. For any concrete discussion or future collaboration, please feel free to contact. â™¥ Work done at Queen's University Belfast and Anyvision.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Why do we design a derivative magnitude function instead of a loss function?</head><p>We present two incompatible perspectives on the robustness of a loss function, which motivates us to design the derivative other than a loss function: (1) Loss value. From this viewpoint, a loss function, which is less sensitive to large errors (i.e. residuals), is more robust and preferred <ref type="bibr" target="#b17">[11,</ref><ref type="bibr" target="#b18">12]</ref>. For example, absolute error is considered more robust than squared error. An outlier has a larger error by definition, but its loss value should not increase dramatically when a robust loss function is applied.</p><p>(2) Derivative magnitude. A more robust model is less affected by noisy data than clean data. Therefore, a noisy example should have a smaller derivative magnitude. Whether a larger loss value corresponds to a larger derivative depends on a specific loss function. When an example has a large loss value, its derivative can be so small that its effect is negligible. As a consequence, two viewpoints are obviously inconsistent. We remark the first one is misleading. We support the second because the role of a loss is offering the gradient to back-propagate as shown in <ref type="figure" target="#fig_1">Figure 1a</ref>. Accordingly, we propose DM which manipulates the derivative directly so that we do not need to derive it from a loss.</p><p>Beyond, the derivative magnitude defines an example weighting scheme. Although it is feasible to design a loss function which offers the desired derivative, we need to consider whether it is differentiable and derive its derivative to get its underlying weighting scheme. This "two-step" procedure makes the design of a loss more complicated than DM.</p><p>1.2 Using an emphasis density function to mathematically express example weighting DM nonlinearly transforms the derivative magnitude, followed by derivative normalisation (DN) so that the total emphasis (weight) is one unit. We term a normalised derivative magnitude function an Emphasis Density Function (EDF), which explicitly defines an example-level weighting scheme over data points. An EDF considers emphasis mode and variance, being analogous to a probability density function (PDF), thus we can design an EDF according to existing PDFs. Emphasis mode represents examples whose weight values are the largest. Depending on a scenario, it can be adjusted to focus on easy, semi-hard, or hard examples. Therefore, DM is a superset of existing (heuristicallydesigned) example weighting methods <ref type="bibr" target="#b19">[13,</ref><ref type="bibr" target="#b20">14,</ref><ref type="bibr" target="#b12">6,</ref><ref type="bibr" target="#b13">7,</ref><ref type="bibr" target="#b21">15]</ref>. Emphasis variance decides the spread of emphasis (weight), i.e., the variance of an EDF curve. Intuitively, examples of greater "interest" should contribute more to a model's update. Emphasis variance controls how significantly they are emphasised. Some representative EDFs are shown in <ref type="figure" target="#fig_1">Figure 1b</ref>.</p><p>We summarise our contributions: (1) We propose a novel approach that unifies the design of example weighting and loss function in a single framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The effects of example weighting and loss function are generally overlapped. To understand why DM works and is superior, we revisit two key concepts: Rethinking existing robustness theorems on loss functions. In the prior work, when judging the robustness of a loss function, its example weighting scheme is not considered. Instead, its robustness is judged according to its sensitivity to large errors <ref type="bibr" target="#b17">[11,</ref><ref type="bibr" target="#b18">12,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b22">16]</ref>. For example, <ref type="bibr">[4]</ref> proposed theorems showing that a deep model is robust to label noise when the loss function is symmetric and bounded. Accordingly, they claimed that Mean Absolute Error (MAE), Mean Square Error (MSE), Categorical Cross Entropy (CCE) are decreasingly robust, which is not the fact. Because in this work we find CCE is very competitive with MAE, MSE and Generalised Cross Entropy (GCE) <ref type="bibr" target="#b11">[5]</ref>. However, CCE is neither bounded nor symmetric.</p><p>Rethinking proposed example weighting schemes. Many weighting schemes have been proposed for different purposes: <ref type="bibr" target="#b7">(1)</ref> Easier examples are preferred (Examples with lower error/loss are inferred to be easier <ref type="bibr" target="#b8">[2]</ref>). For example, curriculum learning <ref type="bibr" target="#b23">[17]</ref> picks easier examples in early training. Self-paced learning <ref type="bibr" target="#b24">[18]</ref> increases the weights of more difficult examples gradually. <ref type="formula" target="#formula_5">(2)</ref> Harder examples are emphasised: Hard example mining is demonstrated to accelerate convergence and improve performance in some cases <ref type="bibr" target="#b25">[19]</ref><ref type="bibr" target="#b26">[20]</ref><ref type="bibr" target="#b27">[21]</ref>. However, note that the derivative magnitude of a loss function also defines a weighting scheme. Then it becomes the interaction between a proposed weighting scheme and the one from a loss function works in previous work. Instead, in DM, there is only one weighting scheme. We compare it with other weighting methods in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Derivative Manipulation</head><p>Let a training set be N training examples</p><formula xml:id="formula_0">X = {(x i , y i )} N i=1 , where (x i , y i ) denotes ith sample with input x i âˆˆ R D and label y i âˆˆ {1, 2, ..., C}.</formula><p>C is the number of classes. Consider a deep neural network z composed of an embedding network f (Â·) : R D â†’ R K and a linear classifier</p><formula xml:id="formula_1">g(Â·) : R K â†’ R C , i.e., z i = z(x i ) = g(f (x i )) : R D â†’ R C .</formula><p>Generally, a linear classifier is the last fully-connected layer which outputs logits z âˆˆ R C . To predict the probabilities of x i belonging to different classes, z is normalised by a softmax function:</p><formula xml:id="formula_2">p(j|x i ) = exp(z ij )/ C m=1 exp(z im ),</formula><p>where p(j|x i ) is the probability of x i belonging to class j. For brevity, we define p i = p(y i |x i ).</p><p>For a loss function with p i as an input, no matter how it's computed, the loss should be monotonically nonincreasing with p i (After a softmax layer, maximising p(y i |x i ) towards one will automatically reduce p(j|x i ), j = y i towards 0). We analyse several well-known losses and find that their derivatives share the same direction, which indicates they share the same optimisation objective although their loss expressions are different. They perform differently only due to the derivate magnitude, which theoretically (mathematically) demonstrates that it is the key.</p><p>We propose DM to systematically study the derivative magnitude in which the derivative's direction is kept the same as those losses. We choose L 1 norm to measure the magnitude of derivative because its expression is simpler than other norms. For clarity, we formally define the emphasis mode and variance. Definition 1 (Emphasis Mode Ïˆ). It refers to those examples that own the largest weight. In DM, an example's weight is only decided by its p i . For brevity, we define Ïˆ to be p i of examples whose weights are the largest, i.e., Ïˆ = arg max pi w i , Ïˆ âˆˆ [0, 1]. Definition 2 (Emphasis Variance Ïƒ). It is the weight variance over all data points in a batch and is defined by the variance of an EDF curve, which affects the impact ratio between two examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The derivative direction and magnitude of common losses</head><p>We present the derivatives of four popular loss functions: CCE, MAE, MSE and GCE. 2 CCE. For a given (x i , y i ), CCE and its derivative with respect to z i are:</p><formula xml:id="formula_3">L CCE (x i , y i ) = âˆ’ log p(y i |x i ) â‡’ âˆ‚L CCE âˆ‚z ij = p(y i |x i ) âˆ’ 1, j = y i p(j|x i ), j = y i .<label>(1)</label></formula><p>With</p><formula xml:id="formula_4">L 1 norm, we have || âˆ‚LCCE âˆ‚zi || 1 = 2(1 âˆ’ p(y i |x i )) = 2(1 âˆ’ p i ). The weight of x i is w CCE i = || âˆ‚LCCE âˆ‚zi || 1 = 2(1 âˆ’ p i )</formula><p>, meaning examples with smaller p i get higher weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAE.</head><p>Similarly as above:</p><formula xml:id="formula_5">L MAE (x i , y i ) = 1 âˆ’ p(y i |x i ) â‡’ âˆ‚L MAE âˆ‚z ij = p(y i |x i )(p(y i |x i ) âˆ’ 1), j = y i p(y i |x i )p(j|x i ), j = y i .<label>(2)</label></formula><p>The weight of</p><formula xml:id="formula_6">x i is w MAE i = || âˆ‚LMAE âˆ‚zi || 1 = 2p(y i |x i )(1 âˆ’ p(y i |x i )) = 2p i (1 âˆ’ p i ). MSE.</formula><p>Similarly as above:</p><formula xml:id="formula_7">L MSE (x i , y i ) = (1 âˆ’ p(y i |x i )) 2 â‡’ âˆ‚L MSE âˆ‚z ij = âˆ’2p(y i |x i )(p(y i |x i ) âˆ’ 1) 2 , j = y i âˆ’2p(y i |x i )(p(y i |x i ) âˆ’ 1)p(j|x i ), j = y i .<label>(3)</label></formula><p>The weight of</p><formula xml:id="formula_8">x i is w MSE i = || âˆ‚LMSE âˆ‚zi || 1 = 4p(y i |x i )(1 âˆ’ p(y i |x i )) 2 = 4p i (1 âˆ’ p i ) 2 . GCE.</formula><p>Similarly as above:</p><formula xml:id="formula_9">L GCE (x i , y i ) = 1 âˆ’ p(y i |x i ) q q â‡’ âˆ‚L GCE âˆ‚z ij = p(y i |x i ) q (p(y i |x i ) âˆ’ 1), j = y i p(y i |x i ) q p(j|x i ), j = y i ,<label>(4)</label></formula><p>where q âˆˆ [0, 1] is a hyperparameter. The weight of </p><formula xml:id="formula_10">x i is w GCE i = || âˆ‚LGCE âˆ‚zi || 1 = 2p(y i |x i ) q (1 âˆ’ p(y i |x i )) = 2p q i (1 âˆ’ p i</formula><formula xml:id="formula_11">âˆ‚L MAE âˆ‚z i = p i Ã— âˆ‚L CCE âˆ‚z i ; âˆ‚L MSE âˆ‚z i = 2p i Ã— (1 âˆ’ p i ) Ã— âˆ‚L CCE âˆ‚z i ; âˆ‚L GCE âˆ‚z i = p q i Ã— âˆ‚L CCE âˆ‚z i . (5)</formula><p>Derivative magnitude. We summarise the weighting schemes of all losses as follows:</p><formula xml:id="formula_12">w CCE i = 2(1 âˆ’ p i ) â‡’ Ïˆ CCE = 0; w MAE i = 2p i (1 âˆ’ p i ) â‡’ Ïˆ MAE = 0.5; w MSE i = 4p i (1 âˆ’ p i ) 2 â‡’ Ïˆ MSE = 1 3 ; w GCE i = 2p q i (1 âˆ’ p i ) â‡’ Ïˆ GCE = q q + 1 .<label>(6)</label></formula><p>3.2 Defining an example weighting scheme by an emphasis density function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Example weighting via derivative manipulation</head><p>Common losses perform differently only due to the derivate magnitude as shown in the previous section. Therefore, we manipulate the derivative magnitude directly. Concretely, given a weighting function w DM i , we scale CCE's derivative by w DM i /(2(1 âˆ’ p i )):</p><formula xml:id="formula_13">z i = w DM i /(2(1 âˆ’ p i )) Ã— âˆ‚L CCE âˆ‚z i .<label>(7)</label></formula><p>Then the gradient magnitude of z i is:</p><formula xml:id="formula_14">|| z i || 1 = ||w DM i /(2(1 âˆ’ p i )) Ã— âˆ‚LCCE âˆ‚zi || 1 = w DM i .</formula><p>Treating p i as a continuous variable, w DM i can be interpreted as an emphasis density function (EDF). Correspondingly, the integral (area under the curve of w DM i ) between a range, e.g., [Ïˆ DM âˆ’ âˆ†, Ïˆ DM + âˆ†], denotes the accumulative weight of examples whose p i is in this range. 2âˆ† is the length of this range, denoting the examples of interest. As p i âˆˆ [0, 1], we normalise an EDF by its integral over [0, 1], termed a derivative normalisation (DN):</p><formula xml:id="formula_15">h(w DM i ) = w DM i 1 0 w DM i d pi â‡’ 1 0 h(w DM i )d pi = 1.<label>(8)</label></formula><p>We name w DM i and h weighting function and EDF, respectively. In Eq. (8), the DN operator is trivial. Therefore, for brevity, we discuss the variants of w DM i instead of the normalised h. Next, we discuss how we express an example weighting function w DM i mathematically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Choosing an example weighting scheme</head><p>We derive w DM i according to the PDFs of probability distributions of the exponential family. Normal distribution variant. Ïˆ â‰¥ 0 denotes the emphasis mode while Î² adjusts the variance:</p><formula xml:id="formula_16">v ND (w DM i ; Ïˆ, Î²) = exp(âˆ’Î²p i (p i âˆ’ 2Ïˆ)).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exponential distribution variant. Harder examples have larger (smaller) weights if</head><formula xml:id="formula_17">Î² &gt; 0 (Î² &lt; 0): v ED (w DM i ; Î²) = exp(Î²(1 âˆ’ p i )).<label>(10)</label></formula><p>Beta distribution variant. It covers all weighting schemes of the common losses shown in Eq. <ref type="bibr" target="#b12">(6)</ref>. We remark the difference of coefficients can be ignored since they are gone after</p><formula xml:id="formula_18">DN. Î±, Î· â‰¥ 0. v BD (w DM i ; Î±, Î·) = p Î±âˆ’1 i (1 âˆ’ p i ) Î·âˆ’1 = ï£± ï£´ ï£´ ï£² ï£´ ï£´ ï£³ w CCE i /2 = (1 âˆ’ p i ), Î± = 1, Î· = 2 w MAE i /2 = p i (1 âˆ’ p i ), Î± = 2, Î· = 2 w MSE i /4 = p i (1 âˆ’ p i ) 2 , Î± = 2, Î· = 3 w GCE i /2 = p q i (1 âˆ’ p i ), Î± = q + 1, Î· = 2<label>(11)</label></formula><p>Both emphasis mode and variance matter. However, adjusting the variance is inconvenient in</p><formula xml:id="formula_19">v BD (w DM i ; Î±, Î·).</formula><p>Although v ND (w i ; Ïˆ, Î²) controls both of them, its mathematical generality to other weighting schemes is not good. Therefore, we design the other variant of w DM i as follows:</p><formula xml:id="formula_20">w DM i = exp(Î²p Î» i (1 âˆ’ p i )), Î» â‰¥ 0 â‡’ Ïˆ DM = Î» Î» + 1 âˆˆ [0, 1),<label>(12)</label></formula><p>where Î» and Î² are the parameters to control the emphasis mode and variance, respectively. Design reasons. By varying Î» and Î² in Eq <ref type="formula" target="#formula_3">(12)</ref>, we can show that <ref type="formula" target="#formula_3">(1)</ref>  <ref type="formula" target="#formula_3">(12)</ref>  Finally, its loss expression is not an elementary function and is represented as:</p><formula xml:id="formula_21">if Î» = 0, w DM i = exp(Î²(1 âˆ’ p i )), it becomes the same as an exponential distribution variant v ED (w DM i ; Î²); (2) If Î» = 1, w DM i is a normal distribution variant; (3) Eq</formula><formula xml:id="formula_22">1 pi w DM i 2pi(1âˆ’py) dp i ,</formula><p>which is unbounded and non-symmetric in multi-class cases. Despite this, the overall optimisation objective is unchanged and consistent with common losses as discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To demonstrate DM's value as a useful example weighting framework, we conduct extensive experiments, including robust image classification on synthetic and real-world large-scale datasets, robust video retrieval on a large-scale dataset and robust sentiment analysis of movie reviews. Beyond label noise, all real-world datasets are highly imbalanced, e.g., the number of videos per person ranges from 1 to 271 in MARS <ref type="bibr" target="#b28">[22]</ref>, while the number of images per class varies between 18,976 and 88,588 in Clothing 1M <ref type="bibr" target="#b29">[23]</ref>. In our experiments, we fix the random seed as 123 and do not apply any random computational accelerator for the purpose of exact reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A premise for setting the emphasis mode when noisy labels exist</head><p>DM has two key components: (1) We optimise the emphasis mode according to an intuition: e.g., when there exists more noise, we use a relatively larger emphasis mode to emphasise on easier examples <ref type="bibr" target="#b30">[24,</ref><ref type="bibr" target="#b12">6]</ref>; (2) After an emphasis mode is set, we can search for the best emphasis variance if a validation set is given. Concretely, when training data is clean, we set Î» = 0 so that w DM</p><formula xml:id="formula_23">i = exp(Î²(1 âˆ’ p i )) â‡’ Ïˆ DM = 0.</formula><p>When label noise exists, we increase Î» so that Ïˆ DM increases and easier samples become the emphasis mode. For more justification, we present the following premise:</p><formula xml:id="formula_24">Premise 1.</formula><p>Difficult examples have smaller probabilities of being predicted to its annotated labels, i.e., smaller p i . Abnormal examples, including noisy ones and outliers, belong to those difficult ones. This premise is justified in recent work: <ref type="bibr" target="#b30">[24]</ref> shows that DNNs do not fit real datasets by brute-force memorisation. Instead, DNNs learn simple shared patterns before memorising difficult abnormal data points. Consequently, abnormal samples have smaller p i than easier ones.</p><p>In addition, we present our empirical evidence in <ref type="figure" target="#fig_2">Figure 2</ref>. The learning dynamics prove this premise and DM is superior to CCE: (a) The p i of clean examples increases while that of noisy ones has no noticeable rise in DM, which means DM hinders fitting of abnormal examples and preserves DNNs' ability to learn on clean data; (b) DM has the best test accuracy. Furthermore, it is robust, i.e., early stopping is unnecessary. (c) As we increase noise rate, the optimised emphasis mode also increases  showing a positive correlation between them. A thorough ablation study on the emphasis mode and variance are reported in the supplementary material. Remark 1. In the early training phase, p i is awfully random and non-informative, what does DM do? At this phase, DM weights examples randomly since p i is random. It is the same in all common losses (weighting schemes). Although nothing makes sense at the beginning, including random initialisation, loss values and weighting schemes, DNNs gradually learn meaningful patterns <ref type="bibr" target="#b30">[24]</ref>. Therefore, DM boosts learning gradually as shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Robust image classification</head><p>Datasets. (1) CIFAR-10/100 <ref type="bibr" target="#b31">[25]</ref>, which contain 10 and 100 classes, respectively. The image size is 32 Ã— 32. In CIFAR-10, the training data contains 5k images per class while the test set includes 1k images per class. CIFAR-100 has 500 images per class for training and 100 images per class for testing. We generate synthetic label noise on them: (a) Symmetric noise. With a probability of r, the original label of an image is changed to one of the other class labels uniformly following <ref type="bibr" target="#b32">[26,</ref><ref type="bibr" target="#b33">27]</ref>. r denotes the noise rate. We remark that some work randomly flips an image's label to one of all labels including the ground-truth <ref type="bibr">[28,</ref><ref type="bibr" target="#b35">29]</ref>. The actual noise rate becomes quite different when the number of classes is small, e.g., CIFAR-10. We do not compare with those results. (b) Asymmetric noise. We generate asymmetric noise for CIFAR-100 following <ref type="bibr" target="#b33">[27]</ref>. CIFAR-100 has 20 superclasses and every superclass has 5 subclasses. In each superclass, two subclasses are randomly selected and their labels are flipped to each other with a probability of r. The overall noise rate is less than r. (2) Clothing 1M <ref type="bibr" target="#b29">[23]</ref>, which is an industrial-level dataset, consisting of around 1 million images of 14 classes from shopping websites. The noise rate is about 38.46% and noise distribution is agnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Results on CIFAR-100</head><p>Training details. We follow the settings of <ref type="bibr" target="#b33">[27]</ref> for a fair comparison. We use SGD with a momentum of 0.9 and a weight decay of 1e âˆ’ 4. We train 30k iterations. The learning rate starts at 0.1, and is divided by 10 at 15k and 22k iterations. Standard data augmentation is used: padding images with 4 pixels on every side, followed by a random crop of 32 Ã— 32 and horizontal flip. The batch size is 256. Competitors. We briefly introduce the compared baselines: (1) Analysed losses (CCE, GCE, MAE and MSE) and their variants after DN (CCE-DN, GCE-DN, MAE-DN and MSE-DN); (2) Bootstrapping trains a model with new labels generated by a convex combination of the original ones and their predictions. A convex combination can be either soft (Boot-soft) or hard (Boot-hard) <ref type="bibr" target="#b7">[1]</ref>; (3) Forward (Backward) uses a noise-transition matrix to multiply the network's predictions (losses) for label correction <ref type="bibr" target="#b37">[31]</ref>; (4) D2L addresses noise-robustness by restricting the dimensionality expansion of learned subspaces during training; (5) SL modifies CCE symmetrically with a reverse cross entropy; (6) LS denotes label smoothing <ref type="bibr" target="#b38">[32]</ref>. Note that we do not compare with <ref type="bibr" target="#b39">[33]</ref>. First, its backbone is not ResNet-44 after checking with the authors. Second, their algorithm is orthogonal to ours because it targets at the inference stage and is a generative classifier on top of deep representations. <ref type="table" target="#tab_1">Table 1</ref>, our observations are: (1) When training data is clean, CCE (CCN-DN) is the best against other common losses. Besides, DM(Î» = 0) is the best compared with other variants. We conclude by Ïˆ = 0, harder examples have higher weights, leading to better performance. (2) When   <ref type="table">Table 3</ref>: DM versus other losses on sentiment classification of movie reviews. The results of common losses after DN are in the brackets. We test on two adverse cases: label noise and sample imbalance. P-N Ratio denotes the ratio of positive reviews to negative ones.    report the cumulated matching characteristics (CMC) and mean average precision (mAP) results <ref type="bibr" target="#b28">[22]</ref>. Implementation details. Following <ref type="bibr" target="#b47">[41,</ref><ref type="bibr" target="#b48">42]</ref>, we train GoogleNet V2 <ref type="bibr" target="#b49">[43]</ref> and process a video as an image set, which means we use only appearance information without exploiting latent temporal information. A video's representation is simply the average fusion of its frames' representations. The learning rate starts from 0.01 and is divided by 2 every 10k iterations. We stop training at 50k iterations. We apply an SGD optimiser with a weight decay of 5e âˆ’ 4 and a momentum of 0.9. The batch size is 180. Data augmentation is the same as Clothing 1M. At testing, we first L 2 normalise videos' features and then calculate the cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. From</head><p>Results. The results are displayed in <ref type="table" target="#tab_5">Table 6</ref>. Although DRSA <ref type="bibr" target="#b50">[44]</ref> and CAE <ref type="bibr" target="#b51">[45]</ref> exploit extra temporal information by incorporating attention mechanisms, DM is superior to them in terms of both effectiveness and simplicity. OSM+CAA <ref type="bibr" target="#b48">[42]</ref> is the only competitive method. However, OSM+CAA combines CCE and weighted contrastive loss to address anomalies, thus being more complex. We highlight that one query may have multiple matching instances in the MARS benchmark so that mAP is a more reliable and accurate performance assessment. DM is the best in terms of mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sentiment analysis of movie reviews</head><p>We report results on the IMDB dataset of movie reviews <ref type="bibr" target="#b52">[46,</ref><ref type="bibr" target="#b53">47]</ref>. We use Paragraph Vector, PV-DBOW, as a document descriptor <ref type="bibr" target="#b54">[48]</ref>. We train a neural network with one 8-neuron hidden layer and display the results in <ref type="table">Table 3</ref>. Due to space, other details are reported in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Further analysis</head><p>We also experimented with Adam optimiser as shown in <ref type="table" target="#tab_3">Table 4</ref>. To explore different networks simultaneously, we train ResNet-56 <ref type="bibr" target="#b36">[30]</ref> instead of GoogLeNet V1 on CIFAR-10 with 40% symmetric label noise. We observe that DM's results are consistently the best. Additionally, experiments about comparison with standard regularisers, performance on a small-scale dataset and detailed training analysis under label noise are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose derivative manipulation for example weighting. DM directly works on gradients bypassing a loss function. As a consequence, it creates great flexibility in designing various example weighting schemes. Extensive experiments on both vision and language tasks empirically show that DM outperforms existing methods despite its simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Information</head><p>For your better reference, DM is a following work of <ref type="bibr">[3]</ref>:  . Out-of-distribution anomalies: 1) The first image in the 3rd row contains only background and no semantic information at all.</p><formula xml:id="formula_25">IMAE</formula><p>2) The 2nd first image or the last one in the 3rd row may contain a person that does not belong to any person in the training set.</p><p>In-distribution anomalies: 1) Some images of deer class are wrongly annotated to horse class. 2) We cannot decide the object of interest without any prior when an image contains more than one object, e.g., some images contain two persons in the 2nd row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Derivation Details of Softmax, CCE, MAE and GCE B.1 Derivation of Softmax Normalisation</head><p>We rewrite p(y i |x i ) as follows:</p><formula xml:id="formula_26">p(y i |x i ) âˆ’1 = 1 + j =yi exp(z ij âˆ’ z iyi ).<label>(13)</label></formula><p>For left and right sides of Eq. (13), we calculate their derivatives w.r.t. z ij simultaneously.</p><formula xml:id="formula_27">If j = y i , âˆ’1 p(y i |x i ) 2 âˆ‚p(y i |x i ) z iyi = âˆ’ j =yi exp(z ij âˆ’ z iyi ) =&gt; âˆ‚p(y i |x i ) z iyi = p(y i |x i )(1 âˆ’ p(y i |x i )).<label>(14)</label></formula><p>If j = y i ,</p><formula xml:id="formula_28">âˆ’1 p(y i |x i ) 2 âˆ‚p(y i |x i ) z ij = exp(z ij âˆ’ z iyi ) =&gt; âˆ‚p(y i |x i ) z ij = âˆ’p(y i |x i )p(j|x i ).<label>(15)</label></formula><p>In summary, the derivation of softmax layer is:</p><formula xml:id="formula_29">âˆ‚p(y i |x i ) âˆ‚z ij = p(y i |x i )(1 âˆ’ p(y i |x i )), j = y i âˆ’p(y i |x i )p(j|x i ), j = y i<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Derivation of CCE</head><p>According to Eq. (1), we have L CCE (x i ; f Î¸ , W) = âˆ’ log p(y i |x i ). (17) Therefore, we obtain (the parameters are omitted for brevity),</p><formula xml:id="formula_30">âˆ‚L CCE âˆ‚p(j|x i ) = âˆ’p(y i |x i ) âˆ’1 , j = y i 0, j = y i .<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Derivation of MAE</head><p>According to Eq. (2), we have</p><formula xml:id="formula_31">L MAE (x i ; f Î¸ , W) = 1 âˆ’ (p(y i |x i ). (19) Therefore, we obtain âˆ‚L MAE âˆ‚p(j|x i ) = âˆ’1, j = y i 0, j = y i .<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Derivation of GCE</head><p>According to Eq. (4), we have</p><formula xml:id="formula_32">L GCE (x i ; f Î¸ , W) = 1 âˆ’ p(y i |x i ) q q .<label>(21)</label></formula><p>Therefore, we obtain If j = y i , we have:</p><formula xml:id="formula_33">âˆ‚L GCE âˆ‚p(j|x i ) = âˆ’p(y i |x i ) qâˆ’1 , j = y i 0, j = y i .<label>(22)</label></formula><formula xml:id="formula_34">âˆ‚L CCE âˆ‚z iyi = C j=1 âˆ‚L CCE âˆ‚p(j|x i ) âˆ‚p(y i |x i ) z ij = p(y i |x i ) âˆ’ 1.<label>(23)</label></formula><p>If j = y i , it becomes:</p><formula xml:id="formula_35">âˆ‚L CCE âˆ‚z ij = C j=1 âˆ‚L CCE âˆ‚p(j|x i ) âˆ‚p(y i |x i ) z ij = p(j|x i ).<label>(24)</label></formula><p>In summary, âˆ‚L CCE /âˆ‚z i can be represented as: </p><formula xml:id="formula_36">âˆ‚L CCE âˆ‚z ij = p(y i |x i ) âˆ’ 1, j = y i p(j|x i ), j = y i .<label>(25)</label></formula><formula xml:id="formula_37">âˆ‚L MAE âˆ‚z iyi = C j=1 âˆ‚L MAE âˆ‚p(j|x i ) âˆ‚p(y i |x i ) z ij = âˆ’p(y i |x i )(1 âˆ’ p(y i |x i )).<label>(26)</label></formula><p>otherwise (j = y i ):</p><formula xml:id="formula_38">âˆ‚L MAE âˆ‚z ij = C j=1 âˆ‚L MAE âˆ‚p(j|x i ) âˆ‚p(y i |x i ) z ij = p(y i |x i )p(j|x i ).<label>(27)</label></formula><p>In summary, âˆ‚L MAE /âˆ‚z i is: If j = y i , we have:</p><formula xml:id="formula_39">âˆ‚L MAE âˆ‚z ij = p(y i |x i )(p(y i |x i ) âˆ’ 1), j = y i p(y i |x i )p(j|x i ), j = y i .<label>(28)</label></formula><formula xml:id="formula_40">âˆ‚L GCE âˆ‚z iyi = C j=1 âˆ‚L GCE âˆ‚p(j|x i ) âˆ‚p(y i |x i ) z ij = p(y i |x i ) q (p(y i |x i ) âˆ’ 1).<label>(29)</label></formula><p>If j = y i , it becomes:</p><formula xml:id="formula_41">âˆ‚L GCE âˆ‚z ij = C j=1 âˆ‚L GCE âˆ‚p(j|x i ) âˆ‚p(y i |x i ) z ij = p(y i |x i ) q p(j|x i ).<label>(30)</label></formula><p>In summary, âˆ‚L GCE /âˆ‚z i can be represented as:</p><formula xml:id="formula_42">âˆ‚L GCE âˆ‚z ij = p(y i |x i ) q (p(y i |x i ) âˆ’ 1), j = y i p(y i |x i ) q p(j|x i ), j = y i .<label>(31)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Beating Standard Regularisers Under Label Noise</head><p>In <ref type="table" target="#tab_8">Table 7</ref>, we compare our proposed DM with other standard ones, i.e., L2 weight decay and Dropout <ref type="bibr" target="#b56">[50]</ref>. We set the dropout rate to 0.2 and L2 weight decay rate to 10 âˆ’4 . For DM, we fix Î² = 8, Î» = 0.5. DM is better than those standard regularisers and their combinations significantly. DM works best when it is together with L2 weight decay. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Small-scale Fine-grained Visual Categorisation of Vehicles</head><p>How does DM perform on small datasets, for example, the number of data points is no more than 5,000? We have tested DM on CIFAR-10 and CIFAR-100 in the main paper. However, both of them contain a training set of 50,000 images.</p><p>For this question, we answer it from different perspectives as follows:</p><p>1. The problem of label noise on CIFAR-10 and CIFAR-100 in Section 4.2 is of similar scale.</p><p>â€¢ In <ref type="table" target="#tab_2">Table 2</ref>, when noise rate is 80% on CIFAR-10, the number of clean training examples is around 50, 000 Ã— 20% = 5, 000 Ã— 2. Therefore, this clean set is only two times as large as 5,000. Beyond, the learning process may be interrupted by other noisy data points.</p><p>â€¢ In <ref type="table" target="#tab_1">Table 1</ref>, when noise rate is 60% on CIFAR-100, the number of clean training data points is about 50, 000 Ã— 40% = 5, 000 Ã— 4, i.e., four times as large as 5,000.</p><p>2. We compare DM with other standard regularisers on a small-scale fine-grained visual categorisation problem in <ref type="table" target="#tab_9">Table 8</ref>.</p><p>Vehicles-10 Dataset. In CIFAR-100 <ref type="bibr" target="#b31">[25]</ref>, there are 20 coarse classes, including vehicles 1 and 2. Vehicles 1 contains 5 fine classes: bicycle, bus, motorcycle, pickup truck, and train. Vehicles 2 includes another 5 fine classes: lawn-mower, rocket, streetcar, tank, and tractor. We build a smallscale vehicle classification dataset composed of these 10 vehicles from CIFAR-100. Specifically, the training set contains 500 images per vehicle class while the testing set has 100 images per class. Therefore, the number of training data points is 5,000 in total. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experimental Details of Robust Sentiment Analysis of Movie Reviews</head><p>We report results on the IMDB dataset of movie reviews <ref type="bibr" target="#b52">[46]</ref> following <ref type="bibr" target="#b53">[47]</ref>. Paragraph Vector (PV-DBOW) is used as a document descriptor <ref type="bibr" target="#b54">[48]</ref>. We train a neural network with one 8-neuron hidden layer and display the results in <ref type="table">Table 3</ref>. We generate symmetric label noise.</p><p>IMDB contains 25,000 positive movie reviews and 25,000 negative ones. We follow <ref type="bibr" target="#b52">[46,</ref><ref type="bibr" target="#b54">48,</ref><ref type="bibr" target="#b53">47]</ref> to split them evenly for training and testing, respectively. PV-DBOW represents every movie review using a fixed-length feature vector. It is a binary classification problem. Our implementation benefits from the codes publicly available at https://github.com/mesnilgr/iclr15 and https://github. com/shaform/experiments/tree/master/caffe_sentiment_analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Label Noise</head><p>It is a binary classification problem so that the maximum noise rate that we can generate is 50%. We test three cases: r = 0.0, 0.2, 0.4. We choose an exponential distribution variant as an EDF, i.e., Î» = 0. Additionally, if r = 0.0, Î² &gt; 0, and Î² &lt; 0 otherwise. Specifically:</p><p>(1) When it is clean, we set Î² = 0.9 so that larger weights are assigned to harder examples.</p><p>(2) When noise exists, for r = 0.2 and r = 0.4, we set Î² = âˆ’0.52 and Î² = âˆ’0.33, respectively. Therefore, easier examples have larger weights. Note that the settings of Î» and Î² change a lot because we have only two classes here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Sample Imbalance</head><p>We use all the positive reviews (12500) and randomly sample a small proportion of negative reviews: (1) When P-N Ratio is 10:1, 1250 negative reviews are sampled.</p><p>(2) When P-N Ratio is 50:1, 250 negative reviews are sampled. We set Î» = 0.3, Î² = 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Net Architecture and Optimisation Solver</head><p>The network architecture is shown in <ref type="figure" target="#fig_10">Figure 4</ref> and its SGD solver is as follows:  </p><formula xml:id="formula_43">#</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F The Effectiveness of Label Correction</head><p>Is it feasible to correct the labels of noisy training data? In label correction, we replace the original labels by their corresponding predictions at the end of training. The results are shown in <ref type="table" target="#tab_11">Table 9</ref>.   Our proposal: DM incorporates emphasis mode and variance, and serves as explicit regularisation in terms of example weighting.</p><p>Important finding: When label noise rate is higher, we can improve a model's robustness by moving emphasis mode towards relatively less difficult examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Empirical Analysis of DM on CIFAR-10</head><p>To empirically understand DM well, we explore the behaviours of DM on CIFAR-10 with r = 20%, 40%, 60%, 80%, respectively. We use ResNet-56 which has larger capacity than ResNet-20.</p><p>Design choices. We mainly analyse the impact of different emphasis modes for different noise rates. We explore five emphasis modes: 1) None: Î² = 0. There is no emphasis mode since all examples are treated equally; 2) 0: Î» = 0; 3) 1 3 : Î» = 0.5; 4) 1 2 : Î» = 1; 5) 2 3 : Î» = 2. We remark that when Î» is larger, the emphasis mode is higher, leading to relatively easier training data points are emphasised. When emphasis mode changes, emphasis variance changes accordingly. Therefore, to set a proper spread for each emphasis mode, we try four emphasis variances and choose the best one 3 to study the impact of emphasis mode.</p><p>Results analysis. We show the results in <ref type="table" target="#tab_1">Table 10</ref>. The intact training set serves as an indicator of meaningful fitting and we observe that its accuracy is always consistent with the final test accuracy. We display the training dynamics in <ref type="figure" target="#fig_8">Figure 5</ref>. We summarise our observations as follows: Fitting and generalisation. We observe that CCE always achieves the best accuracy on corrupted training sets, which indicates that CCE has a strong data fitting ability even if there is severe noise <ref type="bibr" target="#b57">[51]</ref>. As a result, CCE has much worse final test accuracy than most models. Emphasising on harder examples. When there exists label noise, we obtain the worst final test accuracy if emphasis mode is 0, i.e., CCE and DM with Î» = 0. This unveils that in applications where we have to learn from noisy training data, it will hurt the model's generalisation dramatically if we use CCE or simply focus on harder training data points. Emphasis mode. When noise rate is 0, 20%, 40%, 60%, and 80%, we obtain the best final test accuracy when Î» = 0, Î» = 0.5, Î» = 1, Î» = 2, and Î» = 2, respectively. This demonstrates that when noise rate is higher, we can improve a model's robustness by moving emphasis mode towards relatively less difficult examples with a larger Î», which is informative in practice. Emphasis spread. As displayed in <ref type="table" target="#tab_1">Table 10</ref> and <ref type="figure" target="#fig_12">Figures 6-9</ref> in the supplementary material, emphasis variance also matters a lot when fixing emphasis mode, i.e., fixing Î». For example in <ref type="table" target="#tab_1">Table 10</ref> , when Î» = 0, although focusing on harder examples similarly with CCE, DM can outperform CCE by modifying the emphasis variance. As shown in <ref type="figure" target="#fig_12">Figures 6-9</ref>, some models even collapse and cannot converge if the emphasis variance is not rational.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Learning Dynamics on Clean Training Data</head><p>The learning dynamics on clean training data are displayed in the <ref type="figure" target="#fig_1">Figures 10-11.</ref> (a) r = 20%.</p><p>(b) r = 40%.</p><p>(c) r = 60%. <ref type="figure" target="#fig_8">Figure 5</ref>: The learning dynamics of ResNet-56 on CIFAR-10, i.e., training and testing accuracies along with training iterations. The legend in the top left is shared by all subfigures. 'xxx: yyy' means 'method: emphasis mode'. We have two key observations: 1) When noise rate increases, better generalisation is obtained with higher emphasis mode, i.e., focusing on relatively easier examples; 2) Both overfitting and underfitting lead to bad generalisation. For example, 'CCE: 0' fits training data much better than the others while 'DM: None' generally fits it unstably or a lot worse. Better viewed in colour.     <ref type="figure" target="#fig_1">Figure 10</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Common practice and DM. Black and red arrows denote forward process and gradient back-propagation, respectively. An EDF is an example-level weighting function normalised by its integral over [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of DM in terms of optimisation and example weighting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 2 )</head><label>2</label><figDesc>We demonstrate the effectiveness of DM for training robust deep networks on diverse tasks: (a) Image classification under synthetic and realworld label noise; (b) Video retrieval with unknown and diverse abnormal examples; (c) Sentiment classification of movie reviews when label noise and sample imbalance exist. (3) We show DM with diverse network architectures and stochastic optimisers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>can be viewed as an exponential transformation of v BD (w DM i ; Î±, Î·), where Î± = Î»+1, Î· = 2 and scale it by Î² followed by an exponential transformation; (4) w DM i is an extension of w GCE i by making Î» â‰¥ 0, linear scaling and exponential transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) The average pi of different examples as training goes in CCE and DM.(b) The test accuracy of CCE and DM as the iteration increases.(c) We optimise ÏˆDM over four settings from {0, 1/3, 1/2, 2/3}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>We train ResNet-56 on CIFAR-10. In (a) and (b), we observe noisy examples have much less p i than clean ones, thus being more difficult examples in both CCE and DM (Î» = 1). In (c), for each label noise rate, we show the optimised Ïˆ DM from {0, 1/3, 1/2, 2/3}, i.e., Î» âˆˆ {0, 1/2, 1, 2}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>CCE(-DN)GCE(-DN)MAE(-DN)MSE(-DN) DM Label Noise r 0.0 88.9(88.5) 88.9(87.7) 88.9(74.5) 88.9(88.2) 89.1 0.2 87.7(87.2) 88.6(85.5) 88.6(72.8) 88.5(87.0) 88.7 0.4 75.5(75.0) 83.6(75.2) 84.9(80.2 ) 83.7(75.3) 86.4 P-N Ratio 10:1 78.9(77.5) 77.0(59.1) 75.4(0.5) 77.6(78.5) 80.6 50:1 63.4(61.4) 0.5(0.5) 0.5(0.5) 58.6(64.4) 65.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Diverse semantically abnormal training examples highlighted by red boxes. The 1st row shows synthetic abnormal examples from corrupted CIFAR-10 [25]. The 2nd and 3rd rows present realistic abnormal examples from video person re-identification benchmark MARS [22]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>B. 5</head><label>5</label><figDesc>Derivatives w.r.t. Logits z i B.5.1 âˆ‚L CCE /âˆ‚z i The calculation is based on Eq. (18) and Eq. (16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>B. 5 . 3</head><label>53</label><figDesc>âˆ‚L GCE /âˆ‚z i The calculation is based on Eq. (22) and Eq. (16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 :</head><label>4</label><figDesc>A designed neural network with one 8-neuron hidden layer for sentiment analysis on IMDB. It is quite simple and our objective is not to represent the state-of-the-art.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>G</head><label></label><figDesc>More Detailed Empirical Results G.1 Training DNNs under label noise Practical research question: What training examples should be focused on and how much more should they be emphasised when training DNNs under label noise?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 :</head><label>6</label><figDesc>ResNet-56 on CIFAR-10 (r = 20%). From left to right, the results of four emphasis modes 0, 1 3 , 0.5, 2 3 with different emphasis variances are displayed in each column respectively. When Î» is larger, Î² should be larger. Specifically : 1) when Î» = 0: we tried Î² = 0.5, 1, 2, 4; 2) when Î» = 0.5: we tried Î² = 4, 8, 12, 16; 3) when Î» = 1: we tried Î² = 8, 12, 16, 20; 4) when Î» = 2: we tried Î² = 12,16, 20, 24.   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>ResNet-56 on CIFAR-10 (r = 80%). From left to right, the results of four emphasis modes 0, 1 3 , 0.5, 2 3 with different emphasis variances are displayed in each column respectively. When Î» is larger, Î² should be larger. Specifically : 1) when Î» = 0: we tried Î² = 0.5, 1, 2, 4; 2) when Î» = 0.5: we tried Î² = 4, 8, 12, 16; 3) when Î» = 1: we tried Î² = 8, 12, 16, 20; 4) when Î» = 2: we tried Î² = 12, 16, 20, 24.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>The training and test accuracies on clean CIFAR-10 along with training iterations. The training labels are clean. We fix Î» = 0 to focus on harder examples while changing emphasis variance controller Î². The backbone is ResNet-20. The results of ResNet-56 are shown inFigure 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>The training and test accuracies on clean CIFAR-10 along with training iterations. The training labels are clean. We fix Î» = 0 to focus on more difficult examples while changing emphasis variance controller Î². The backbone is ResNet-56. The results of ResNet-20 are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) on CIFAR-100. The best results on each block are bolded. Italic row is the most basic baseline where examples have identical weights. All methods use ResNet-44<ref type="bibr" target="#b36">[30]</ref>.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Clean Labels</cell><cell cols="3">Symmetric Noisy Labels r=0.2 r=0.4 r=0.6</cell><cell cols="3">Asymmetric Noisy Labels r=0.2 r=0.3 r=0.4</cell></row><row><cell></cell><cell>LS</cell><cell>63.7</cell><cell>58.8</cell><cell>50.1</cell><cell>24.7</cell><cell>63.0</cell><cell>62.3</cell><cell>61.6</cell></row><row><cell>Results</cell><cell>Boot-hard</cell><cell>63.3</cell><cell>57.9</cell><cell>48.2</cell><cell>12.3</cell><cell>63.4</cell><cell>63.2</cell><cell>62.1</cell></row><row><cell>From</cell><cell>Forward</cell><cell>64.0</cell><cell>59.8</cell><cell>53.1</cell><cell>24.7</cell><cell>64.1</cell><cell>64.0</cell><cell>60.9</cell></row><row><cell>SL</cell><cell>D2L</cell><cell>64.6</cell><cell>59.2</cell><cell>52.0</cell><cell>35.3</cell><cell>62.4</cell><cell>63.2</cell><cell>61.4</cell></row><row><cell></cell><cell>SL</cell><cell>66.8</cell><cell>60.0</cell><cell>53.7</cell><cell>41.5</cell><cell>65.6</cell><cell>65.1</cell><cell>63.1</cell></row><row><cell></cell><cell>CCE</cell><cell>70.0</cell><cell>60.4</cell><cell>53.2</cell><cell>42.1</cell><cell>66.4</cell><cell>64.7</cell><cell>60.3</cell></row><row><cell></cell><cell>GCE</cell><cell>63.6</cell><cell>62.4</cell><cell>58.6</cell><cell>50.6</cell><cell>62.8</cell><cell>62.2</cell><cell>58.7</cell></row><row><cell></cell><cell>MAE</cell><cell>8.2</cell><cell>6.4</cell><cell>7.3</cell><cell>5.2</cell><cell>7.3</cell><cell>6.3</cell><cell>7.3</cell></row><row><cell></cell><cell>MSE</cell><cell>28.0</cell><cell>24.6</cell><cell>21.3</cell><cell>18.0</cell><cell>24.5</cell><cell>24.3</cell><cell>23.0</cell></row><row><cell>Our</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Trained</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) of DM and other baselines on CIFAR-10 under symmetric label noise. The best results on each block are bolded. Number format of this table follows MentorNet. '-' denotes result was not reported.</figDesc><table><row><cell></cell><cell>Method</cell><cell>r=0 r=0.2r=0.4r=0.8</cell></row><row><cell></cell><cell>Forgetting</cell><cell>-0.76 0.71 0.44</cell></row><row><cell></cell><cell>Self-paced</cell><cell>-0.80 0.74 0.33</cell></row><row><cell>Results From MentorNet</cell><cell cols="2">Focal Loss Boot-soft MentorNet PD -0.79 0.74 0.44 -0.77 0.74 0.40 -0.78 0.73 0.39</cell></row><row><cell></cell><cell cols="2">MentorNet DD -0.79 0.76 0.46</cell></row><row><cell></cell><cell>CCE</cell><cell>0.85 0.74 0.74 0.35</cell></row><row><cell></cell><cell>GCE</cell><cell>0.83 0.81 0.77 0.18</cell></row><row><cell></cell><cell>MAE</cell><cell>0.57 0.50 0.45 0.19</cell></row><row><cell></cell><cell>MSE</cell><cell>0.80 0.78 0.73 0.29</cell></row><row><cell>Our Trained</cell><cell cols="2">CCE-DN 0.84 0.75 0.76 0.18</cell></row><row><cell>Results</cell><cell></cell></row></table><note>GCE-DN 0.77 0.82 0.79 0.19 MAE-DN 0.10 0.10 0.10 0.19 MSE-DN 0.85 0.51 0.76 0.53 DM(Î² = 0) 0.86 0.66 0.56 0.18 DM(Î» = 0.0) 0.86 0.77 0.75 0.18 DM(Î» = 0.5) 0.86 0.83 0.80 0.57</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of common stochastic optimisers.</figDesc><table><row><cell cols="2">Adam [36] is an adaptive gradient method. We</cell></row><row><cell>report three settings of it.</cell><cell></cell></row><row><cell></cell><cell>CCE GCE MAE MSE DM</cell></row><row><cell>SGD (lr: 0.01)</cell><cell>64.6 68.8 39.3 58.4 82.0</cell></row><row><cell cols="2">SGD + Momentum (lr: 0.01) 61.7 80.7 64.7 76.7 83.8</cell></row><row><cell>Nesterov (lr: 0.01)</cell><cell>57.3 80.0 63.9 76.8 84.0</cell></row><row><cell cols="2">Adam (lr: 0.01, delta: 0.1) 39.3 75.7 57.5 66.8 78.2</cell></row><row><cell cols="2">Adam (lr: 0.005, delta: 0.1) 44.3 72.6 60.8 67.9 80.8</cell></row><row><cell cols="2">Adam (lr: 0.005, delta: 1) 52.0 67.7 37.3 58.5 79.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Boot-hard</cell><cell>D2L Forward SL</cell><cell>S-adaptation</cell><cell>Masking</cell><cell>Joint Optim.</cell><cell>MD-DYR-SH</cell><cell>Our Trained Results CCE(-DN) GCE(-DN) MAE(-DN) MSE(-DN) DM</cell></row><row><cell cols="2">68.9 69.5 69.8 71.0</cell><cell>70.3</cell><cell>71.1</cell><cell>72.2</cell><cell cols="2">71.0 71.7(72.5) 72.4(64.5) 39.7(16.4) 71.7(69.9) 73.3</cell></row></table><note>Accuracy (%) on Clothing1M. The leftmost block's results are from SL [27] while the middle block's are from Masking [39]. Results of common losses after DN are in the brackets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Video retrieval results on MARS. All other methods use GoogLeNet V2 except that DRSA and CAE use more complex ResNet-50.</figDesc><table><row><cell>Metric</cell><cell cols="2">DRSA CAE</cell><cell>OSM+CAA</cell><cell></cell><cell cols="3">Our Trained Results</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CCE GCE MAE MSE</cell><cell>DM</cell></row><row><cell>mAP (%)</cell><cell>65.8</cell><cell>67.5</cell><cell>72.4</cell><cell>58.1</cell><cell>31.6</cell><cell>12.0</cell><cell>19.6</cell><cell>72.8</cell></row><row><cell>CMC-1 (%)</cell><cell>82.3</cell><cell>82.4</cell><cell>84.7</cell><cell>73.8</cell><cell>51.5</cell><cell>26.0</cell><cell>39.3</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Is it the right time to make a change from the design of loss function to the design of derivative directly?2. We show that the loss values do not matter, i.e., they only tell where the current model is.To update a model, the derivative (i.e. gradient of the final layer) used for back-propagation is the key. We advocate rethinking existing robustness theorems on loss functions in deep learning (gradient-based optimisation algorithms).</figDesc><table><row><cell>Supplementary Material of Derivative Manipulation</cell></row><row><cell>3. A derivative magnitude function = An example-level weighting scheme!</cell></row><row><cell>4. Future work on how to design the derivative direction?</cell></row><row><cell>We remark that we can leverage the interesting idea explored in [49]:</cell></row><row><cell>ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks</cell></row></table><note>for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters. IMAE focuses on studying MAE and how to improve it. IMAE's independent contributions are: 1. We correct the previous claim that MAE treats examples equally.2. We demonstrate that emphasis variance matters, consequently we propose Improved MAE.Open Discussion on Machine Learning Research Concepts and Future Work 1.A Display of Semantically Abnormal Training Examples</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>B.5.2 âˆ‚L MAE /âˆ‚z iThe calculation is analogous with that of âˆ‚L CCE /âˆ‚z i .</figDesc><table /><note>According to Eq. (20) and Eq. (16), if j = y i :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Results of DM and other standard regularisers on CIFAR-100. We set r = 40%, i.e., the label noise is severe but not belongs to the majority. We train ResNet-44. We report the average test accuracy and standard deviation (%) over 5 trials. Baseline is CCE without regularisation.</figDesc><table><row><cell>Baseline</cell><cell>L2</cell><cell>Dropout Dropout+L2</cell><cell>DM</cell><cell cols="2">DM+L2 DM+Dropout DM+L2+Dropout</cell></row><row><cell cols="5">44.7Â±0.1 51.5Â±0.4 46.7Â±0.5 52.8Â±0.4 55.7Â±0.3 59.3Â±0.2</cell><cell>54.3Â±0.4</cell><cell>58.3Â±0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>The test accuracy (%) of DM and other standard regularisers on Vehicles-10. We train ResNet-44. Baseline denotes CCE without regularisation. We test two cases: symmetric label noise rate is r = 40%, and clean data r = 0.</figDesc><table><row><cell>r</cell><cell>Baseline</cell><cell>L2</cell><cell cols="6">Dropout Dropout+L2 DM DM+L2 DM+Dropout DM+L2+Dropout</cell></row><row><cell>0</cell><cell>75.4</cell><cell>76.4</cell><cell>77.9</cell><cell>78.7</cell><cell>83.8</cell><cell>84.4</cell><cell>84.5</cell><cell>84.7</cell></row><row><cell>40%</cell><cell>42.3</cell><cell>44.8</cell><cell>41.6</cell><cell>47.4</cell><cell>45.8</cell><cell>55.7</cell><cell>48.8</cell><cell>58.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Is it feasible to correct the labels of noisy training data? Our results demonstrate the effectiveness of label correction using DM. When retraining from scratch on the relabelled training data, we do not adjust the hyper-parameters Î² and Î». Therefore, the reported results of retraining on relabelled datasets are not the optimal. In label correction, the original labels are replaced by their corresponding predictions.</figDesc><table><row><cell>Noise Rate r</cell><cell>Emphasis Mode</cell><cell>Model</cell><cell>Testing Accuracy (%)</cell><cell cols="2">Accuracy on Training Sets (%)</cell><cell cols="2">Fitting degree of subsets (%)</cell><cell>Retrain after</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Best Final Noisy</cell><cell>Intact</cell><cell cols="2">Clean Noisy</cell><cell>label correction</cell></row><row><cell></cell><cell>0</cell><cell>CCE</cell><cell>86.5 76.8</cell><cell>95.7</cell><cell>80.6</cell><cell>99.0</cell><cell>85.9</cell><cell>-</cell></row><row><cell>20%</cell><cell cols="3">1/3(Î» = 0.5) DM (Î² = 12) 89.4 87.8</cell><cell>81.5</cell><cell>95.0</cell><cell>98.8</cell><cell>11.7</cell><cell>89.3 (+1.5)</cell></row><row><cell></cell><cell>0</cell><cell>CCE</cell><cell>82.8 60.9</cell><cell>83.0</cell><cell>64.4</cell><cell>97.0</cell><cell>81.1</cell><cell>-</cell></row><row><cell>40%</cell><cell cols="3">1/2(Î» = 1) DM (Î² = 16) 84.7 83.3</cell><cell>60.3</cell><cell>88.9</cell><cell>94.8</cell><cell>7.5</cell><cell>85.3 (+2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Results of CCE, DM on CIFAR-10 under noisy labels. For every model, we show its best test accuracy during training and the final test accuracy when training terminates, which are indicated by 'Best' and 'Final', respectively. We also present the results on corrupted training sets and original intact one. The overlap rate between corrupted and intact sets is (1 âˆ’ r). When Î» is larger, Î² should be larger for adjusting emphasis variance. The intact training set serves as an indicator of meaningful fitting and we observe that its accuracy is always consistent with the final test accuracy.</figDesc><table><row><cell>Noise Rate r</cell><cell>Emphasis Mode</cell><cell>Model</cell><cell cols="2">Testing Accuracy (%)</cell><cell></cell><cell>Accuracy on Training Sets (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Best</cell><cell>Final</cell><cell cols="2">Corrupted Intact (Meaningful Fitting)</cell></row><row><cell></cell><cell>0</cell><cell>CCE</cell><cell>86.5</cell><cell>76.8</cell><cell>95.7</cell><cell>80.6</cell></row><row><cell></cell><cell>None</cell><cell>DM (Î²=0)</cell><cell>83.5</cell><cell>58.1</cell><cell>50.6</cell><cell>60.2</cell></row><row><cell>20%</cell><cell cols="3">0 (Î» = 0) 1/3(Î» = 0.5) DM (Î² = 12) 89.4 DM (Î² = 2) 84.9</cell><cell>76.4 87.8</cell><cell>85.3 81.5</cell><cell>80.5 95.0</cell></row><row><cell></cell><cell>1/2(Î» = 1)</cell><cell cols="2">DM (Î² = 16) 87.3</cell><cell>86.7</cell><cell>78.4</cell><cell>93.8</cell></row><row><cell></cell><cell>2/3(Î» = 2)</cell><cell cols="2">DM (Î² = 24) 85.8</cell><cell>85.5</cell><cell>76.0</cell><cell>91.4</cell></row><row><cell></cell><cell>0</cell><cell>CCE</cell><cell>82.8</cell><cell>60.9</cell><cell>83.0</cell><cell>64.4</cell></row><row><cell></cell><cell>None</cell><cell>DM (Î²=0)</cell><cell>71.8</cell><cell>44.9</cell><cell>31.3</cell><cell>45.8</cell></row><row><cell>40%</cell><cell cols="3">0 (Î» = 0) 1/3(Î» = 0.5) DM (Î² = 12) 85.1 DM (Î² = 1) 78.4</cell><cell>65.6 79.9</cell><cell>63.3 67.7</cell><cell>66.6 85.7</cell></row><row><cell></cell><cell>1/2(Î» = 1)</cell><cell cols="2">DM (Î² = 16) 84.7</cell><cell>83.3</cell><cell>60.3</cell><cell>88.9</cell></row><row><cell></cell><cell>2/3(Î» = 2)</cell><cell cols="2">DM (Î² = 20) 52.7</cell><cell>52.7</cell><cell>35.4</cell><cell>53.6</cell></row><row><cell></cell><cell>0</cell><cell>CCE</cell><cell>69.5</cell><cell>37.2</cell><cell>84.1</cell><cell>40.5</cell></row><row><cell></cell><cell>None</cell><cell>DM (Î²=0)</cell><cell>69.9</cell><cell>57.9</cell><cell>40.1</cell><cell>58.6</cell></row><row><cell>60%</cell><cell cols="3">0 (Î» = 0) 1/3(Î» = 0.5) DM (Î² = 12) 77.5 DM (Î² = 0.5) 72.3</cell><cell>53.9 58.5</cell><cell>42.1 55.5</cell><cell>55.1 62.6</cell></row><row><cell></cell><cell>1/2(Î» = 1)</cell><cell cols="2">DM (Î² = 12) 71.9</cell><cell>70.0</cell><cell>41.0</cell><cell>73.9</cell></row><row><cell></cell><cell>2/3(Î» = 2)</cell><cell cols="2">DM (Î² = 12) 80.2</cell><cell>72.5</cell><cell>44.9</cell><cell>75.4</cell></row><row><cell></cell><cell>0</cell><cell>CCE</cell><cell>36.1</cell><cell>16.1</cell><cell>54.3</cell><cell>18.4</cell></row><row><cell></cell><cell>None</cell><cell>DM (Î²=0)</cell><cell>44.4</cell><cell>28.2</cell><cell>20.6</cell><cell>28.8</cell></row><row><cell>80%</cell><cell>0 (Î» = 0) 1/3(Î» = 0.5)</cell><cell cols="2">DM (Î² = 0.5) 46.2 DM (Î² = 8) 51.6</cell><cell>21.3 22.4</cell><cell>27.8 46.1</cell><cell>23.1 24.4</cell></row><row><cell></cell><cell>1/2(Î» = 1)</cell><cell>DM (Î² = 8)</cell><cell>35.5</cell><cell>31.5</cell><cell>19.8</cell><cell>32.3</cell></row><row><cell></cell><cell>2/3(Î» = 2)</cell><cell cols="2">DM (Î² = 12) 33.0</cell><cell>32.8</cell><cell>14.2</cell><cell>32.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The derivation details of all losses are given in the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Since there is a large interval between different Î² in our four trials, we deduce that the chosen one is not the optimal. The focus of this work is not to optimize the hyper-parameters. Instead, we focus more on the practical research question: What training examples should be focused on and how much more should they be emphasised when training DNNs under label noise?</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">We follow the same settings as MentorNet [6] and train GoogLeNet V1 to compare fairly with its reported results. Optimiser and data augmentation are the same as CIFAR-100. Competitors. Self-paced</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note>Training details. Focal Loss [34], and MentorNet are representatives of example weighting algorithms. Forgetting [24] searches the dropout parameter in the range of (0.2, 0.9. All methods use GoogLeNet V1 [35</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The results are shown in Table 2: (1) When looking at common losses, they perform differently in different cases. For example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Results</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mse-Dn</forename><surname>Cce</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data augmentation: first resize a raw input image to 256 Ã— 256, and then crop it randomly at 224 Ã— 224 followed by random horizontal flipping</title>
	</analytic>
	<monogr>
		<title level="m">We set Î» = 1, Î² = 2 for DM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">We compare with recent algorithms: (1) S-adaptation applies an auxiliary softmax layer to estimate a noise-transition matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Competitors</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
	<note>Masking is a human-assisted approach that conveys human cognition to speculate the structure of a noise. transition matrix [39]; (3) Joint Optim</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">learns latent true labels and model&apos;s parameters iteratively. Two regularisation terms are added for label estimation and adjusted in practice</title>
	</analytic>
	<monogr>
		<title level="m">) MD-DYR-SH [40] combines dynamic mixup (MD), dynamic bootstrapping plus regularisation (DYR) from soft to hard (SH)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">We display the results in Table 5. Under real-world agnostic noise, DM outperforms the state-of-the-art. It is worth noting that the burden of noise-transition matrix estimation in Forward, S-adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Results</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Masking and Joint Optim. is heavy, whereas DM is simple and effective</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">There are 1,067,516 frames in total. Because person videos are collected by tracking and detection algorithms, abnormal examples exist as shown in Figure 3 in the supplementary material: Some frames contain only background or an out-of-distribution person. Exact noise type and rate are unknown. We use 8,298 videos of 625 persons for training and 12</title>
		<imprint/>
	</monogr>
	<note>MARS contains 20,715 videos of 1,261 persons. 180 videos of the other 636 persons for testing. We References</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<editor>NeurIPS.</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">IMAE for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude&apos;s variance matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12141</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The approach based on influence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Hampel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Ronchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stahel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robust Statistics</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A general and adaptive robust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Statistical learning with sparsity: the lasso and generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
		<title level="m">Robust statistics</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On symmetric losses for learning from corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Charoenphakdee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive sampling for sgd by exploiting side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online batch selection for faster training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In: ECCV.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>JastrzÄ™bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Nlnl: Negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Quality aware network for set to set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep metric learning by online soft mining and class-aware attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video person re-identification with competitive snippetsimilarity aggregation and co-attentive snippet embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<editor>ACL.</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03788</idno>
		<title level="m">Proselflc: Progressive self label correction for training robust deep neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">From left to right, the results of four emphasis modes 0, 1 3 , 0.5, 2 3 with different emphasis variances are displayed in each column respectively</title>
	</analytic>
	<monogr>
		<title level="m">ResNet-56 on CIFAR-10 (r = 40%)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>When Î» is larger, Î² should be larger. Specifically</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">From left to right, the results of four emphasis modes 0, 1 3 , 0.5, 2 3 with different emphasis variances are displayed in each column respectively</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Figure 8: ResNet-56 on CIFAR-10 (r = 60%). When Î» is larger, Î² should be larger. Specifically</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
