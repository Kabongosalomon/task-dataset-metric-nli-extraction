<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Search is State of the Art for Neural Architecture Search Benchmarks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>White</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Realityengines</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin@realityengines</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Nolen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Realityengines</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam@realityengines</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Savani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Realityengines</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Local Search is State of the Art for Neural Architecture Search Benchmarks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Local search is one of the simplest families of algorithms in combinatorial optimization, yet it yields strong approximation guarantees for canonical NP-Complete problems such as the traveling salesman problem and vertex cover. While it is a ubiquitous algorithm in theoretical computer science, local search is often neglected in hyperparameter optimization and neural architecture search.</p><p>We show that the simplest local search instantiations achieve state-of-the-art results on multiple NAS benchmarks (NASBench-101 and NASBench-201), outperforming the most popular recent NAS algorithms. However, local search fails to perform well on the much larger DARTS search space. Motivated by these observations, we present a theoretical study which characterizes the performance of local search on graph optimization problems, backed by simulation results. This may be of independent interest beyond NAS. All code and materials needed to reproduce our results are publicly available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural architecture search (NAS) is a widely popular area of machine learning, with the goal of automating the development of the best neural network for a given dataset. Hundreds of NAS algorithms have been proposed <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref>, and with the release of two NAS benchmark datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref>, the extreme computational cost for NAS is no longer a barrier, and it is easier to fairly compare different NAS algorithms. Most of the recently proposed state-of-the-art algorithms are becoming increasingly more complex, many of which use neural networks as subroutines <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. This trend is problematic because as the complexity of NAS algorithms increases, the amount of necessary "hyper-hyperparameter tuning", or tuning the NAS algorithm itself, increases. Not only is this a vicious cycle (will we start using AutoML algorithms to tune AutoML algorithms?), but the runtime for any hyper-hyperparameter tuning on a new dataset must be added to the total runtime of the NAS algorithm <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref>. Since this information is not always recorded, some NAS algorithms may have under-reported runtimes, making it harder to compare different algorithms.</p><p>In contrast to prior work, we study a NAS algorithm which can be implemented in five lines of code (Algorithm 1). Local search is a simple and canonical family of greedy algorithms in combinatorial optimization and has led to famous results in the study of approximation algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>. The most basic form of local search, often called the hill-climbing algorithm, consists of starting with a random architecture, and then iteratively training all architectures in its neighborhood, choosing the best one for the next iteration. The neighborhood is typically defined as all architectures which differ by one operation or edge. Local search finishes when it reaches a (local or global) optimum, or when it exhausts its runtime budget.</p><p>Despite its simplicity, we show that local search achieves state-of-the-art results on all four datasets from the NASBench-101 and NASBench-201 benchmarks, beating out many recent algorithms which claimed state of the art. However, these benchmark datasets contain at most 4 × 10 5 architectures. On the DARTS <ref type="bibr" target="#b20">[21]</ref> search space which contains 10 18 architectures, we show that local search performs worse than random search. This suggests more generally that strong performance on NAS benchmark datasets does not necessarily imply strong performance on large-scale NAS applications.</p><p>Motivated by the stark contrast between the performance of local search on NASBench datasets and DARTS, we present a theoretical study to better understand the performance of local search for NAS on different search spaces. The underlying optimization problem in NAS is a hybrid between discrete optimization, on a graph topology, and continuous optimization, on the distribution of architecture accuracies. We formally define a NAS problem instance by the graph topology, a global probability density function (PDF) on the architecture accuracies, and a local PDF on the accuracies between neighboring architectures, and we derive a set of equations which calculate the probability that a randomly drawn architecture will converge to within of the global optimum, for all &gt; 0. As a corollary, we give equations for the expected number of local minima, and the expected size of the preimage of a local minimum. These results completely characterize the performance of local search. To the best of our knowledge, this is the first result which theoretically predicts the performance of a NAS algorithm, and may be of independent interest within discrete optimization. We run simulations which suggest that our theoretical results predict the performance of real datasets reasonably well. Altogether, our results show that the performance of local search depends on the level of locality of the search space, as well as the average neighborhood size in the search space. We make available all code and materials needed to reproduce our results.</p><p>Our contributions. We summarize our main contributions below.</p><p>• We implement the simple local search algorithm as a baseline for NAS, showing that it achieves stateof-the-art performance on two NASBench datasets (which both have size &lt; 10 6 ) as well as subpar performance on a large search space (of size 10 18 ). We suggest that existing NAS benchmarks may be too small to adequately evaluate NAS algorithms. • We give a full theoretical characterization of the properties of a dataset necessary for local search to give strong performance. We experimentally validate these results on real datasets. Our results improve the theoretical understanding of local search and lay the groundwork for future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Local search in theoretical computer science. Local search has been studied since at least the 1950s in the context of the traveling salesman problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, machine scheduling <ref type="bibr" target="#b25">[26]</ref>. and graph partitioning <ref type="bibr" target="#b16">[17]</ref>. Local search has consistently seen significant attention in theory <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15]</ref> and practice <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Neural architecture search. NAS has gained significant attention in recent years <ref type="bibr" target="#b39">[40]</ref>, although the first few techniques have been around since at least the 1990s <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29]</ref>. Popular techniques include Bayesian optimization <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b12">13]</ref>, reinforcement learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b19">20]</ref>, gradient descent <ref type="bibr" target="#b20">[21]</ref>, neural prediction <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30]</ref>, and evolution <ref type="bibr" target="#b27">[28]</ref>. NAS can be broadly split into macro search, in which the search is over the entire neural network, or micro search, in which the search is over a small 'cell', which is then duplicated many times to create a large neural network. Recent papers have highlighted the need for fair and reproducible NAS comparisons <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, spurring the release of two cell-based NAS benchmark datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref>, each of which include tens of thousands of pretrained neural networks. See the recent survey <ref type="bibr" target="#b9">[10]</ref> for a more comprehensive overview on NAS. Prior work has performed local search for NAS using network morphisms, guided by cosine annealing <ref type="bibr" target="#b8">[9]</ref>. This is a more complex variation of local search for NAS. Very recently, concurrent work has also shown that simple local search is a strong baseline for NAS <ref type="bibr" target="#b24">[25]</ref>. Their work considers multi-objective NAS (the objective is a function of accuracy and network complexity), focuses on macro search rather than cell-based search, and gives no theoretical results. Therefore, the concurrent work uses different techniques to achieve a similar conclusion. The existence of their work strengthens our conclusions, as they were independently and simultaneously verified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Broader Impact</head><p>Our work seeks to improve the understanding of neural architecture search by analyzing the differences between benchmark vs. large-scale search spaces, characterizing the performance of local search, and helping to lay the groundwork for future theoretical results. While this does not have an immediate societal impact the same way that a GAN for deep fakes or a deep learning optimizer for CO 2 emissions would, our work, and more broadly the field of NAS, will be the algorithms powering and refining the deep learning applications with direct societal implications.</p><p>Since our work only indirectly affects society, we have less control over its net implications. However, we see two benefits to the AI community that will foster a positive impact. First, we help to lay the groundwork for future theoretical results which can improve our understanding of the workings of different algorithms. Second, we present an easily implementable NAS algorithm which gives strong performance under some settings, and we advocate for simpler NAS algorithms in the future. Not only will this exhibit better baselining of NAS algorithms, but it also helps to democratize neural architecture search. Easily implementable and understandable NAS algorithms will lead to more widespread use. Because of the recent push for explicitly reasoning about the impact of research in AI <ref type="bibr" target="#b11">[12]</ref>, we are hopeful that downstream deep learning applications will be used to benefit society.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preliminaries</head><p>In this section, we formally define the local search algorithm, and we define notation that will be used for the rest of the paper. Given a set A, denote an objective function : A → [0, 1]. Although our theory works generally for any A and , we refer to A as a search space of neural architectures, and (v) as the validation loss of v ∈ A over a fixed dataset and training pipeline. The goal is to find v * = argmin v∈A (v), the neural architecture with the minimum validation loss, or else find an architecture whose validation loss is within of the minimum, for some small &gt; 0. We define a neighborhood function N : A → 2 A . For instance, N (v) might represent the set of all neural architectures which differ from v by one operation or edge.</p><p>Local search in its simplest form (also called the hill-climbing algorithm) is defined as follows. Start with a random architecture v and evaluate (v) by training v. Iteratively train all architectures in N (v), and then replace v with the architecture u such that u = argmin w∈N (v) (w). Continue until we reach an architecture v such that ∀u ∈ N (v), (v) ≤ (u), i.e., we reach a local minimum. See Algorithm 1. We often place a runtime bound on the algorithm, in which case the algorithm returns the architecture v with the lowest value of (v) when it exhausts the runtime budget. In Section 6, we also consider two simple variants. In the query_until_lower variant, instead of evaluating every architecture in the neighborhood N (v) and picking the best one, we draw architectures u ∈ N (v) at random without replacement and move to the next iteration as soon as (u) &lt; (v). In the continue_at_min variant, we do not stop at a local minimum, instead moving to the second-best architecture found so far and continuing until we exhaust the runtime budget. One final variant, which we explore in Appendix B, is choosing k initial architectures at random instead of just one, and setting v 1 to be the architecture with the lowest objective value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Local search</head><p>Input: Search space A, objective function , neighborhood function N 1. Pick an architecture v 1 ∈ A uniformly at random 2. Evaluate (v 1 ); denote a dummy variable</p><formula xml:id="formula_0">(v 0 ) = ∞; set i = 1 3. While (v i ) &lt; (v i−1 ) : i. Evaluate (u) for all u ∈ N (v i ) ii. Set v i+1 = argmin u∈N (v i ) (u); set i = i + 1 Output:</formula><p>Architecture v i Now we define the notation used in Sections 5 and 6. Given a search space A and a neighborhood function N , we define the neighborhood graph G N = (A, E N ) such that for u, v ∈ A, the edge (u, v) is in E N if and only if v ∈ N (u). We only consider symmetric neighborhood functions, that is, v ∈ N (u) implies u ∈ N (v). Therefore, we may assume that the neighborhood graph is undirected. Given G, N , and a loss function , define LS :</p><formula xml:id="formula_1">A → A such that ∀v ∈ A, LS(v) = argmin u∈N (v) (u) if min u∈N (v) (u) &lt; (v),</formula><p>and LS(v) = ∅ otherwise. In other words, LS(v) denotes the architecture after performing one iteration of local search starting from v.</p><formula xml:id="formula_2">For integers k ≥ 1, recursively define LS k (v) = LS(LS k−1 (v)). We set LS 0 (v) = v and denote LS * (v) = min k|LS k (v) =∅ LS k (v)</formula><p>, that is, the output when running local search to convergence, starting at v. Similarly, define the preimage</p><formula xml:id="formula_3">LS −k (v) = {u | LS k (u) = v} for integers k ≥ 1 and LS − * (v) = {u | ∃k ≥ 0 s.t. LS −k (u) = v}. That is, LS − * (v)</formula><p>is a multifunction which defines the set of all points u which reach v at some point during local search. We refer to LS − * as the full preimage of v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A theory of local search</head><p>In this section, we give a theoretical analysis of local search for NAS, including a complete characterization of its performance. We present both a general result on local search, as well as a closed-form solution for search spaces satisfying certain constraints. We give an experimental validation of our theoretical results in the next section.</p><p>In a NAS application, the topology of the search space is fixed and discrete, while the distribution of validation losses for architectures is randomized and continuous, due to the non-deterministic nature of training a neural network. For example, both NASBench-101 and NASBench-201 include validation and test accuracies for three different random seeds for each architecture, to better simulate real NAS experiments. Therefore, we assume that the validation loss for a trained architecture is sampled from a global probability distribution, and for each architecture, the validation losses of its neighbors are sampled from a local probability distribution. Given a graph G N = (A, E N ), each node v ∈ A has a loss (v) ∈ R sampled from a PDF which we denote by pdf n . For any two neighbors (v, u) ∈ E N , the PDF for the validation loss x of architecture u is given by pdf e ( (v), x). Choices for the distribution pdf e are constrained by the fixed topology of the search space, as well as the distribution pdf n . In Appendix A, we discuss this further by formally defining measurable spaces for all random variables in our framework.</p><p>Our main result is a formula for the fraction of nodes in the search space which are local minima, as well as a formula for the fraction of nodes v such that the loss of LS * (v) is within of the loss of the global optimum, for all ≥ 0. In other words, we give a formula for the probability that the local search algorithm outputs a solution that is close to optimal. Note that such a formula characterizes the performance of local search. We give the full proofs for all of our results in Appendix A. For the rest of this section, we assume for all v ∈ A, |N (v)| = s, and we assume G N is vertex transitive (given u, v ∈ A, there exists an automorphism of G N which maps u to v). Let v * denote the architecture with the global minimum loss, therefore the support of the distribution of validation losses is a subset of [ (v * ), ∞). Technically, the integrals in this section are Lebesgue integrals. However, we use the more standard Riemann-Stieltjes notation for clarity. We also slightly abuse notation and define</p><formula xml:id="formula_4">LS − * (v) = LS − * (x) when (v) = x.</formula><p>In the following theorems and lemmas, we assume there is a fixed graph G N , and the validation accuracies are randomly assigned from a distribution defined by pdf n and pdf e . Therefore, the expectations are over the random draws from pdf n and pdf e . 1 Theorem 5.1. Given |A| = n, , s, , pdf n , and pdf e , we have</p><formula xml:id="formula_5">E[|{v ∈ A | LS * (v) = v}|] = n ∞ (v * ) pdf n (x) ∞ x pdf e (x, y)dy s dx, and E[|{v ∈ A | (LS * (v)) − (v * ) ≤ }|] = n (v * )+ (v * ) pdf n (x) ∞ x pdf e (x, y)dy s E[|LS − * (x)|]dx.</formula><p>Proof sketch. To prove the first statement, we introduce an indicator random variable on the architecture space to test if the architecture is a local minimum:</p><formula xml:id="formula_6">I(v) = I{LS * (v) = v}. Then E[|{v ∈ A | LS * (v) = v}|] = n · P({v ∈ A | I(v) = 1}) = n ∞ (v * ) pdf n (x) · P({x &lt; (u)∀u s.t. (u, v) ∈ E N , x = (v)})dx = n ∞ (v * ) pdf n (x) ∞ x pdf e (x, y)dy s dx.</formula><p>Intuitively, in the proof of the second statement, we follow similar reasoning but multiply the probability in the outer integral by the expected size of v's full preimage to weight the integral by the probability a random point will converge to v. Formally, we introduce an indicator random variable on the architecture space that tests if a node will terminate on a local minimum that is within of the global minimum:</p><formula xml:id="formula_7">I (v) = I{LS * (v) = u ∧ l(u) − l(v * ) ≤ } = I{∃S ∈ {LS − * (u) : LS * (u) = u ∧ l(u) − l(v * ) ≤ }, v ∈ S}</formula><p>We use this random variable to prove the second statement of the theorem.</p><formula xml:id="formula_8">E[|{v ∈ A | (LS * (v)) − (v * ) ≤ }|] = n · P({I = 1}) = n (v * )+ (v * ) P({v ∈ A | I(v) = 1, (v) = x})E[|LS − * (x)|]dx = n (v * )+ (v * ) pdf n (x) ∞ (v) pdf e (x, y)dy s E[|LS − * (x)|]dx</formula><p>where the last equality follows from the first half of this theorem.</p><p>In the next lemma, we derive a recursive equation for |LS − * (v)|. We define the branching fraction of</p><formula xml:id="formula_9">graph G N as b k = |N k (v)|/ (|N k−1 (v)| · |N (v)|), where N k (v)</formula><p>denotes the set of nodes which are distance k to v in G N . For example, the branching fraction of a tree with degree d is 1 for all k, and the branching fraction of a clique is b 1 = 1 and b k = 0 for all k &gt; 1. One more example is as follows. In Appendix B, we show that the neighborhood graph of the NASBench-201 search space is (K 5 ) 6 and therefore its branching factor is b k = 6−k+1 6k .</p><p>Lemma 5.2. Given A, , s, pdf n , and pdf e , then for all v ∈ A, we have the following equations.</p><formula xml:id="formula_10">E[|LS −1 (v)|] = s ∞ (v) pdf e ( (v), y) ∞ (v) pdf e (y, z)dz s−1 dy, and (5.1) E[|LS −k (v)|] = b k−1 · E[|LS −1 (v)|] ∞ (v) pdf e ( (v), y)E[|LS −(k−1) (y)|]dy ∞ (v) pdf e ( (v), y)dy . (5.2)</formula><p>For some PDFs, it is not possible to find a closed-form solution for E[|LS −k (v)|] because arbitrary functions may not have closed-form antiderivatives. By assuming there exists a function g such that pdf e (x, y) = g(y) for all x, we can use induction to find a closed-form expression for E[|LS −k (v)|]. This includes the uniform distribution (g(y) = 1 for y ∈ [0, 1]), as well as distributions that are polynomials in x.</p><p>In Appendix A, we use this to show that E[|LS − * (v)|] can be approximated by</p><formula xml:id="formula_11">1 + s · G( (v)) s · e G( (v)) s , where G(x) = ∞</formula><p>x g(y)dy. Now we use a similar technique to give a closed-form expression for Theorem 5.1 when the local and global distributions are uniform.   x dy = (1 − x). We use this in combination with Theorem 5.1 to prove the first statement:</p><formula xml:id="formula_12">Lemma 5.3. If pdf n (x) = pdf e (x, y) = U ([0, 1]) ∀x ∈ A, then E[|{v | v = LS * (v)}|] = n s+1 and E[|{v | (LS * (v)) − (v * ) ≤ }|] = n ∞ i=0   s i 1 − (1 − ) (i+1)s+1 (i + 1)s + 1 · i−1 j=0 b j js + 1   .</formula><formula xml:id="formula_13">E[|{v | v = LS * (v)}|] = n ∞ (v * ) 1 · (1 − x) s dx = n s + 1 .</formula><p>To prove the second statement, first we use induction on the recursive expression in Lemma 5.2 to show that for all v ∈ A,</p><formula xml:id="formula_14">E[|LS − * (v)|] = ∞ k=0 E[|LS −k (v)|] = ∞ k=0 s k (1 − (v)) sk · k−1 i=0 b i is + 1 .</formula><p>We plug this into the second part of Theorem 5.1:</p><formula xml:id="formula_15">E[|{v | (LS * (v)) − (v * ) ≤ }|] = n (v * )+ (v * ) 1 · (1 − x) s ∞ k=0 E[|LS −k (x)|]dx = n (v * )+ (v * ) (1 − x) s ∞ k=0 s k (1 − x) sk · k−1 i=0 b j is + 1 dx = n ∞ i=0   s i 1 − (1 − ) (i+1)s+1 (i + 1)s + 1 · i−1 j=0 b j js + 1   .</formula><p>In the next section, we will show that Theorem 5.1 and Lemma 5.3 can be used to predict the performance of local search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we discuss our experimental setup and results. To promote reproducible research, we discuss how our experiments follow the best practices checklist <ref type="bibr" target="#b18">[19]</ref> in Appendix B, and we release our code at https://github.com/realityengines/local_search. In particular, we run experiments on NAS benchmark datasets, we run enough trials to reach statistical significance, and we release our code and all materials needed to reproduce our results. We start by describing the search spaces used in our experiments.</p><p>NASBench-101 <ref type="bibr" target="#b38">[39]</ref>. The NASBench-101 dataset consists of over 423,000 unique neural architectures from a cell-based search space, and each architecture comes with precomputed validation, and test accuracies for 108 epochs on CIFAR-10. The search space consists of a cell with 7 nodes. The first node is the input, and the last node is the output. The remaining five nodes can be either 1 × 1 convolution, 3 × 3 convolution, or 3 × 3 max pooling. The cell can take on any DAG structure from the input to the output with at most 9 edges.</p><p>NASBench-201 <ref type="bibr" target="#b7">[8]</ref>. The NASBench-201 dataset consists of 5 6 = 15, 625 unique neural architectures, with precomputed training, validation, and test accuracies for 200 epochs on CIFAR-10, CIFAR-100, and ImageNet-16-120. The search space consists of a cell which is a complete directed acyclic graph over 4 nodes. Therefore, there are 4 2 = 6 edges. Each edge takes an operation, and there are five possible operations: 1 × 1 convolution, 3 × 3 convolution, 3 × 3 avg. pooling, skip connect, or none.</p><p>DARTS search space <ref type="bibr" target="#b20">[21]</ref>. The DARTS search space is a popular search space for large-scale cell-based NAS experiments on CIFAR-10. The search space consists of roughly 10 18 architectures. It consists of two cells: a convolutional cell and a reduction cell, each with six nodes. The first two nodes are input from previous layers, and the last four nodes can take on any DAG structure such that each node has degree two. Each edge can take one of eight operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Local search performance</head><p>We evaluate the effectiveness of local search for NAS. First we compare local search to other NAS algorithms on the four benchmark dataset/search space pairs. On the three NASBench-201 datasets, we compare local search to random search, DNGO <ref type="bibr" target="#b30">[31]</ref>, Regularized Evolution <ref type="bibr" target="#b27">[28]</ref>, Bayesian Optimization, BANANAS <ref type="bibr" target="#b35">[36]</ref>, and NASBOT <ref type="bibr" target="#b15">[16]</ref>. On NASBench-101, we test local search with the aforementioned algorithms, as well as REINFORCE <ref type="bibr" target="#b36">[37]</ref> and AlphaX <ref type="bibr" target="#b32">[33]</ref>. For every algorithm, we used the code directly from the corresponding open source repositories and kept the hyperparameters unchanged for almost all algorithms. For more details on the implementations, see Appendix B. We gave each algorithm a budget of 300 queries. For each algorithm, we recorded the test loss of the architecture with the best validation loss that has been queried so far. We ran 200 trials of each algorithm and averaged the results. For local search, we set N (v) to denote all architectures which differ by one operation or edge. If local search converged before its budget, it started a new run. On NASBench-101 and ImageNet-16-120, we used the query_until_lower variant of local search, and on NASBench-201 CIFAR-10 and CIFAR-100, we used the continue_at_min variant. (In Appendix B, we evaluate all four variants.) See <ref type="figure">Figure 6</ref>.3. Local search consistently performs the strongest on all four datasets. Random search performed significantly worse than all other algorithms on NASBench-201, so we omitted it from the plots. Note that on ImageNet16-120, some algorithms such as NASBOT overfit to the training set, causing performance to decline over time.</p><p>In Appendix B, we evaluate local search with a different number k of random initializations, showing that there is little effect on the performance when k ≤ 20. We also report local search statistics such as the number of local minima and the average number of iterations to convergence.</p><p>Next, we evaluate local search with the query_until_lower variant on the DARTS search space. We ran one trial training all queried architecture to 25 epochs, and another trial training to 50 epochs. The runtime is 11.8 GPU days on a Tesla V100. We trained the final returned architectures for 600 epochs, using the same training pipeline as prior work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>. See <ref type="table" target="#tab_0">Table 1</ref>. Local search performed worse than random search, and significantly worse than DARTS. One reason for the subpar performance is because the degree of the neighborhood graph is 136, much larger than NASBench-201's 24. For instance, in the 50 epoch trial, 100 queries and 11.8 GPU days was not sufficient to get through a single iteration of local search even with the query_until_lower variant.</p><p>Discussion. The simple local search algorithm achieved state-of-the-art performance on all four NAS benchmark search space/datasets, beating out several popular NAS algorithms. However, the poor performance on DARTS shows that local search is not efficient on search spaces with high degree. As a consequence, we suggest that existing NAS benchmarks including NASBench-101 and -201 may be too small and/or simple to adequately evaluate NAS algorithms. For example, while NASBench-201 contains 15k architectures, the DARTS search space contains 10 18 architectures, and similarly the search spaces from ENAS <ref type="bibr" target="#b26">[27]</ref> and PNAS <ref type="bibr" target="#b19">[20]</ref> contain 10 11 and 10 14 architectures. Since local search can be implemented in five lines of code, we encourage local search to be used as a benchmark in future work, especially when experimenting on smaller search spaces.   Simulation Results. We run a local search simulation using the equations in Section 5 as a means of experimentally validating our theoretical results with real data (we use NASBench-201). In order to use these equations, first we must approximate the local and global probability density functions of the three datasets in NASBench-201. We start by visualizing the probability density functions of the three datasets See <ref type="figure">Figure 6</ref>.1. We see the most density along the diagonal, meaning that architectures with similar accuracy are more likely to be neighbors. Therefore, we can approximate the PDFs by using the following equation:</p><formula xml:id="formula_16">pdf(u) =    1 σ √ 2π · e − 1 2 ( u−v σ ) 2 · 1 0 1 σ √ 2π · e − 1 2 ( w−v σ ) 2 dw −1 if u ∈ [0, 1], 0 otherwise. (6.1)</formula><p>This is a normal distribution with mean u − v and standard deviation of σ, truncated so that it is a valid PDF in     defined as the autocorrelation of the accuracies of points visited during a random walk on the neighborhood graph <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32]</ref>, and was used to measure locality in NASBench-101 in prior work <ref type="bibr" target="#b38">[39]</ref>. For the full details of how we modeled the datasets in NASBench-201, see Appendix B. Now we use Theorem 5.1 to compute the probability that a randomly drawn architecture will converge to within of the global minimum when running local search. Since there is no closed-form solution for the expression in Lemma 5.2, we compute Theorem 5.1 up to the 5th preimage. We compare this to the experimental results on NASBench-201. We also compare the performance of the NASBench-201 search space with validation losses drawn uniformly at random, to the performance predicted by Lemma 5.3. Finally, we compare the preimage sizes of the architectures in NASBench-201 with randomly drawn validation losses to the sizes predicted in Lemma 5.2. See <ref type="figure">Figure 6</ref>.3. Our theory exactly predicts the performance and the preimage sizes of the uniform random NASBench-201 dataset. On the three image datasets, our theory predicts the performance fairly accurately, but is not perfect due to our assumption that the distribution of accuracies is unimodal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We show that the simplest local search algorithm achieves state-of-the-art results on the most popular existing NAS benchmarks (NASBench-101 and NASBench-201). We also show that it has subpar performance on the DARTS search space, suggesting that the NAS benchmarks may be too simple and/or small to adequately evaluate NAS methods. Since local search is a simple technique that sometimes gives state-of-the-art performance, we encourage local search to be used as a benchmark for NAS in the future, especially for smaller search spaces.</p><p>Motivated by the stark contrast between the performance of local search on NASBench datasets and DARTS, we give a theoretical study which explains the performance of local search for NAS on different search spaces. We define a probabilistic graph optimization framework to study NAS problems, and we give a characterization of the performance of local search for NAS in our framework. In particular, we find that local search performs well on search spaces with high locality and with a neighborhood graph of low degree. Our theoretical results may be of independent interest. We validate our theory with experimental results. Investigating more sophisticated variants of local search for NAS such as Tabu search, simulated annealing, or multi-fidelity local search, are interesting next steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details from Section 5</head><p>In this section, we give details from Section 5. For convenience, we restate all theorems and lemmas here.</p><p>We start by formally defining all measurable spaces in our theoretical framework. Recall that the topology of the search space is fixed and discrete, while the distribution of validation losses for architectures is randomized and continuous. This is because training a neural network is not deterministic; in fact, both NASBench-101 and NASBench-201 include validation and test accuracies for three different random seeds for each architecture, to better simulate real NAS experiments. Therefore, we assume that the validation loss for a trained architecture is sampled from a global probability distribution, and for each architecture, the validation losses of its neighbors are sampled from a local probability distribution.</p><p>Let <ref type="figure">(R, B(R)</ref>) denote a measurable space for the global validation losses induced by the dataset on the architectures, where B(R) is the Borel σ-algebra on R. The distribution for the validation loss of any architecture in the search space is given by pdf n (x)∀x ∈ R.</p><p>Let (R 2 , B(R 2 )) denote a measurable space for validation losses in a neighborhood of an architecture. Let E : R 2 → R denote a random variable mapping the validation losses of two neighboring architectures to the loss of the second architecture, E(x, y) → y. E has a distribution that is characterized by probability density function pdf e (x, y)∀x, y ∈ R. This gives us a probability over the validation loss for a neighboring architecture.</p><p>Every architecture v ∈ A has a loss (v) ∈ R that is sampled from pdf n . For any two neighbors (v, u) ∈ E N , the PDF for the validation loss x of architecture u is given by pdf e ( (v), x). Note that choices for the distribution pdf e are constrained by the fixed topology of the search space, as well as the selected distribution pdf n . Let (A, 2 A ) denote a measurable space over the nodes of the graph.</p><p>For the rest of this section, we fix an arbitrary neighborhood graph G N with vertex set A such that for all v ∈ A, |N (v)| = s, i.e., G N has regular degree s, and we assume that G N is vertex transitive. Each vertex in A is assigned a validation loss according to pdf n and pdf e defined above. The expectations in the following theorem and lemmas are over the random draws from pdf n and pdf e . Theorem 5.1. Given |A| = n, , s, , pdf n , and pdf e , we have</p><formula xml:id="formula_17">E[|{v ∈ A | LS * (v) = v}|] = n ∞ (v * ) pdf n (x) ∞ x pdf e (x, y)dy s dx, and E[|{v ∈ A | (LS * (v)) − (v * ) ≤ }|] = n (v * )+ (v * ) pdf n (x) ∞ x pdf e (x, y)dy s E[|LS − * (x)|]dx.</formula><p>Proof. To prove the first statement, we introduce an indicator random variable on the architecture space to test if the architecture is a local minimum I : A → R, where</p><formula xml:id="formula_18">I(v) = I{LS * (v) = v} = I{ (v) &lt; (u) ∀u s.t. (u, v) ∈ E N }.</formula><p>The expected number of local minima in |A| is equal to |A| times the fraction of nodes in A which are local minima. Therefore, we have</p><formula xml:id="formula_19">E[|{v ∈ A | LS * (v) = v}|] = n · P({I = 1}) = n ∞ −∞ pdf n (x) · P({x &lt; (u)∀u s.t. (u, v) ∈ E N , x = (v)})dx = n ∞ −∞ pdf n (x) ∞ x pdf e (x, y)dy s dx</formula><p>In line one we use the notation P({I = 1}) ≡ P({v ∈ A | I(v) = 1}). To prove the second statement, we introduce an indicator random variable on the architecture space that tests if a node will terminate on a local minimum that is within of the global minimum, I : A → R, where</p><formula xml:id="formula_20">I (v) = I{LS * (v) = u ∧ l(u) − l(v * ) ≤ } = I{∃S ∈ {LS − * (u) : LS * (u) = u ∧ l(u) − l(v * ) ≤ }, v ∈ S}</formula><p>We use this random variable to prove the second statement of the theorem.</p><formula xml:id="formula_21">E[|{v ∈ A | (LS * (v)) − (v * ) ≤ }|] = n · P({I = 1}) = n (v * )+ (v * ) P({v ∈ A | I(v) = 1, (v) = x})E[|LS − * (x)|]dx = n (v * )+ (v * ) pdf n (x) ∞ (v) pdf e (x, y)dy s E[|LS − * (x)|]dx</formula><p>where the last equality follows from the first half of this theorem. This concludes the proof.</p><p>Recall that we defined the branching fraction of graph</p><formula xml:id="formula_22">G N as b k = |N k (v)|/ (|N k−1 (v)| · |N (v)|),</formula><p>where N k (v) denotes the set of nodes which are distance k to v in G N . For example, the branching fraction of a tree with degree d is 1 for all k, and the branching fraction of a clique is b 1 = 1 and b k = 0 for all k &gt; 1. Also, for any graph, b 1 = 1. We will see in Section B that the neighborhood graph of the NASBench-201 search space is (K 5 ) 6 and therefore its branching factor is b k = 6−k+1 6k . Now we restate and prove Lemma 5.2, which gives a formula for the k'th preimage of the local search function.</p><p>Lemma 5.2. Given A, , s, pdf n , and pdf e , then for all v ∈ A, we have the following equations. </p><formula xml:id="formula_23">E[|LS −1 (v)|] = s ∞ (v) pdf e ( (v), y) ∞ (v)</formula><formula xml:id="formula_24">E[|LS −k (v)|] = b k−1 · E[|LS −1 (v)|] ∞ (v) pdf e ( (v), y)E[|LS −(k−1) (y)|]dy ∞ (v) pdf e ( (v), y)dy . (5.2)</formula><p>Proof. The function LS −1 (v) ∈ 2 A returns a set of nodes which form the preimage of node v ∈ A, namely, the set of all neighbors u ∈ N (v) with higher validation loss than v, and whose neighbors w ∈ N (u) excluding v have higher validation loss than (v). Formally,</p><formula xml:id="formula_25">LS −1 (v) = {u ∈ A | LS(u) = v} = {u ∈ A | (v, u) ∈ E N , (v) &lt; (u), {v ∈ A\{v} | (v , u) ∈ E N , (v ) &lt; (v)} = ∅}. Let LS −1 v : A → R denote a random variable where LS −1 v (u) = I{u ∈ LS −1 (v)}.</formula><p>The probability distribution for LS −1 v gives the probability that a neighbor of v is in the preimage of v. We can multiply this probability by |N (v)| = s to express the expected number of nodes in the preimage of v.</p><formula xml:id="formula_26">E[|LS −1 (v)|] = s · P({LS −1 v = 1}) = s ∞ (v) pdf e ( (v), y) ∞ l(v) pdf e (y, z)dz s−1 dy.</formula><p>Note that the inner integral is raised to the power of s − 1, not s, so as not to double count node v. We can use this result to find the preimage of node v after m steps. Let LS −m v : A → R denote a random variable where</p><formula xml:id="formula_27">LS −m v (u) = I{u ∈ LS −m (v)} = I{∀w ∈ LS −1 (v), u ∈ LS −(m−1) (w)}.</formula><p>Following a similar argument as above, we compute the expected size of the m'th preimage set.</p><formula xml:id="formula_28">E[|LS −m (v)|] = b k−1 · E[|LS −1 (v)|] · E[|{∀w ∈ A | ∀u ∈ LS −1 (v), LS −(m−1) u (w) = 1}|] E[|LS −m (v)|] = b k−1 · E[|LS −1 (v)|] ∞ (v) pdf e ( (v), y)E[|LS −(m−1) (y)|]dy ∞ (v) pdf e ( (v), y)dy</formula><p>Closed-form solution for single-variate PDFs. Now we give the details for Lemma 5.3. We start with a lemma that will help us prove Lemma 5.3. This lemma uses induction to derive a closed-form solution to Lemma 5.2 in the case where pdf e (x, y) is independent of x.</p><p>Lemma A.1. Assume there exists a function g such that pdf e (x, y) = g(y) for all x. Given v ∈ A, for k ≥ 1,</p><formula xml:id="formula_29">E[|LS −k (v)|] = s k ∞ (v) g(y)dy sk · k−1 i=0 b i is + 1 . Proof. Given v ∈ A, E[|LS −1 (v)|] = s ∞ (v) pdf e ( (v), y) ∞ (v)</formula><p>pdf e (y, z)dz</p><formula xml:id="formula_30">s−1 dy = s ∞ (v) g(y) ∞ (v) g(z)dz s−1 dy = s ∞ (v) g(y)dy s ,</formula><p>where the first equality follows from Lemma 5.2. Now we give a proof by induction for the closed-form equation. The base case, m = 1, is proven above. Given an integer m ≥ 1, assume that</p><formula xml:id="formula_31">E[|LS −m (v)|] = s m ∞ (v) g(y)dy sn · m−1 i=0 b i is + 1 . Then E[|LS −(m+1) (v)|] = b n · E[|LS −1 (v)|] · ∞ (v) g(y)dy −1 ∞ (v) g(y)E[|LS −m (y)|]dy = b n · s ∞ (v) g(y)dy s ∞ (v) g(y)dy −1 ∞ (v) g(y) · E[|LS −m (y)|]dy = b n · s ∞ (v) g(y)dy s−1 ∞ (v) g(y) · s m ∞ y g(z)dz sn · m−1 i=0 b i is + 1 · dy = b n · s m+1 ∞ (v) g(y)dy s−1 · m−1 i=0 b i is + 1 ∞ (v) g(y) ∞ y g(z)dz sn dy = b n · s m+1 ∞ (v) g(y)dy s−1 · m−1 i=0 b i is + 1 ∞ (v) g(z)dz sn+1 1 sn + 1 = s m+1 ∞ (v) g(y)dy s(m+1) · m i=0 b i is + 1 .</formula><p>In the first equality, we used Lemma 5.2, and in the fourth equality, we used the fact that This concludes the proof.</p><p>Next, we prove a lemma which gives strong approximation guarantees on the size of the full preimage of an architecture, again assuming that pdf e (x, y) is independent if x. For this lemma , we need to assume that n is large compared to s. However, this is the only lemma that assumes n is large. In particular, Lemma 5.3 will not need this assumption.</p><p>Lemma A.2. Assume there exists g such that pdf e (x, y) = g(y) for all x. Denote G(x) = ∞ x g(y)dy. Given s, there exists N such that for all n &gt; N , for all v, we have</p><formula xml:id="formula_32">1 + s · G( (v)) s e s s+1 G( (v)) s ≤ E[|LS − * (v)|] ≤ 1 + s · G( (v)) s · e G( (v)) s . Proof. From Lemma A.1, we have E[|LS − * (v)|] = ∞ m=1 E[|LS −m (v)|] = ∞ m=1 s m G( (v)) sm · m−1 i=0 b i is + 1 . (A.1)</formula><p>We start with the upper bound. For all j ≥ 1,</p><formula xml:id="formula_33">s·b j js+1 ≤ s js+1 ≤ s sj = 1 j because 0 ≤ b j ≤ 1 for all 1 ≤ j. Therefore for all i, i−1 j=1 s · b j js + 1 ≤ i−1 j=1 1 j = 1 (i − 1)! .</formula><p>It follows that</p><formula xml:id="formula_34">E[|LS −m (v)|] = ∞ i=1 s i G( (v)) si i−1 j=0 b j js + 1 = s ∞ i=1 G( (v)) s i−1 j=1 s · b j js + 1 ≤ sG( (v)) s ∞ i=1 1 (i − 1)! · G( (v)) s = sG( (v)) s e G( (v)) s .</formula><p>The final equality comes from the well-known Taylor series e x = ∞ n=0</p><p>x n n! (e.g. <ref type="bibr" target="#b1">[2]</ref>) evaluated at x = G( (v)) s . Now we prove the lower bound. b 1 = 1 by definition for all graphs, and for 1 &lt; j ≤ D, 0 ≤ b j ≤ 1, where D denotes the diameter of the graph. (Since N D (v) = n for all v, b j is meaningless for j ≥ D.) Recall that all of our arguments assume vertex transitivity. It follows that b D−1 ≤ b D−2 ≤ · · · ≤ b 1 . Now, for a fixed s, b D−1 approaches 1 as n approaches infinity. Therefore, given s, there exists N such that for all n &gt; N , b D−1 &gt; 2s+1 2(s+1) . Then for all i,</p><formula xml:id="formula_35">1 j(s + 1) ≤ 1 js + 1 js + 1 j(s + 1) ≤ 1 js + 1 2s + 1 2(s + 1) ≤ 1 js + 1 (b D−1 ) ≤ b j js + 1 .</formula><p>Therefore,</p><formula xml:id="formula_36">s i−1 (i − 1)!(s + 1) i−1 = i−1 j=1 s j(s + 1) ≤ i−1 j=1 s · b j js + 1 .</formula><p>It follows that</p><formula xml:id="formula_37">E[|LS − * (v)|] = ∞ i=1 s i G( (v)) si i−1 j=0 b j js + 1 = s ∞ i=1 G( (v)) si i−1 j=1 s · b j js + 1 ≥ sG( (v)) s ∞ i=1 1 (i − 1)! · sG( (v)) s s + 1 i−1 = sG( (v)) s · e s s+1 G( (v)) s .</formula><p>The final equality again comes from the Taylor series e x , this time evaluated at x = s s+1 · G( (v)) s .</p><p>Note that Equation A.1 does not require the assumption that n is large. Now we can use Equation A.1 and Theorem 5.1 to prove Lemma 5.3. </p><formula xml:id="formula_38">Lemma 5.3. If pdf n (x) = pdf e (x, y) = U ([0, 1]) ∀x ∈ A, then E[|{v | v = LS * (v)}|] = n s+1 and E[|{v | (LS * (v)) − (v * ) ≤ }|] = n ∞ i=0   s i 1 − (1 − ) (i+1)s+1 (i + 1)s + 1 · i−1 j=0 b j js + 1   .</formula><formula xml:id="formula_39">E[|{v | (LS * (v)) − (v * ) ≤ }|] = n (v * )+ (v * ) 1 · (1 − x) s ∞ k=0 E[|LS −k (x)|]dx = n (v * )+ (v * ) (1 − x) s ∞ k=0 s k (1 − x) sk · k−1 i=0 b i is + 1 dx = n ∞ k=0 s k 1 − (1 − ) (k+1)s+1 (k + 1)s + 1 · k−1 i=0 b i is + 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details from Section 6</head><p>In this section, we give details and supplementary results for Section 6. We start by giving full details for all search spaces used in our experiments.</p><p>NASBench-101 <ref type="bibr" target="#b38">[39]</ref>. The NASBench-101 benchmark dataset consists of over 423,000 unique neural architectures from a cell-based search space, and each architecture comes with precomputed validation, and test accuracies for 108 epochs on CIFAR-10. The search space consists of a cell with 7 nodes. The first node is the input, and the last node is the output. The remaining five nodes can be either 1 × 1 convolution, 3 × 3 convolution, or 3 × 3 max pooling. The cell can take on any DAG structure from the input to the output with at most 9 edges. The hyper-architecture consists of nine cells stacked sequentially, with each set of three cells separated by downsampling layers. The first layer before the first cell is a convolutional layer, and the hyper-architecture ends with a global average pooling layer and a fully connected layer.</p><p>The neighbors of a cell consist of the set of all cells that differ by one operation or edge. Since each cell can have between 1 and 9 edges, the number of neighbors of a cell can range from 1 to 29.</p><p>NASBench-201 <ref type="bibr" target="#b7">[8]</ref>. The NASBench-201 dataset consists of over 15, 000 unique neural architectures, with precomputed training, validation, and test accuracies for 200 epochs on CIFAR-10, CIFAR-100, and ImageNet-16-120.</p><p>The search space consists of a cell which is a complete directed acyclic graph over 4 nodes. Therefore, there are 4 2 = 6 edges. Each edge takes an operation, and there are five possible operations: 1 × 1 convolution, 3 × 3 convolution, 3 × 3 avg. pooling, skip connect, or none. The hyper-architecture consists of 15 cells stacked sequentially, with each set of five cells separated by residual blocks. The first layer before the first cell is a convolution layer, and the hyper-architecture ends with a global average pooling layer and a fully connected layer.</p><p>Since every cell has exactly 6 edges, the total number of possible cells is 5 6 = 15625. We say that two cells are neighbors if they differ by exactly one operation. Then the diameter of the neighborhood graph is 6, because any cell can reach any other cell by swapping out all 6 of its operations. Each cell has exactly 24 neighbors, because there are 6 edges, and each edge has 4 other choices for an operation. The neighborhood graph is (K 5 ) 6 , that is, the Cartesian product of six cliques of size five. DARTS search space <ref type="bibr" target="#b20">[21]</ref>. The DARTS search space is a popular search space for large-scale NAS experiments. It is a convolutional cell-based search space used for CIFAR-10. The search space consists of two cells, a convolutional cell and a reduction cell, each with six nodes. The hyper-architecture stacks k convolutional cells together with one reduction cell. For each cell, the first two nodes are the outputs from the previous two cells in the hyper-architecture. The next four nodes contain two edges as input, creating a DAG. Each edge can take on one of seven operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Details and additional local search experiments</head><p>In this section, we give details and additional experiments for local search on the datasets described above.</p><p>For every benchmark NAS algorithm, we used the code directly from its corresponding open-source repository. For regularized evolution, we changed the population size from 50 to 30 to account for fewer queries. For NASBOT, which was designed for macro (non cell-based) NAS, we computed the distance between two cells by taking the earth-mover's distance between the set of row sums, column sums, and node operations, similar to <ref type="bibr" target="#b35">[36]</ref>. We did not change any hyperparameters for the other baseline algorithms. For vanilla Bayesian optimization, we used the ProBO implementation <ref type="bibr" target="#b23">[24]</ref>. Our experimental setup is the same as prior work (e.g., <ref type="bibr" target="#b38">[39]</ref>). At each timestep t, we report the test error of the architecture with the best validation error found so far, and we run 200 trials of each algorithm and average the result.  First, we compute local search statistics for each of the datasets in NASBench-201. The experimental setup is the same as in Section 6. We also construct a randomized dataset by replacing the validation error for each architecture in NASBench-201 with a number drawn from U ([0, 1]). For the three image datasets, we ran standard local search as well as the query_until_lower variant. For each experiment, we started local search from all 15625 initial seeds for local search, and averaged the results. See <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Now we evaluate all four variants of local search on all three NASBench-201 datasets. We use the same experimental setup as described in Section 6. See <ref type="figure">Figure B</ref>.1. See Section 5 for an explanation of the query_until_lower and continue_at_min variants to local search. Now we evaluate the performance of local search as a function of the number of initial random architectures drawn at the beginning. We run local search with the number of initial random architectures set to 1, and 10 to 100 in increments of 10. For each number of initial random architectures, we ran 2000 trials and averaged the results. See <ref type="figure">Figure B</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Details from simulation experiments</head><p>In this section, we give more details for our simulation experiment described in Section 6.</p><p>For convenience, we restate Equation 6.1, the function used to approximate the datasets in NASBench-201.</p><formula xml:id="formula_40">pdf(u) =    1 σ √ 2π · e − 1 2 ( u−v σ ) 2 · 1 0 1 σ √ 2π · e − 1 2 ( w−v σ ) 2 dw −1 if u ∈ [0, 1], 0 otherwise.</formula><p>This is a normal distribution with mean u − v and standard deviation of σ, truncated so that it is a valid PDF  in [0, 1]. For a visualization, see <ref type="figure">Figure B</ref>.3. In order to choose an appropriate probability density function for modelling the datasets in NASBench-201, we approximate the σ values for both the local and global PDFs.</p><p>To model the global PDF for each dataset, we plot a histogram of the validation losses and match them to the closest-fitting values of σ and v. See <ref type="figure">Figure 6</ref>.2 in Section 6. The best values are σ = 0.18, 0.1, and 0.22 for CIFAR-10, CIFAR-100, and ImageNet16-120, respectively. Now we plot the random-walk autocorrelation (RWA) described in Section 6. Recall that RWA is defined as the autocorrelation of the accuracies of points visited during a walk of random single changes through the search space <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32]</ref>, and was used to measure locality in NASBench-101 in prior work <ref type="bibr" target="#b38">[39]</ref>. We compute the RWA for all three datasets in NASBench-201, by performing a random walk of length 100,000. See <ref type="figure">Figure B</ref>.4. We see that all three datasets in NASBench-201, as evidenced because there is a high correlation at distances close to 0. As the diameter of NASBench-201 is 6, the correlation approaches zero at distances beyond about 3.5. In order to model the local pdfs of each dataset, we also compute the RWA for Equation 6.1, and match each dataset with the closest value of σ. We see that a value of σ = 0.35 is the closest match for all three datasets. Now for each of the three NASBench-201 datasets, we have estimates for the pdf e and pdf n distributions. We plug each (pdf e , pdf n ) pair into Theorem 5.1, which gives a plot of vs. percent of architectures that  <ref type="figure">Figure B</ref>.4: RWA vs. distance for three datasets in NASBench-201, as well as three values of σ in Equation 6.1. Since a random walk reaches a mean distance of √ N after N steps, we plot the x-axis as the square root as the autocorrelation shift, similar to prior work <ref type="bibr" target="#b38">[39]</ref>.</p><p>converge to within of the global optimum after running local search. We compare these to the true plot in <ref type="figure">Figure 6</ref>.3. For the random simulation, we are modeling the case where pdf e = pdf n = U ([0, 1]), so we can use Lemma 5.3 directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Best practices for NAS research</head><p>The area of NAS research has had issues with reproducibility and fairness in empirical comparisons <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>, and there is now a checklist for best practices <ref type="bibr" target="#b18">[19]</ref>. In order to promote best practices, we discuss each point on the list, and encourage all NAS research papers to do the same.</p><p>• Releasing code. Our code is publicly available at https://github.com/realityengines/local_search. The training pipelines and search spaces are from popular existing NAS work: NASBench-101, NASBench-201, and DARTS. One thing that is missing is the set of random seeds we used for the DARTS experiments.</p><p>• Comparing NAS methods. We made fair comparisons due to our use of NASBench-101 and NASBench-201. For baseline comparisons, we used open-source code, a few times adjusting hyperparameters to be more appropriate for the search space. We ran ablation studies, compared to random search, and compared performance over time. We performed 200 trials on tabular benchmarks.</p><p>• Reporting important details. Local search only has two boolean hyperparameters, so we did not need to tune hyperparameters. We reported the times for the full NAS method and all details for our experimental setup.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proof sketch. The probability density function of U ([0, 1]) is equal to 1 on [0, 1] and 0 otherwise. Then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>∞x</head><label></label><figDesc>pdf e (x, y)dy =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 . 1 :</head><label>61</label><figDesc>Probability density function for CIFAR-10 (left), CIFAR-100 (middle), and ImageNet16-120 (right) on NASBench-201. For each coordinate (u, v), a darker color indicates that architectures with accuracy u and v are more likely to be neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>[0, 1]. To model the global PDF for each dataset, we plot a histogram of the validation losses and match them to the closest-fitting values of σ and v. See Figure 6.2. The best values of σ are 0.18, 0.1, and 0.22 for CIFAR-10, CIFAR-100, and ImageNet16-120, respectively, and the best values for v are all 0.25. 2 To model the local PDF for each dataset, we compute the random walk autocorrelation (RWA) on each dataset. RWA is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 . 2 :</head><label>62</label><figDesc>Histogram of validation losses for the three datasets in NASBench-201, fitted with the best values of σ and v in Equation 6.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 . 3 :</head><label>63</label><figDesc>Results for NAS algorithms on NASBench-201 (top) and NASBench-101 (bottom left). Probability that local search will converge to within of the global optimum, compared to Theorem 5.1 (bottom middle). Validation loss vs. size of preimages, compared to Lemma 5.2 (bottom right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Proof. 1 x+ 1 .</head><label>11</label><figDesc>The probability density function of U ([0, 1]) is equal to 1 on [0, 1] and 0 otherwise. Let (v) = x. Then ∞ x pdf e (x, y)dy = dy = (1 − x). Using Theorem 5.1, we have E[|{v | v = LS * (v)}|] = n ∞ (v * ) 1 · (1 − x) s dx = n s Now we plug in Equation A.1 to the second part of Theorem 5.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure B. 1 :</head><label>1</label><figDesc>Results for local search variants on CIFAR-10 (left), CIFAR-100 (middle), and ImageNet-16-120 (right) on NASBench-201.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure B. 2 :Figure B. 3 :</head><label>23</label><figDesc>Results for local search performance vs. number of inital randomly drawn architectures on NASBench-201 for CIFAR-10 (left), CIFAR-100 (middle), and ImageNet-16-120 (right). Normal PDF from Equation 6.1 plotted with three values of v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Percent error on the test set of the best architectures returned by several NAS algorithms. The runtime is in total GPU-days on a Tesla V100.</figDesc><table><row><cell>NAS Algorithm</cell><cell>Source</cell><cell>Test error</cell><cell>Queries</cell><cell>Runtime</cell></row><row><cell>Random search</cell><cell>[21]</cell><cell>3.29</cell><cell></cell><cell>4</cell></row><row><cell>DARTS</cell><cell>[21]</cell><cell>2.68</cell><cell></cell><cell>5</cell></row><row><cell>ASHA</cell><cell>[18]</cell><cell>3.08</cell><cell>700</cell><cell>9</cell></row><row><cell>BANANAS</cell><cell>[36]</cell><cell>2.64</cell><cell>100</cell><cell>11.8</cell></row><row><cell>Local Search 50 epochs</cell><cell>Ours</cell><cell>3.49</cell><cell>100</cell><cell>11.8</cell></row><row><cell>Local Search 25 epochs</cell><cell>Ours</cell><cell>3.93</cell><cell>200</cell><cell>11.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of local search for NASBench-201 datasets. Dataset query_until_lower Avg. path length # local min. % reached global min.</figDesc><table><row><cell cols="3">CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell cols="2">No</cell><cell></cell><cell></cell><cell></cell><cell cols="2">5.36</cell><cell></cell><cell></cell><cell>21</cell><cell></cell><cell></cell><cell></cell><cell cols="2">47.4</cell></row><row><cell cols="3">CIFAR-100</cell><cell></cell><cell></cell><cell></cell><cell cols="2">No</cell><cell></cell><cell></cell><cell></cell><cell cols="2">5.59</cell><cell></cell><cell></cell><cell>29</cell><cell></cell><cell></cell><cell></cell><cell cols="2">58.5</cell></row><row><cell cols="5">ImageNet-16-120</cell><cell></cell><cell cols="2">No</cell><cell></cell><cell></cell><cell></cell><cell cols="2">4.67</cell><cell></cell><cell></cell><cell>47</cell><cell></cell><cell></cell><cell></cell><cell cols="2">10.9</cell></row><row><cell cols="2">Random</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">No</cell><cell></cell><cell></cell><cell></cell><cell cols="2">2.56</cell><cell></cell><cell cols="2">616</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.717</cell></row><row><cell cols="3">CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Yes</cell><cell></cell><cell></cell><cell></cell><cell cols="2">9.59</cell><cell></cell><cell></cell><cell>21</cell><cell></cell><cell></cell><cell></cell><cell cols="2">59.8</cell></row><row><cell cols="3">CIFAR-100</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Yes</cell><cell></cell><cell></cell><cell></cell><cell cols="2">7.31</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">100.0</cell></row><row><cell cols="5">ImageNet-16-120</cell><cell></cell><cell cols="2">Yes</cell><cell></cell><cell></cell><cell></cell><cell cols="2">13.33</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">100.0</cell></row><row><cell>test error of best neural net</cell><cell>5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4</cell><cell></cell><cell></cell><cell cols="4">CIFAR-10 continue_at_min, query_until_lower query_until_lower continue_at_min Original</cell><cell>test error of best neural net</cell><cell>27.0 27.5 28.0 28.5 29.0</cell><cell></cell><cell></cell><cell cols="4">CIFAR-100 continue_at_min, query_until_lower query_until_lower continue_at_min Original</cell><cell>test error of best neural net</cell><cell>53.75 54.00 54.25 54.50 54.75 55.00 55.25</cell><cell></cell><cell cols="3">ImageNet-16-120 continue_at_min, query_until_lower query_until_lower continue_at_min Original</cell></row><row><cell></cell><cell>5.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>53.50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50 time in TPU hours 60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell></cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50 time in TPU hours 60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell></cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50 time in TPU hours 60</cell><cell>70</cell><cell>80</cell><cell>90</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In particular, given a node v with validation loss (v) the probability distribution for the validation loss of a neighbor depends only on (v) and pdf e , which makes the local search procedure similar to a Markov process.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that plotting a histogram of all validation losses is impractical for real-world NAS search spaces; we do this on NASBench-201 as a means of validating our theoretical results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Willie Neiswanger and Murali Narayanaswamy for their help with this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Local search in combinatorial optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aarts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lenstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Handbook of mathematical functions with formulas, graphs, and mathematical tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milton</forename><surname>Abramowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><forename type="middle">A</forename><surname>Stegun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
			<biblScope unit="volume">55</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">K-center clustering under perturbation resilience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nika</forename><surname>Haghtalab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Algorithms</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast algorithms for geometric traveling salesman problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon Jouis</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ORSA Journal on computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="387" to="411" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An algorithm for solving travelling-salesman and related network optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Operations Research</title>
		<imprint>
			<date type="published" when="1958" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="897" to="897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Local search yields approximation schemes for k-means and k-median in euclidean and minor-free metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Cohen-Addad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mathieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS)</title>
		<meeting>the Annual Symposium on Foundations of Computer Science (FOCS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A method for solving traveling-salesman problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Georges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Croes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="791" to="812" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Simple and efficient architecture search for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04528</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural architecture search: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local search yields a ptas for k-means in doubling metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Friggstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">R</forename><surname>Salavatipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="452" to="480" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">It&apos;s time to do something: Mitigating the negative impacts of computing through a change to the peer review process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Schöning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">De</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lana</forename><surname>Russis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bushra</forename><surname>Yarosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Anjum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathy</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Future of Computing Blog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Auto-keras: Efficient neural architecture search with network morphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10282</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The traveling salesman problem: A case study in local optimization. Local search in combinatorial optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcgeoch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="215" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How easy is local search?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><forename type="middle">H</forename><surname>David S Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihalis</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer and system sciences</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="100" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2016" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An efficient heuristic procedure for partitioning graphs. The Bell system technical journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Kernighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="291" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02453</idno>
		<title level="m">Best practices for scientific research on neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Evolutionaryneural hybrid agents for architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Khorlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09828</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Theoretical aspects of local search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wil</forename><surname>Michiels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Aarts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Korst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Probo: a framework for using probabilistic programming in bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11515</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Local search is a remarkably strong baseline for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>T Den Ottelander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dushatskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Virgolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bosman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08996</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An approach to the scheduling of jobs on machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Es Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="484" to="492" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Amoebanet: An sdn-enabled network service for big data science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raza</forename><surname>Syed Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenji</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajith</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Sasidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Demar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Guok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Macauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Pouyoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="70" to="82" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-objective neural architecture search via predictive network performance optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09336</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable bayesian optimization using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mr</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2171" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Landscapes and their correlation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical chemistry</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Alphax: exploring neural architectures with deep neural networks and monte carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuu</forename><surname>Jinnai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07440</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Correlated and uncorrelated fitness landscapes and how to tell the difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00848</idno>
		<title level="m">Neural predictor for neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Bananas: Bayesian optimization with neural architectures for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Savani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11858</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nas evaluation is frustratingly hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperança</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09635</idno>
		<title level="m">Nas-bench-101: Towards reproducible neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
