<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Multilingual Named Entity Recognition with Shallow Semi-Supervised Features * †</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-01-31">31 Jan 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Agerri</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IXA NLP Group</orgName>
								<orgName type="institution">University of the Basque Country (UPV/EHU) Donostia-San Sebastián</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IXA NLP Group</orgName>
								<orgName type="institution">University of the Basque Country (UPV/EHU) Donostia-San Sebastián</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Multilingual Named Entity Recognition with Shallow Semi-Supervised Features * †</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-01-31">31 Jan 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.artint.2016.05.003</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Named Entity Recognition</term>
					<term>Information Extraction</term>
					<term>Word representations</term>
					<term>Semi- supervised learning</term>
					<term>Natural Language Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a multilingual Named Entity Recognition approach based on a robust and general set of features across languages and datasets. Our system combines shallow local information with clustering semi-supervised features induced on large amounts of unlabeled text. Understanding via empirical experimentation how to effectively combine various types of clustering features allows us to seamlessly export our system to other datasets and languages. The result is a simple but highly competitive system which obtains state of the art results across five languages and twelve datasets. The results are reported on standard shared task evaluation data such as CoNLL for English, Spanish and Dutch. Furthermore, and despite the lack of linguistically motivated features, we also report best results for languages such as Basque and German. In addition, we demonstrate that our method also obtains very competitive results even when the amount of supervised data is cut by half, alleviating the dependency on manually annotated data. Finally, the results show that our emphasis on clustering features is crucial to develop robust out-of-domain models. The system and models are freely available to facilitate its use and guarantee the reproducibility of results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A named entity can be mentioned using a great variety of surface forms (Barack Obama, President Obama, Mr. Obama, B. Obama, etc.) and the same surface form can refer to a variety of named entities. For example, according to the English Wikipedia, the form 'Europe' can ambiguously be used to refer to 18 different entities, including the continent, the European Union, various Greek mythological entities, a rock band, some music albums, a magazine, a short story, etc. 1 Furthermore, it is possible to refer to a named entity by means of anaphoric pronouns and coreferent expressions such as 'he <ref type="bibr">', 'her', 'their', 'I', '</ref>the 35 year old', etc. Therefore, in order to * Please cite this paper as: R. Agerri, G. Rigau. Robust multilingual Named Entity Recognition with shallow semisupervised features, Artificial Intelligence <ref type="bibr">(2016)</ref> provide an adequate and comprehensive account of named entities in text it is necessary to recognize the mention of a named entity and to classify it by a pre-defined type (e.g, person, location, organization). Named Entity Recognition and Classification (NERC) is usually a required step to perform Named Entity Disambiguation (NED), namely to link 'Europe' to the right Wikipedia article, and to resolve every form of mentioning or co-referring to the same entity. Nowadays NERC systems are widely being used in research for tasks such as Coreference Resolution <ref type="bibr" target="#b50">(Pradhan et al., 2012)</ref>, Named Entity Disambiguation <ref type="bibr" target="#b18">(Cucerzan, 2007;</ref><ref type="bibr" target="#b26">Han and Sun, 2011;</ref><ref type="bibr" target="#b28">Hoffart et al., 2011;</ref><ref type="bibr" target="#b37">Mendes et al., 2011;</ref><ref type="bibr" target="#b25">Hachey et al., 2013)</ref> for which a lot of interest has been created by the TAC KBP shared tasks <ref type="bibr" target="#b31">(Ji and Grishman, 2011)</ref>, Machine Translation <ref type="bibr" target="#b2">(Al-Onaizan and Knight, 2002;</ref><ref type="bibr" target="#b32">Koehn et al., 2007;</ref><ref type="bibr" target="#b4">Babych and Hartley, 2003;</ref><ref type="bibr" target="#b34">Li et al., 2013)</ref>, Aspect Based Sentiment Analysis <ref type="bibr" target="#b36">(Liu, 2012;</ref><ref type="bibr" target="#b10">Cambria et al., 2013;</ref><ref type="bibr" target="#b49">Pontiki et al., 2014</ref><ref type="bibr" target="#b48">Pontiki et al., , 2015</ref>, Event Extraction <ref type="bibr" target="#b21">(Doddington et al., 2004;</ref><ref type="bibr" target="#b1">Ahn, 2006;</ref><ref type="bibr" target="#b30">Ji and Grishman, 2008;</ref><ref type="bibr" target="#b19">Cybulska and Vossen, 2013;</ref><ref type="bibr" target="#b29">Hong et al., 2011)</ref> and Event Ordering <ref type="bibr" target="#b40">(Minard et al., 2015)</ref>.</p><p>Moreover, NERC systems are integrated in the processing chain of many industrial software applications, mostly by companies offering specific solutions for a particular industrial sector which require recognizing named entities specific of their domain. There is therefore a clear interest in both academic research and industry to develop robust and efficient NERC systems: For industrial vendors it is particularly important to diversify their services by including NLP technology for a variety of languages whereas in academic research NERC is one of the foundations of many other NLP end-tasks.</p><p>Most NERC taggers are supervised statistical systems that extract patterns and term features which are considered to be indications of Named Entity (NE) types using the manually annotated training data (extracting orthographic, linguistic and other types of evidence) and often external knowledge resources. As in other NLP tasks, supervised statistical NERC systems are more robust and obtain better performance on available evaluation sets, although sometimes the statistical models can also be combined with specific rules for some NE types. For best performance, supervised statistical approaches require manually annotated training data, which is both expensive and time-consuming. This has seriously hindered the development of robust high performing NERC systems for many languages but also for other domains and text genres <ref type="bibr" target="#b44">(Nobata et al., 2000;</ref><ref type="bibr" target="#b53">Ritter et al., 2011)</ref>, in what we will henceforth call 'out-of-domain' evaluations.</p><p>Moreover, supervised NERC systems often require fine-tuning for each language and, as some of the features require language-specific knowledge, this poses yet an extra complication for the development of robust multilingual NERC systems. For example, it is well-known that in German every noun is capitalized and that compounds including named entities are pervasive. This also applies to agglutinative languages such as Basque, Korean, Finnish, Japanese, Hungarian or Turkish. For this type of languages, it had usually been assumed that linguistic features (typically Part of Speech (POS) and lemmas, but also semantic features based on WordNet, for example) and perhaps specific hand-crafted rules, were a necessary condition for good NERC performance as they would allow to capture better the most recurrent declensions (cases) of named entities for Basque <ref type="bibr" target="#b3">(Alegria et al., 2006)</ref> or to address problems such as sparsity and capitalization of every noun for German <ref type="bibr" target="#b22">(Faruqui et al., 2010;</ref><ref type="bibr" target="#b6">Benikova et al., 2014</ref><ref type="bibr" target="#b7">Benikova et al., , 2015</ref>. This language dependency was easy to see in the CoNLL 2002 and 2003 tasks, in which systems participating in the two available languages for each edition obtained in general different results for each language. This suggests that without fine-tuning for each corpus and language, the systems did not generalize well across languages .</p><p>This paper presents a multilingual and robust NERC system based on simple, general and shallow features that heavily relies on word representation features for high performance. Even though we do not use linguistic motivated features, our approach also works well for inflected languages such as Basque and German. We demonstrate the robustness of our approach by reporting best results for five languages (Basque, Dutch, German, English and Spanish) on 12 different datasets, including seven in-domain and eight out-of-domain evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>The main contributions of this paper are the following: First, we show how to easily develop robust NERC systems across datasets and languages with minimal human intervention, even for languages with declension and/or complex morphology. Second, we empirically show how to effectively use various types of simple word representation features thereby providing a clear methodology for choosing and combining them. Third, we demonstrate that our system still obtains very competitive results even when the supervised data is reduced by half (even less in some cases), alleviating the dependency on costly hand annotated data. These three main contributions are based on: 1. A simple and shallow robust set of features across languages and datasets, even in out-ofdomain evaluations.</p><p>2. The lack of linguistic motivated features, even for languages with agglutinative (e.g., Basque) and/or complex morphology (e.g., German).</p><p>3. A clear methodology for using and combining various types of word representation features by leveraging public unlabeled data.</p><p>Our approach consists of shallow local features complemented by three types of word representation (clustering) features: Brown clusters <ref type="bibr" target="#b9">(Brown et al., 1992)</ref>, Clark clusters <ref type="bibr" target="#b14">(Clark, 2003)</ref> and K-means clusters on top of the word vectors obtained by using the Skip-gram algorithm <ref type="bibr" target="#b38">(Mikolov et al., 2013)</ref>. We demonstrate that combining and stacking different clustering features induced from various data sources (Reuters, Wikipedia, Gigaword, etc.) allows to cover different and more varied types of named entities without manual feature tuning. Even though our approach is much simpler than most, we obtain the best results for Dutch, Spanish and English and comparable results in German <ref type="bibr">(on CoNLL 2002 and</ref><ref type="bibr" target="#b14">2003)</ref>. We also report best results for German using the GermEval 2014 shared task data and for Basque using the Egunkaria testset <ref type="bibr" target="#b3">(Alegria et al., 2006)</ref>.</p><p>We report out-of-domain evaluations in three languages (Dutch, English and Spanish) using four different datasets to compare our system with the best publicly available systems for those languages: Illinois NER <ref type="bibr" target="#b51">(Ratinov and Roth, 2009)</ref> for English, Stanford NER <ref type="bibr" target="#b23">(Finkel et al., 2005)</ref> for English and Spanish, SONAR-1 NERD for Dutch <ref type="bibr" target="#b20">(Desmet and Hoste, 2014)</ref> and Freeling for Spanish <ref type="bibr" target="#b46">(Padró and Stanilovsky, 2012)</ref>. We outperform every other system in the eight out-ofdomain evaluations reported in Section 4.3. Furthermore, the out-of-domain results show that our clustering features provide a simple and easy method to improve the robustness of NERC systems.</p><p>Finally, and inspired by previous work <ref type="bibr" target="#b33">(Koo et al., 2008;</ref><ref type="bibr" target="#b8">Biemann, 2009)</ref> we measure how much supervision is required to obtain state of the art results. In Section 4.2 we show that we can still obtain very competitive results reducing the supervised data by half (and sometimes even more). This, together with the lack of linguistic features, means that our system considerably saves data annotation costs, which is quite convenient when trying to develop a NERC system for a new language and/or domain.</p><p>Our system learns Perceptron models <ref type="bibr" target="#b16">(Collins, 2002)</ref> using the Machine Learning machinery provided by the Apache OpenNLP project 2 with our own customized (local and clustering) features.</p><p>Our NERC system is publicly available and distributed under the Apache 2.0 License and part of the IXA pipes tools <ref type="bibr" target="#b0">(Agerri et al., 2014)</ref>. Every result reported in this paper is obtained using the conlleval script from the CoNLL 2002 and CoNLL 2003 shared tasks 3 . To guarantee reproducibility of results we also make publicly available the models and the scripts used to perform the evaluations. The system, models and evaluation scripts can be found in the ixa-pipe-nerc website 4 .</p><p>Next Section reviews related work, focusing on best performing NERC systems for each language evaluated on standard shared evaluation task data. Section 3 presents the design of our system and our overall approach to NERC. In Section 4 we report the evaluation results obtained by our system for 5 languages (Basque, Dutch, German, English and Spanish) on 12 different datasets, distributed in 7 in-domain and 8 out-of-domain evaluations. Section 5 discusses the results and contributions of our approach. In Section 6 we highlight the main aspects of our work providing some concluding remarks and future work to be done using our NERC approach applied to other text genres, domains and sequence labeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The Named Entity Recognition and Classification (NERC) task was first defined for the Sixth Message Understanding Conference (MUC 6) <ref type="bibr" target="#b43">(Nadeau and Sekine, 2007)</ref>. The MUC 6 tasks focused on Information Extraction (IE) from unstructured text and NERC was deemed to be an important IE sub-task with the aim of recognizing and classifying nominal mentions of persons, organizations and locations, and also numeric expressions of dates, money, percentage and time. In the following years, research on NERC increased as it was considered to be a crucial source of information for other Natural Language Processing tasks such as Question Answering (QA) and Textual Entailment (RTE) <ref type="bibr" target="#b43">(Nadeau and Sekine, 2007)</ref>. Furthermore, while MUC 6 was solely devoted to English as target language, the CoNLL shared tasks <ref type="bibr">(2002 and 2003)</ref> boosted research on language independent NERC for 3 additional target languages: Dutch, German and Spanish <ref type="bibr" target="#b56">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b57">Tjong Kim Sang and De Meulder, 2003)</ref>.</p><p>The various MUC, ACE and CoNLL evaluations provided a very convenient framework to test and compare NERC systems, algorithms and approaches. They provided manually annotated data for training and testing the systems as well as an objective evaluation methodology. Using such framework, research rapidly evolved from rule-based approaches (consisting of manually handcrafted rules) to language independent systems focused on learning supervised statistical models. Thus, while in the MUC 6 competition 5 out of 8 systems were rule-based, in CoNLL 2003 16 teams participated in the English task all using statistical-based NERC <ref type="bibr" target="#b43">(Nadeau and Sekine, 2007)</ref>. <ref type="table" target="#tab_2">Table 1</ref> describes the 12 datasets used in this paper. The first half lists the corpora used for indomain evaluation whereas the lower half contains the out-of-domain datasets. The CoNLL NER shared tasks focused on language independent machine learning approaches for 4 entity types: person, location, organization and miscellaneous entities. The 2002 edition provided manually annotated data in Dutch and Spanish whereas in 2003 the languages were German and English. In addition to the CoNLL data, for English we also use the formal run of MUC 7 and Wikigold for out-of-domain evaluation. Very detailed descriptions of CoNLL and MUC data can easily be found in the literature, including the shared task descriptions themselves <ref type="bibr" target="#b12">(Chinchor and Marsh, 1998;</ref><ref type="bibr" target="#b56">Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b57">Tjong Kim Sang and De Meulder, 2003)</ref>, so in the following we will describe the remaining, newer datasets.   . For Spanish and Dutch, we also use Ancora 2.0 <ref type="bibr" target="#b55">(Taulé et al., 2008)</ref> and SONAR-1 <ref type="bibr" target="#b20">(Desmet and Hoste, 2014)</ref> respectively. SONAR-1 is a one million word Dutch corpus with both coarse-grained and fine-grained named entity annotations. The coarse-grained level includes product and event entity types in addition to the four types defined in CoNLL data. Ancora adds date and number types to the CoNLL four main types. In Basque the only gold standard corpus is Egunkaria <ref type="bibr" target="#b3">(Alegria et al., 2006)</ref>. Although the Basque Egunkaria dataset is annotated with four entity types, the miscellaneous class is extremely sparse, occurring only in a proportion of 1 to 10. Thus, in the training data there are 156 entities annotated as MISC whereas each of the other three classes contain around 1200 entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>In the datasets described so far, named entities were assumed to be non-recursive and nonoverlapping. During the annotation process, if a named entity was embedded in a longer one, then only the longest mention was annotated. The exceptions are the GermEval 2014 shared task data for German and MEANTIME, where nested entities are also annotated (both inner and outer spans).</p><p>The GermEval 2014 NER shared task <ref type="bibr" target="#b6">(Benikova et al., 2014)</ref> aimed at improving the state of the art of German NERC which was perceived to be comparatively lower than the English NERC. Two main extensions were introduced in GermEval 2014; (i) fine grained named entity sub-types to indicate derivations and compounds; (ii) embedded entities (and not only the longest span) are annotated. In total, there are 12 types for classification: person, location, organization, other plus their sub-types annotated at their inner and outer levels.</p><p>Finally, the MEANTIME corpus <ref type="bibr" target="#b41">(Minard et al., 2016</ref>) is a multilingual (Dutch, English, Italian and Spanish) publicly available evaluation set annotated within the Newsreader project 5 . It consists of 120 documents, divided into 4 topics: Apple Inc., Airbus and Boeing, General Motors, Chrysler and Ford, and the stock market. The articles are selected in such a way that the corpus contains different articles that deal with the same topic over time (e.g. launch of a new product, discussion of the same financial indexes). Moreover, it contains nested entities so the evaluation results will be provided in terms of the outer and the inner spans of the named entities. MEANTIME includes six named entity types: person, location, organization, product, financial and mixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Approaches</head><p>Named entity recognition is a task with a long history in NLP. Therefore, we will summarize those approaches that are most relevant to our work, especially those we will directly compared with in Section 4. Since CoNLL shared tasks, the most competitive approaches have been supervised systems learning CRF, SVM, Maximum Entropy or Averaged Perceptron models. In any case, while the machine learning method is important, it has also been demonstrated that good performance might largely be due to the feature set used <ref type="bibr" target="#b15">(Clark and Curran, 2003)</ref>. <ref type="table">Table 2</ref> provides an overview of the features used by previous best scoring approaches for each of the five languages we address in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Local Ling Global Gaz WR Rules Ensemble Public Res <ref type="bibr" target="#b51">Ratinov and Roth 2009</ref><ref type="bibr" target="#b47">Passos et al. 2014</ref><ref type="bibr">ExB Faruqui et al. 2010</ref><ref type="bibr" target="#b11">Carreras et al. 2002</ref><ref type="bibr" target="#b15">Clark and Curran 2003</ref><ref type="bibr">Sonar nerd Alegria et al. 2006</ref> ixa-pipe-nerc <ref type="table">Table 2</ref>: Features of best previous in-domain results. Local: shallow local features including capitalization, word shape, etc.; Ling: linguistic features such as POS, lemma, chunks and semantic information from Wordnet; Global: global features; Gaz: gazetteers; WR: word representation features; Rules: manually encoded rules; Ensemble: stack of classifiers or ensemble system; Public: if the system is publicly distributed. Res: If any external resources used are publicly distributed to allow re-training.</p><p>Traditionally, local features have included contextual and orthographic information, affixes, character-based features, prediction history, etc. As argued by the CoNLL 2003 organizers, no feature set was deemed to be ideal for NERC <ref type="bibr" target="#b57">(Tjong Kim Sang and De Meulder, 2003)</ref>, although many approaches for English refer to <ref type="bibr" target="#b60">Zhang and Johnson (2003)</ref> as a useful general approach.</p><p>Linguistic Information Some of the CoNLL participants use linguistic information (POS, lemmas, chunks, but also specific rules or patterns) for Dutch and English <ref type="bibr" target="#b11">(Carreras et al., 2002;</ref><ref type="bibr" target="#b15">Clark and Curran, 2003)</ref>, although these type of features was deemed to be most important for German, for which the use of linguistic features is pervasive <ref type="bibr" target="#b6">(Benikova et al., 2014)</ref>. This is caused by the sparsity caused by the declension cases, the tendency to form compounds containing named entities and by the capitalization of every noun <ref type="bibr" target="#b22">(Faruqui et al., 2010)</ref>. For example, the best system among the 11 participants in GermEval 2014, ExB, uses morphological features and specific suffix lists aimed at capturing frequent patterns in the endings of named entities <ref type="bibr" target="#b27">(Hänig et al., 2014)</ref>.</p><p>In agglutinative languages such as Basque, which contains declension cases for named entities, linguistic features are considered to be a requirement. For example, the country name 'Espainia' (Spain in Basque) can occur in several forms, Espainian, Espainiera, Espainiak, Espainiarentzat, Espainiako, and many more. 6 Linguistic information has been used to treat this phenomenon. The only previous work for Basque developed Eihera, a rule-based NERC system formalized as finite state transducers to take into account declension classes <ref type="bibr" target="#b3">(Alegria et al., 2006)</ref>. The features of Eihera include word, lemma, POS, declension case, capitalized lemma, etc. These features are complemented with gazetteers extracted from the Euskaldunon Egunkaria newspaper and semantic information from the Basque WordNet.</p><p>Gazetteers Dictionaries are widely used to inject world knowledge via gazetteer matches as features in machine learning approaches to NERC. The best performing systems carefully compile their own gazetteers from a variety of sources <ref type="bibr" target="#b11">(Carreras et al., 2002)</ref>. <ref type="bibr" target="#b51">Ratinov and Roth (2009)</ref> leverage a collection of 30 gazetteers and matches against each one are weighted as a separate feature. In this way they trust each gazetteer to a different degree. <ref type="bibr" target="#b47">Passos et al. (2014)</ref> carefully compiled a large collection of English gazetteers extracted from US Census data and Wikipedia and applied them to the process of inducing word embeddings with very good results.</p><p>While it is possible to automatically extract them from various corpora or resources, they still require careful manual inspection of the target data. Thus, our approach only uses off the shelf gazetteers whenever they are publicly available. Furthermore, our method collapses every gazetteer into one dictionary. This means that we only add a feature per token, instead of a feature per token and gazetteer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Features</head><p>The intuition behind non-local (or global) features is to treat similarly all occurrences of the same named entity in a text. <ref type="bibr" target="#b11">Carreras et al. (2002)</ref> proposed a method to produce the set of named entities for the whole sentence, where the optimal set of named entities for the sentence is the coherent set of named entities which maximizes the summation of confidences of the named entities in the set. <ref type="bibr" target="#b51">Ratinov and Roth (2009)</ref> developed three types of non-local features, analyzing global dependencies in a window of between 200 and 1000 tokens.</p><p>Word representations Semi-supervised approaches leveraging unlabeled text had already been applied to improve results in various NLP tasks. More specifically, it had been previously shown how to apply Brown clusters <ref type="bibr" target="#b9">(Brown et al., 1992)</ref> for Chinese Word Segmentation <ref type="bibr" target="#b35">(Liang, 2005)</ref>, dependency parsing <ref type="bibr" target="#b33">(Koo et al., 2008)</ref>, NERC <ref type="bibr" target="#b54">(Suzuki and Isozaki, 2008)</ref> and POS tagging <ref type="bibr" target="#b8">(Biemann, 2009</ref>). <ref type="bibr" target="#b51">Ratinov and Roth (2009)</ref>   <ref type="bibr" target="#b17">(Collobert and Weston, 2008)</ref> and HLBL embeddings <ref type="bibr" target="#b42">(Mnih and Hinton, 2007)</ref> to improve chunking and NERC. They show that in some cases the combination of word representation features was positive but, although they used <ref type="bibr" target="#b51">Ratinov and Roth's (2009)</ref> system as starting point, they did not manage to improve over the state of the art. Furthermore, they reported that Brown clustering features performed better than the word embeddings. <ref type="bibr" target="#b47">Passos et al. (2014)</ref> extend the Skip-gram algorithm to learn 50-dimensional lexicon infused phrase embeddings from 22 different gazetteers and the Wikipedia. The resulting embeddings are used as features by scaling them by a hyper-parameter which is a real number tuned on the development data. <ref type="bibr" target="#b47">Passos et al. (2014)</ref>  The best German CoNLL 2003 system (an ensemble) was outperformed by <ref type="bibr" target="#b22">Faruqui et al. (2010)</ref>. They trained the Stanford NER system <ref type="bibr" target="#b23">(Finkel et al., 2005)</ref>, which uses a linear-chain Conditional Random Field (CRF) with a variety of features, including lemma, POS tag, etc. Crucially, they included "distributional similarity" features in the form of Clark clusters <ref type="bibr" target="#b14">(Clark, 2003)</ref> induced from large unlabeled corpora: the Huge German Corpus (HGC) of around 175M tokens of newspaper text and the deWac corpus <ref type="bibr" target="#b5">(Baroni et al., 2009</ref>) consisting of 1.71B tokens of webcrawled data. Using the clusters induced from deWac as a form of semi-supervision improved the results over the best CoNLL 2003 system by 4 points in F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble Systems</head><p>The best participant of the English CoNLL 2003 shared task used the results of two externally trained NERC taggers to create an ensemble system <ref type="bibr" target="#b24">(Florian et al., 2003)</ref>. <ref type="bibr" target="#b47">Passos et al. (2014)</ref> develop a stacked linear-chain CRF system: they train two CRFs with roughly the same features; the second CRF can condition on the predictions made by the first CRF. Their "baseline" system uses a similar local featureset as <ref type="bibr" target="#b51">Ratinov and Roth's (2009)</ref> but complemented with gazetteers. Their baseline system combined with their phrase embeddings trained with infused lexicons allow them to report the best CoNLL 2003 result so far.</p><p>The best system of the GermEval 2014 task built an ensemble of classifiers and pattern extractors to find the most likely tag sequence <ref type="bibr" target="#b27">(Hänig et al., 2014)</ref>. They paid special attention to out of vocabulary words which are addressed by semi-supervised word representation features and an ensemble of POS taggers. Furthermore, remaining unknown candidate mentions are tackled by look-up via the Wikipedia API.</p><p>Apart from the feature types, the last two columns of <ref type="table">Table 2</ref> refer to whether the systems are publicly available and whether any external resources used for training are made available (e.g., induced word embeddings, gazetteers or corpora). This is desirable to be able to re-train the systems on different datasets. For example, we would have been interested in training the Stanford NER system with the full Ancora corpus for the evaluation presented in <ref type="table" target="#tab_2">Table 16</ref>, but their Spanish cluster lexicon is not available. Alternatively, we would have liked to train our system with the same Ancora partition used to train Stanford NER, but that is not available either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Description</head><p>The design of ixa-pipe-nerc aims at establishing a simple and shallow feature set, avoiding any linguistic motivated features, with the objective of removing any reliance on costly extra gold annotations (POS tags, lemmas, syntax, semantics) and/or cascading errors if automatic language processors are used. The underlying motivation is to obtain robust models to facilitate the development of NERC systems for other languages and datasets/domains while obtaining state of the art results. Our system consists of:</p><p>• Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context.</p><p>• Three types of simple clustering features, based on unigram matching.</p><p>• Publicly available gazetteers, widely used in previous NERC systems <ref type="bibr" target="#b57">(Tjong Kim Sang and De Meulder, 2003;</ref><ref type="bibr" target="#b43">Nadeau and Sekine, 2007)</ref>. <ref type="table" target="#tab_6">Table 3</ref> provides an example of the features generated by our system 7 .</p><p>7 To avoid too much repetition, Brown, Trigram and character n-gram features have been abbreviated.</p><p>Feature  </p><formula xml:id="formula_0">w i−2 w i−1 w i w i+1 w</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local Features</head><p>The local features constitute our baseline system on top of which the clustering features are added. We implement the following feature set, partially inspired by previous work <ref type="bibr" target="#b60">(Zhang and Johnson, 2003)</ref>:</p><p>• Token: Current lowercase token (w), namely, ekuadorko in <ref type="table" target="#tab_6">Table 3</ref>.</p><p>• Token Shape: Current lowercase token (w) plus current token shape (wc), where token shape consist of: (i) The token is either lowercase or a 2 digit word or a 4 digit word; (ii) If the token contains digits, then whether it also contains letters, or slashes, or hyphens, or commas, or periods or is numeric; (iii) The token is all uppercase letters or is an acronym or is a one letter uppercase word or starts with capital letter. Thus, in <ref type="table" target="#tab_6">Table 3</ref> 1994an is a 4 digit word (4d), Ekuadorko has an initial capital shape (ic) and hiriburuan is lowercase (lc).</p><p>• Previous prediction: the previous outcome (pd) for the current token. The previous predictions in our example are null because these words have not been seen previously, except for the comma.</p><p>• Sentence: Whether the token is the beginning of the sentence. None of the tokens in our example is at the beginning of the sentence, so this feature is not active in <ref type="table" target="#tab_6">Table 3</ref>.</p><p>• Prefix: Two prefixes consisting of the first three and four characters of the current token: Eku and Ekua.</p><p>• Suffix: The four suffixes of length one to four from the last four characters of the current token.</p><p>• Bigram: Bigrams including the current token and the token shape.</p><p>• Trigram: Trigrams including the current token and the token shape.</p><p>• Character n-gram: All lowercase character bigrams, trigrams, fourgrams and fivegrams from the current token (ng).</p><p>Token, token shape and previous prediction features are placed in a 5 token window, namely, for these these three features we also consider the previous and the next two words, as shown in <ref type="table" target="#tab_6">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gazetteers</head><p>We add gazetteers to our system only if they are readily available to use, but our approach does not fundamentally depend upon them. We perform a look-up in a gazetteer to check if a named entity occurs in the sentence. The result of the look-up is represented with the same encoding chosen for the training process, namely, the BIO or BILOU scheme 8 . Thus, for the current token we add the following features:</p><p>1. The current named entity class in the encoding schema. Thus, in the BILOU encoding we would have "unit", "beginning", "last", "inside", or if not match is found, "outside", combined with the specific named entity type (LOC, ORG, PER, MISC, etc.).</p><p>2. The current named entity class as above and the current token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Clustering Features</head><p>The general idea is that by using some type of semantic similarity or word cluster induced over large unlabeled corpora it is possible to improve the predictions for unseen words in the test set. This type of semi-supervised learning may be aimed at improving performance over a fixed amount of training data or, given a fixed target performance level, to establish how much supervised data is actually required to reach such performance <ref type="bibr" target="#b33">(Koo et al., 2008)</ref>. So far the most successful approaches have only used one type of word representation <ref type="bibr" target="#b47">(Passos et al., 2014;</ref><ref type="bibr" target="#b22">Faruqui et al., 2010;</ref><ref type="bibr" target="#b51">Ratinov and Roth, 2009</ref>). However, our simple baseline combined with one type of word representation features are not able to compete with previous, more complex, systems. Thus, instead of encoding more elaborate features, we have devised a simple method to combine and stack various types of clustering features induced over different data sources or corpora. In principle, our method can be used with any type of word representations. However, for comparison purposes, we decided to use word representations previously used in successful NERC approaches: Brown clusters <ref type="bibr" target="#b51">(Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b59">Turian et al., 2010)</ref>, Word2vec clusters <ref type="bibr" target="#b47">(Passos et al., 2014)</ref> and Clark clusters <ref type="bibr" target="#b23">(Finkel et al., 2005;</ref><ref type="bibr" target="#b22">Faruqui et al., 2010)</ref>. As can be observed in <ref type="table" target="#tab_6">Table 3</ref>, our clustering features are placed in a 5 token window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Brown Features</head><p>The Brown clustering algorithm <ref type="bibr" target="#b9">(Brown et al., 1992)</ref> is a hierarchical algorithm which clusters words to maximize the mutual information of bigrams. Thus, it is a class-based bigram model in which:</p><p>• The probability of a document corresponds to the product of the probabilities of its bigrams, <ref type="bibr">8</ref> The BIO scheme suggests to learn models that identify the Beginning, the Inside and the Outside of sequences. The BILOU scheme proposes to learn models Beginning, the Inside and the Last tokens of multi-token chunks as well as Unit-length chunks.</p><p>• the probability of each bigram is calculated by multiplying the probability of a bigram model over latent classes by the probability of each class generating the actual word types in the bigram, and</p><p>• each word type has non-zero probability only on a single class.</p><p>The Brown algorithm takes a vocabulary of words to be clustered and a corpus of text containing these words. It starts by assigning each word in the vocabulary to its own separate cluster, then iteratively merges the pair of clusters which leads to the smallest decrease in the likelihood of the text corpus. This produces a hierarchical clustering of the words, which is usually represented as a binary tree, as shown in <ref type="figure">Figure 1</ref>. In this tree every word is uniquely identified by its path from the root, and the path can be represented by a bit string. It is also possible to choose different levels of word abstraction by choosing different depths along the path from the root to the word. Therefore, by using paths of various lengths, we obtain clustering features of different granularities <ref type="bibr" target="#b39">(Miller et al., 2004)</ref>. We use paths of length 4, 6, 10 and 20 as features <ref type="bibr" target="#b51">(Ratinov and Roth, 2009</ref>). However, we introduce several novelties in the design of our Brown clustering features:</p><p>1. For each feature which is token-based, we add a feature containing the paths computed for the current token. Thus, taking into account our baseline system, we will add the following Brown clustering features:</p><p>(a) Brown Token: existing paths of length 4, 6, 10 and 20 for the current token.</p><p>(b) Brown Token Shape: existing paths of length 4, 6, 10, 20 for the current token and current token shape.</p><p>(c) Brown Bigram: existing paths of length 4, 6, 10, 20 for bigrams including the current token.</p><p>2. Brown clustering features benefit from two additional features:</p><p>(a) Previous prediction plus token: the previous prediction (pd) for the current token and the current token.</p><p>(b) Previous two predictions: the previous prediction for the current and the previous token.</p><p>For space reasons, <ref type="table" target="#tab_6">Table 3</ref> only shows the Brown Token (bt) and Brown Token Shape (c) features for paths of length 4 and 6. We use the publicly available tool 9 implemented by <ref type="bibr" target="#b35">Liang (2005)</ref> with default settings. The input consists of a corpus tokenized and segmented one sentence per line, without punctuation. Furthermore, we follow previous work and remove all sentences which consist of less than 90% lowercase characters <ref type="bibr" target="#b35">(Liang, 2005;</ref><ref type="bibr" target="#b59">Turian et al., 2010)</ref> before inducing the Brown clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Clark Features</head><p>Clark <ref type="formula">(2003)</ref> presents a number of unsupervised algorithms, based on distributional and morphological information, for clustering words into classes from unlabeled text. The focus is on clustering infrequent words on a small numbers of clusters from comparatively small amounts of data. In particular, <ref type="bibr" target="#b14">Clark (2003)</ref> presents an algorithm combining distributional information with morphological information of words "by composing the Ney-Essen clustering model with a model for the morphology within a Bayesian framework". The objective is to bias the distributional information to put words that are morphologically similar in the same cluster. We use the code released by <ref type="bibr" target="#b14">Clark (2003)</ref> off the shelf 10 to induce Clark clusters using the Ney-Essen with morphological information method. The input of the algorithm is a sequence of lowercase tokens without punctuation, one token per line with sentence breaks.</p><p>Our Clark clustering features are very simple: we perform a look-up of the current token in the clustering lexicon. If a match is found, we add as a feature the clustering class, or the lack of match if the token is not found (see <ref type="bibr">Clark-a and Clark-b in</ref>  <ref type="table" target="#tab_6">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Word2vec Features</head><p>Another family of language models that produces word representations are the neural language models. These approaches produce representation of words as continuous vectors <ref type="bibr" target="#b17">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b42">Mnih and Hinton, 2007)</ref>, also called word embeddings. Nowadays, perhaps the most popular among them is the Skip-gram algorithm <ref type="bibr" target="#b38">(Mikolov et al., 2013)</ref>. The Skip-gram algorithm uses shallow log-linear models to compute vector representation of words which are more efficient than previous word representations induced on neural language models. Their objective is to produce word embeddings by computing the probability of each n-gram as the product of the conditional probabilities of each context word in the n-gram conditioned on its central word <ref type="bibr" target="#b38">(Mikolov et al., 2013)</ref>.</p><p>Instead of using continuous vectors as real numbers, we induce clusters or word classes from the word vectors by applying K-means clustering. In this way we can use the cluster classes as simple binary features by injecting unigram match features. We use the Word2vec tool 11 released by <ref type="bibr" target="#b38">Mikolov et al. (2013)</ref> with a 5 window context to train 50-dimensional word embeddings and to obtain the word clusters on top of them. The input of the algorithm is a corpus tokenized, lowercased, with punctuation removed and in one line. The Word2vec features are implemented exactly like the Clark features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Stacking and Combining Clustering Features</head><p>We successfully combine clustering features from different word representations. Furthermore, we also stack or accumulate features of the same type of word representation induced from different data sources, trusting each clustering lexicon to a different degree, as shown by the five encoded clustering features in <ref type="table" target="#tab_6">Table 3</ref>: two Clark and Word2vec features from different source data and one Brown feature. When using word representations as semi-supervised features for a task like NERC, two principal factors need to be taken into account: (i) the source data or corpus used to induce the word representations and (ii) the actual word representation used to encode our features which in turn modify the weight of our model's parameters in the training process.</p><p>For the clustering features to be effective the induced clusters need to contain as many words appearing in the training, development and test sets as possible. This can be achieved by using corpora closely related to the text genre or domain of the data sets or by using very large unlabeled corpora which, although not closely domain-related, be large enough to include many relevant words. For example, with respect to the CoNLL 2003 English dataset an example of the former would be the Reuters corpus while the Wikipedia would be an example of the latter.</p><p>The word representations obtained by different algorithms would capture different distributional properties of words in a given corpus or data source. Therefore, each type of clustering would allow us to capture different types of occurring named entity types. In other words, combining and stacking different types of clustering features induced over a variety of data sources should help to capture more similarities between different words in the training and test sets, increasing the contribution to the weights of the model parameters in the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this Section we report on the experiments performed with the ixa-pipe-nerc system as described in the previous section. The experiments are performed in 5 languages: Basque, Dutch, English, German and Spanish. For comparison purposes, in-domain results are presented in Section 4.1 using the most common NERC datasets for each language as summarized in <ref type="table" target="#tab_2">Table 1</ref>  The results for Dutch, English and Spanish do not include trigrams and character n-grams in the local featureset described in Section 3.1, except for the models in each in-domain evaluation which are marked with "charngram 1:6".</p><p>We also experiment with dictionary features but, in contrast to previous approaches such as <ref type="bibr" target="#b47">Passos et al. (2014)</ref>, we only use currently available gazetteers off-the-shelf. For every model marked with "dict" we use the thirty English Illinois NER gazetteers <ref type="bibr" target="#b51">(Ratinov and Roth, 2009</ref>), irrespective of the target language. Additionally, the English models use six gazetteers about the Global Automotive Industry provided by LexisNexis to the Newsreader project 12 , whereas the German models include, in addition to the Illinois gazetteers, the German dictionaries distributed in the CoNLL 2003 shared task. The gazetteers are collapsed into one large dictionary and deployed as described in Section 3.2.</p><p>Finally, the clustering features are obtained by processing the following clusters from publicly available corpora: (i) 1000 Brown clusters; (ii) Clark and Word2vec clusters in the 100-600 range. To choose the best combination of clustering features we test the available permutations of Clark and Word2vec clusters with and without the Brown clusters on the development data. <ref type="table" target="#tab_8">Table 4</ref> provides details of every corpus used to induce the clusters. For example, the first row reads: "Reuters RCV1 was used; the original 63 million words were reduced to 35 million after preprocessing for inducing Brown clusters. Clark and Word2vec clusters were trained on the whole corpus". The pre-processing and tokenization is performed with the IXA pipes tools <ref type="bibr" target="#b0">(Agerri et al., 2014)</ref>.</p><p>Every evaluation is carried out using the CoNLL NER evaluation script 13 . The results are obtained with the BILOU encoding for every experimental setting except for German CoNLL 2003.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">In-domain evaluation</head><p>In this section the results are presented by language. In two cases, Dutch and German, we use two different datasets, making it a total of seven in-domain evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">English</head><p>We tested our system in the highly competitive CoNLL 2003 dataset. <ref type="table" target="#tab_10">Table 5</ref> shows that three of our models outperform previous best results reported for English in the CoNLL 2003 dataset <ref type="bibr" target="#b47">(Passos et al., 2014)</ref>. Note that the best F1 score (91.36) is obtained by adding trigrams and character n-gram features to the best model (91.18). The results also show that these models improve the baseline provided by the local features by around 7 points in F1 score. The most significant gain is in terms of recall, almost 9 points better than the baseline.</p><p>We also report very competitive results, only marginally lower than <ref type="bibr" target="#b47">Passos et al. (2014)</ref>, based on the stacking and combination of clustering features as described in Section 3.3.4. Thus, both best cluster and comp models, based on local plus clustering features only, outperform very competitive and more complex systems such as those of <ref type="bibr" target="#b51">Ratinov and Roth (2009)</ref> and <ref type="bibr" target="#b59">Turian et al. (2010)</ref>, and obtain only marginally lower results than <ref type="bibr" target="#b47">Passos et al. (2014)</ref>. The stacking and combining effect manifests itself very clearly when we compare the single clustering feature models (BR, CW600, W2VG200 and W2VW400) with the light, comp and best cluster models which improve the overall F1 score by 1.30, 1.72 and 1.85 respectively over the best single clustering model (CW600).</p><p>It is worth mentioning that our models do not score best in the development data. As the development data is closer in style and genre to the training data <ref type="bibr" target="#b51">(Ratinov and Roth, 2009</ref>), this may suggest that our system generalizes better on test data that is not close to the training data; indeed, the results reported in Section 4.3 seem to confirm this hypothesis.  We also compared our results with respect to the best two publicly available English NER systems trained on the same data. We downloaded the Stanford NER system distributed in the 2015-01-30 package. We evaluated their CoNLL model and, while the result is substantially better than their reference paper <ref type="bibr" target="#b23">(Finkel et al., 2005)</ref>, our clustering models obtain better results. The Illinois NER tagger is used by <ref type="bibr" target="#b51">Ratinov and Roth (2009)</ref> and <ref type="bibr" target="#b59">Turian et al. (2010)</ref>, both of which are outperformed by our system.  We tested our system in the GermEval 2014 dataset. <ref type="table" target="#tab_12">Table 6</ref> compares our results with the best two systems (ExB and UKP) by means of the M3 metric, which separately analyzes the performance in terms of the outer and inner named entity spans. <ref type="table" target="#tab_12">Table 6</ref> makes explicit the significant improvements achieved by the clustering features on top of the baseline system, particularly in terms of recall (almost 11 points in the outer level). The official results of our best configuration (de-cluster-dict) are reported in <ref type="table" target="#tab_14">Table 7</ref> showing that our system marginally improves the best systems' results on that task (ExB and UKP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">German</head><p>We also compare our system, in the last three rows, with the publicly available GermaNER <ref type="bibr" target="#b7">(Benikova et al., 2015)</ref>, which reports results for the 4 main outer level entity types <ref type="bibr">(person, location, organization and other)</ref>. For this experiment we trained the de-cluster and de-cluster + dict models on the four main classes, improving GermaNER's results by almost 3 F1 points. The GermaNER method of evaluation is interesting because allows researchers to directly compare their systems with a publicly available system trained on GermEval data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Precision     <ref type="bibr" target="#b22">(Faruqui et al., 2010)</ref> using public data. <ref type="bibr" target="#b22">Faruqui et al. (2010)</ref> also report 78.20 F1 with a model trained with Clark clusters induced using the Huge German Corpus (HGC). Unfortunately, the corpus or the induced clusters were not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Spanish</head><p>The best system up to date on the CoNLL 2002 dataset, originally published by <ref type="bibr" target="#b11">Carreras et al. (2002)</ref>, is distributed as part of the Freeling library <ref type="bibr" target="#b46">(Padró and Stanilovsky, 2012)</ref>. <ref type="table" target="#tab_18">Table 9</ref> lists four models that improve over their reported results, almost by 3 points in F1 measure in the case of the es-cluster model (with our without trigram and character n-gram features).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Dutch</head><p>Despite using clusters from one data source only (see <ref type="table" target="#tab_8">Table 4</ref>), results in <ref type="table" target="#tab_2">Table 10</ref> show that our nl-cluster model outperforms the best result published on CoNLL 2002 <ref type="bibr" target="#b15">(Clark and Curran, 2003)</ref> by 3.83 points in F1 score. Adding the English Illinois NER gazetteers <ref type="bibr" target="#b51">(Ratinov and Roth, 2009)</ref> and trigram and character n-gram features increases the score to 85.04 F1, 5.41 points better than previous published work on this dataset.  We also compared our system with the more recently developed SONAR-1 corpus and the companion NERD system distributed inside its release <ref type="bibr" target="#b20">(Desmet and Hoste, 2014)</ref>. They report 84.91 F1 for the six main named entity types via 10-fold cross validation. For this comparison we chose the local, nl-cluster and nl-cluster-dict configurations from <ref type="table" target="#tab_2">Table 10</ref> and run them on SONAR-1 using the same settings. The results reported in <ref type="table" target="#tab_2">Table 11</ref>   <ref type="table" target="#tab_2">Table 11</ref>: SONAR-1 10-fold cross validation results. <ref type="table" target="#tab_2">Table 12</ref> reports on the experiments using the Egunkaria NER dataset provided by <ref type="bibr" target="#b3">Alegria et al. (2006)</ref>. Due to the sparsity of the MISC class mentioned in Section 2.1, we decided to train our models on three classes only (location, organization and person). Thus, the results are obtained training our models in the customary manner and evaluating on 3 classes. However, for direct comparison with previous work <ref type="bibr" target="#b3">(Alegria et al., 2006)</ref>, we also evaluate our best eu-cluster model (trained on 3 classes) on 4 classes. The results show that our eu-cluster model clearly improves upon previous work by 4 points in F1 measure (75.40 vs 71.35). These results are particularly interesting as it had been so far assumed that complex linguistic features and language-specific rules were required to perform well for agglutinative languages such as Basque <ref type="bibr" target="#b3">(Alegria et al., 2006)</ref>. Finally, it is worth noting that the eu-cluster model increases the overall F1 score by 11.72 over the baseline, of which 10 points are gained in precision and 13 in terms of recall.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Basque</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reducing training data</head><p>So far, we have seen how, given a fixed amount of supervised training data, leveraging unlabeled data using multiple cluster sources helped to obtain state of the art results in seven different indomain settings for five languages. In this section we will investigate to what extent our system allows to reduce the dependency on supervised training data. We first use the English CoNLL 2003 dataset for this experiment. The training set consists of around 204K words and we use various smaller versions of it to test the performance of our best cluster model reported in <ref type="table" target="#tab_10">Table 5</ref>. <ref type="table" target="#tab_2">Table 13</ref> displays the F1 results of the baseline system consisting of local features and the best cluster model. The ∆ column refers to the gains of our best cluster model with respect to the baseline model for every portion of the training set.  While we have already commented the substantial gains obtained simply by adding our clustering features, it is also interesting to note that the gains are much substantial when less supervised training data is available. Furthermore, it is striking that training our clustering features using only one eight of the training data (30K words) allows to obtain similar performance to the baseline system trained on the full training set. Equally interesting is the fact that cutting by half the training data only marginally harms the overall performance. Finally, training on just a quarter of the training set (60K) results in a very competitive model when compared with other publicly available NER systems for English trained on the full training set: it roughly matches Stanford NER's performance, it outperforms models using external knowledge or non-local features reported by <ref type="bibr" target="#b51">Ratinov and Roth (2009)</ref>, and also several models reported by <ref type="bibr" target="#b59">Turian et al. (2010)</ref>, which use one type of word representations on top of the baseline system.</p><p>We have also re-trained the Illinois NER system <ref type="bibr" target="#b51">(Ratinov and Roth, 2009</ref>) and our best CoNLL 2003 model (en-91-18 ) for comparison. First, we can observe that for every portion of the training set, both our best cluster and en-91-18 model outperform the Illinois NER system. The best cluster results are noteworthy because, as opposed to Illinois NER, it does not use gazetteers or global features for extra performance.</p><p>These results are mirrored by those obtained for the rest of the languages and datasets. Thus, <ref type="table" target="#tab_2">Table 14</ref> displays, for each language, the F1 results of the baseline system and of the best cluster  models on top of the baseline. 14 Overall, it confirms that our cluster-based models obtain state of the art results using just one half of the data. Furthermore, using just one quarter of the training data we are able to match results of other publicly available systems for every language, outperforming in some cases, such as Basque, much complex systems of classifiers exploiting linguistic specific rules and features (POS tags, lemmas, semantic information from WordNet, etc.). Considering that Basque is a low-resourced language, it is particularly relevant to be able to reduce as much as possible the amount of gold supervised data required to develop a competitive NERC system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Out-of-domain evaluations</head><p>NERC systems are often used in out-of-domain settings, namely, to annotate data that greatly differs from the data from which the NERC models were learned. These differences can be of text genre and/or domain, but also because the assumptions of what constitutes a named entity might differ. It is therefore interesting to develop robust NERC systems across both domains and datasets. In this section we demonstrate that our approach, consisting of basic, general local features and the combination and stacking of clusters, produces robust NERC systems in three out-of-domain evaluation settings:</p><p>• Class disagreements: Named entities are assigned to different classes in training and test.</p><p>• Different text genre: The text genre of training and test data differs.</p><p>• Annotation guidelines: The gold annotation of the test data follows different guidelines from the training data. This is usually reflected in different named entity spans.</p><p>The datasets and languages chosen for these experiments are based on the availability of both previous results and publicly distributed NERC systems to facilitate direct comparison of our system with other approaches.  14 The Basque dataset was far too small to train models with 1/16 of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Class Disagreements</head><p>MUC 7 annotates seven entity types, including four that are not included in CoNLL data: DATE, MONEY, NUMBER and TIME entities. Furthermore, CoNLL includes the MISC class, which was absent in MUC 7. This means that there are class disagreements in the gold standard annotation between the training and test datasets. In addition to the four CoNLL classes, SONAR-1 includes PRODUCT and EVENT whereas Ancora also annotates DATE and NUMBER. For example, consider the following sentence of the MUC 7 gold standard (example taken from <ref type="bibr" target="#b51">Ratinov and Roth (2009)</ref>):</p><p>"...baloon, called the Virgin Global Challenger."</p><p>The gold annotation in MUC 7 establishes that there is one named entity: In this setting some adjustments are made to the NERC systems' output. Following previous work <ref type="bibr" target="#b51">(Ratinov and Roth, 2009)</ref>, every named entity that is not LOC, ORG, PER or MISC is labeled as 'O'. Additionally for MUC 7 every MISC named entity is changed to 'O'. For English we used the models reported in Section 4.1.1. For Spanish and Dutch we trained our system with the Ancora and SONAR-1 corpora using the configurations described in Sections 4.1.3 and 4.1.4 respectively. <ref type="table" target="#tab_2">Table 16</ref> compares our results with previous approaches: using MUC 7, <ref type="bibr" target="#b59">Turian et al. (2010)</ref> provide standard phrase results whereas <ref type="bibr" target="#b51">Ratinov and Roth (2009)</ref> score token based F1 results, namely, each token is considered a chunk, instead of considering multi-token spans too. For Spanish we use the Stanford NER Spanish model (2015-01-30 version) trained with Ancora. For Dutch we compare our SONAR-1 system with the companion system distributed with the SONAR-1 corpus <ref type="bibr" target="#b20">(Desmet and Hoste, 2014)</ref>. The results are summarized in <ref type="table" target="#tab_2">Table 16</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Text Genre</head><p>In this setting the out-of-domain character is given by the differences in text genre between the English CoNLL 2003 set and the Wikigold corpus. We compare our system with English models trained on large amounts of silver-standard text (3.5M tokens) automatically created from the Wikipedia . They report results on Wikigold showing that they outperformed their own CoNLL 2003 gold-standard model by 10 points in F1 score. We compare their result with our best cluster model in <ref type="table" target="#tab_2">Table 17</ref>. While the results of our baseline model confirms theirs, our clustering model score is slightly higher. This result is interesting because it is arguably more simple to induce the clusters we use to train ixa-pipe-nerc rather than create the silver standard training set from Wikipedia as described in <ref type="bibr" target="#b45">Nothman et al. (2013)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Annotation Guidelines</head><p>In this section the objective is studying not so much the differences in textual genre as the influence of substantially different annotation standards. We only use three classes (location, organization and person) to evaluate the best models presented for in-domain evaluations labeling 'O' every entity which is not LOC, ORG or PER. The text genre of MEANTIME is not that different from CoNLL data. However, differences in the gold standard annotation result in significant disagreements regarding the span of the named entities <ref type="bibr" target="#b58">(Tonelli et al., 2014)</ref>. For example, the following issues are markedly different with respect to the training data we use for each language:</p><p>• Different criteria to decide when a named entity is annotated: in the expression "40 billion US air tanker contract" the MEANTIME gold standard does not mark 'US' as location, whereas in the training data this is systematically annotated.</p><p>• Mentions including the definite article within the name entity span: 'the United States' versus 'United States'.</p><p>• Longer extents containing common nouns: in the MEANTIME corpus there are many entities such as "United States airframer Boeing", which in this case is considered an organization, whereas in the training data this span will in general consists of two entities: 'United States' as location and 'Boeing' as organization.</p><p>• Common nouns modifying the proper name: 'Spokeswoman Sandy Angers' is annotated as a named entity of type PER whereas in the training data used the span of the named entity would usually be 'Sandy Angers'.</p><p>CoNLL NER phrase based evaluation punishes any bracketing error as both false positive and negative. Thus, these span-related disagreements make this setting extremely hard for models trained according to other annotation guidelines, as shown by <ref type="table" target="#tab_2">Table 18</ref>. Our baseline models degrade around 40 F1 points and the cluster-based models around 35. Other systems' results worsen much more, especially for Spanish and Dutch. The token-based scores are in general better but the proportion in performance between systems across languages is similar.   <ref type="table">)</ref> whereas ixa-pipe-nerc is trained with CoNLL data. T-F1: token-based F1. Local : baseline system; best-clusters: nl-clusters, es-cluster and en-best-cluster; best-overall : best configuration previously presented for each language for the in-domain evaluations.</p><formula xml:id="formula_1">English Spanish Dutch Outer Inner Outer Inner Outer Inner Features F1 T-F1 F1 T-F1 F1 T-F1 F1 T-F1 F1 T-F1 F1 T-F1</formula><p>As an additional experiment, we also tested the English model recommended by Stanford NER which is trained for three classes (LOC, PER, ORG) using a variety of public and (not identified) private corpora (referred to as Stanford NER 3 class (ALL) in <ref type="table" target="#tab_2">Table 19</ref>). The results with respect to their CoNLL model improved by around 3 points in F1 score across named entity labels and evaluation types (phrase or token based). In view of these results, we experimented with multicorpora training data added to our best <ref type="bibr">CoNLL 2003 model (en-91-18 )</ref>. Thus, we trained using three public training sets: MUC 7, CoNLL 2003 and Ontonotes 4.0. The local model with the three training sets (Local ALL) improved 12 and 17 points in F1 score across evaluations and entity types, outperforming our best model trained only with CoNLL 2003. Adding the clustering features gained between 2 and 5 points more surpassing the Stanford NER 3 class multi-corpora model in every evaluation. We believe that the main reason to explain these improvements is the variety and quantity of annotations provided by Ontonotes (1M word corpus), and to a lesser extent by MUC 7, which includes some spans containing common nouns and determiners making the model slightly more robust regarding the mention spans.  <ref type="table" target="#tab_2">Table 19</ref>: MEANTIME English multi-corpus out-of-domain evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Despite the simplicity of the ixa-pipe-nerc approach, we report best results for English in 4 different datasets: for CoNLL 2003 and for the three English out-of-domain evaluations. For German we improve the results of the best system in the GermEval 2014 task and obtain comparable results to previous work in the CoNLL 2003 dataset using publicly available data. In Spanish we provide results on CoNLL 2002 and in two out-of-domain evaluations clearly outperforming previous best results. For Dutch we improve over previous results in CoNLL 2002 and SONAR-1 data and two out-of-domain evaluations. Finally, for Basque (Egunkaria) the improvements are considerable.</p><p>Simple and shallow features These results are obtained without linguistic or global features. Instead, injecting unigram knowledge from the combination and stacking of clusters allows to obtain a robust NERC system across languages, outperforming other, more complex <ref type="bibr" target="#b51">(Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b59">Turian et al., 2010;</ref><ref type="bibr" target="#b20">Desmet and Hoste, 2014;</ref><ref type="bibr" target="#b47">Passos et al., 2014)</ref> and language-specific systems. This is also the case for languages such as Basque or German, where the use of linguistic features (lemmas, POS tags, curated suffix lists and rules, etc.) has so far been pervasive <ref type="bibr" target="#b3">(Alegria et al., 2006;</ref><ref type="bibr" target="#b6">Benikova et al., 2014</ref><ref type="bibr" target="#b7">Benikova et al., , 2015</ref>.</p><p>Minimal human intervention Each of the datasets used displays an idiosyncratic annotation and genre. This is even the case for the NER tasks organized at CoNLL 2002 and 2003: "For instance, Spanish marks no lowercase adjectival nationalities and includes 192 instances where surrounding quotes are included in the entity annotation; Dutch has as PER the initials of photographers; and English has lots of financial and sports data in tables" .</p><p>In despite of this, our best in-domain results were obtained using the same set of features for all seven evaluations, which included trigrams and character n-grams. The only variable across datasets and languages was the number of classes of the clustering lexicons used.</p><p>However, the in-domain results also manifest that trigrams and character n-grams can be omitted for languages without declension cases or repeated suffixes in the named entities (e.g., Dutch, English and Spanish) without it being too detrimental. In fact, we started experimenting without trigrams and character n-grams for Dutch, English and Spanish. When we added them to the best model of each language (e.g, charngram 1:6 en-91-18 in <ref type="table" target="#tab_10">Table 5</ref>), the in-domain results improved or remained quite similar but at the cost of making the models less robust in the outof-domain evaluations. In contrast, trigrams and character n-grams were highly beneficial in both in-domain and out-of-domain settings for Basque and German.</p><p>Our take on this issue is that trigram and character n-gram features would only be required to address inflected named entities (in Basque) or to learn repeated suffixes appearing in named entities and to tackle sparsity (in both Basque and German). For example, <ref type="table" target="#tab_6">Table 3</ref> shows the utility of character n-gram features capturing the Basque locative declension case -ko, which is repeated for many location entities in the training data.</p><p>The emphasis on clustering features for good performance (as opposed to local features) produces an easily exportable and robust system for both in-domain and out-of-domain evaluations and across languages. It is therefore crucial, for competitive performance, to understand which clustering methods and corpora use as well as how to combine them effectively.</p><p>Choosing the right corpus and clustering method Contrary to previous suggestions that the larger the number of classes and the corpus used to induced the clusters the better <ref type="bibr" target="#b59">(Turian et al., 2010)</ref>, our results provide a number of interesting pointers to choose the appropriate type of corpus and clustering method required for optimal performance.</p><p>With respect to Brown clusters, all our results are better when we induce 1000 classes. We systematically tried for every language and data source with less (320) and more (3200) classes without performance improvement. Moreover, in every evaluation setting the best results with Brown clusters were obtained when a corpus relatively closed in-domain, genre and date was used, even if significantly smaller. This is especially clear for Basque, English and Spanish where the best Brown clusters were induced over the smallest corpora (Egunkaria, Reuters RCV1 and El Periodico, respectively).</p><p>In contrast, results show that Word2vec clusters, unlike Brown, always benefit from very large amounts of data, regardless of domain or temporal issues. Our experiments also suggest that Clark clusters seem to behave more robustly than Brown clusters with respect to the size and type of text, performing well with large unrelated and smaller domain-specific corpora. For best performance, <ref type="bibr" target="#b14">Clark (2003)</ref> recommends that the proportion of clusters k with respect to the source data should be of k 3 ≈ n where n is the number of words in the corpus. Instead, we systematically induce, for every corpus, Clark clusters in the range of 100-600 classes, because preliminary experiments proved that over 600 classes, even if the proportion proposed by Clark holds, performance starts to deteriorate 15 . Following this, we are now in better position to address the questions posed by <ref type="bibr" target="#b59">Turian et al. (2010)</ref>:</p><p>• Brown clusters benefit from source data closely related to the testset, even if small in size.</p><p>• Clark clusters behave robustly with respect to the size or type of data sources from which they are induced.</p><p>• For Word2vec clusters size is the most important factor: the larger the corpus the better.</p><p>Should we prefer certain word features? While Clark features seem to obtain the best results overall, our work provides a very simple method of effectively combining them, depending on the data sources we have available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combination and Stacking</head><p>We use three different data sources, namely, Wikipedia, Egunkaria and Berria (see <ref type="table" target="#tab_8">Table 4</ref> for the list of unlabeled corpora used) for the Basque experiments. In order to understand better our approach, we annotated the Basque testset with every model in <ref type="table" target="#tab_2">Table  12</ref> and manually inspected their output. The following two examples illustrate how our approach works:</p><p>• Ekuadorko: In addition to the eu-cluster model, the Brown model (BE), and the Clark Wikipedia model (CW200) provide the correct annotation. The assigned clusters in BE and CW200 clustering lexicons clearly consist of locations. For example, the 176 cluster of CW200 contains Gasteizko, Arabako, Espainiako, etc., which, unlike Ekuadorko, do occur in the training set. <ref type="bibr">16</ref> Most interestingly, while Gasteizko is only labeled as location in the training set, both Arabako and Espainiako are labeled as, depending on the context, organization or location.</p><p>• Ameriketara (to America): Only the Brown model (and the eu-cluster ) correctly labels it as a location. In this case, the 011110011100 Brown path clusters Ameriketara with other locations such as Baionara and Espainiara, among others, which, unlike Ameriketara, are contained in the training set. 17</p><p>The same phenomenon can be observed for languages quite different from Basque such as English or Spanish. For example, the named entity 'Uzbekistan' is not present in the English CoNLL 2003 training data, whereas in the test set can be found four times, all of them locations. The local model annotates all four occurrences as organization (see <ref type="table" target="#tab_10">Table 5</ref> for references to models). The Brown and the Word2vec models, two as locations and two as organizations, because the cluster companions of 'Uzbekistan' are of mixed nature. Finally, the Clark model (CW600) does correctly annotate them as locations (also best-cluster, en-91-18, en-91-36 . . .): 'Uzbekistan' is placed in the 145 cluster, which contains mostly locations contained in the training data <ref type="bibr">(Spain, Ireland, etc.)</ref>. <ref type="bibr">15</ref> Note that we did not experiment with any of the clustering algorithms' parameters, vector dimensions, etc, just with the number of classes.</p><p>16 English: in Vitoria-Gasteiz, in Alava, in Spain. <ref type="bibr">17</ref> To Bayonne, to Spain.</p><p>Previous approaches to NERC combining clusters or word embeddings have obtained mixed results <ref type="bibr" target="#b59">(Turian et al., 2010)</ref>. Up until now best results have been based on rather complex systems which also used one type clustering or embedding feature <ref type="bibr" target="#b47">(Passos et al., 2014;</ref><ref type="bibr" target="#b51">Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b22">Faruqui et al., 2010;</ref><ref type="bibr" target="#b6">Benikova et al., 2014)</ref>. In other sequence labeling tasks, Biemann (2009) reports a slight improvement (from 97.33 to 97.43 word accuracy) in POS tagging combining two types of clustering methods (one of them was Clark <ref type="formula">(2003)</ref>) for German.</p><p>Our system displays two important differences with respect to previous approaches. First, the differences between our baseline system and the, for example, Clark features are much larger than in previous work (with the exception of <ref type="bibr" target="#b22">Faruqui et al. (2010)</ref>), ranging from 2.2 and 5.5 points in F1 measure across the in-domain evaluations to 2-8 points for out-of-domain results. For example, our English CoNLL 2003 single clustering models are similar to the best CoNLL 2003 model distributed by the Stanford NER. If we consider the combined clustering models, the differences over the baseline increase to 5-10 points of F1 measure for in-domain evaluations and between 4-22 in out-of-domain settings.</p><p>Second, our combination of clustering features significantly increases the performance over the models using only one type of clustering feature. The improvements range over 2 to 6 points in F1 measure for in-domain and out-of-domain results.</p><p>In our opinion, these results are quite interesting as previous experiments combining features of different word representations for NERC <ref type="bibr" target="#b59">(Turian et al., 2010)</ref>, while increasing the overall result, did not improve over the state of the art at the time <ref type="bibr" target="#b51">(Ratinov and Roth, 2009</ref>). The results also show that leaning heavily on the clustering features (instead of specific feature tuning) for performance proves very beneficial in out-of-domain settings.</p><p>Robust in out-of-domain settings The results of the eight out-of-domain evaluations undertaken suggest that differences regarding named entities spans as described in Section 4.3.3 are harder to overcome than disagreements in text genre (e.g. Section 4.3.2) or entity type (Section 4.3.1). Thus, our method using multiple clustering sources allow to overcome better any differences in named entity type or text genre. However, and even though our system obtains state of the art results in every evaluation, trying to adapt to differences in named entity shape and span proves to be a much more difficult task, hence the comparatively lower results obtained in the MEANTIME evaluations.</p><p>Robust reducing training data <ref type="bibr" target="#b33">Koo et al. (2008)</ref> present learning curves showing the increase in performance when using Brown clusters for dependency parsing whereas Biemann (2009) provides learning curves to measure the impact of clusters for NERC and chunking. Inspired by those two previous works we measured the performance when training data is reduced. Unlike these two approaches, the differences between adding clusters or not to our system with less training data is huge. <ref type="table" target="#tab_2">Table 14</ref> shows that differences adding the clustering features with half the data is around 8 points in F1 score (for Spanish the difference is 3.42 F1).</p><p>Another common point of our clustering features with <ref type="bibr" target="#b33">Koo et al. (2008)</ref> is that when gold training data is reduced, the system still obtains competitive results with respect to previous approaches or publicly available systems using only a fraction (half or a quarter) of the data. If we consider the ability of our system to be competitive with substantial reductions in gold training data plus the fact that no linguistic motivated features are required, we are providing a system which is much cheaper to train for new languages and/or domains contributing therefore to alleviate the dependency on gold training data to obtain good performing NERC systems for new languages, domains or genres.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We have shown how to develop robust NERC systems across languages and datasets with minimal human intervention, even for languages with inflected named entities. This is based on adequately combining word representation features on top of shallow and general local features. Crucially, we have empirically demonstrate how to effectively combine various types of simple word representation features depending on the source data available. This has resulted in a clear methodology for using the three types of clustering features which produces very competitive results in both in-domain and out-of-domain settings.</p><p>Thus, despite the relative simplicity of our approach, we report state of the art results for Dutch, English, German, Spanish and Basque in seven in-domain evaluations.</p><p>We also outperform previous work in eight out-of-domain evaluations, showing that our clustering features improve the robustness of NERC systems across datasets. Finally, we have measured how much our system's performance degrades when the amount of supervised data is drastically cut. The results show our models are still very competitive even when reducing the supervised data by half or more. This, together with the lack of linguistic features, facilitates the easy and fast development of NERC systems for new domains or languages.</p><p>In future work we would like to explore more the various types of domain adaptation required for robust performance across text genres and domains, perhaps including micro-blog and noisy text such as tweets. Furthermore, we are also planning to adapt our techniques to other sequence labeling problems such as Opinion Target Extraction <ref type="bibr" target="#b49">(Pontiki et al., 2014</ref><ref type="bibr" target="#b48">(Pontiki et al., , 2015</ref> and Super Sense tagging <ref type="bibr" target="#b13">(Ciaramita and Altun, 2006)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A Brown clustering hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. http://dx.doi.org/10.1016/j.artint.2016.05.003. c 2016. /creativecommons.org/licenses/by-nc-nd/4.0/. Paper submitted 1 September 2015, Revised 11 May 2016, Accepted 15 May 2016.</figDesc><table><row><cell>† This</cell><cell>manuscript</cell><cell>version</cell><cell>is</cell><cell>made</cell><cell>available</cell><cell>under</cell><cell>the</cell><cell>CC-BY-NC-ND</cell><cell>4.0</cell><cell>license</cell></row><row><cell>http:/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>‡ Corresponding author: rodrigo.agerri@ehu.eus1 http://en.wikipedia.org/wiki/Europe_(disambiguation)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Datasets used for training, development and evaluation. MUC7: only three classes (LOC,</cell></row><row><cell>ORG, PER) of the formal run are used for out-of-domain evaluation. As there are not standard</cell></row><row><cell>partitions of SONAR-1 and Ancora 2.0, the full corpus was used for training and later evaluated</cell></row><row><cell>in-out-of-domain settings.</cell></row><row><cell>The Wikigold corpus consists of 39K words of English Wikipedia manually annotated following</cell></row><row><cell>the CoNLL 2003 guidelines</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>used Brown clusters as features obtaining what was at the time the best published result of an English NERC system on the CoNLL 2003 testset. Turian et al. (2010) made a rather exhaustive comparison of Brown clusters, Collobert and Weston's embeddings</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>report best results up to date for English NERC on CoNLL 2003 test data, 90.90 F1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Features generated for the Basque sentence "Morras munduko txapeldun izan zen ju- niorretan 1994an, Ekuadorko hiriburuan, Quiton". English: Morras was junior world champion in 1994, in the capital of Ecuador, Quito. Current token is 'Ekuadorko'.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>. Section 4.2 analyzes the performance when reducing training data and Section 4.3 presents eight out-of-domain evaluations for three languages: Dutch, English and Spanish.</figDesc><table><row><cell></cell><cell>million words in corpus</cell><cell></cell><cell cols="3">million words for training</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Brown</cell><cell cols="2">Clark Word2vec</cell></row><row><cell></cell><cell>Reuters RCV1</cell><cell>63</cell><cell>35</cell><cell>63</cell><cell>63</cell></row><row><cell>en</cell><cell>Wikipedia (20141208)</cell><cell>1700</cell><cell>790</cell><cell>790</cell><cell>1700</cell></row><row><cell></cell><cell>Gigaword 5th ed.</cell><cell>4000</cell><cell>-</cell><cell>-</cell><cell>4000</cell></row><row><cell>de</cell><cell cols="2">Wikipedia (20140725) deWac (Baroni et al., 2009) 1100 650</cell><cell>190 500</cell><cell>190 500</cell><cell>650 1100</cell></row><row><cell></cell><cell>Wikipedia (20140810)</cell><cell>428</cell><cell>246</cell><cell>246</cell><cell>428</cell></row><row><cell>es</cell><cell>elperiodico (1998-2002)</cell><cell>60</cell><cell>35</cell><cell>60</cell><cell>60</cell></row><row><cell></cell><cell>Gigaword 3rd ed.</cell><cell cols="3">1150 330 (afp) 330 (afp)</cell><cell>1150</cell></row><row><cell cols="2">nl Wikipedia (20140804)</cell><cell>235</cell><cell>128</cell><cell>128</cell><cell>235</cell></row><row><cell></cell><cell>Wikipedia (20141208)</cell><cell>60</cell><cell>12</cell><cell>60</cell><cell>60</cell></row><row><cell>eu</cell><cell>Egunkaria (1999-2003)</cell><cell>38</cell><cell>28</cell><cell>38</cell><cell>38</cell></row><row><cell></cell><cell>Berria (2003-2014)</cell><cell>90</cell><cell>78</cell><cell>90</cell><cell>90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Unlabeled corpora used to induced clusters. For each corpus and cluster type the number of words (in millions) is specified. Average training times: depending on the number of words, Brown clusters training time required between 5h and 48h. Word2vec required 1-4 hours whereas Clark clusters training lasted between 5 hours and 10 days.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>CoNLL 2003 English results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>GermEval 2014 M3 metric results and comparison to GermaNER system on the outer spans.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>GermEval 2014 Official results.</figDesc><table><row><cell>Development</cell><cell>Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>CoNLL 2003 German results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8</head><label>8</label><figDesc>compares our German CoNLL 2003 results with the best previous work trained on public data. Our best CoNLL 2003 model obtains results similar to the state of the art performance with respect to the best system published up to date</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>CoNLL 2002 Spanish results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 10 :</head><label>10</label><figDesc>CoNLL 2002 Dutch results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>shows our system's improvement over previous results on this dataset.</figDesc><table><row><cell>Features</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>Local (L)</cell><cell>86.66</cell><cell cols="2">85.57 86.11</cell></row><row><cell>nl-cluster</cell><cell>87.89</cell><cell cols="2">87.56 87.72</cell></row><row><cell>nl-cluster + dict</cell><cell>88.08</cell><cell cols="2">87.91 88.00</cell></row><row><cell>Sonar-nerd</cell><cell>-</cell><cell>-</cell><cell>84.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 12 :</head><label>12</label><figDesc>Basque Egunkaria results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 13 :</head><label>13</label><figDesc>CoNLL 2003 English results reducing training data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 14</head><label>14</label><figDesc></figDesc><table><row><cell>: Multilingual results reducing training data. Datasets employed: Basque (egunkaria),</cell></row><row><cell>Dutch and Spanish (CoNLL 2002) and German (GermEval 2014 outer). L: Local model. C:</cell></row><row><cell>cluster model. ∆: difference between them.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 15</head><label>15</label><figDesc>specifies the datasets used for each out-of-domain setting and language. Details of each dataset can be foundTable 1.</figDesc><table><row><cell></cell><cell cols="2">Class Disagreements</cell><cell cols="2">Text Genre</cell><cell cols="2">Annotation Guidelines</cell></row><row><cell></cell><cell>Train</cell><cell>Test</cell><cell>Train</cell><cell>Test</cell><cell>Train</cell><cell>Test</cell></row><row><cell cols="2">en CoNLL</cell><cell cols="5">MUC 7 CoNLL Wikigold CoNLL/Ontonotes/MUC 7 MEANTIME</cell></row><row><cell>es</cell><cell>Ancora</cell><cell>CoNLL</cell><cell>-</cell><cell>-</cell><cell>Ancora/CoNLL</cell><cell>MEANTIME</cell></row><row><cell cols="3">nl SONAR-1 CoNLL</cell><cell>-</cell><cell>-</cell><cell>SONAR-1</cell><cell>MEANTIME</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 15 :</head><label>15</label><figDesc></figDesc><table /><note>Testsets and languages for out-of-domain evaluations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 16</head><label>16</label><figDesc></figDesc><table><row><cell>: Out-of-domain evaluation based on class disagreements. English models trained on</cell></row><row><cell>CoNLL 2003; Spanish models trained with Ancora; Dutch models trained with SONAR-1. T-F1:</cell></row><row><cell>token-based F1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 17 :</head><label>17</label><figDesc>Wikigold out-of-domain evaluation based on text genre.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head></head><label></label><figDesc>67.36 64.95 71.98 58.94 65.63 62.14 65.54 63.40 70.68 63.93 70.24 Stanford NER 53.14 64.62 62.45 69.76 46.42 54.40 47.</figDesc><table><row><cell>Local</cell><cell cols="10">41.83 54.17 48.57 57.85 34.42 42.95 37.14 41.93 48.49 54.84 49.77 55.86</cell></row><row><cell>best-cluster</cell><cell cols="10">54.04 65.96 63.72 71.13 56.78 62.55 59.77 63.04 59.94 66.03 60.27 65.42</cell></row><row><cell>best-overall</cell><cell cols="8">55.48 48 54.27</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Illinois NER</cell><cell cols="4">53.24 65.68 62.72 71.04</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Freeling 3.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">38.27 48.06 40.93 46.52</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Sonar nerd</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">48.60 53.60 48.44 52.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head>Table 18</head><label>18</label><figDesc></figDesc><table><row><cell>: MEANTIME out-of-domain evaluation. English systems trained on CoNLL data. Dutch</cell></row><row><cell>systems trained with SONAR-1. Stanford NER Spanish model is trained with Ancora (20150130</cell></row><row><cell>version</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://opennlp.apache.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.cnts.ua.ac.be/conll2002/ner/bin/conlleval.txt 4 https://github.com/ixa-ehu/ixa-pipe-nerc</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.newsreader-project.eu</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">English: in Spain, to Spain, Spain (in transitive clause), for Spain, in Spain.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://github.com/percyliang/brown-cluster 10 https://github.com/ninjin/clark_pos_induction 11 https://code.google.com/p/Word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their comments to improve this paper. We would also like to thank Sebastian Padó for his help training the Clark clusters. This work has been supported by the European projects NewsReader, EC/FP7/316404 and QTLeap -EC/FP7/610516, and by the Spanish Ministry for Science and Innovation (MICINN) SKATER, Grant No. TIN2012-38584-C06-01 and TUNER, TIN2015-65308-C5-1-R.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">IXA pipeline: Efficient and ready to use multilingual NLP tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bermudez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3823" to="3828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The stages of event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Annotating and Reasoning about Time and Events</title>
		<meeting>the Workshop on Annotating and Reasoning about Time and Events</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating named entities using monolingual and bilingual resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL 2002)</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics (ACL 2002)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="400" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lessons from the development of a named entity recognizer for Basque</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alegria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arregi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ezeiza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fernández</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="25" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving machine translation quality with automatic named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International EAMT workshop on MT and other Language Technology Tools</title>
		<meeting>the 7th International EAMT workshop on MT and other Language Technology Tools</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zanchetta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Germeval 2014 named entity recognition shared task: Companion paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Benikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kisselew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GermEval 2014 Named Entity Recognition Shared Task</title>
		<meeting>the GermEval 2014 Named Entity Recognition Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Germaner: Free open german named entity recognition tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Benikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Yimam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of the German Society for Computational Linguistics and Language Technology (GSCL-2015)</title>
		<meeting>the International Conference of the German Society for Computational Linguistics and Language Technology (GSCL-2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised part-of-speech tagging in the large</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="101" to="135" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">New avenues in opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="15" to="21" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Named entity extraction using AdaBoost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Padro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th conference on Natural language learning</title>
		<meeting>the 6th conference on Natural language learning</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Muc-7 information extraction task definition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chinchor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Seventh Message Understanding Conference (MUC-7), Appendices</title>
		<meeting>eeding of the Seventh Message Understanding Conference (MUC-7), Appendices</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining distributional and morphological information for part of speech induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics</title>
		<meeting>the tenth conference on European chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language Independent NER using a Maximum Entropy Tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-03)</title>
		<meeting>the Seventh Conference on Natural Language Learning (CoNLL-03)<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="164" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale named entity disambiguation based on wikipedia data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic relations between events and their time, locations and participants for event coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cybulska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Recent Advances in Natural Language Processing RANLP 2013</title>
		<meeting>the International Conference Recent Advances in Natural Language Processing RANLP 2013<address><addrLine>Hissar, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="156" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fine-grained dutch named entity recognition. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Desmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hoste</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="307" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The automatic content extraction (ace) program-tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>LREC</publisher>
			<biblScope unit="page" from="837" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training and evaluating a german named entity recognizer with semantic generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sprachverarbeitung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KONVENS</title>
		<meeting>KONVENS</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Named entity recognition through classifier combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<editor>Daelemans, W., Osborne, M.</editor>
		<meeting>CoNLL-2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="168" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating entity linking with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hachey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="130" to="150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A generative entity-mention model for linking entities with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="945" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modular classifier ensemble architecture for named entity recognition on low resource systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hänig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bordag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the KONVENS Germ-Eval Shared Task on Named Entity Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Frstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="27" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using cross-entity inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1127" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACL</publisher>
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge base population: Successful approaches and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1148" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Name-aware machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="604" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semi-supervised learning for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluating DBpedia Spotlight for the TAC-KBP entity linking task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TAC-KBP 2011 Workshop</title>
		<meeting>the TAC-KBP 2011 Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="118" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Name tagging with word clusters and discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamanian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>HLT-NAACL</publisher>
			<biblScope unit="page" from="337" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Minard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speranza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Aldabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urizar</surname></persName>
		</author>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="778" to="786" />
		</imprint>
	</monogr>
	<note>Semeval-2015 task 4: Timeline: Cross-document event ordering</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MEANTIME, the NewsReader Multilingual Event and Time Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Minard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speranza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Altuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Van Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning, ACM</title>
		<meeting>the 24th international conference on Machine learning, ACM</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingvisticae Investigationes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Comparison between tagged corpora for the named entity task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nobata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Comparing corpora</title>
		<meeting>the workshop on Comparing corpora</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning multilingual named entity recognition from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Freeling 3.0: Towards wider multilinguality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stanilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Calzolari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Odijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)</title>
		<editor>Piperidis, S.</editor>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC-2012)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2473" to="2479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics, Denver</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Sixteenth Conference on Computational Natural Language Learning<address><addrLine>Jeju, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nested named entity recognition with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckle-Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schnober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the KONVENS Germ-Eval Shared Task on Named Entity Recognition</title>
		<meeting>the KONVENS Germ-Eval Shared Task on Named Entity Recognition<address><addrLine>Hildesheim, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">AnCora: Multilevel Annotated Corpora for Catalan and Spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Recasens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2008</title>
		<meeting>LREC 2008</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="96" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong</forename><surname>Kim Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2002</title>
		<meeting>CoNLL-2002<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="155" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong</forename><surname>Kim Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>De Meulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">NewsReader Guidelines for Annotation at Document Level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sprugnoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speranza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Minard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Fondazione Bruno Kessler</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A robust risk minimization based named entity recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="204" to="207" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
