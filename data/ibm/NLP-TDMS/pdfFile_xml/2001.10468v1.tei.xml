<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firas</forename><surname>Kassawat</surname></persName>
							<email>kassawat@cs.uni-bonn.dechaudhur</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Smart Data Analytics Group (SDA)</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debanjan</forename><surname>Chaudhuri</surname></persName>
							<email>debanjan.chaudhuri@iais.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Smart Data Analytics Group (SDA)</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Enterprise Information Systems Department, Fraunhofer IAIS</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
							<email>jens.lehmann@cs.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Smart Data Analytics Group (SDA)</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Enterprise Information Systems Department, Fraunhofer IAIS</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Dialogue Systems · Knowledge Graphs · Joint Embeddings · Neural Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention-based encoder-decoder neural network models have recently shown promising results in goal-oriented dialogue systems. However, these models struggle to reason over and incorporate state-full knowledge while preserving their end-to-end text generation functionality. Since such models can greatly benefit from user intent and knowledge graph integration, in this paper we propose an RNN-based end-to-end encoder-decoder architecture which is trained with joint embeddings of the knowledge graph and the corpus as input. The model provides an additional integration of user intent along with text generation, trained with multi-task learning paradigm along with an additional regularization technique to penalize generating the wrong entity as output. The model further incorporates a Knowledge Graph entity lookup during inference to guarantee the generated output is state-full based on the local knowledge graph provided. We finally evaluated the model using the BLEU score, empirical evaluation depicts that our proposed architecture can aid in the betterment of task-oriented dialogue system's performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is a considerable rise in the need for effective task-oriented dialogue agents that can help the user to achieve specific goals and everyday tasks such as weather forecast assistance, schedule arrangement, and location navigation. Neural network based dialogue models proved to be the most promising architectures so far which can effectively use dialogue corpora in order to build such agents. However, these models struggle to reason over and incorporate state-full knowledge while preserving their end-to-end text generation functionality. They often require additional user supervision or additional data annotation to incorporate some dialogue-state information along with additional knowledge.</p><p>To help the dialogue agents overcome these problems, they have often been built with several pipelined modules such as language understanding, dialog management, question answering components, and natural language generation <ref type="bibr" target="#b15">[16]</ref>. However, modeling the dependencies between such modules is complex and may not result in very natural conversations.</p><p>The problem becomes much more complicated in multi-domain scenarios, because the text generation in such scenarios hugely depends on the domain the user is talking about, where each domain will have its own vocabulary that differs from other domains. Hence, understanding user intent during the text generation process can be beneficial. <ref type="table" target="#tab_0">Table 1</ref> shows sample user utterances along with its intent. While generating new tokens, the model can benefit from this because separate intents will follow different vocabulary distributions. In order to tackle the aforementioned problems, we propose a novel neural network architecture trained using a multi-task learning paradigm, which along with generating tokens from the vocabulary also tries to predict the intent of the user utterances. By doing so, the model is able to relate the generated words to the predicted intent and uses this to exclude words unrelated to the current conversation from the next word candidate prediction. Our model utilizes joint text and knowledge graph embeddings as inputs to the model inspired by the works of <ref type="bibr" target="#b18">[18]</ref>.</p><p>Additionally, after projecting the entities and text into the same vector space, we propose an additional training criterion to use as a novel regularization technique called entity loss, which further penalizes the model if the predicted entity is different from the original. The loss is the mean of the vector distance between the predicted and correct entities.</p><p>Furthermore, to guarantee a state-full knowledge incorporation we included a knowledge graph (KG) key-value look-up to implement a way of knowledge tracking during the utterances of a dialogue.</p><p>A KG in the context of this paper is a directed, multi-relational graph that represents entities as nodes, and their relations as edges, and can be used as an abstraction of the real world. KGs consists of triples of the form (h,r,t) ∈ KG, where h and t denote the head and tail entities, respectively, and r denotes their relation. Our main contributions in this paper are as follows:</p><p>-Utilizing joint text and knowledge graph embeddings into an end-to-end model for improving the performance of task-oriented dialogue systems.</p><p>-A multi-task learning of dialogue generation and learning user intent during decoding. -A novel entity loss based regularization technique which penalizes the model if the predicted entity is further from the true entity in vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, several works have tried incorporating external knowledge, both structured and unstructured, into dialogue systems. <ref type="bibr" target="#b12">[13]</ref> [27] <ref type="bibr" target="#b9">[10]</ref> proposed architectures for incorporating unstructured knowledge into retrieval based dialogue systems targeting the Ubuntu dialogue corpus <ref type="bibr" target="#b13">[14]</ref>. More recently, <ref type="bibr" target="#b2">[3]</ref> used domain description based unstructured knowledge using an additional recurrent neural network (GRU) architecture added to the word embeddings for domain keywords to further improve the performance of such models. There are also many recent works that incorporated end-to-end models with structured knowledge sources, such as knowledge graphs <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b25">[25]</ref>. <ref type="bibr" target="#b5">[6]</ref> used a key-value retrieval network to augment the vocabulary distribution of the keys of the KG with the attentions of their associated values. We were inspired by this model to add the key-value lookup in the inference stage of the model. Although the work they represented was inspiring, it turns out that using attention over a knowledge graph is inefficient for long sequences. Moreover, they didn't utilize user intent during modeling. In <ref type="bibr" target="#b15">[16]</ref>, the authors used a memory-to-sequence model that uses multi-hop attention over memories to help in learning correlations between memories which results in faster trained model with a stable performance. As for using joint learning to support end-to-end dialogue agent the work introduced by <ref type="bibr" target="#b10">[11]</ref> showed state-of-the-art results where they used an attention based RNN for the joint learning of intent detection and slot filling. They proposed two methods, an encoder-decoder model with aligned inputs and an attention-Based model. Our intent predictor during decoding is influenced by their architecture. <ref type="bibr" target="#b22">[22]</ref> implemented a multi-domain statistical dialogue system toolkit in which they created a topic tracker, which is used to load only the domain-specific instances of each dialogue model. In other words, all the elements in the pipeline (semantic decoder, belief tracker and the language generator) would consist of only domain-specific instances.</p><p>Graph embeddings have been used by <ref type="bibr" target="#b3">[4]</ref> to predict missing relationships between entities, later on, <ref type="bibr" target="#b26">[26]</ref> used knowledge graph embeddings with text descriptions to propose a semantic space projection model that learns from both symbolic triples and their textual description. <ref type="bibr" target="#b0">[1]</ref> proposed a model for jointly learning word embeddings using a corpus and a knowledge graph that was used for representing the meaning of words in vector space. They also proposed two methods to dynamically expand a KG. Taking inspirations from these joint embeddings, we incorporate them into our model as inputs for better capturing grounded knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Description</head><p>In this section, we present our model architecture. Firstly, we discuss the attention based RNN encoder-decoder architecture, and then we explain the intent predictor, joint embedding vector space and additional entity loss based regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention Based Seq-to-seq Model</head><p>Our model is quintessentially based on an RNN-based encoder-decoder architecture taking inspirations from the works proposed by <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b21">[21]</ref> and <ref type="bibr" target="#b14">[15]</ref>. Given the input utterances from the user in a dialogue scenario D, the system has to produce an output utterance o t for each time-step t, i.e. there is a sequence</p><formula xml:id="formula_0">(i 1 , o 1 ), (i 2 , o 2 ), ..., (i n , o n )</formula><p>where n denotes the total number of utterances in a dialogue. We reprocess these utterances and present them as:</p><formula xml:id="formula_1">(i 1 , o 1 ), ([i 1 ; o 1 ; i 2 ], o 2 ), ..., ([i 1 ; o 1 ; ...; o n−1 ; i n ], o n ) ,</formula><p>where we present an input as the sequence of all previous utterances concatenated with the current input. This is done in order to maintain the context in the dialogue. Let x 1 , ...x m denote the words in each input utterance (where m is the length of the utterance). We firstly map these words to their embedding vector representations using Φ xt which can be either a randomly initialized or a pretrained embedding. Then these distributed representations are fed into the RNN-encoder (LSTM <ref type="bibr" target="#b7">[8]</ref> in this case) as follows:</p><formula xml:id="formula_2">h t = LST M (Φ xt , h t−1 )<label>(1)</label></formula><p>h t = (h 0 , ....h m ) represents the hidden states of the encoder. These encoder inputs are fed into a decoder which is again a recurrent module. It generates a token for every time-stamp t given by the probability</p><formula xml:id="formula_3">p(y t |y t−1 , ..., y 1 , x t ) = g(y t−1 , s t , c t )<label>(2)</label></formula><p>Where, g(.) is a softmax function, s t is the hidden state of the decoder at timestep t given by</p><formula xml:id="formula_4">s t = LST M (s t−1 , y t−1 , c t )<label>(3)</label></formula><p>Additionally, we use an attention mechanism over the encoder hidden-states as proposed by <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[7]</ref>. The attention mechanism is calculated as</p><formula xml:id="formula_5">α ij = exp(e ij ) m k=1 exp(e ik )<label>(4)</label></formula><formula xml:id="formula_6">e ij = tanh(W c [s i−1 , h j ])<label>(5)</label></formula><p>The final weighted context from the encoder is given by</p><formula xml:id="formula_7">c i = m α ij h j<label>(6)</label></formula><p>Where, α ij represent the attention weights learned by the model, c i the context vector, W c a weight parameter that has to be learned by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Predicting Intent</head><p>The intent predictor module is used to predict the intent from the user utterance. It is computed using the encoder LSTM's hidden representation h t as computed at every timestamp t. The intent output prediction is given by:</p><formula xml:id="formula_8">i out = W o (tanh(W i [h t ; c t ])))<label>(7)</label></formula><p>Where, i out is the intent score ∈ R ic for the t th time-step, W i and W o are the trainable weight parameters for the intent predictor and i c is the total number of intent classes. The latter can be the different domains the dialogue system wants to converse upon, for example restaurant booking, flight booking etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Joint Text and Knowledge Graph Embeddings</head><p>Learning distributed embeddings for words has been widely studied for neural network based natural language understanding. <ref type="bibr" target="#b17">[17]</ref> was the first to propose a model to learn such distributed representations. In order to leverage additional information from knowledge graphs in goal-oriented dialogues, we need to first project the embeddings for both texts appearing in the dialogue corpus and knowledge graphs in the same vector space. For learning such word embeddings, we adopted the work done by <ref type="bibr" target="#b18">[18]</ref>, which proposes a model for training the embeddings from both a corpus and a knowledge graph jointly. The proposed method is described in three steps: firstly training the Global Vectors (GloVe) following <ref type="bibr" target="#b20">[20]</ref>, secondly incorporating the knowledge graph, and jointly learning the former two using the proposed model with a linear combination of both their objective functions.</p><p>Glove Vectors (GloVe) Firstly, we train the embedding vectors by creating a global word co-occurrence matrix X from both the input and target sentences, where each word in these sentences is represented by a row in X containing the co-occurrence of context words in a given contextual window. Finally, the GloVe embedding learning method minimizes the weighted least squares loss presented as:</p><formula xml:id="formula_9">J C = 1 2 i∈V j∈V f (X ij )(w i w j + bi + bj − log(X ij )) 2<label>(8)</label></formula><p>Where X ij denote the total occurrences of target word w i and the context word w j and the weighting function f assigns a lower weight for extremely frequent co-occurrences to prevent their over-emphasis.</p><p>Incorporating the knowledge graph The GloVe embeddings themselves do not take the semantic relation between corresponding words into account. Therefore, in order to leverage a knowledge graph while creating the word embeddings, we define an objective J s that only considers the three-way co-occurrences between a target word w i and one of its context word w j along with the semantic relation R that exists between them in the KG. The KG-based objective is defined as follows:</p><formula xml:id="formula_10">J S = 1 2 i∈V j∈V R(w i , w j )(w i − w j ) 2<label>(9)</label></formula><p>Where R(w i , w j ) indicates the strength of the relation R between w i and w j , which assigns a lower weight for extremely frequent co-occurrences to prevent over-emphasising such co-occurrences and if there is no semantic relation between w i and w j then R(w i , w j ) will be set to zero.</p><p>For the final step, to train both the corpus and the KG objectives jointly. We define the combined objective function J as their linearly weighted combination</p><formula xml:id="formula_11">J = J C + λJ S<label>(10)</label></formula><p>λ is the regularization coefficient that regulates the influence of the knowledge graph on those learned from the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Regularizing using additional Entity Loss</head><p>After projecting the knowledge graphs and the text in the same vector space, we compute an additional loss which is equal to the vector distances between the predicted entity and correct entity. The intuition behind using this loss is to penalize the model for predicting the wrong entity during decoding, in vector space. We use cosine distance to measure the entity loss as given by:</p><formula xml:id="formula_12">Entity l = 1 − φ ecorr .φ e pred φ ecorr φ e pred<label>(11)</label></formula><p>φ ecorr and φ e pred being the vector embeddings for the correct and predicted entities, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Final objective Function</head><p>The final objective function for training the model is the summation of the cross-entropy (CE) loss from the decoder (Vocab l ), the cross-entropy loss from the intent predictor (Intent l ) and the entity loss (Entity l ). The total loss is given by:</p><formula xml:id="formula_13">L tot = Vocab l + Intent l + Entity l<label>(12)</label></formula><p>The model is trained with back-propagation using Adam <ref type="bibr" target="#b8">[9]</ref> optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Key-Value Entity Look-up</head><p>As mentioned before the knowledge graph (KG) can change from one dataset to another or even as in our case from one dialogue to another. This will result in the generation of KG entities that do not belong to the current dialogue. Since <ref type="figure">Fig. 1</ref>. Sequence-to-Sequence based module with multi-task learning of user intent regularized with additional Entity Loss we are incorporating global knowledge information using the joint embedding, this additional step would ensure local KG integration aiding in the betterment of the models. To accomplish this, we added an additional lookup step during decoding. While generating a new token during decoding at time-step t, we firstly check if the predicted token is an entity. If it's an entity, we first do a lookup into the local dialogue KG to check its presence. If the entity is not present, then we pick the one with the entity with the highest softmax probability that is present in the local KG using greedy search. The technique is illustrated in <ref type="figure" target="#fig_0">figure 2</ref>. As seen in the figure, for the query what is the weather like in New York, today?, during decoding at t = 2, the model outputs a softmax distribution with highest probability 3 (0.08) for the token raining after predicting it is. We keep a copy of all the global KG entities and first check whether the predicted token is an entity in the global KG or not. Since raining is indeed an entity, we further do a look up into the local dialogue KG if it exists. For this scenario it doesn't, hence we do a greedy search for the next best-predicted tokens which are present in the local KG. In this specific case, sunny is the next most probable entity that exists in the local KG, We pick this entity for the time-step and feed it into the next. The final output will be it is sunny today in New York.</p><p>One such example in a real-case setting is shown in table 6. The distance for the gas station was generated as 2_miles, whereas we can observe that in the local KG, the correct distance entity is 5_miles. Hence by performing the additional entity lookup, we replace 2_miles with the correct entity during decoding. Empirical evidence suggests that this strategy can gain in performance as depicted in table 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>To test our hypothesis of multi-task learning of intent and dialogues, we needed a dataset which has multiple domains and user intents annotated along with a knowledge graph for task-oriented dialogues. <ref type="bibr" target="#b5">[6]</ref> introduced such a multi-turn, multi-domain task-oriented dialogue dataset. They performed a Wizard-of-Oz based data collection scheme inspired by <ref type="bibr" target="#b24">[24]</ref>. They used the Amazon Mechanical Turk (AMT) platform for data collection. The statics of the datasets are mentioned in <ref type="table" target="#tab_1">Table 2</ref>. The dataset is a conversation workflow in an in-car setting where the driver (user) is asking the assistant for scheduling appointments, weather forecasts, and navigation. Both roles are played by AMT turkers (workers). In driver mode, the turker asked a question while the assistant turker responds to the query using the provided knowledge graph. The Knowledge graph statistics are provided in <ref type="table" target="#tab_2">Table 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-processing and Model Hyperparameters</head><p>We processed the dataset by creating inputs out of the driver's utterances and the accompanied KG and the model outputs would be the current task intent and the assistants' output. We also processed the entire knowledge graph to identify the entities and convert them into canonicalized forms. We also split entities with multiple objects. For instance, a weather forecast that looks like "frost, low of 20F, high of 30F" can be split into (weather-condition="frost", low-temperature="low of 20F", high-temperature="high of 30F").</p><p>For training the Joint Embedding we used a contextual window for the cooccurrence matrix equal to 15 which is 1 4 th of the mean of input size. We trained the model with α = 0.01 and λ = 10000 and the embedding vectors with dimensions of 300 for 500 epochs with learning rate equals to 1e−4 and a weight-decay of 1e − 6. The stats regarding the knowledge graph is provided in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>As for the sequence-to-sequence model, we trained each of them on a GPU with 3072 CUDA cores and a VRAM of 12GB. The model is trained for 1000 epoch with a batch size of 128 and a hidden layer size equal to the embedding dimension of 300. We set the learning rate to be 1e − 4 for the encoder and 5e − 4 for the decoder, we also added a gradient clipping of 50.0 for countering the 'exploding gradient' problem; by doing so we prevent the gradients from growing exponentially and either overflow (undefined values), or overshoot steep cliffs in the cost function. We didn't do any exhaustive hyper-parameter optimization, the values reported here are the ones used for all models. Our codes and preprocessing scripts are available in 4 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>In this section, we report the results from our model on the mentioned dataset. We evaluate our system using BLEU scores as suggested by <ref type="bibr" target="#b19">[19]</ref>. BLEU as defined by <ref type="bibr" target="#b19">[19]</ref> analyzes the co-occurrences of n-grams in the reference and the proposed responses. It computes the n-gram precision for the whole dataset, which is then multiplied by a brevity penalty to penalize short translations We are reporting the geometric means of BLEU-1, BLEU-2, BLEU-3, and BLEU-4.</p><p>In addition to that we evaluated the both our model and the Mem2Seq model proposed by <ref type="bibr" target="#b15">[16]</ref> using an Embedding-based metrics such as Greedy Matching, Embedding Average and Vector Extrema as proposed by <ref type="bibr" target="#b11">[12]</ref> 5 . We are using an Attention based seq-to-seq model as a baseline along with the other state-ofthe-art models on this corpora <ref type="bibr" target="#b5">[6]</ref> (KV Retrieval Net) and <ref type="bibr" target="#b15">[16]</ref>  <ref type="table" target="#tab_1">(Mem2Seq H3)</ref>.</p><p>We are reporting our best model which is attention based Sequence-to-sequence model (S2S+Intent+JE+EL) with Joint Embeddings (JE) as inputs and trained as a multi-task objective of learning response utterance along with user Intent further, regularized using Entity Loss (EL). This model is further improved with an additional step called Key-Value Entity Look-up (KVL) which is done during inference. The method was explained in 3.6. As seen from the results, our proposed architecture has absolute improvements of 0.92 over KV Retrieval Net and 1.52 over Mem2Seq (H3) on BLEU. Although, the results are not directly comparable with the latter because we use canonicalized entity forms like <ref type="bibr" target="#b5">[6]</ref>. We are reporting BLEU scores because for task-oriented dialogues there's not much variance between the generated answers, unlike open-domain dialogues <ref type="bibr" target="#b11">[12]</ref>. We are using the official version (i.e. moses multi-bleu.perl script) <ref type="bibr" target="#b5">6</ref> for doing all the evaluations. Also, as seen in table 4, our proposed model performs significantly better than Mem2Seq on emedding-based metrics too. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Study</head><p>As mentioned in the results, our best model is based on Joint embeddings as inputs, followed by jointly learning user intent and text generation; with entity loss based on further regularization techniques. To analyze which specific part is influencing the performance, we did an ablation study dissecting the different components of the model one by one. The performances are reported in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>The first model is a sequence-to-sequence (S2S) model with Glove <ref type="bibr" target="#b20">[20]</ref> embeddings as input. These embeddings are trained on the training set. It can be seen that with the domain-vocabulary knowledge the model already performs better than vanilla attention sequence-to-sequence model. The next (S2S+JE) is the same model with Joint text and knowledge graph embeddings (also trained on the dialogue corpus and provided knowledge graph). This model has an absolute improvement of 2.93 in BLEU scores. Interestingly, adding the intent along with Joint Embeddings drops the performance by 0.7 but the encounters a little improvement with glove vectors. The model sees a further boost in performances (relative improvement of 3.7) over vanilla models with GloVe. All the models encounter better performances with the proposed key-value lookup (KVL) technique during inferencing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Qualitative Analysis</head><p>To qualitatively understand the performance improvements of the model with a different setting as in <ref type="table" target="#tab_4">Table 5</ref> we analyze the outputs from all of them for a given user query. For the user (driver) query Where is the nearest gas station ?, predicted responses are given in <ref type="table">Table 6</ref>. The knowledge graph snapshot for this particular dialogue scenario is in <ref type="table">Table 7</ref>. <ref type="table">Table 6</ref>. Example of generated responses on the Navigation domain explaining the improvement process through the model stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Driver</head><p>Where is the nearest gas station? S2S chevron is 5_pm away at &lt;UNK&gt; it is going to snow S2S+GloVe chevron is 3_pm at room_215 S2S+JE chevron is 5_miles at &lt;UNK&gt; high_of_30_f S2S+Intent+JE chevron is at 783_arcadia_pl S2S+Intent+JE+EL chevron is 2_miles away at 783_arcadia_pl S2S+Intent+JE+EL+KVL chevron is 5_miles away at 783_arcadia_pl Target chevron is 5_miles away at 783_arcadia_pl</p><p>As observed, the sequence-to-sequence (S2S) model is not able to produce knowledge grounded responses since it has no information about the background knowledge graph. Using Joint Embeddings (S2S+JE), although produces more knowledge grounded responses (since chevron is an entity of type gas_station and is at a distance of 5_miles), it outputs entities like high_of_30_f which is not related to navigation intent but the weather. Incorporating intent into the model (S2S+Intent+JE) ensures that it is generating words conditionally dependent on the intent also. Further grammatical improvements in the model response are seen with entity loss (S2S+Intent+JE+EL) which is further made more knowledge grounded with the proposed KVL method as seen in the last response (S2S+Intent+JE+EL+KVL). To further understand the quality of the produced responses, we did a human evaluation of a subset of the predicted responses from the test set. We asked the annotators to judge the response whether it is human-like or not on a scale of 1-5. The average score given by the annotators is 4.51. <ref type="table">Table 7</ref>. KG triples for the dialogue in <ref type="table">Table 6</ref>, for navigation.</p><p>Subject Relation Object chevron distance 5_miles chevron traffic_info moderate_traffic chevron poi_type gas_station chevron address 783_arcadia_pl</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Error Analysis</head><p>To further understand the model performance, we did a further analysis of the responses from the test set which gave very low BLEU scores during evaluation. The target and the predicted response are in <ref type="table" target="#tab_5">Table 8</ref>. As observed, the model produces fewer word overlaps for generic responses like have a good day. But, the responses are grammatically and semantically correct, which cannot be measured using BLEU scores. The other types of errors are factual errors where the model fails because it requires reasoning as in case of the 4 th example. The user is asking if the weather is cloudy in this case, in Fresno. <ref type="table">Table 9</ref>. KG triples and context for the error analysis in <ref type="table" target="#tab_5">Table 8</ref> for weather.</p><p>Subject Relation Object fresno monday clear_skies fresno monday low_40f</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>In this paper, we proposed techniques to improve goal-oriented dialogue systems using joint text and knowledge graph embeddings with learning user query intent as a multi-task learning paradigm. We also further suggested regularization techniques based on entity loss to improve upon the model. Empirical evaluations suggest that our suggested method can gain over existing state-of-the-art systems on BLEU scores for multi-domain, task-oriented dialogue systems. We are not reporting entity-f1 scores since we are treating canonicalized entity forms as vocabulary tokens and BLEU scores will already reflect the presence of the correct entity in the responses. For future endeavors, we would like to include KVL in the training using memory network modules instead of using it as a separate module during inference. We also observed that our model can benefit from better out-of-vocabulary (OOV) words handling which we would like to keep as a future work. Also, as observed in predicted responses for generic dialogues in first 3 rows in table 8, it would make sense to incorporate evaluations which also captures semantic similarities between predicted responses, we would like to work on this in future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>An example of how KVL works to replace predicted entities with correct ones from the local KG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>User query and respective intents User utterance Intent Book a seat for me in any good Chinese restaurant nearby Restaurant Booking Book a flight ticket for me from Berlin to Paris for tomorrow Flight Booking Please show me the direction to the main train station Navigation Please cancel my appointment with the dentist for tomorrow Scheduling</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Statistics of the in-car multi-turn, multi-domain, goal-oriented dialogue dataset.</figDesc><table><row><cell>Training Dialogues</cell><cell>2,425</cell></row><row><cell>Validation Dialogues</cell><cell>302</cell></row><row><cell>Test Dialogues</cell><cell>304</cell></row><row><cell cols="2">Avg. # Utterances Per Dialogue 5.25</cell></row><row><cell>Avg. # Tokens Per Utterance</cell><cell>9</cell></row><row><cell>Vocabulary Size</cell><cell>1,601</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Statistics of the trained Joint Embedding.</figDesc><table><row><cell>Number of entities</cell><cell>291</cell></row><row><cell>Number of Triples</cell><cell>2,512</cell></row><row><cell cols="2">Number of Triples found in context 1,242</cell></row><row><cell>Max Triple co-occurrence</cell><cell>750</cell></row><row><cell>Unique words co-occurrences</cell><cell>1,30,803</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results compared to baseline and State-of-the-art models.</figDesc><table><row><cell>Model</cell><cell cols="4">BLEU Emb. Avg. Vec. Ext. Greedy</cell></row><row><cell>Attention based Seq-to-Seq</cell><cell>8.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">KV Retrieval Net(no enc attention) 10.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KV Retrieval Net</cell><cell>13.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mem2Seq H3</cell><cell>12.6</cell><cell>0.668</cell><cell>0.742</cell><cell>0.528</cell></row><row><cell>S2S+Intent+JE+EL</cell><cell cols="2">14.12 -</cell><cell>-</cell><cell>-</cell></row><row><cell>S2S+Intent+JE+EL+KVL</cell><cell cols="2">18.31 0.955</cell><cell>0.974</cell><cell>0.625</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation Study.</figDesc><table><row><cell>Model Used</cell><cell cols="2">BLEU Score BLEU with KVL</cell></row><row><cell>S2S+glove</cell><cell>10.42</cell><cell>14.63</cell></row><row><cell>S2S+JE</cell><cell>13.35</cell><cell>15.29</cell></row><row><cell>S2S+Intent+JE</cell><cell>12.65</cell><cell>17.89</cell></row><row><cell cols="2">S2S+Intent+glove 13.25</cell><cell>17.27</cell></row><row><cell cols="2">S2S+Intent+JE+EL 14.12</cell><cell>18.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Error Analysis.</figDesc><table><row><cell>Predicted Sentence</cell><cell>Target Sentence</cell></row><row><cell>You're welcome</cell><cell>Have a good day</cell></row><row><cell>What city do you want the forecast for</cell><cell>What city can i give you this weather for</cell></row><row><cell>Setting gps for quickest route now</cell><cell>I picked the route for you drive carefully</cell></row><row><cell>It is cloudy with a low of 80f in fresno</cell><cell>There are no clouds in fresno right now</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">this is a fictitious example for explaining the algorithm, the scores are not what is being predicted from the real case scenarios</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/s6fikass/Chatbot_KVNN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We report these metrics for our best model and only for Mem2Seq because their implementation is open-source.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">multi-bleu: https://raw.githubusercontent.com/moses-smt/mosesdecoder/ master/scripts/generic/multi-bleu.perl</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>This work was partly supported by the European Union's Horizon 2020 funded projects WDAqua (grant no. 642795) and Cleopatra (grant no. 812997) as well as the BmBF funded project Simple-ML.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jointly learning word embeddings using a corpus and a knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alsuhaibani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">193094</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving response selection in multi-turn dialogue systems by incorporating domain knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kristiadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01476v6</idno>
		<title level="m">Convolutional 2d knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Towards end-to-end reinforcement learning of dialogue agents for information access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00777</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Key-value retrieval networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W17-366" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrucken, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dialogue response generation using neural networks with attention and background knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kosovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<ptr target="http://jens-lehmann.org/files/2017/cscubs_dialogues.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Science Conference for University of Bonn Students (CSCUBS)</title>
		<meeting>the Computer Science Conference for University of Bonn Students (CSCUBS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01454</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incorporating unstructured textual knowledge sources into neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Workshop on Machine Learning for Spoken Language Understanding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno>abs/1506.08909</idno>
		<ptr target="http://arxiv.org/abs/1506.08909" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Association for Computational Linguistics</title>
		<meeting>eeding of Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th</title>
		<meeting>the 56th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<ptr target="http://aclweb.org/anthology/P18-1136" />
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1468" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Jointly learning word embeddings using a corpus and a knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Danushka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takanori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kenichi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">193094</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pydial: A multi-domain statistical dialogue system toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Rojas Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P17-4013" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning: Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
		<title level="m">A network-based end-to-end trainable task-oriented dialogue system</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">End-to-end lstm-based dialog control optimized with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssp: Semantic space projection for knowledge graph embedding with text descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiaoyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. State Key Lab. of Intelligent Technology and Systems</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence. State Key Lab. of Intelligent Technology and Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Incorporating loose-structured knowledge into lstm with recall gate for conversation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05110</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
