<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Swift</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Data61 / CSIRO 3</orgName>
								<orgName type="laboratory">Australian Centre for Robotic Vision (ACRV) 2</orgName>
								<orgName type="institution" key="instit1">The Australian National University (ANU</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney (UTS</orgName>
								<address>
									<addrLine>) 4</addrLine>
									<region>Tencent AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Lab 5</orgName>
								<orgName type="institution">University of Turku</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sign language translation (SLT) aims to interpret sign video sequences into textbased natural language sentences. Sign videos consist of continuous sequences of sign gestures with no clear boundaries in between. Existing SLT models usually represent sign visual features in a frame-wise manner so as to avoid needing to explicitly segmenting the videos into isolated signs. However, these methods neglect the temporal information of signs and lead to substantial ambiguity in translation. In this paper, we explore the temporal semantic structures of sign videos to learn more discriminative features. To this end, we first present a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation. Taking advantage of the proposed segment representation, we develop a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called TSPNet. Specifically, TSPNet introduces an inter-scale attention to evaluate and enhance local semantic consistency of sign segments and an intra-scale attention to resolve semantic ambiguity by using non-local video context. Experiments show that our TSPNet outperforms the state-of-the-art with significant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96) on the largest commonly-used SLT dataset. Our implementation is available at https://github.com/verashira/TSPNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sign language translation (SLT), as an essential sign language interpretation task, aims to provide textbased natural language translation for continuously signing videos. Since sign languages are distinct linguistic systems <ref type="bibr" target="#b0">[1]</ref> which differ from natural languages, signed sentence and their translation into natural languages do not syntactically align. For instance, sign languages have different word ordering rules from their natural language counterparts. Because of such discrepancies between a sign language and its natural language translation, SLT methods are often required to jointly learn embedding space of sign sentence videos and natural languages as well as mappings between them, leading to a difficult sequential learning problem.</p><p>Existing SLT approaches can be categorized into two-staged and bootstrapping approaches depending on whether they require additional annotations for video and text alignments or not. Two-staged models require extra annotations, namely gloss, to describe sign videos with word labels in their occurring order. These models first learn to recognize gestures using gloss annotations and then rearrange the recognition results into natural language sentences. Gloss annotations significantly ease the syntactic alignment in these approaches. However, gloss annotations are not easy to acquire since they require expertise in sign languages <ref type="bibr" target="#b1">[2]</ref>. In contrast, bootstrapping models directly learn to translate from video inputs to natural language sentences without gloss annotations. These models extend easily to a wider range of sign language resources, and have recently attracted great research interests. This paper also investigates bootstrapping methods and aims to minimize the translation accuracy gap between these two approaches by learning more expressive sign features.</p><p>Sign gestures are the minimal units that preserve semantics in sign language videos. However, because of motion blurs, fine-grained gestural details, and the transitions between different sign gestures, inferring boundaries between sign gestures is difficult. Thus, current approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> extract sign features in a frame-wise fashion. By doing so, only spatial appearance features are captured while neglecting the temporal dependencies between sign gestures. However, temporal information is helpful in distinguishing different signs when similar body poses appear, and therefore we expect that this information to be useful in SLT models.</p><p>In this paper, we propose a Temporal Semantic Pyramid Network (TSPNet) to learn features from video segments instead of single frames. Particularly, we aim to learn sign video representations that encode both spatial appearance and temporal dynamics. However, obtaining accurate gesture segments from a continuous sign video is difficult, while noisy segments bring substantial ambiguity for feature learning. Here, we observe two important factors impacting the semantics of sign segments. First, sign video semantics are coherent, implying that segments that are temporally close share consistent semantics locally. Second, the semantics of sign gestures are context-dependent. Namely, non-local information is helpful to disambiguate the semantics of local gestures. Motivated by these, we divide each video into segments of different granularities. Our proposed TSPNet then exploits the semantic consistency among them to further enhance sign representations. To be specific, after organizing multiple video segments of different granularities in a hierarchy, our TSPNet enforces local semantic consistency by aggregating features of segments in each semantic neighborhood using an inter-scale attention. When tackling local ambiguity caused by imprecise segmentation, we develop an intra-scale attention to re-weight the local gesture features along the whole video sequence. By learning features from sign segments in a hierarchical approach, our TSPNet captures temporal information in sign gestures thus producing more discriminative sign video features. As a result of stronger feature semantics, we ease the difficulty in constructing mappings between sign videos and natural language sentences, thus improving the translation results.</p><p>Our model significantly improves the translation quality on the largest public sign language translation dataset RWTH-PHOENIX-WEATHER-2014T, increasing the BLEU score from 9.58 [2] to 13.41 and ROUGE score from 31.80 <ref type="bibr" target="#b1">[2]</ref> to 34.96, greatly relaxing the constraint on expensive gloss annotations for sign language translation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sign Word Recognition. Most sign language works focus on word-level sign language recognition (WSLR), aiming to recognize a single gesture from an input video <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Although many efforts have been devoted to WSLR, few works investigate the connections between WSLR and SLT, thus hindering the usage of WSLR models in practice. Earlier WSLR works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13]</ref> conduct studies on constrained subsets of vocabulary, resulting in less generalizable features. The recent research outcomes <ref type="bibr" target="#b9">[10]</ref> show that large-scale WLSR datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> facilitate the generalization ability of sign feature learning. Motivated by this, we make the first attempt to apply the knowledge of WLSR models to the SLT task by reusing WLSR backbones to extract video features. Interestingly, our experiments indicate that an American Sign Language (ASL) WLSR backbone network is even effective for unseen sign languages, such as German Sign Language (GSL).</p><p>Sign Language Translation. A major challenge in sign language translation is the alignment between sign gestures and words of a natural language. One solution is to manually provide gloss annotations for each gesture in a video as aforementioned. However, glosses often require sign language expertise to annotate. Thus, they are expensive to label and not always available. Cihan et al. <ref type="bibr" target="#b1">[2]</ref> propose a sign2text (S2T) model to predict translations directly from sign videos without glosses, which is referred to as a bootstrapping approach. In particular, their models learn video features in a frame-wise manner. Since sign gestures span over multiple continuous frames, their approach overlooks the temporal dependencies in sign gestures. Different from their work, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Input</head><p>..  <ref type="figure">Figure 1</ref>: Overview of the workflow of our proposed TSPNet, which generates spoken language translations directly from sign language videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frames in Pivot Other Frames</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Weights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Translation Output</head><p>propose to learn video features from segments, and model both spatial appearance and temporal dynamics of sign gestures simultaneously. Our feature learning method exploits local and non-local temporal structure in a hierarchical way to learn discriminative sign video representations while counteracting the effect of inaccurate sign gesture segmentation.</p><p>Neural Machine Translation (NMT). The NMT task aims to translate one natural language to another. Most NMT models follow an encoder-decoder paradigm. Earlier works use RNN to model temporal semantics <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Later, attention mechanisms are adopted to deal with long-term dependencies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Instead of RNNs, recent Transformer models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> fully rely on attention and feed-forward layers for sequence modeling. They obtain a large improvement in both translation quality and efficiency. In our work, we develop a novel encoder architecture in order to fully exploit the local and non-local semantics in sign videos, while reusing Transformer decoder to produce translations in natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Temporal Semantic Pyramid Network</head><p>Our TSPNet employs an encoder-decoder architecture. The encoder learns discriminative sign video representations by exploiting the semantic hierarchical structure among video segments. The output of the encoder is fed to a Transformer decoder to acquire the translation. In this section, we first describe our proposed multi-scale segment representation for sign videos. Then we introduce the proposed hierarchical feature learning method. To focus on our main contributions, we omit the detailed decoder architecture and refer readers to <ref type="bibr" target="#b18">[19]</ref> for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-scale Segment Representation</head><p>Previous SLT approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> learn video features in a frame-wise manner. Since a sign gesture usually lasts for around half a second (∼12 frames) <ref type="bibr" target="#b20">[21]</ref>, these features from static images neglect the temporal semantics of gestures. Different from their approaches, we develop a segment representation for sign videos, and aim to learn both spatial and temporal semantics of sign gestures. However, as aforementioned it is difficult to obtain accurate sign gesture boundaries. To alleviate the influence of imprecise sign video segmentation, we exploit the semantic consistency among segments of different granularities in a hierarchy. Specifically, we employ a sliding window approach to create video segments with multiple window widths.</p><p>Windowing Segments. Given a video of N frames x = {x 0 , x 1 , ..., x N −1 } with x i a video frame, a video segment x m, n is a subsequence of x, denoted as {x m , x m+1 , ..., x m+n−1 }. For a window width w ∈ N and a stride s ∈ N, we define windowing segments of x with width w and stride s as</p><formula xml:id="formula_0">Φ(x, w, s) = {x ks, w | 0 ≤ k &lt; N w }.</formula><p>Windowing segments evenly divide an input video into overlapping clips. However, since sign gestures in a video vary in length, it is hard to choose an appropriate window width: smaller segments tend to capture finer-grained gestures but provide weaker contextual semantics, while larger ones are inferior to capture short gesture semantics but provide stronger context knowledge. To make the most out of the segment representation, we introduce multi-scale segment representation and complement the semantics of short and long segments with each other. Specifically, a multi-scale segment representation of video x is a set of windowing segments</p><formula xml:id="formula_1">{Φ(x, w i , s i ) | 0 ≤ i &lt; M },</formula><p>where M , w i , and s i denote the number of scales, a window width and a stride. In the following, we refer to the scales with short and long segments as small and large scales, respectively. Given the multi-scale segment representation of a video, we employ a 3D convolution network I3D <ref type="bibr" target="#b21">[22]</ref> to extract video features for each segment. In order to adapt our backbone network to sign language gestures, we further finetune I3D on two WSLR datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Video Feature Learning for Sign Language Translation</head><p>Inaccurate sign video segmentation leads to substantial ambiguity in gesture semantics. As a result, straightforward combinations of multi-scale segments, such as pooling or concatenation, do not necessarily improve the overall translation results. To deal with the issue, we start from two key observations on the semantic structure of sign language videos, i.e. local consistency and context dependency. First, gestures in sign language videos continuously evolve. This implies that video semantics change coherently. Therefore, segments that are temporally close are expected to share consistent semantics. Second, similar sign gestures translate to different words according to the context <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>, and non-local video information is important for resolving semantic ambiguity in the individual gestures especially when the video segmentation is noisy. Driven by these observations, we develop a hierarchical feature learning method that utilizes local temporal structure to enforce semantic consistency and non-local video context to reduce semantic ambiguity. <ref type="figure">Figure 1</ref> illustrates an overview of our TSPNet. For a given sign video, we first generate its multi-scale segment representation and extract features from our I3D network G I3D (·). We also develop a Shared Positional Embedding layer (Section 3.2.1) to inform the positions of segments in a sequence. Next, we propose to learn semantically consistent representations by aggregating features in each local neighborhood (Section 3.2.2). Lastly, TSPNet collects all the aggregated features and uses them to provide non-local video context to resolve the ambiguity of local gestures (Section 3.2.3). As an alternative, we also introduce a joint learning layer to consolidate the feature learning by utilizing local and non-local information simultaneously (Section 3.2.4). For notational convenience, we denote the windowing segments at i-th scale Φ(x, w i , s i ) as Φ i , and its k-th segment x ksi,wi as φ i,k .</p><p>We use φ f i,k = G I3D (φ i,k ) ∈ R D to represent the feature of the segment φ i,k from the backbone, with D the feature dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Shared Positional Encoding</head><p>Similar to words in spoken languages, the position of a sign gesture in the whole video sequence is important for interpretation. Inspired by recent works on sequence modeling <ref type="bibr" target="#b18">[19]</ref>, we inject the position information to video segments by representing position indices in an embedding space. Specifically, we learn a function G spe (·) that maps each position index into an embedding with the same length of the segment feature. Such positional embeddings are then added to the segment features at the corresponding position in each scale, resulting in position-informed segment representation</p><formula xml:id="formula_2">φ f i,k = φ f i,k + G spe (k)</formula><p>. We pad each video by repeating the last frame to ensure the number of segments in each scale is equal. As a result, we have the same number of position indices in each scale. This allows us to share the weights of G spe (·) across all the scales. By sharing weights of G spe (·), we reduce the number of model parameters, especially when the number of segments is large, thus benefiting the training efficiency and alleviating overfitting when data is limited. The output of the shared positional embedding layer is the position-informed video representation in M scales, i.e.</p><formula xml:id="formula_3">{ Φ f 0 , Φ f 1 , ..., Φ f M −1 }, where each scale has L segment features Φ f i = { φ f i,0 , φ f i,1 , ..., φ f i,L−1 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Enforcing Local Semantic Consistency</head><p>Taking advantage of the multi-scale segment representation, we tackle the issue of imperfect video segmentation by complementing smaller segments with larger but semantically relevant ones. This is  From the 26-th to 37-th frame, the GSL sign is the word süden (south).</p><p>achieved by an inter-scale attention-based aggregation of segments within surrounding neighborhoods. A surrounding neighborhood consists of features of one pivot segment (see <ref type="figure" target="#fig_3">Figure 2</ref>) from the smallest scale Φ f 0 , and features of multiple neighbor segments from larger scales. Specifically, for each pivot segment, we construct its surrounding neighborhood by including segments from larger scales if their frames superset those of the pivot segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surrounding Neighborhood. Given position-informed video representation</head><formula xml:id="formula_4">{ Φ f 0 , Φ f 1 , ..., Φ f M −1 }</formula><p>, with window widths arranged in an ascending order w 0 &lt; w 1 &lt; ... &lt; w M −1 , we name the feature of a pivot segment, φ f ∈ Φ f 0 , as a pivot feature. We define the surrounding neighborhood of a pivot feature φ f as a set Surrounding neighborhood imposes a containment relation between a pivot and its neighbors. As a result, we ensure gestures appearing in the pivot segment are also included in the neighbors. In this way, we use neighbors to provide more contextual clues for the pivot, and encourage the model to learn an aggregated feature that best represents the local temporal region.</p><formula xml:id="formula_5">N φ f = φ f ∪ { ψ f | ψ f ∈ M −1 i=1 Φ f i , φ ⊂ ψ}, with φ f ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-scale Attention Aggregation. Given position-informed video features</head><formula xml:id="formula_6">{ Φ f 0 , Φ f 1 , ..., Φ f M −1 }</formula><p>, our hierarchical feature learning method first enforces local semantic consistency to compensate for the effect of inaccurate video segmentation. As <ref type="figure" target="#fig_3">Figure 2</ref> shows, longer segments in the neighborhood capture more transitional gestures while shorter segments focus on fine-grained movements, both of which are helpful to recognize sign gestures in the local region. Therefore, for each pivot feature Φ f 0 , we retrieve its surrounding neighbors and aggregate them using an inter-scale attention. Specifically, since segments of different scales capture different semantic aspects of the gesture, they may not reside in the same embedding space. In this regard, we first apply a linear mapping W g ∈ R D ×D to their features and map them into a shared space in R D . We then perform scaled dot-product attention to aggregate neighbor features into the pivot feature φ f 0,k ∈ Φ f 0 ,</p><formula xml:id="formula_7">Z k = [W g z 0 , W g z 1 , ..., W g z P −1 ] T , where z j ∈ N φ f 0,k , P = N φ f 0,k ,<label>(1)</label></formula><formula xml:id="formula_8">c k = G attn (W g φ f 0,k , Z k , Z k ),<label>(2)</label></formula><p>where G attn (·) is the scaled dot-product attention G attn (Q, K, V) = softmax(QK T / √ d)V. G attn (·) measures the correlation between Q, K and uses it to re-weight V; with d the dimension of vectors in K. The scaling factor √ d handles the effect of growing magnitude of dotproduct with larger d <ref type="bibr" target="#b18">[19]</ref>. In order to learn more expressive features, we add two linear layers <ref type="bibr" target="#b23">[24]</ref> in between as follows,</p><formula xml:id="formula_9">W 1 ∈ R D×D , W 2 ∈ R D×D with a GELU activation</formula><formula xml:id="formula_10">h k = W 2 · GELU(W 1 c k + b 1 ) + b 2 ,<label>(3)</label></formula><p>where b 1 and b 2 ∈ R D are biases of the corresponding fully-connected layers. Each h k encodes the aggregated semantics of segments across all the scales, thus providing a locally consistent representation of the sign gestures in the temporal region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Non-local Semantic Disambiguation</head><p>The interpretation of individual sign language gestures depends on the sentence-level context. First, the sign gesture of a word is usually a composition of two or even more "meta-signs". For example, the word "driver" requires to perform signs of "car" and "person" in order <ref type="bibr" target="#b0">[1]</ref>. The sign of "person" may translate to "teacher" or "student" depending on the accompanying word. Therefore, semantics for these signs are clarified only in the presence of context information. Second, there exist quite a few similar sign gestures <ref type="bibr" target="#b8">[9]</ref>. For instance, signs of "wish" and "hungry" are very similar and are hardly distinguishable without the context. Due to the imprecise gesture segments, these ambiguities become even more severe. It is thereby important for an SLT model to consider non-local sentence information in order to resolve the semantic ambiguity. Hence, we propose to model non-local video context by an intra-scale attention over the sequence of enriched pivot features.</p><p>Intra-scale Attention Aggregation. After we aggregate multi-scale features into pivots, we design an intra-scale attention which takes h = [h 0 , h 1 , ..., h L−1 ] T , h k ∈ R D as input, in order to enhance features across all the local regions. This is achieved by a self-attention operation on the aggregated local features, i.e. e = G attn (W e h, W e h, W e h), where W e ∈ R D ×D and D denotes the dimension of the hidden embedding space. Similar to the inter-scale attention, the self-attention layer is followed by two fully-connected layers for feature transformation, and then we acquire the output, namely o ∈ R D . We feed the output into a Transformer decoder for translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Joint Learning of Local and Non-local Video Semantics</head><p>In the previous sections, the intra-scale attention follows sequentially the inter-scale attention. Therefore, the model does not have knowledge of non-local video context when enforcing local semantic consistency. This is not ideal when the non-local context is helpful for recognizing local sign gestures and easing the noisy segmentation issue. As an alternative, we propose to jointly learn local and non-local video semantics so that the two information sources interact thoroughly. In this way, non-local information helps to better recognize local gestures. In the meanwhile, enhanced local gesture semantics contribute to resolve ambiguity in turn. For this purpose, we include all the pivot segments into each surrounding neighborhood, leading to extended surrounding neighborhood.</p><formula xml:id="formula_11">Extended Surrounding Neighborhood. For segment representation { Φ f 0 , Φ f 1 , ..., Φ f M −1 }</formula><p>of an input video, whose window widths are arranged in an ascending order</p><formula xml:id="formula_12">w 0 &lt; w 1 &lt; ... &lt; w M −1 . An extended surrounding neighborhood of a pivot feature φ f is a set N * φ f = N φ f ∪ Φ f 0 .</formula><p>As an extended surrounding neighborhood includes semantically relevant multi-scale segments and all the pivot segments, we aggregate to learn both local and non-local sign video features, as follows,</p><formula xml:id="formula_13">Z * k = [W c z * 0 , W c z * 1 , ..., W c z * Q−1 ] T where z * j&gt;0 ∈ N * φ f 0,k , Q = N * φ f 0,k ,<label>(4)</label></formula><formula xml:id="formula_14">c * k = G attn (Z * k , Z * k , Z * k ).<label>(5)</label></formula><p>In this way, we bring forward the non-local video context and encourage models to jointly learn to recognize sign gestures locally and mitigate the semantic ambiguity due to the inaccurate video segmentation. We eventually pass c * = [c * 0 , c * 1 , ..., c * L−1 ] T to two fully-connected layers to acquire the encoder output, which later passes into the Transformer decoder for generating the translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup and Implementation Details</head><p>Dataset. We evaluate TSPNet on RWTH-PHOENIX-Weather 2014T (RPWT) dataset <ref type="bibr" target="#b1">[2]</ref>. It is the only publicly available standard SLT dataset that is used for large-scale training and inference. We follow the official RPWT data partition protocol, where 7096, 519, 642 videos are used for training, validation and testing sets, respectively. These samples are performed by nine different signers in German Sign Language (GSL) and the translations in German are also provided. RPWT dataset contains a diverse vocabulary of around 3k German words. This distinguishes SLT from most visionand-language tasks that usually have a limited vocabulary and simple sentence structure <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Metrics. We adopt BLEU <ref type="bibr" target="#b26">[27]</ref> and ROUGE-L <ref type="bibr" target="#b27">[28]</ref> scores, two commonly used machine translation metrics, for evaluation. BLEU-n measures the precision of translation up to n-grams. For instance, BLEU-4 summarizes precision scores of 1, 2, 3 and 4-grams. We use ROUGE-L that measures the F1 score based on the longest common sub-sequences between predictions and ground-truth translations. In general, both metrics are expected to be significantly lower than 100, as there are multiple valid translations of the same meaning in natural language. This phenomenon, however, is not well quantified by existing translation metrics. Implementation and Optimization. We implement the proposed TSPNet using FAIRSEQ <ref type="bibr" target="#b28">[29]</ref> framework in PYTORCH <ref type="bibr" target="#b29">[30]</ref>. Since a sign gesture on average lasts around half a second (∼12 frames) <ref type="bibr" target="#b20">[21]</ref>, we determine the minimal segment width to be 8 and enlarge it by √ 2 to another two scales, i.e., 12 and 16 frame segments. In each scale, we apply a stride of 2 frames to reduce the feature sequence lengths while keeping the most semantic information. In order to extract video features, we start with a pretrained I3D networks on Kinetics <ref type="bibr" target="#b30">[31]</ref>, and then finetune it on two WSLR datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> in ASL to adapt to sign gesture videos. To represent texts in the feature space, we adopt SENTENCEPIECE <ref type="bibr" target="#b31">[32]</ref> German subword embedding <ref type="bibr" target="#b32">[33]</ref>, which are based on character units to handle low-frequency words. We optimize TSPNet using Adam optimizer <ref type="bibr" target="#b33">[34]</ref> with a cross-entropy loss as in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref>. We set an initial learning rate to 10 −4 and a weight decay to 10 −4 . We train our networks for 200 epochs, which is sufficient for all the models to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with the State-of-the-art</head><p>Competing Methods. We compare our TSPNet with two bootstrapping SLT methods. (i) Conv2d-RNN <ref type="bibr" target="#b1">[2]</ref> achieves the state-of-the-art performance on RPWT dataset. It extracts features using AlexNet <ref type="bibr" target="#b35">[36]</ref> and employs GRU-based <ref type="bibr" target="#b36">[37]</ref> encoder-decoder architecture for sequence modeling. It also exploits multiple attention variants on top of recurrent units <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. We also conduct comparisons with those attention based variants. (ii) TSPNet-Single: in this baseline, we feed segments from only a single scale into our TSPNet. With merely single-scale feature used, it only applies intra-scale attention aggregation, which is equivalent to vanilla self-attention. As a result, this baseline degenerates to a Transformer model. Quantitative Comparison. We report translation results of our TSPNet and the competing models in <ref type="table" target="#tab_0">Table 1</ref>. The row of TSPNet-Sequential refers to the setting in Section 3.2.2 and Section 3.2.3, where we apply inter-and intra-scale attentions sequentially. TSPNet-Joint refers to the setting in Section 3.2.4, where we enhance local and non-local video semantics by jointly modeling them. As indicated in <ref type="table" target="#tab_0">Table 1</ref>, both settings outperform the state-of-the-art SLT model, Conv2d-RNN, by a large margin, with relative improvements on BLEU-4 score by 39.80% (9.58 → 13.41) and on ROUGE-L by 9.94% (31.80 → 34.96). Benefiting from our proposed sign segment video representation, our features encode not only spatial appearance information but also the temporal information of sign gestures, and thus is more discriminative. Compared to TSPNet-Single, the multi-scale settings improve SLT performance on all metrics. This shows the effectiveness of our hierarchical feature learning. In addition, TSPNet-Joint achieves superior performance to TSPNet-Sequential. This reflects that including non-local video context is beneficial to resolve local ambiguity caused by imprecise gesture segments. Compared with previous bootstrapping approaches, TSPNet significantly narrows the performance gap between bootstrapping approaches and two-staged ones. Computationally, it costs around two hours to train a TSPNet-Joint model on a single NVIDIA V100 GPU, excluding the time for the one-off offline visual feature extraction.</p><p>Qualitative Comparison. <ref type="table" target="#tab_1">Table 2</ref> shows two example translations produced by TSPNet and the stateof-the-art model, Conv2d-RNN. In the first example, TSPNet produces a very accurate translation while Conv2d-RNN fails to interpret the original meanings. In the second example, the translation from our model retains the meaning of the sign by using the synonym of the word "rain", (i.e., "shower"), while Conv2d-RNN does not capture the correct intent. However, this difference is not fully reflected on the adopted metrics. More results are provided in the supplementary material. Ground Truth: der wind weht meist schwach aus unterschiedlichen richtungen .</p><p>( mostly windy, blowing in weakly from various directions . ) Conv2d-RNN [2]+ <ref type="bibr" target="#b16">[17]</ref>: der wind weht schwach bis mäßig .</p><p>( windy, blows weak to moderate . ) Ours: der wind weht meist schwach aus unterschiedlichen richtungen .</p><p>( mostly windy, blowing in weakly from various directions . )</p><p>Ground Truth: im süden und südwesten gebietsweise regen sonst recht freundlich .</p><p>( in the south and southwest locally rain otherwise quite friendly . ) Conv2d-RNN [2]+ <ref type="bibr" target="#b16">[17]</ref>: von der südhälfte beginntes vielerorts .</p><p>( from the southpart it starts in many places . ) Ours: im süden gibt es heute nacht noch einzelne schauer .</p><p>( in the south there are still some showers tonight . ) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Analysis and Discussions</head><p>In this section, we investigate the effects of different components and design choices of TSPNet-Joint, our best model, on the translation performance.</p><p>Multi-scale Segment Representation. We first study the impacts of feature in different scales. As seen in <ref type="table" target="#tab_2">Table 3</ref>, the performance of our model increases as we progressively incorporate multi-scale features. This demonstrates that learning sign representations from hierarchical features mitigates the inaccurate video segment issue. When only single-scale features are exploited, our TSPNet-Joint model degenerates to the Transformer model. We notice that the inclusion of segments with a width 16 improves the model most. This is also consistent with the finding that the 16-frame segments offer the most expressive features in a single-scale model, as indicated in <ref type="table" target="#tab_0">Table 1</ref>. However, by incorporating segments of larger widths (e.g., 24 frames), we observe a slight performance drop. This is because 24 frame segments (around 1 second) usually contain more than one sign gesture, where local semantic consistency may not hold.</p><p>Hierarchical Feature Learning. To investigate the effectiveness of our hierarchical feature learning method, we compare our method with three aggregation methods that do not fully consider semantic relevance among segments. (1) position-wise pooling: different from Section 3.2.2, we fuse features at the same temporal position across scales. Specifically, we first encode features on each scale using a separate encoder, and then apply a position-wise max-pooling operation over multi-scale segment features.</p><p>(2) position-wise FC: we first concatenate position-wise features and then employ two fully-connected layers for aggregating features. (3) nonrestrictive attention: unlike Section 3.2.3, this method allows each pivot to attend to all the segments on different scales to verify the importance of enforcing local consistency. As <ref type="table" target="#tab_2">Table 3</ref> shows, non-structural methods fail to utilize the multi-scale segments. On the contrary, all three settings result in worse translation quality TSPNet-Single <ref type="bibr" target="#b15">(16)</ref>. This signifies the role of semantic structures when combining multi-scale segment features.</p><p>Other Design Choices. (i) Without finetuning the I3D networks on the WLSR datasets, the best BLEU-4 score drops to 11.23. This shows our backbone features are generalizable to even unseen sign languages (e.g., GSL). (ii) When not sharing weights but learning a separate position embedding layer for each scale, we observe a slight drop of 0.08 in BLEU-4 compared to TSPNet-Joint. When we share positioning embedding layer weights, we not only reduce the model parameters but also further avoid overfitting. (iii) As indicated by TSPNet-Single (8), <ref type="bibr" target="#b11">(12)</ref> results, simply utilizing a Transformer encoder does not achieve better performance than <ref type="bibr" target="#b1">[2]</ref>. This implies that the performance gain mainly comes from the proposed hierarchical feature learning method. For the concern of training efficiency, recurrent operations are rather time-consuming (up to two orders of magnitude more training time in our case). Thus, we opt to avoid using recurrent units in our model. Additionally, TSPNet-Single <ref type="bibr" target="#b15">(16)</ref> achieves better results than <ref type="bibr" target="#b1">[2]</ref>. This shows even with a proper uniform segmentation, our segment representation is effective in capturing temporal semantics of signs.</p><p>Limitations and Discussions. Although the proposed hierarchical feature learning method proves effective in modeling sign language videos, we notice several limitations of our model. For example, low-frequency words such as city names are very challenging to translate. In addition, facial expressions typically reflect the extent of signs, e.g. shower versus rain storm, which are not explicitly modeled in our approach. We further note that the work <ref type="bibr" target="#b37">[38]</ref> achieves 20.17 BLEU-4 score when reusing the visual backbone networks from <ref type="bibr" target="#b38">[39]</ref>, which is reliant on the gloss annotations. In this regard, our proposed method greatly eases the requirements of expensive annotations, thus having the potential to learn sign language translation models directly from natural language sources, e.g. subtitled television footage. We therefore plan to resolve the aforementioned issues and further mitigate the performance gap between gloss-free and gloss-reliant approaches as future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented a Temporal Semantic Pyramid Network (TSPNet) for video sign language translation. To address the unavailability of sign gesture boundaries, TSPNet exploits semantic relevance of multi-scale video segments for learning sign video features, thus mitigating the issue of inaccurate video segmentation. In particular, TSPNet introduces a novel hierarchical feature learning procedure by taking advantage of inter-and intra-scale attention mechanism to learn features from nosily segmented video clips. As a result, the model learns more expressive sign video features. Experiments demonstrate that our method outperforms previous bootstrapping models by a large margin and significantly relaxes the requirement on expensive gloss annotations in video sign language translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>As of the year 2020, 466 million people worldwide, one in every ten people, has disabling hearing loss. And by the year of 2050, it is estimated that this number will grow to over 900 million <ref type="bibr" target="#b39">[40]</ref>. Assisting deaf and hard-of-hearing people to participate fully and feel entirely included in our society is critical and can be facilitated by maximizing their ability to communicate with others in sign languages, thereby minimizing the impact of disability and disadvantage on performance. Communication difficulties experienced by deaf and hard-of-hearing people may lead to unwelcome feelings of isolation, frustration and other mental health issues. Their global cost, including the loss of productivity and deaf service support packages, is US$ 750 billion per annum in the healthcare expenditure alone <ref type="bibr" target="#b39">[40]</ref>.</p><p>The technique developed in this work contributes to the design of automated sign language interpretation systems. Successful applications of such communication technologies would facilitate access and inclusion of all community members. Our work also promotes the public awareness of people living with hearing or other disabilities, who are commonly under-representative in social activities. With more research works on automated sign language interpretation, our ultimate goal is to encourage equitable distribution of health, education, and economic resources in the society.</p><p>Failure in translation leads to potential miscommunication. However, achieving highly-accurate automated translation systems that are trustworthy even in life-critical emergency and health care situations requires further studies and regulation. In scenarios of this kind, automated sign language translators are recommended to serve as auxiliary communication tools, rather an alternative to human interpreters. Moreover, RPWT dataset was sourced from TV weather forecasting, consequently, is biased towards this genre. Hence, its applicability to real-life use may be limited. Despite this linguistic limitation, RPWT remains the only existing large-scale dataset for sign language translation; this under-resourced area deserves more attention. Both datasets and models ought to be developed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 9 O i h S s l K j O / B 2 J f X 2 L 1 c 9 1 G + H 1 0 = " &gt; A A A B 8 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 8 V 7 A c 0 t W y 2 k 3 b p Z h N 2 N 0 I J / R t e P C j i 1 T / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 m w F 9 G h 5 C F n 1 F j J 9 z O / M e K P Y Z / 7 0 3 6 5 4 l b d O c g q 8 X J S g R y N f v n L H 8 Q s j V A a J q j W X c 9 N T C + j y n A m c F r y U 4 0 J Z W M 6 x K 6 l k k a o e 9 n 8 5 i k 5 s 8 q A h L G y J Q 2 Z q 7 8 n M h p p P Y k C 2 x l R M 9 L L 3 k z 8 z + u m J r z u Z V w m q U H J F o v C V B A T k 1 k A Z M A V M i M m l l C m u L 2 V s B F V l B k b U 8 m G 4 C 2 / v E p a F 1 W v V r 2 8 r 1 X q N 3 k c R T i B U z g H D 6 6 g D n f Q g C Y w S O A Z X u H N S Z 0 X 5 9 3 5 W L Q W n H z m G P 7 A + f w B C 5 u R s w = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 9 t s S K Q K M t e V Q U U L p 5 E w x 5 b 2 B l I I = " &gt; A A A B / 3 i c b V D L S s N A F J 3 U V 6 2 v q O D G z W A R X J V E K r o s u n F Z w T 6 g i W E y m T R D J 5 M w M 1 F K z M J f c e N C E b f + h j v / x m m b h b Y e u H A 4 5 1 7 u v c d P G Z X K s r 6 N y t L y y u p a d b 2 2 s b m 1 v W P u 7 n V l k g l M O j h h i e j 7 S B J G O e k o q h j p p 4 K g 2 G e k 5 4 + u J n 7 v n g h J E 3 6 r x i l x Y z T k N K Q Y K S 1 5 5 o G T O w 8 0 I B F S u d O O a H E X e t Q p P L N u N a w p 4 C K x S 1 I H J d q e + e U E C c 5 i w h V m S M q B b a X K z Z F Q F D N S 1 J x M k h T h E R q S g a Y c x U S 6 + f T + A h 5 r J Y B h I n R x B a f q 7 4 k c x V K O Y 1 9 3 x k h F c t 6 b i P 9 5 g 0 y F F 2 5 O e Z o p w v F s U Z g x q B I 4 C Q M G V B C s 2 F g T h A X V t 0 I c I Y G w 0 p H V d A j 2 / M u L p H v a s J u N s 5 t m v X V Z x l E F h + A I n A A b n I M W u A Z t 0 A E Y P I J n 8 A r e j C f j x X g 3 P m a t F a O c 2 Q d / Y H z + A K M k l o I = &lt; / l a t e x i t &gt; Inter-scale Attention h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q y O 3 y L I C z W u E F 2 p Y 9 J a c B 3 k s 6 3 c = " &gt; A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I R Z d F N y 4 r 2 A e 2 Q 8 m k m T Y 0 k x m S O 0 I Z + h d u X C j i 1 r 9 x 5 9 + Y t r P Q 1 g O B w z n 3 k n N P k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l x 1 5 E c R S E 2 W j a L 1 f c q j s H W S V e T i q Q o 9 E v f / U G M U s j r p B J a k z X c x P 0 M 6 p R M M m n p V 5 q e E L Z m A 5 5 1 1 J F I 2 7 8 b J 5 4 S s 6 s M i B h r O 1 T S O b q 7 4 2 M R s Z M o s B O z h K a Z W 8 m / u d 1 U w y v / U y o J E W u 2 O K j M J U E Y z I 7 n w y E 5 g z l x B L K t L B Z C R t R T R n a k k q 2 B G / 5 5 F X S u q h 6 t e r l f a 1 S v 8 n r K M I J n M I 5 e H A F d b i D B j S B g Y J n e I U 3 x z g v z r v z s R g t O P n O M f y B 8 / k D 5 4 G R F A = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " Z g g L e h e 4 + 1 D s 1 2 t q K M f Z k e P 3 R a g = " &gt; A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I R Z d F N y 4 r 2 A e 2 Q 8 m k d 9 r Q T G Z I M k I Z + h d u X C j i 1 r 9 x 5 9 + Y t r P Q 1 g O B w z n 3 k n N P k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z 0 2 I u o G Q V h h t N + u e J W 3 T n I K v F y U o E c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d S y W N U P v Z P P G U n F l l Q M J Y 2 S c N m a u / N z I a a T 2 J A j s 5 S 6 i X v Z n 4 n 9 d N T X j t Z 1 w m q U H J F h + F q S A m J r P z y Y A r Z E Z M L K F M c Z u V s B F V l B l b U s m W 4 C 2 f v E p a F 1 W v V r 2 8 r 1 X q N 3 k d R T i B U z g H D 6 6 g D n f Q g C Y w k P A M r / D m a O f F e X c + F q M F J 9 8 5 h j 9 w P n 8 A 4 v K R E Q = = &lt; / l a t e x i t &gt; o &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z H b y w f w h j v L B G 4 U 4 0 U k P V + W 5 d 4 g = " &gt; A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I R Z d F N y 4 r 2 A e 2 Q 8 m k m T Y 0 k w x J R i h D / 8 K N C 0 X c + j f u / B s z 0 1 l o 6 4 H A 4 Z x 7 y b k n i D n T x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T z q a J k o Q t t E c q l 6 A d a U M 0 H b h h l O e 7 G i O A o 4 7 Q b T 2 8 z v P l G l m R Q P Z h Z T P 8 J j w U J G s L H S 4 y D C Z h K E q Z w P q z W 3 7 u Z A q 8 Q r S A 0 K t I b V r 8 F I k i S i w h C O t e 5 7 b m z 8 F C v D C K f z y i D R N M Z k i s e 0 b 6 n A E d V + m i e e o z O r j F A o l X 3 C o F z 9 v Z H i S O t Z F N j J L K F e 9 j L x P 6 + f m P D a T 5 m I E 0 M F W X w U J h w Z i b L z 0 Y g p S g y f W Y K J Y j Y r I h O s M D G 2 p I o t w V s + e Z V 0 L u p e o 3 5 5 3 6 g 1 b 4 o 6 y n A C p 3 A O H l x B E + 6 g B W 0 g I O A Z X u H N 0 c 6 L 8 + 5 8 L E Z L T r F z D H / g f P 4 A 8 i S R G w = = &lt; / l a t e x i t &gt; c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R X 1 3 9 E M h 6 l R 0 d N R B / 7 v 6 T a 3 S y Y o = " &gt; A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I R Z d F N y 4 r 2 A e 2 Q 8 m k d 9 r Q T G Z I M k I Z + h d u X C j i 1 r 9 x 5 9 + Y t r P Q 1 g O B w z n 3 k n N P k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z 0 2 I u o G Q V h x q b 9 c s W t u n O Q V e L l p A I 5 G v 3 y V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 1 E s 1 J p S N 6 R C 7 l k o a o f a z e e I p O b P K g I S x s k 8 a M l d / b 2 Q 0 0 n o S B X Z y l l A v e z P x P 6 + b m v D a z 7 h M U o O S L T 4 K U 0 F M T G b n k w F X y I y Y W E K Z 4 j Y r Y S O q K D O 2 p J I t w V s + e Z W 0 L q p e r X p 5 X 6 v U b / I 6 i n A C p 3 A O H l x B H e 6 g A U 1 g I O E Z X u H N 0 c 6 L 8 + 5 8 L E Y L T r 5 z D H / g f P 4 A 3 + i R D w = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>8 &lt;</head><label>8</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " x g 9 9 O f t 8 q W a 7 X i 2 i 7 X p W v Y H 6 d + o = " &gt; A A A B + 3 i c b V D N S 8 M w H E 3 n 1 5 x f d R 6 9 B I f g a b Q y c R d h 6 M X j B P c B W y l p m m 1 h a V K S V B 2 l / 4 o X D 4 p 4 9 R / x 5 n 9 j u v W g m w 9 C H u / 9 f u T l B T G j S j v O t 1 V a W 9 / Y 3 C p v V 3 Z 2 9 / Y P 7 M N q V 4 l E Y t L B g g n Z D 5 A i j H L S 0 V Q z 0 o 8 l Q V H A S C + Y 3 u R + 7 4 F I R Q W / 1 7 O Y e B E a c z q i G G k j + X Z 1 G A g W q l l k r v T R d 6 + a m W / X n L o z B 1 w l b k F q o E D b t 7 + G o c B J R L j G D C k 1 c J 1 Y e y m S m m J G s s o w U S R G e I r G Z G A o R x F R X j r P n s F T o 4 R w J K Q 5 X M O 5 + n s j R Z H K 4 5 n J C O m J W v Z y 8 T 9 v k O h R 0 0 s p j x N N O F 4 8 N E o Y 1 A L m R c C Q S o I 1 m x m C s K Q m K 8 Q T J B H W p q 6 K K c F d / v I q 6 Z 7 X 3 U b 9 4 q 5 R a 1 0X d Z T B M T g B Z 8 A F l 6 A F b k E b d A A G T + A Z v I I 3 K 7 N e r H f r Y z F a s o q d I /A H 1 u c P B Q u U b g = = &lt; / l a t e x i t &gt; w2 = 12 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C 3 U l k R I 3 p M j 4 j R k r G e m C D p V D L R k = " &gt; A A A B / H i c b V D N S 8 M w H E 3 n 1 5 x f 1 R 2 9 B I f g a b R j o h d h 6 M X j B P c B W y l p m m 5 h a V K S V B l l / i t e P C j i 1 T / E m / + N 6 d a D b j 4 I e b z 3 + 5 G X F y S M K u 0 4 3 1 Z p b X 1 j c 6 u 8 X d n Z 3 d s / s A + P u k q k E p M O F k z I f o A U Y Z S T j q a a k X 4 i C Y o D R n r B 5 C b 3 e w 9 E K i r 4 v Z 4 m x I v R i N O I Y q S N 5 N v V Y S B Y q K a x u b J H v 3 H l N m a + X X P q z h x w l b g F q Y E C b d / + G o Y C p z H h G j O k 1 M B 1 E u 1 l S G q K G Z l V h q k i C c I T N C I D Q z m K i f K y e f g Z P D V K C C M h z e E a z t X f G x m K V Z 7 P T M Z I j 9 W y l 4 v / e Y N U R 5 d e R n m S a s L x 4 q E o Z V A L m D c B Q y o J 1 m x q C M K S m q w Q j 5 F E W J u + K q Y E d / n L q 6 T b q L v N + v l d s 9 a 6 L u o o g 2 N w A s 6 A C y 5 A C 9 y C N u g A D K b g G b y C N + v J e r H e r Y / F a M k q d q r g D 6 z P H 3 O S l K Q = &lt; / l a t e x i t &gt; w3 = 16 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m D G D e G g X S N 7 e W I I V q K m / j w Z V e f Y = " &gt; A A A B / H i c b V A 7 T 8 M w G H T K q 5 R X o C O L R Y X E V C V Q H g t S B Q t j k e h D a q P I c Z z W q m N H t g O q o v J X W B h A i J U f w s a / w W k z Q O E k y 6 e 7 7 5 P P F y S M K u 0 4 X 1 Z p a X l l d a 2 8 X t n Y 3 N r e s X f 3 O k q k E p M 2 F k z I X o A U Y Z S T t q a a k V 4 i C Y o D R r r B + D r 3 u / d E K i r 4 n Z 4 k x I v R k N O I Y q S N 5 N v V Q S B Y q C a x u b I H / + T S P Z v 6 d s 2 p O z P A v 8 Q t S A 0 U a P n 2 5 y A U O I 0 J 1 5 g h p f q u k 2 g v Q 1 J T z M i 0 M k g V S R A e o y H p G 8 p R T J S X z c J P 4 a F R Q h g J a Q 7 X c K b + 3 M h Q r P J 8 Z j J G e q Q W v V z 8 z + u n O r r w M s q T V B O O 5 w 9 F K Y N a w L w J G F J J s G Y T Q x C W 1 G S F e I Q k w t r 0 V T E l u I t f / k s 6 x 3 W 3 U T + 9 b d S a V 0 U d Z b A P D s A R c M E 5 a I I b 0 A J t g M E E P I E X 8 G o 9 W s / W m / U + H y 1 Z x U 4 V / I L 1 8 Q 1 7 L p S p &lt; / l a t e x i t &gt; . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>A surrounding neighborhood consists of features of a pivot segment and neighboring segments from multiple scales. Here we show 4 segments with the highest inter-scale attention scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ψ f the position-informed representation of segments φ and ψ, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of translation results on RWTH-PHOENIX-Weather 2014T dataset.</figDesc><table><row><cell>Methods</cell><cell>Width(s)</cell><cell cols="5">ROUGE-L BLEU-1 BLEU-2 BLEU-3 BLEU-4</cell></row><row><cell cols="2">Conv2d-RNN [2] + Luong Attn. [2]+[18] + Bahdanau Attn. [2]+[17] {1} {1} {1} TSPNet-Single (Transformer) {8} {12} {16} TSPNet-Sequential {8, 12, 16} TSPNet-Joint {8, 12, 16}</cell><cell>29.70 30.70 31.80 28.93 28.10 32.36 34.77 34.96</cell><cell>27.10 29.86 32.24 30.29 29.02 32.52 35.65 36.10</cell><cell>15.61 17.52 19.03 17.75 17.03 20.33 22.80 23.12</cell><cell>10.82 11.96 12.83 12.35 12.08 14.75 16.60 16.88</cell><cell>8.35 9.00 9.58 9.41 9.39 11.61 12.97 13.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the example translation results of TSPNet and the previous state-of-the-art model. We highlight correctly translated 1-grams in blue, semantically correct translation in red.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Analysis into the effects of the proposed segment representation and hierarchical feature learning method in TSPNet. We report ROUGE-L scores in R column; BLEU-n in B-n columns. Left: impact of multi-scale segments. Right: impact of the hierarchical feature learning method.</figDesc><table><row><cell>{w}</cell><cell>R</cell><cell>B1</cell><cell>B2</cell><cell>B3</cell><cell>B4</cell><cell>Methods</cell><cell>R</cell><cell>B1</cell><cell>B2</cell><cell>B3</cell><cell>B4</cell></row><row><cell>{8, 12}</cell><cell cols="5">30.49 32.04 19.01 13.42 10.40</cell><cell>Pooling</cell><cell cols="5">31.21 32.80 20.17 14.39 11.13</cell></row><row><cell>{8, 16}</cell><cell cols="5">33.97 34.58 21.99 16.10 12.81</cell><cell>FC</cell><cell cols="5">33.84 34.20 21.52 15.24 11.80</cell></row><row><cell>{12, 16}</cell><cell cols="5">33.19 34.03 21.53 15.66 12.36</cell><cell cols="5">NonRest. 29.01 28.95 17.17 12.15</cell><cell>9.35</cell></row><row><cell cols="6">{8, 12, 16} 34.96 36.10 23.12 16.88 13.41</cell><cell>TSPNet</cell><cell cols="5">34.96 36.10 23.12 16.88 13.41</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>HL's research is funded in part by the ARC Centre of Excellence for Robotics Vision (CE140100016), ARC-Discovery (DP 190102261) and ARC-LIEF (190100080) grants. CX receives research support from ANU and Data61/CSIRO Ph.D. scholarship. We gratefully acknowledge the GPUs donated by NVIDIA Corporation, and thank all the anonymous reviewers and ACs for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The syntax of sign language agreement: Common ingredients, but unusual recipe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Steinbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Glossa: A Journal of General Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7784" to="7793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural sign language translation based on human keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Ki</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Jo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyedong</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choongsang</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">2683</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modelling and recognition of the linguistic components in american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liya</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1826" to="1844" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sign language recognition using sub-units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng-Jon</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2205" to="2231" />
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ms-asl: A large-scale data set and benchmark for understanding american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaezi</forename><surname>Hamid Reza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flickernet: Adaptive 3d gesture recognition from sparse point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuecong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujuan</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient pointlstm for point clouds based gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuecong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujuan</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5761" to="5770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1459" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transferring cross-domain knowledge for video sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sth: Spatio-temporal hybrid convolution for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengzong</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanhui</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08042</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Purdue rvl-slll asl database for automatic recognition of american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aleix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronnie</forename><forename type="middle">B</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Wilbur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash C</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Fourth IEEE International Conference on Multimodal Interfaces</title>
		<meeting>Fourth IEEE International Conference on Multimodal Interfaces</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="167" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gesture and sign language recognition with temporal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mieke</forename><surname>Van Herreweghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni</forename><surname>Dambre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning sign language by watching tv (using weakly aligned subtitles)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2961" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Interaction of morphology and syntax in American Sign Language. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carol A Padden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">605</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bpemb: Tokenization-free pre-trained subword embeddings in 275 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Heinzerling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koiti</forename><surname>Hasida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Hélène Mazo, Asuncion Moreno, Jan Odijk; Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Stelios Piperidis, and Takenobu Tokunaga. European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sign language transformers: Joint end-to-end sign language recognition and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Weakly supervised learning with multi-stream cnn-lstm-hmms to discover sequential parallelism in sign language videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihan</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">World Health Organization. Deafness and hearing loss</title>
		<ptr target="https://www.who.int/news-room/fact-sheets/detail/deafness-and-hearing-loss" />
		<imprint/>
	</monogr>
	<note>accessed 05.20.2020</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
