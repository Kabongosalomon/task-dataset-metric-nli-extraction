<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DC-UNet: Rethinking the U-Net Architecture with Dual Channel Efficient CNN for Medical Images Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ange</forename><surname>Lou</surname></persName>
							<email>angelou@gwu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyue</forename><surname>Guan</surname></persName>
							<email>frankshuyueguan@gwu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">The George Washington University Medical Center Washington DC</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Loew</surname></persName>
							<email>loew@gwu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">The George Washington University Medical Center Washington DC</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Medical Imaging and Image Analysis Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DC-UNet: Rethinking the U-Net Architecture with Dual Channel Efficient CNN for Medical Images Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>convolution neural network</term>
					<term>MultiResUnet</term>
					<term>deep- learning</term>
					<term>medical image segmentation</term>
					<term>computer aided diagnosis</term>
					<term>DC-UNet</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep learning has become much more popular in computer vision area. The Convolution Neural Network (CNN) has brought a breakthrough in images segmentation areas, especially, for medical images. In this regard, U-Net is the predominant approach to medical image segmentation task. The U-Net not only performs well in segmenting multimodal medical images generally, but also in some tough cases of them. However, we found that the classical U-Net architecture has limitation in several aspects. Therefore, we applied modifications: 1) designed efficient CNN architecture to replace encoder and decoder, 2) applied residual module to replace skip connection between encoder and decoder to improve based on the-state-of-the-art U-Net model. Following these modifications, we designed a novel architecture--DC-UNet, as a potential successor to the U-Net architecture. We created a new effective CNN architecture and build the DC-UNet based on this CNN. We have evaluated our model on three datasets with tough cases and have obtained a relative improvement in performance of 2.90%, 1.49% and 11.42% respectively compared with classical U-Net. In addition, we used the Tanimoto similarity to replace the Jaccard similarity for gray-to-gray image comparisons.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overview of MultiResUNet Architecture</head><p>In the medical images, the objects in which we are interested sometimes have different scales. For example, the scale of skin lesions can greatly vary in dermoscopy images.</p><p>We can find these problems in different medical image segmentation tasks.</p><p>Therefore, for better segmentation results, a network needs have the ability to analyze objects at different scales. Previous researcher applied a sequence of Gabor filters with varying scales to acknowledge the variation of scale in the images <ref type="bibr" target="#b18">[26]</ref>.</p><p>Based on this idea, Szegedy <ref type="bibr" target="#b19">[27]</ref> introduced a revolutionary architectureinception blocks. The inception blocks utilize convolutional layers of varying kernel size in parallel to extract features with different scales from images. The inception block is illustrated in <ref type="figure">Fig. 2</ref>. In the naïve version, the inception block simply combined 1 × 1, 3 × 3, 5 × 5 convolutional layers and 3 × 3 max pooling layers in parallel. Then, it concatenated different scales features and sent them to next layer. One big problem in this naïve version, however, is the number of dimensions will cause a computational blow up. Also, the merging of output of the pooling layer with outputs of the convolutional layers will increase the number of outputs from block to block. The dimension reduction version as shown in <ref type="figure">Fig.2 (b)</ref> solves the problems. A 1 × 1 convolutional layer <ref type="bibr" target="#b20">[28]</ref> are used to reduce dimensions before computing the 3 × 3 and 5 × 5 convolutions. dimensions, convolution with larger spatial filters (e.g. 5 × 5 or 7 × 7) is also time-consuming. For example, a 7 × 7 convolution is 49 / 9 = 5.44 times more computationally expensive than a 3 × 3 convolution with same filter number. Using three 3 × 3 convolution layers can obtain a same receptive field output with a 7 × 7 convolution <ref type="bibr" target="#b21">[29]</ref> but this sequence 3 × 3 convolutions are only 27 / 9 = 3 times than a 3 × 3 convolution with same filter number. It will be the same with 5 × 5 convolution. Based on the replacement, the inception block can also be simplified as in <ref type="figure" target="#fig_1">Fig. 3</ref>. In the U-Net architecture, after each pooling and transposed convolutional layer, a sequence of two 3 × 3 convolutional layers, which can be consider as a 5 × 5 convolution, is applied.</p><p>Like the inception block, to incorporate 3 × 3 and 7 × 7 convolution operations in parallel to the 5 × 5 convolution operation makes the U-Net have multi-resolution analysis ability ( <ref type="figure" target="#fig_2">Fig. 4 (a)</ref>). To apply this inception-like block makes U-Net architecture have the ability to concatenate features learnt from the image at different scales. In order to reduce computation and memory requirement, we accepted ideas from Szegedy. They utilized a succession of smaller 3 × 3 convolutional layers to replace the bigger 5 × 5 and 7 × 7 convolutional layers, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref> (b). Moreover, they also add the 1 × 1 convolutional layer called residual connection <ref type="bibr" target="#b22">[30]</ref>, which can provide some additional spatial features. And this structure is called MultiRes block <ref type="bibr" target="#b23">[31]</ref>, as shown in <ref type="figure" target="#fig_2">Fig.4 (c)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Besides</head><p>is number of filters in the corresponding CNN block and is a scalar coefficient.</p><p>In  </p><formula xml:id="formula_1">Conv2D(1,1) Conv2D(3,3) Conv2D(1,1) Conv2D(3,3) Conv2D(1,1) Conv2D(3,3) Conv2D(1,1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>In the experiments, the network models were built by using Keras <ref type="bibr" target="#b25">[33]</ref> with Tensorflow backend <ref type="bibr" target="#b26">[34]</ref> in Python 3 <ref type="bibr" target="#b24">[32]</ref>. The experiments were conducted in a desktop computer with Intel core i7-9700K processor (3.6 GHz) CPU, 16.0 GB RAM, and NVIDIA GeForce RTX 2070 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline model</head><p>In these experiments, we chose the U-Net as baseline model and compare its performance with MultiResUNet and DC-UNet. In order to show the advantage in parameters, we implemented the classical U-Net with five stages encoder and decoder, and the filter numbers are {64, 128, 256, 512, 1024}.</p><p>For the MultiResUNet and DC-UNet, we also set five stages encoder and decoder, and each layer's filter number can be found in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pre-processing</head><p>The goal of our experiments is to show the performance of the DC-UNet and compare with the classical U-Net and</p><p>MultiResUNet. The pre-processing we applied for thermography database are converting 16-bit images to 8-bit and resize the image to 256 × 128. Due to the limitation of GPU memory, the pre-processing for other databases is to resize the weight and height of images no larger than 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head><p>The goal of semantic segmentation is to predict whether a pixel belongs to the object. Therefore, this problem can be considered as a pixel-wise binary classification problem. Hence, we chose the binary cross-entropy as loss function and minimized it.</p><p>For the input image , the prediction of model is ̂ and the ground truth is . Thus, the binary cross-entropy is defined as:</p><formula xml:id="formula_2">( ,̂) = ∑ −( log(̂) + (1 − ) log(1 −̂)) ∈ (2)</formula><p>For a batch containing n images, the loss function is defined as:</p><formula xml:id="formula_3">= 1 ∑ ( ,̂) =1</formula><p>(3)</p><p>We trained those models using the Adam optimizer <ref type="bibr" target="#b27">[35]</ref> with the parameter 1 = 0.9 and 2 = 0.999. Epochs are varied by datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Measurement metric</head><p>To evaluate the performance of segmentation, we need a method to compare the segmented region with ground truth Tanimoto Similarity <ref type="bibr" target="#b31">[39]</ref> (Extended Jaccard Similarity);</p><p>Structural similarity (SSIM) <ref type="bibr" target="#b32">[40]</ref> In our previous studies, we used the JS. JS is for binary to binary comparison; to consider two binary images as two set A and B, their JS value is:</p><formula xml:id="formula_4">( , ) = | ∩ | | ∪ | (4)</formula><p>The used conversion method is Otsu <ref type="bibr" target="#b33">[41]</ref> based algorithm which is designed by us. The workflow is shown in the <ref type="figure">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 9. Workflow of Jaccard Similarity</head><p>Otsu's is an automatic image thresholding method to binarize images, however, its outcomes may vary greatly when the breast boundaries of segmented images are not very clear.</p><p>Hence, we turn to consider some gray to gray comparisons.</p><p>A simple and widely used gray to gray comparison based on pixel by pixel comparison is the Mean Absolute Error (MAE). In general, for two images (same size) A and B, their MAE value is:</p><formula xml:id="formula_5">( , ) = 1 − | − | (5)</formula><p>The maxE is maximum error value, to 8-bit gray-scale images Since:</p><formula xml:id="formula_6">| ∩ | = ∑ 2<label>(7)</label></formula><p>And,</p><formula xml:id="formula_7">| ∪ | = | | + | | − | ∩ | = ∑( 2 + 2 − ) (8)</formula><p>For gray-gray comparison, according to ( , ), the value of Tanimoto similarity is:</p><formula xml:id="formula_8">( , ) = ∑ ∑( 2 + 2 − )<label>(9)</label></formula><p>By definition, Tanimoto similarity is similar to JS but more general than JS and has wider applications. Therefore, it is a good alternative method for segmentation evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Cross-validation</head><p>Cross-Validation is widely used to test model's performance.</p><p>In the k-Fold cross-validation test, the dataset is randomly split into mutually exclusive subsets 1 , 2 , … , of approximately equal size <ref type="bibr" target="#b34">[42]</ref>. The model is run k times; for each time, one of the subsets is chosen as the validation set and all others as training set. We estimated the performance of model via overall results from k times training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASETS</head><p>Compared with traditional computer vision datasets, current medical imaging datasets are more challenging. Expensive medical equipment, complex image acquisition pipelines, diagnose of expert and tedious manual labelingthey all make medical datasets hard to build. Currently, there are some public medical imaging benchmark datasets containing medical images and their ground truth. We have selected two public datasets and our own infrared breast dataset to test the performance of the three U-Net based models. The datasets used in the experiments are briefly described in <ref type="table">Table 3</ref>. for training due to the limitation of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Models in experiments</head><p>To evaluate the performance of DC-UNet, we have designed experiments with three medical datasets and shown the parameter numbers of models in <ref type="table">Table 4</ref>.  From the results in <ref type="table" target="#tab_3">Table 5,</ref>     <ref type="table" target="#tab_4">Table 6</ref>. The result of EM through a 5-fold cross-validation.</p><p>Bold values are the maximum for each case.</p><p>From the <ref type="table" target="#tab_5">Table 7</ref>, we can find that the DC-UNet gives the best results for all cases. Segmentation results of one case are shown in <ref type="figure" target="#fig_11">Fig. 16</ref>. And the average accuracies and standard deviations for each fold are shown in <ref type="figure" target="#fig_10">Fig. 17</ref>. In the <ref type="figure" target="#fig_11">Fig. 16</ref>, we can find that DC-UNet has a better result     <ref type="figure">Fig. 19</ref>. The DC-UNet can successfully segment images with vague boundaries and successfully detect small objects in images, as shown in <ref type="figure">Fig. 20</ref>. The segmentation accuracy has been improved 11.42% to the U-Net.  Inception module like Inception-v4 <ref type="bibr" target="#b45">[51]</ref> and Inception-v3 <ref type="bibr" target="#b46">[52]</ref>, which give a good idea that using asymmetric convolution to replace the original convolution kernel. For example, the 3 × 3 convolution operator can be replaced by a 3 × 1 convolution following a 1 × 3 convolution to minimize the parameters further.</p><p>In the future, we will test our model on more datasets.</p><p>Moreover, we will study on how data augmentation and preprocessing could improve the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this work, we analyzed the classical U-Net and the recent MultiResUNet architecture and found potential improvements.</p><p>We noticed that the results of our own infrared breast dataset still have many limitations for using classical U-Net and</p><p>MultiResUNet. The author of MultiResUNet paper <ref type="bibr" target="#b23">[31]</ref> has verified that Res-Path can slightly improve the segmentation accuracy. Thus, we designed the Dual-Channel CNN block to give more effective features with less parameters to overcome those limitations. To incorporate this dual-channel CNN architecture with Res-Path, we developed a novel U-Net-like architecture--DC-UNet.</p><p>We selected two public medical datasets and our own infrared breast dataset to test and compare the performance of these three models. Each dataset contains some challenging For those challenging cases, the performance of DC-UNet was better than MultiResUNet and DC-UNet. Generally, for the infrared breast, ISBI-2012 and CVC-ClinicDB dataset, a relative improvement of segmentation accuracy 2.90%, 1.49% and 11.42% has been observed in using DC-UNet over U-Net.</p><p>And DC-UNet also has 1.20%, 0.66% and 2.02% improvements over Multi-ResUNet. Besides higher segmentation accuracies DC-UNet achieved, the segmentation results are much closer to the ground truth by observation. As shown in <ref type="figure" target="#fig_5">Fig. 17 and Fig. 18</ref>, U-Net and MultiResUNet tend to under-segment and even miss the objects completely. On the contrary, DC-UNet seems more reliable and robust. DC-UNet can detect vague boundaries and avoid the interference of noise.</p><p>Even for the challenging cases, the DC-UNet shows a stronger ability to capture the fine details.</p><p>Therefore, we believed that the DC-UNet architecture can be an effective model for medical image segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Inception block, naïve version (b) Inception block with dimensionality reduction Fig. 2. Inception block Although using 1 × 1 convolutional layer to reduce</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Inception block where each 5 × 5 convolution is replaced by two 3 × 3 convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>MultiRes block. (a) A simple inception-like block by using 3 × 3, 5 × 5 and 7 × 7 convolutional filters in parallel and concatenating the generated feature maps. (b) Using a succession of 3 × 3 filters to simplify inception-like block. (c) Add a residual connection to build MultiRes block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>MultiRes block, they also made some modification in skip connection called Res-Path between encoder and decoder. Dataflow pass through a chain of 3 × 3 convolutional layers with residual connections, and then concatenate the decoder feature. The Res-Path is illustrated in Fig. 5. From the Res-path in stage NO. 1 to NO. 4 in Fig. 6, the numbers of 3 × 3 convolutional layers with residual connections are {4, 3, 2, 1}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Res-PathThe MultiRes block and Res-Path form MultiResUNet model, as shown inFig. 6. For the MultiRes block, the filters number of each layer is , which can be computed as:= × .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Architecture of DC-UNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>similarity, also called extended JS, can be seen as a grayscale version JS. For binary image, JS compares images by union and intersection operations. The union operation could be considered as sum of products. For two set A and B: | ∩ | = ∑ (6) Where ∈ , ∈ . This equation holds if , ∈ {0, 1}, which are binary values. But if , are not binary, we use sum of products (right part) instead of the union operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Size and ratio change of images Ideal evaluation of images will not be affected by image sizes and object area ratio; as shown in the Fig. 10. We tested the JS, MAE, Tanimoto similarity and SSIM on different size images. Each result is the average value of all (15) samples from one patient. For every sample, the value is calculated by comparing ground truth image with C-DCNN segmented image. We changed image size by down-sampling and changed object area ratio by adding blank margin around the object. From Fig. 11, results show that SSIM is not stable to image size change and only the Tanimoto similarity keeps stable to changes of object area ratio. (a) accuracy value vs image size, (b) accuracy vs ratio Furthermore, Fig. 12 shows comparison results by Tanimoto similarity, JS and MAE for the 15 samples in size 200x400. Results indicate that for majority samples (9/15, yellow mark), Tanimoto similarity values are close to JS. Therefore, Tanimoto similarity is a good alternative measure instead of JS for grayscale image comparisons. Fig. 12. Comparison of four measurement matric In conclusion, Jaccard similarity is a proper measure if segmentation output is binary but for grayscale images, Tanimoto similarity is better. In this study, we use Tanimoto similarity to all evaluations for all datasets segmentation because segmented images from neural networks are grayscale, thus, using Tanimoto similarity avoids binarization so that keeps more information of segmented images and time cost is less, Tanimoto similarity is stable on different image size and object ratio, in addition, results are close to JS. Although ground truth images are binary, it is simple to convert them to 8-bit grayscale by multiplying by 255.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Breast segmentation results of the models for each subject. The "P" and "V" corresponds to patient and volunteer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 . 15 .Fig. 15 .</head><label>141515</label><figDesc>DC-UNet provides more accurate segmentation results both for simple and challenging cases. For example, Fig. 14 shows a simple case. The segmentation accuracies of U-Net, MultiResUNet and DC-UNet were 92.47%, 93.86% and 95.38%, respectively. DC-UNet gives the best segmentation results compared to the other models because of using our new DC blocks. Segmentation result of volunteer 7. (a) Original image (b) Manual ground-truth (c) U-Net (d) MultiResUNet (e) DC-UNet For the challenging cases, DC-UNet also gives inspiring results. For the example of patient No.11, its image contains many interferences like medical equipment and other parts of body as shown in Fig. The segmentation accuracies of U-Net, MultiResUNet and DC-UNet are 86.47%, 84.01% and 92.62%, respectively. We can find that only DC-UNet can clearly separate breast boundary from belly. Thus, the DC-UNet gives an outstanding result than other models. Segmentation result of patient 13. (a) Original image (b) Manual ground-truth (c) U-Net (d) MultiResUNet (e) DC-UNet C. Results of electron microscopy image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 17 .</head><label>17</label><figDesc>Segmentation results of the models for each fold For the electron microscopy (EM) dataset, we have performed 5-fold cross-validation and compared the performance of DC-UNet with MultiResUnet and the baseline U-Net. Every model has been trained 50 epochs for each run and recorded the Tanimoto accuracy. The results of EM dataset were shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 16 .</head><label>16</label><figDesc>Segmentation results. (a) Original image (b) Ground truth (c) U-Net (89.8) (d) MultiResUNet (91.4) (e) DC-Net (93.8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>than</head><label></label><figDesc>MultiResUNet and the visually segmentation result are very different. The segmentation results show that the DC-UNet can capture some separating lines that U-Net and MultiResUNet missed. Moreover, the DC-UNet can distinguish the boundary from interferences so that the segmentation result of DC-UNet is much clearer than other two models. D. Results of endoscopy image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 18 .</head><label>18</label><figDesc>Segmentation results of the models for each fold For the endoscopy dataset, we performed 5-fold crossvalidation and compared the performance of DC-UNet with MultiResUNet and the baseline U-Net. In the experiments, each model has been trained 150 epochs for each run and recorded the Tanimoto accuracy. The results of endoscopy dataset were shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 19 .Fig. 20 .Fig. 21 .</head><label>192021</label><figDesc>Segment images with vague boundaries. (a) Original image (b) Ground truth (c) U-Net (72.25%) (d) MultiResUNet (73.04%) (e) DC-UNet (96.45%) Segment images with small objects. (a) Original image (b) Ground truth (c) U-Net (0%) (d) MultiResUNet (11.76%) (e) DC-UNet (69.00%) For some easy cases in CVC-ClinicDB, results of MultiResUNet and DC-UNet are similar, but much better than classical U-Net, as shown in Fig. 21. Segment images with vague boundaries. (a) Original image (b) Ground truth (c) U-Net (72.07%) (d) MultiResUNet (96.44%) (e) DC-UNet (96.41%) VI. DISCUSSION A. Comparison of the three models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>cases. The infrared breast dataset contains small-size breast images with unclear boundaries. Some images in ISBI-2012 Electron Microscopy dataset contain many interferences like noise and other parts of cell will influence the model to recognize the boundaries. For colon endoscopy images in CVC-ClinicDB, the boundaries of polyps are very vague and hard to distinguish and the shapes, sizes, structures and positions of polyps are different. Those factors make this dataset most challenging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>the MultiResUNet, the number of filters is equal to {64, 128, 256, 512, 1024}. And we set = 1.67 as a constant. Thus, the numbers of filters for three 3 × 3 convolutional layers in the MultiRes are 6 , 3 and 2 . Moreover, the numbers of filters in the Res-path are {64, 128, 256, 512}. And the numbers Net has been a remarkable and the most popular architecture in medical image segmentation and the MultiResUNet can provide a much better output than the U-Net, because it can provide different scales features. For some extremely challenging medical image cases, however, the MultiResUNet cannot perform well, such as fuzzy objects and</figDesc><table><row><cell>C. DC-UNet</cell></row></table><note>of each layers' filters are shown in Table 1. All convolutional layers in the MultiResUNet are activated by the ReLU function and use batch normalization to avoid overfitting. And the final output layer is activated by Sigmoid function.. Details of MultiResUNet Fig. 6. Architecture of MultiResUNet We applied the same connection (Res-Path) between encoder and decoder like the MultiResUNet. Then, we utilized the Res-Path and Dual-Channel block to build a new U-Net architecture -DC-UNet whose architecture is illustrated in Fig. 8. Each channel in Dual-Channel block has half filter numbers of MultiRes block :{32, 64, 128, 256, 512}. is each layers' filter number. And meets the equation (1) as mentioned before. We applied the same and value, and the filter number of three 3 × 3 are also divided into 6 , 3 and 2 .Moreover, the number of filters in Res-path are {32, 64, 128, 256}. And the number of each layers' filter is shown in Table 2. All convolutional layers in the DC-UNet are activated by the ReLU function and use batch normalization to avoid overfitting. And the final output layer is activated by Sigmoid function.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Parameters</cell></row><row><cell>U-Net (baseline)</cell><cell>31,031,685</cell></row><row><cell>MultiResUNet</cell><cell>29,061,741</cell></row><row><cell>DC-UNet</cell><cell>10,069,640</cell></row><row><cell cols="2">Table 4. Models used in experiments.</cell></row><row><cell>B. Results of infrared breast images</cell><cell></cell></row><row><cell cols="2">The infrared breast dataset contains 450 images and was</cell></row><row><cell cols="2">divided into 30 subsets ( = 30) by participant. Every model</cell></row><row><cell cols="2">has been trained 50 epochs at each run. The overall average</cell></row><row><cell cols="2">accuracies of U-Net, MultiResUNet and DC-UNet are 89.80%,</cell></row><row><cell cols="2">91.47% and 92.71%, respectively, after applying the 30-Fold</cell></row><row><cell cols="2">cross validation for three models. The average accuracies and</cell></row><row><cell cols="2">standard deviations for each participant are shown in Fig 13,</cell></row><row><cell cols="2">the DC-UNet performs better than the other models for most</cell></row><row><cell cols="2">test cases. Table 5 shows the specific average accuracy values</cell></row><row><cell>of each patients.</cell><cell></cell></row></table><note>. Average segmentation accuracy for each sample. Bold values are the maximum for each participant.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="6">Fold 1 Fold 2 Fold 3 Fold 4 Fold 5 Average</cell></row><row><cell>U-Net</cell><cell>90.75</cell><cell>89.31</cell><cell>91.33</cell><cell>92.65</cell><cell>91.60</cell><cell>91.13</cell></row><row><cell cols="2">MultiResUNet 90.83</cell><cell>91.16</cell><cell>92.64</cell><cell>93.11</cell><cell>92.08</cell><cell>91.96</cell></row><row><cell>DC-UNet</cell><cell>91.79</cell><cell>91.27</cell><cell>93.21</cell><cell>93.44</cell><cell>93.38</cell><cell>92.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>The average accuracies and standard deviations for each fold are shown inFig. 18.</figDesc><table><row><cell>Model</cell><cell cols="6">Fold 1 Fold 2 Fold 3 Fold 4 Fold 5 Average</cell></row><row><cell>U-Net</cell><cell>74.03</cell><cell>70.81</cell><cell>67.96</cell><cell>63.26</cell><cell>71.52</cell><cell>69.52</cell></row><row><cell>MultiResUNet</cell><cell>81.82</cell><cell>80.34</cell><cell>79.57</cell><cell>74.23</cell><cell>78.66</cell><cell>78.92</cell></row><row><cell>DC-UNet</cell><cell>83.11</cell><cell>82.51</cell><cell>81.10</cell><cell>78.14</cell><cell>79.84</cell><cell>80.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>The result of endoscopy through a 5-fold crossvalidation. Bold values are the maximum for each case.</figDesc><table><row><cell>From the Table 8, we can find that MultiResUNet gives</cell></row><row><cell>much better results than U-Net in this challenging dataset. The</cell></row><row><cell>average accuracy has been improved 9.4%. However,</cell></row><row><cell>MultiResUNet does not perform well for some challenging</cell></row><row><cell>tasks, as shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>From these experiments, DC-UNet shows great potential in multimodal medical image segmentation. In our infrared breast dataset, DC-UNet can separate the belly and breast region even they have similar temperature and DC-UNet also provides a more accurate contour. . From the results of EM dataset as shown inFig. 16, the DC-UNet shows a good robustness to noise because compared with results of MultiResUNet and U-Net, the result of DC-UNet contains less noise. In the CVC-</figDesc><table><row><cell>ClinicDB dataset, DC-UNet shows a great ability, which</cell></row><row><cell>MultiResUnet and U-Net do not have to segment small objects</cell></row><row><cell>and vague boundaries without any data augmentation</cell></row><row><cell>techniques.</cell></row><row><cell>In addition, DC-UNet is more efficient because its</cell></row><row><cell>parameters are much less than MultiResUNet and classical U-</cell></row><row><cell>Net. The number of parameters is related to convolutional</cell></row><row><cell>kernel size, input's and output's channel numbers. In our DC</cell></row><row><cell>block, each channel's filter number is half of the corresponding</cell></row><row><cell>MultiRes block. After passing the add layer, we calculate sum</cell></row><row><cell>of these two channels instead of concatenating. Thus, the</cell></row><row><cell>dimension of current output layer and next input layer are half</cell></row><row><cell>of the corresponding layer in MultiRes block. Moreover, half</cell></row><row><cell>output dimension in DC block also leads to the filter number of</cell></row><row><cell>Res-Paths is half of that in MultiResUNet. Based on reduced</cell></row><row><cell>dimension of input and output, the parameters in DC-UNet are</cell></row><row><cell>much less than MultiResUNet and U-Net. Nevertheless, it</cell></row><row><cell>contains doubled multi-resolution features that makes the</cell></row><row><cell>results better than compared models.</cell></row><row><cell>B. Improvements and future work</cell></row><row><cell>In order to get better results, data augmentation [46] like</cell></row><row><cell>flipping, rotation and randomly cropping to enlarge the datasets</cell></row><row><cell>and image enhancement [47] are very helpful techniques. Data</cell></row><row><cell>augmentation operations can help models avoid overfitting</cell></row><row><cell>during training [48]. Moreover, objects in medical images</cell></row><row><cell>sometimes do not have clear boundaries because of poor</cell></row><row><cell>illumination, noise and tissue properties. Thus, the histogram</cell></row><row><cell>equalization technique, which can improve the contrast, such as</cell></row><row><cell>CLAHE [49] would be greatly helpful.</cell></row><row><cell>In spite of data augmentation and image enhancement</cell></row><row><cell>techniques, there are also potential in the dual-channel CNN</cell></row><row><cell>architectures. In our experiments, we only use dual-channel</cell></row><row><cell>model for segmentation. Adding more channels like blocks in</cell></row><row><cell>ResNeXt [50] will provide more effective features, but it will</cell></row><row><cell>cause the increment of parameters and floating points</cell></row></table><note>operations (FLOPs). Moreover, there are also other versions'</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for computer-aided detection or diagnosis in medical image analysis: An overview</title>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Imagenet classification with deep convolutional neural networks</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E A</forename><surname>Elshaer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ettlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tatavarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic liver and lesion segmentation in CT using cascaded fully convolutional neural networks and 3D conditional random fields</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="415" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-04" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weighted Res-UNet for High-Quality Retina Vessel Segmentation</title>
	</analytic>
	<monogr>
		<title level="m">2018 9th International Conference on Information Technology in Medicine and Education (ITME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="327" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A guide to convolution arithmetic for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07285</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6830</idno>
		<title level="m">Learning activation functions to improve deep neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust object recognition with cortex-like mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="426" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MultiResUNet: Rethinking the U-Net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Python for scientific computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="10" to="20" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Survey over image thresholding techniques and quantitative performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sezgin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic imaging</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="146" to="166" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A note on the triangle inequality for the jaccard distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kosub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="36" to="38" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Root mean square error (RMSE) or mean absolute error (MAE)?-Arguments against avoiding RMSE in the literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Draxler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geoscientific model development</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1247" to="1250" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A computer program for classifying plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Tanimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">3434</biblScope>
			<biblScope unit="page" from="1115" to="1118" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2003-11" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A threshold selection method from graylevel histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on systems, man, and cybernetics</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A study of cross-validation and bootstrap for accuracy estimation and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Ijcai</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1137" to="1145" />
			<date type="published" when="1995-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Arganda-Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Crowdsourcing the creation of image segmentation algorithms for connectomics</title>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroanatomy</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">142</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An integrated micro-and macroarchitectural analysis of the Drosophila brain by computer-assisted serial section electron microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saalfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Preibisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pulokas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Hartenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS biology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1000502</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernández-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04621</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aggarwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1003.4053</idno>
		<title level="m">A comprehensive review of image enhancement techniques</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Understanding data augmentation for classification: when to warp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stamatescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 international conference on digital image computing: techniques and applications (DICTA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Realization of the contrast limited adaptive histogram equalization (CLAHE) for real-time image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Reza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
	<note>Journal of VLSI signal processing systems for signal, image and video technology</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
