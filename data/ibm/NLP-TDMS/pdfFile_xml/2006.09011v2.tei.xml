<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Techniques for Training Score-Based Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
							<email>yangsong@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Techniques for Training Score-Based Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32 × 32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score-based models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can scale scorebased generative models to various image datasets, with diverse resolutions ranging from 64 × 64 to 256 × 256. Our score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, including CelebA, FFHQ, and several LSUN categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Score-based generative models <ref type="bibr" target="#b0">[1]</ref> represent probability distributions through score-a vector field pointing in the direction where the likelihood of data increases most rapidly. Remarkably, these score functions can be learned from data without requiring adversarial optimization, and can produce realistic image samples that rival GANs on simple datasets such as CIFAR-10 <ref type="bibr" target="#b1">[2]</ref>.</p><p>Despite this success, existing score-based generative models only work on low resolution images <ref type="bibr">(32 × 32)</ref> due to several limiting factors. First, the score function is learned via denoising score matching <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Intuitively, this means a neural network (named the score network) is trained to denoise images blurred with Gaussian noise. A key insight from <ref type="bibr" target="#b0">[1]</ref> is to perturb the data using multiple noise scales so that the score network captures both coarse and fine-grained image features. However, it is an open question how these noise scales should be chosen. The recommended settings in <ref type="bibr" target="#b0">[1]</ref> work well for 32 × 32 images, but perform poorly when the resolution gets higher. Second, samples are generated by running Langevin dynamics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. This method starts from white noise and progressively denoises it into an image using the score network. This procedure, however, might fail or take an extremely long time to converge when used in high-dimensions and with a necessarily imperfect (learned) score network.</p><p>We propose a set of techniques to scale score-based generative models to high resolution images. Based on a new theoretical analysis on a simplified mixture model, we provide a method to analytically compute an effective set of Gaussian noise scales from training data. Additionally, we propose an efficient architecture to amortize the score estimation task across a large (possibly infinite) number of noise scales with a single neural network. Based on a simplified analysis of the convergence properties of the underlying Langevin dynamics sampling procedure, we also derive a technique to approximately optimize its performance as a function of the noise scales. Combining these techniques with an exponential moving average (EMA) of model parameters, we are able to significantly improve the sample quality, and successfully scale to images of resolutions ranging from 64 × 64 to 256 × 256, which was previously impossible for score-based generative models. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the samples are sharp and diverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Langevin dynamics</head><p>For any continuously differentiable probability density p(x), we call ∇ x log p(x) its score function. In many situations the score function is easier to model and estimate than the original probability density function <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. For example, for an unnormalized density it does not depend on the partition function. Once the score function is known, we can employ Langevin dynamics to sample from the corresponding distribution. Given a step size α &gt; 0, a total number of iterations T , and an initial sample x 0 from any prior distribution π(x), Langevin dynamics iteratively evaluate the following</p><formula xml:id="formula_0">x t ← x t−1 + α ∇ x log p(x t−1 ) + √ 2α z t , 1 ≤ t ≤ T<label>(1)</label></formula><p>where z t ∼ N (0, I). When α is sufficiently small and T is sufficiently large, the distribution of x T will be close to p(x) under some regularity conditions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Suppose we have a neural network s θ (x) (called the score network) parameterized by θ, and it has been trained such that s θ (x) ≈ ∇ x log p(x). We can approximately generate samples from p(x) using Langevin dynamics by replacing ∇ x log p(x t−1 ) with s θ (x t−1 ) in Eq. <ref type="bibr" target="#b0">(1)</ref>. Note that Eq. (1) can be interpreted as noisy gradient ascent on the log-density log p(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Score-based generative modeling</head><p>We can estimate the score function from data and generate new samples with Langevin dynamics. This idea was named score-based generative modeling by ref. <ref type="bibr" target="#b0">[1]</ref>. Because the estimated score function is inaccurate in regions without training data, Langevin dynamics may not converge correctly when a sampling trajectory encounters those regions (see more detailed analysis in ref. <ref type="bibr" target="#b0">[1]</ref>). As a remedy, ref. <ref type="bibr" target="#b0">[1]</ref> proposes to perturb the data with Gaussian noise of different intensities and jointly estimate the score functions of all noise-perturbed data distributions. During inference, they combine the information from all noise scales by sampling from each noise-perturbed distribution sequentially with Langevin dynamics.</p><p>More specifically, suppose we have an underlying data distribution p data (x) and consider a sequence of noise scales {σ i } L i=1 that satisfies σ 1 &gt; σ 2 &gt; · · · &gt; σ L . Let p σ (x | x) = N (x | x, σ 2 I), and denote the corresponding perturbed data distribution as p σ (x) p σ (x | x)p data (x)dx. Ref. <ref type="bibr" target="#b0">[1]</ref> proposes to estimate the score function of each p σi (x) by training a joint neural network s θ (x, σ) (called the noise conditional score network) with the following loss:</p><formula xml:id="formula_1">1 2L L i=1 E pdata(x) E pσ i (x|x) σ i s θ (x, σ i ) +x − x σ i 2 2 ,<label>(2)</label></formula><p>where all expectations can be efficiently estimated using empirical averages. When trained to the optimum (denoted as s θ * (x, σ)), the noise conditional score network (NCSN) satisfies ∀i : <ref type="bibr" target="#b0">[1]</ref>, assuming enough data and model capacity.</p><formula xml:id="formula_2">s θ * (x, σ i ) = ∇ x log p σi (x) almost everywhere</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Annealed Langevin dynamics [1]</head><p>Require:</p><formula xml:id="formula_3">{σ i } L i=1 , , T . 1: Initialize x 0 2: for i ← 1 to L do 3: α i ← · σ 2 i /σ 2 L α i is the step size.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for t ← 1 to T do 5:</p><formula xml:id="formula_4">Draw z t ∼ N (0, I) 6: x t ← x t−1 + α i s θ (x t−1 , σ i ) + √ 2α i z t 7: x 0 ← x T 8: if denoise x T then 9: return x T + σ 2 T s θ (x T , σ T ) 10: else 11: return x T</formula><p>After training an NCSN, ref. <ref type="bibr" target="#b0">[1]</ref> generates samples by annealed Langevin dynamics, a method that combines information from all noise scales. We provide its pseudo-code in Algorithm 1. The approach amounts to sampling from p σ1 (x), p σ2 (x), · · · , p σ L (x) sequentially with Langevin dynamics with a special step size schedule α i = σ 2 i /σ 2 L for the i-th noise scale. Samples from each noise scale are used to initialize Langevin dynamics for the next noise scale until reaching the smallest one, where it provides final samples for the NCSN.</p><p>Following the first public release of this work, ref. <ref type="bibr" target="#b8">[9]</ref> noticed that adding an extra denoising step after the original annealed Langevin dynamics in <ref type="bibr" target="#b0">[1]</ref>, similar to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, often significantly improves FID scores <ref type="bibr" target="#b12">[13]</ref> without affecting the visual appearance of samples. Instead of directly returning x T , this denoising step returns <ref type="bibr">Algorithm 1)</ref>, which essentially removes the unwanted noise N (0, σ 2 T I) from x T using Tweedie's formula <ref type="bibr" target="#b13">[14]</ref>. Therefore, we have updated results in the main paper by incorporating this denoising trick, but kept some original results without this denoising step in the appendix for reference.</p><formula xml:id="formula_5">x T + σ 2 T s θ (x T , σ T ) (see</formula><p>There are many design choices that are critical to the successful training and inference of NCSNs, including (i) the set of noise scales {σ i } L i=1 , (ii) the way that s θ (x, σ) incorporates information of σ, (iii) the step size parameter and (iv) the number of sampling steps per noise scale T in Algorithm 1. Below we provide theoretically motivated ways to configure them without manual tuning, which significantly improve the performance of NCSNs on high resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Choosing noise scales</head><p>Noise scales are critical for the success of NCSNs. As shown in <ref type="bibr" target="#b0">[1]</ref>, score networks trained with a single noise can never produce convincing samples for large images. Intuitively, high noise facilitates the estimation of score functions, but also leads to corrupted samples; while lower noise gives clean samples but makes score functions harder to estimate. One should therefore leverage different noise scales together to get the best of both worlds.</p><p>When the range of pixel values is [0, 1], the original work on NCSN <ref type="bibr" target="#b0">[1]</ref> recommends choosing {σ i } L i=1 as a geometric sequence where L = 10, σ 1 = 1, and σ L = 0.01. It is reasonable that the smallest noise scale σ L = 0.01 1, because we sample from perturbed distributions with descending noise scales and we want to add low noise at the end. However, some important questions remain unanswered, which turn out to be critical to the success of NCSNs on high resolution images: (i) Is σ 1 = 1 appropriate? If not, how should we adjust σ 1 for different datasets? (ii) Is geometric progression a good choice? (iii) Is L = 10 good across different datasets? If not, how many noise scales are ideal?</p><p>Below we provide answers to the above questions, motivated by theoretical analyses on simple mathematical models. Our insights are effective for configuring score-based generative modeling in practice, as corroborated by experimental results in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Initial noise scale</head><p>The algorithm of annealed Langevin dynamics (Algorithm 1) is an iterative refining procedure that starts from generating coarse samples with rich variation under large noise, before converging to fine samples with less variation under small noise. The initial noise scale σ 1 largely controls the diversity of the final samples. In order to promote sample diversity, we might want to choose σ 1 to be as large as possible. However, an excessively large σ 1 will require more noise scales (to be discussed in Section 3.2) and make annealed Langevin dynamics more expensive. Below we present an analysis to guide the choice of σ 1 and provide a technique to strike the right balance.</p><p>Real-world data distributions are complex and hard to analyze, so we approximate them with empirical distributions. Suppose we have a dataset {x <ref type="bibr" target="#b0">(1)</ref> </p><formula xml:id="formula_6">, x (2) , · · · , x (N ) } which is i.i.d. sampled from p data (x). Assuming N is sufficiently large, we have p data (x) ≈p data (x) 1 N N i=1 δ(x = x (i) ), where δ(·)</formula><p>denotes a point mass distribution. When perturbed with N (0, σ 2 1 I), the empirical distribution becomeŝ</p><formula xml:id="formula_7">p σ1 (x) 1 N N i=1 p (i) (x), where p (i) (x) N (x | x (i) , σ 2 1 I).</formula><p>For generating diverse samples regardless of initialization, we naturally expect that Langevin dynamics can explore any component p (i) (x) when initialized from any other component p (j) (x), where i = j. The performance of Langevin dynamics is governed by the score function ∇ x logp σ1 (x) (see Eq. (1)).</p><formula xml:id="formula_8">Proposition 1. Letp σ1 (x) 1 N N i=1 p (i) (x), where p (i) (x) N (x | x (i) , σ 2 1 I). With r (i) (x) p (i) (x) N k=1 p (k) (x) , the score function is ∇ x logp σ1 (x) = N i=1 r (i) (x)∇ x log p (i) (x). Moreover, E p (i) (x) [r (j) (x)] ≤ 1 2 exp − x (i) − x (j) 2 2 8σ 2 1 .<label>(3)</label></formula><p>In order for Langevin dynamics to transition from</p><formula xml:id="formula_9">p (i) (x) to p (j) (x) easily for i = j, E p (i) (x) [r (j) (x)] has to be relatively large, because otherwise ∇ x logp σ1 (x) = N k=1 r (k) (x)∇ x log p (k) (x) will ig- nore the component p (j) (x) (on average) when initialized with x ∼ p (i) (x)</formula><p>and in such case Langevin dynamics will act as if p (j) (x) did not exist. The bound of Eq. (3) indicates that E p (i) (x) [r (j) (x)] can decay exponentially fast if σ 1 is small compared to x (i) − x (j) 2 . As a result, it is necessary for σ 1 to be numerically comparable to the maximum pairwise distances of data to facilitate transitioning of Langevin dynamics and hence improving sample diversity. In particular, we suggest: Technique 1 (Initial noise scale). Choose σ 1 to be as large as the maximum Euclidean distance between all pairs of training data points.  Taking CIFAR-10 as an example, the median pairwise distance between all training images is around 18, so σ 1 = 1 as in <ref type="bibr" target="#b0">[1]</ref> implies E[r(x)] &lt; 10 −17 and is unlikely to produce diverse samples as per our analysis. To test whether choosing σ 1 according to Technique 1 (i.e., σ 1 = 50) gives significantly more diverse samples than using σ 1 = 1, we run annealed Langevin dynamics to sample from a mixture of Gaussian with 10000 components, where each component is centered at one CIFAR-10 test image. All initial samples are drawn from a uniform distribution over [0, 1] 32×32×3 . This setting allows us to avoid confounders introduced by NCSN training because we use ground truth score functions. As shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Other noise scales</head><p>After setting σ L and σ 1 , we need to choose the number of noise scales L and specify the other elements of {σ i } L i=1 . As analyzed in <ref type="bibr" target="#b0">[1]</ref>, it is crucial for the success of score-based generative models to ensure that p σi (x) generates a sufficient number of training data in high density regions of p σi−1 (x) for all 1 &lt; i ≤ L. The intuition is we need reliable gradient signals for p σi (x) when initializing Langevin dynamics with samples from p σi−1 (x).</p><p>However, an extensive grid search on {σ i } L i=1 can be very expensive. To give some theoretical guidance on finding good noise scales, we consider a simple case where the dataset contains only one data point, or equivalently,</p><formula xml:id="formula_10">∀1 ≤ i ≤ L : p σi (x) = N (x | 0, σ 2 i I).</formula><p>Our first step is to understand the distributions of p σi (x) better, especially when x has high dimensionality. We can decompose p σi (x) in hyperspherical coordinates to p(φ)p σi (r), where r and φ denote the radial and angular coordinates of x respectively. Because p σi (x) is an isotropic Gaussian, the angular component p(φ) is uniform and shared across all noise scales. As for p σi (r), we have the following Proposition 2. Let x ∈ R D ∼ N (0, σ 2 I), and r = x 2 . We have</p><formula xml:id="formula_11">p(r) = 1 2 D/2−1 Γ(D/2) r D−1 σ D exp − r 2 2σ 2 and r − √ Dσ d → N (0, σ 2 /2) when D → ∞.</formula><p>In practice, dimensions of image data can range from several thousand to millions, and are typically large enough to warrant p(r) ≈ N (r| √ Dσ, σ 2 /2) with negligible error. We therefore take p σi (r) = N (r|m i , s 2 i ) to simplify our analysis, where m i √ Dσ, and s 2 i σ 2 /2.</p><p>Recall that our goal is to make sure samples from p σi (x) will cover high density regions of p σi−1 (x).</p><p>Because p(φ) is shared across all noise scales, p σi (x) already covers the angular component of p σi−1 (x). Therefore, we need the radial components of p σi (x) and p σi−1 (x) to have large overlap.</p><p>Since p σi−1 (r) has high density in</p><formula xml:id="formula_12">I i−1 [m i−1 − 3s i−1 , m i−1 + 3s i−1 ] (employing the "three- sigma rule of thumb" [15]), a natural choice is to fix p σi (r ∈ I i−1 ) = Φ( √ 2D(γ i − 1) + 3γ i ) − Φ( √ 2D(γ i − 1) − 3γ i ) = C with some moderately large constant C &gt; 0 for all 1 &lt; i ≤ L, where γ i σ i−1 /σ i and Φ(·)</formula><p>is the CDF of standard Gaussian. This choice immediately implies that</p><formula xml:id="formula_13">γ 2 = γ 3 = · · · γ L and thus {σ i } L i=1</formula><p>is a geometric progression. Ideally, we should choose as many noise scales as possible to make C ≈ 1. However, having too many noise scales will make sampling very costly, as we need to run Langevin dynamics for each noise scale in sequence. On the other hand, L = 10 (for 32 × 32 images) as in the original setting of <ref type="bibr" target="#b0">[1]</ref> is arguably too small, for which C = 0 up to numerical precision. To strike a balance, we recommend C ≈ 0.5 which performs well in our experiments. In summary, Technique 2 (Other noise scales). Choose {σ i } L i=1 as a geometric progression with common ratio γ, For high resolution images, we need a large σ 1 and a huge number of noise scales as per Technique 1 and 2. Recall that the NCSN is a single amortized network that takes a noise scale and gives the corresponding score. In <ref type="bibr" target="#b0">[1]</ref>, authors use a separate set of scale and bias parameters in normalization layers to incorporate the information from each noise scale. However, its memory consumption grows linearly w.r.t. L, and it is not applicable when the NCSN has no normalization layers.</p><formula xml:id="formula_14">such that Φ( √ 2D(γ − 1) + 3γ) − Φ( √ 2D(γ − 1) − 3γ) ≈ 0.5.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Incorporating the noise information</head><p>We propose an efficient alternative that is easier to implement and more widely applicable. For</p><formula xml:id="formula_15">p σ (x) = N (x | 0, σ 2 I) analyzed in Section 3.2, we observe that E[ ∇ x log p σ (x) 2 ] ≈ √ D /σ.</formula><p>Moreover, as empirically noted in <ref type="bibr" target="#b0">[1]</ref>, s θ (x, σ) 2 ∝ 1 /σ for a trained NCSN on real data. Because the norm of score functions scales inverse proportionally to σ, we can incorporate the noise information by rescaling the output of an unconditional score network s θ (x) with 1/σ. This motivates our following recommendation Technique 3 (Noise conditioning). Parameterize the NCSN with s θ (x, σ) = s θ (x)/σ, where s θ (x) is an unconditional score network.</p><p>It is typically hard for deep networks to automatically learn this rescaling, because σ 1 and σ L can differ by several orders of magnitude. This simple choice is easier to implement, and can easily handle a large number of noise scales (even continuous ones). As shown in <ref type="figure" target="#fig_4">Fig. 3</ref> (detailed settings in Appendix B), it achieves similar training losses compared to the original noise conditioning approach in <ref type="bibr" target="#b0">[1]</ref>, and generate samples of better quality (see Appendix C.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Configuring annealed Langevin dynamics</head><p>In order to sample from an NCSN with annealed Langevin dynamics, we need to specify the number of sampling steps per noise scale T and the step size parameter in Algorithm 1. Authors of <ref type="bibr" target="#b0">[1]</ref> recommends = 2 × 10 −5 and T = 100. It remains unclear how we should change and T for different sets of noise scales.</p><p>To gain some theoretical insight, we revisit the setting in Section 3.2 where the dataset has one point (i.e., p σi (x) = N (x | 0, σ 2 i I)). Annealed Langevin dynamics connect two adjacent noise scales σ i−1 &gt; σ i by initializing the Langevin dynamics for p σi (x) with samples obtained from p σi−1 (x). When applying Langevin dynamics to p σi (x), we have</p><formula xml:id="formula_16">x t+1 ← x t + α∇ x log p σi (x t ) + √ 2αz t , where x 0 ∼ p σi−1 (x) and z t ∼ N (0, I).</formula><p>The distribution of x T can be computed in closed form:</p><formula xml:id="formula_17">Proposition 3. Let γ = σi−1 σi . For α = · σ 2 i σ 2 L (as in Algorithm 1), we have x T ∼ N (0, s 2 T I), where s 2 T σ 2 i = 1 − σ 2 L 2T γ 2 − 2 σ 2 L − σ 2 L 1 − σ 2 L 2 + 2 σ 2 L − σ 2 L 1 − σ 2 L 2 .<label>(4)</label></formula><p>When</p><formula xml:id="formula_18">{σ i } L i=1</formula><p>is a geometric progression as advocated by Technique 2, we immediately see that</p><formula xml:id="formula_19">s 2 T/σ 2 i is identical across all 1 &lt; i ≤ T because of the shared γ. Furthermore, the value of s 2 T/σ 2 i has no explicit dependency on the dimensionality D.</formula><p>For better mixing of annealed Langevin dynamics, we hope s 2 T/σ 2 i approaches 1 across all noise scales, which can be achieved by finding and T that minimize the difference between Eq. (4) and 1. Unfortunately, this often results in an unnecessarily large T that makes sampling very expensive for large L. As an alternative, we propose to first choose T based on a reasonable computing budget (typically T × L is several thousand), and subsequently find by making Eq. (4) as close to 1 as possible. In summary: Technique 4 (selecting T and ). Choose T as large as allowed by a computing budget and then select an that makes Eq. (4) maximally close to 1.</p><p>We follow this guidance to generate all samples in this paper, except for those from the original NCSN where we adopt the same settings as in <ref type="bibr" target="#b0">[1]</ref>. When finding with Technique 4 and Eq. (4), we recommend performing grid search over , rather than using gradient-based optimization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Improving stability with moving average</head><p>Unlike GANs, score-based generative models have one unified objective (Eq. (2)) and require no adversarial training. However, even though the loss function of NCSNs typically decreases steadily over the course of training, we observe that the generated image samples sometimes exhibit unstable visual quality, especially for images of larger resolutions. We empirically demonstrate this fact by training NCSNs on CIFAR-10 32 × 32 and CelebA <ref type="bibr" target="#b15">[16]</ref> 64 × 64 following the settings of <ref type="bibr" target="#b0">[1]</ref>, which exemplifies typical behavior on other image datasets. We report FID scores <ref type="bibr" target="#b12">[13]</ref> computed on 1000 samples every 5000 iterations. Results in <ref type="figure">Fig. 4</ref> are computed with the denoising step, but results without the denoising step are similar (see <ref type="figure">Fig. 8</ref> in Appendix C.1). As shown in <ref type="figure">Figs. 4  and 8</ref>, the FID scores for the vanilla NCSN often fluctuate significantly during training. Additionally, samples from the vanilla NCSN sometimes exhibit characteristic artifacts: image samples from the same checkpoint have strong tendency to have a common color shift. Moreover, samples are shifted towards different colors throughout training. We provide more samples in Appendix C.3 to manifest this artifact.</p><p>This issue can be easily fixed by exponential moving average (EMA). Specifically, let θ i denote the parameters of an NCSN after the i-th training iteration, and θ be an independent copy of the parameters. We update θ with θ ← mθ + (1 − m)θ i after each optimization step, where m is the  momentum parameter and typically m = 0.999. When producing samples, we use s θ (x, σ) instead of s θi (x, σ). As shown in <ref type="figure">Fig. 4</ref>, EMA can effectively stabilize FIDs, remove artifacts (more samples in Appendix C.3) and give better FID scores in most cases. Empirically, we observe the effectiveness of EMA is universal across a large number of different image datasets. As a result, we recommend the following rule of thumb: Technique 5 (EMA). Apply exponential moving average to parameters when sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Combining all techniques together</head><p>Employing Technique 1-5, we build NCSNs that can readily work across a large number of different datasets, including high resolution images that were previously out of reach with score-based generative modeling. Our modified model is named NCSNv2. For a complete description on experimental details and more results, please refer to Appendix B and C.</p><p>Quantitative results: We consider CIFAR-10 32×32 and CelebA 64×64 where NCSN and NCSNv2 both produce reasonable samples. We report FIDs (lower is better) every 5000 iterations of training on 1000 samples and give results in <ref type="figure">Fig. 5</ref> (with denoising) and <ref type="figure">Fig. 9</ref> (without denoising, deferred to Appendix C.1). As shown in Figs. 5 and 9, we observe that the FID scores of NCSNv2 (with all techniques applied) are on average better than those of NCSN, and have much smaller variance over the course of training. Following <ref type="bibr" target="#b0">[1]</ref>, we select checkpoints with the smallest FIDs (on 1000 samples) encountered during training, and compute full FID and Inception scores on more samples from them.</p><p>As shown by results in <ref type="table" target="#tab_0">Table 1</ref>, NCSNv2 (w/ denoising) is able to significantly improve the FID scores of NCSN on both CIFAR-10 and CelebA, while bearing a slight loss of Inception scores on CIFAR-10. However, we note that Inception and FID scores have known issues <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> and they should be interpreted with caution as they may not correlate with visual quality in the expected way.</p><p>In particular, they can be sensitive to slight noise perturbations <ref type="bibr" target="#b22">[23]</ref>, as shown by the difference of  scores with and without denoising in <ref type="table" target="#tab_0">Table 1</ref>. To verify that NCSNv2 indeed generates better images than NCSN, we provide additional uncurated samples in Appendix C.4 for visual comparison.</p><p>Ablation studies: We conduct ablation studies to isolate the contributions of different techniques. We partition all techniques into three groups: (i) Technique 5, (ii) Technique 1,2,4, and (iii) Technique 3, where different groups can be applied simultaneously. Technique 1,2 and 4 are grouped together because Technique 1 and 2 collectively determine the set of noise scales, and to sample from NCSNs trained with these noise scales we need Technique 4 to configure annealed Langevin dynamics properly. We test the performance of successively removing groups (iii), (ii), (i) from NCSNv2, and report results in <ref type="figure">Fig. 5</ref> for sampling with denoising and in <ref type="figure">Fig. 9</ref> (Appendix C.1) for sampling without denoising. All groups of techniques improve over the vanilla NCSN. Although the FID scores are not strictly increasing when removing (iii), (ii), and (i) progressively, we note that FIDs may not always correlate with sample quality well. In fact, we do observe decreasing sample quality by visual inspection (see Appendix C.4), and combining all techniques gives the best samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards higher resolution:</head><p>The original NCSN only succeeds at generating images of low resolution. In fact, <ref type="bibr" target="#b0">[1]</ref> only tested it on MNIST 28 × 28 and CelebA/CIFAR-10 32 × 32. For slightly larger images such as CelebA 64 × 64, NCSN can generate images of consistent global structure, yet with strong color artifacts that are easily noticeable (see <ref type="figure">Fig. 4</ref> and compare <ref type="figure" target="#fig_0">Fig. 10a</ref> with <ref type="figure" target="#fig_0">Fig. 10b</ref>). For images with resolutions beyond 96 × 96, NCSN will completely fail to produce samples with correct structure or color (see <ref type="figure" target="#fig_7">Fig. 7</ref>). All samples shown here are generated without the denoising step, but since σ L is very small, they are visually indistinguishable from ones with the denoising step.</p><p>By combining Technique 1-5, NCSNv2 can work on images of much higher resolution. Note that we directly calculated the noise scales for training NCSNs, and computed the step size for annealed Langevin dynamics sampling without manual hyper-parameter tuning. The network architectures are the same across datasets, except that for ones with higher resolution we use more layers and more filters to ensure the receptive field and model capacity are large enough (see details in Appendix B.1). In <ref type="figure" target="#fig_6">Fig. 6</ref> and 1, we show NCSNv2 is capable of generating high-fidelity image samples with resolutions ranging from 96 × 96 to 256 × 256. To show that this high sample quality is not a result of dataset memorization, we provide the loss curves for training/test, as well as nearest neighbors for samples in Appendix C.5. In addition, NCSNv2 can produce smooth interpolations between two given samples as in <ref type="figure" target="#fig_6">Fig. 6</ref> (details in Appendix B.2), indicating the ability to learn generalizable image representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Motivated by both theoretical analyses and empirical observations, we propose a set of techniques to improve score-based generative models. Our techniques significantly improve the training and sampling processes, lead to better sample quality, and enable high-fidelity image generation at high resolutions. Although our techniques work well without manual tuning, we believe that the performance can be improved even more by fine-tuning various hyper-parameters. Future directions include theoretical understandings on the sample quality of score-based generative models, as well as alternative noise distributions to Gaussian perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our work represents another step towards more powerful generative models. While we focused on images, it is quite likely that similar techniques could be applicable to other data modalities such as speech or behavioral data (in the context of imitation learning). Like other generative models that have been previously proposed, such as GANs and WaveNets, score models have a multitude of applications. Among many other applications, they could be used to synthesize new data automatically, detect anomalies and adversarial examples, and also improve results in key tasks such as semi-supervised learning and reinforcement learning. In turn, these techniques can have both positive and negative impacts on society, depending on the application. In particular, the models we trained on image datasets can be used to synthesize new images that are hard to distinguish from real ones by humans. Synthetic images from generative models have already been used to deceive humans in malicious ways. There are also positive uses of these technologies, for example in the arts and as a tool to aid design in engineering. We also note that our models have been trained on datasets that have biases (e.g., CelebA is not gender-balanced), and the learned distribution is likely to have inherited them, in addition to others that are caused by the so-called inductive bias of models.</p><p>A Proofs</p><formula xml:id="formula_20">Proposition 1. Letp σ1 (x) 1 N N i=1 p (i) (x), where p (i) (x) N (x | x (i) , σ 2 1 I). With r (i) (x) p (i) (x) N k=1 p (k) (x) , the score function is ∇ x logp σ1 (x) = N i=1 r (i) (x)∇ x log p (i) (x). Moreover, E p (i) (x) [r (j) (x)] ≤ 1 2 exp − x (i) − x (j) 2 2 8σ 2 1 .<label>(5)</label></formula><p>Proof. According to the definition of p σ1 (x) and r(x), we have</p><formula xml:id="formula_21">∇ x logp σ1 (x) = ∇ x log 1 N N i=1 p (i) (x) = N i=1 ∇ x p (i) (x) N j=1 p (j) (x) = N i=1 p (i) (x)∇ x log p (i) (x) N j=1 p (j) (x) = N i=1 r (i) (x)∇ x log p (i) (x).</formula><p>Next, assuming x ∈ R D , we have</p><formula xml:id="formula_22">E p (i) (x) [r (j) (x)] = p (i) (x)p (j) (x) N k=1 p (k) (x) dx ≤ p (i) (x)p (j) (x) p (i) (x) + p (j) (x) dx = 1 2 2 1 p (i) (x) + 1 p (j) (x) dx (1) ≤ 1 2 p (i) (x)p (j) (x)dx = 1 2 1 (2πσ 2 1 ) D/2 exp − 1 4σ 2 1 x − x (i) 2 2 + x − x (j) 2 2 dx = 1 2 1 (2πσ 2 1 ) D/2 exp − 1 4σ 2 1 x − x (i) 2 2 + x − x (i) + x (i) − x (j) 2 2 dx = 1 2 1 (2πσ 2 1 ) D/2 exp − 1 2σ 2 1 x − x (i) 2 2 + (x − x (i) ) T (x (i) − x (j) ) + x (i) − x (j) 2 2 2 dx = 1 2 1 (2πσ 2 1 ) D/2 exp − 1 2σ 2 1 x − x (i) + x (i) − x (j) 2 2 2 + x (i) − x (j) 2 2 4 dx = 1 2 exp − x (i) − x (j) 2 2 8σ 2 1 1 (2πσ 2 1 ) D/2 exp − 1 2σ 2 1 x − x (i) + x (i) − x (j) 2 2 2 dx = 1 2 exp − x (i) − x (j) 2 2 8σ 2 1 ,</formula><p>where (1) is due to the geometric mean-harmonic mean inequality.</p><p>Proposition 2. Let x ∈ R D ∼ N (0, σ 2 I), and r = x 2 . We have</p><formula xml:id="formula_23">p(r) = 1 2 D/2−1 Γ(D/2) r D−1 σ D exp − r 2 2σ 2 and r − √ Dσ d → N (0, σ 2 /2) when D → ∞.</formula><p>Proof. Since x ∼ N (0, σ 2 I), we have s x </p><formula xml:id="formula_24">r D−1 σ D exp − r 2 2σ 2 ,</formula><p>which proves our first result. Next, we notice that if x ∼ N (0, σ 2 ), we have x 2 /σ 2 ∼ χ 2 1 and thus</p><formula xml:id="formula_25">E[x] = σ 2 , Var[x] = 2σ 4 . As a result, if x 1 , x 2 , · · · , x D i.i.d.</formula><p>∼ N (0, σ 2 ), the law of large numbers and the central limit theorem will imply that as D → ∞, both of the following hold:</p><formula xml:id="formula_26">x 2 1 + x 2 2 + · · · + x 2 D D p → σ 2 √ D x 2 1 + x 2 2 + · · · + x 2 D D − σ 2 d → N (0, 2σ 4 ).</formula><p>Equivalently,</p><formula xml:id="formula_27">√ D r 2 D − σ 2 d → N (0, 2σ 4 ).</formula><p>Applying the delta method, we obtain</p><formula xml:id="formula_28">√ D r √ D − σ d → N (0, σ 2 /2), and therefore r − √ Dσ d → N (0, σ 2 /2). Proposition 3. Let γ = σi−1 σi . For α = · σ 2 i σ 2 L (as in Algorithm 1), we have x T ∼ N (0, s 2 T I), where s 2 T σ 2 i = 1 − σ 2 L 2T γ 2 − 2 σ 2 L − σ 2 L 1 − σ 2 L 2 + 2 σ 2 L − σ 2 L 1 − σ 2 L 2 .<label>(6)</label></formula><p>Proof. First, the conditions we know are</p><formula xml:id="formula_29">x 0 ∼ p σi−1 (x) = N (0, σ 2 i−1 I), x t+1 ← x t + α∇ x log p σi (x t ) + √ 2αz t = x t − α x t σ 2 i + √ 2αz t ,</formula><p>where z t ∼ N (0, I). Therefore, the variance of x t satisfies</p><formula xml:id="formula_30">Var[x t ] =    σ 2 i−1 I if t = 0 1 − α σ 2 i 2</formula><p>Var[x t−1 ] + 2αI otherwise.</p><formula xml:id="formula_31">Now let v 2α 1− 1− α σ 2 i 2 I, we have Var[x t ] − v = 1 − α σ 2 i 2 (Var[x t−1 ] − v).</formula><p>Therefore,</p><formula xml:id="formula_32">Var[x T ] − v = 1 − α σ 2 i 2T (Var[x 0 ] − v) =⇒ Var[x T ] = 1 − α σ 2 i 2T (Var[x 0 ] − v) + v =⇒ s 2 T = 1 − α σ 2 i 2T σ 2 i−1 − 2α 1 − 1 − α σ 2 i 2 + 2α 1 − 1 − α σ 2 i 2 .<label>(7)</label></formula><p>Substituting σ 2 i /σ 2 L for α in Eq. <ref type="formula" target="#formula_32">(7)</ref>, we immediately obtain Eq. (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental details B.1 Network architectures and hyperparameters</head><p>The original NCSN in <ref type="bibr" target="#b0">[1]</ref> uses a network structure based on RefineNet [24]-a classical architecture for semantic segmentation. There are three major modifications to the original RefineNet in NCSN: (i) adding an enhanced version of conditional instance normalization (designed in <ref type="bibr" target="#b0">[1]</ref> and named CondInstanceNorm++) for every convolutional layer; (ii) replacing max pooling with average pooling in RefineNet blocks; and (iii) using dilated convolutions in the ResNet backend of RefineNet. We use exactly the same architecture for NCSN experiments, but for NCSNv2 or any other architecture implementing Technique 3, we apply the following modifications: (i) setting the number of classes in CondInstanceNorm++ to 1 (which we name as InstanceNorm++); (ii) changing average pooling back to max pooling; and (iii) removing all normalization layers in RefineNet blocks. Here (ii) and (iii) do not affect the results much, but they are included because we hope to minimize the number of unnecessary changes to the standard RefineNet architecture (the original RefineNet blocks in <ref type="bibr" target="#b23">[24]</ref> use max pooling and have no normalization layers). We name a ResNet block (with InstanceNorm++ instead of BatchNorm) "ResBlock", and a RefineNet block "RefineBlock". When CondInstanceNorm++ is added, we name them "CondResBlock" and "CondRefineBlock" respectively. We use the ELU activation function <ref type="bibr" target="#b24">[25]</ref> throughout all architectures.</p><p>To ensure sufficient capacity and receptive fields, the network structures for images of different resolutions have different numbers of layers and filters. We summarize the architectures in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref>.  We use the Adam optimizer <ref type="bibr" target="#b25">[26]</ref> for all models. When Technique 3 is not in effect, we choose the learning rate 0.001; otherwise we use a learning rate 0.0001 to avoid loss explosion. We set the parameter of Adam to 10 −3 for FFHQ and 10 −8 otherwise. We provide other hyperparameters in <ref type="table" target="#tab_3">Table 4</ref>, where σ 1 , L, T , and of NCSNv2 are all chosen in accordance with our proposed techniques. When the number of training data is larger than 60000, we randomly sample 10000 of them and compute the maximum pairwise distance, which is set as σ 1 for NCSNv2. Training: We use the Adam <ref type="bibr" target="#b25">[26]</ref> optimizer with default hyperparameters. The learning rates and batch sizes are provided in Appendix B.1 and <ref type="table" target="#tab_3">Table 4</ref>. We observe that for images at resolution 128 × 128 or 256 × 256, training can be unstable when the loss is near convergence. We note, however, this is a well-known problem of the Adam optimizer, and can be mitigated by techniques such as AMSGrad <ref type="bibr" target="#b30">[31]</ref>. We trained all models on Nvidia Tesla V100 GPUs.</p><p>Settings for Section 3.3: The loss curves in <ref type="figure" target="#fig_4">Fig. 3</ref> are results of two settings: (i) Technique 1, 2, 4 and 5 are in effect, but the model architecture is the same as the original NCSN (i.e., <ref type="table" target="#tab_1">Table 2a</ref>); and (ii) all techniques are in effect, i.e., the model is the same as NCSNv2 depicted in <ref type="table" target="#tab_2">Table 3a</ref>. We apply EMA with momentum 0.9 to smooth the curves in <ref type="figure" target="#fig_4">Fig. 3</ref>. We observe that despite being simpler to implement, the new noise conditioning method proposed in Technique 3 performs as well as the original and arguably more complex one in <ref type="bibr" target="#b0">[1]</ref> in terms of the training loss. See the ablation studies in Section 6 and Appendix C.4 for additional results.</p><p>Interpolation: We can interpolate between two different samples from NCSN/NCSNv2 via interpolating the Gaussian random noise injected by annealed Langevin dynamics. Specifically, suppose we have a total of L noise levels, and for each noise level we run T steps of Langevin dynamics. Let {z ij } 1≤i≤L,1≤j≤T {z 11 , z 12 , · · · , z 1T , z 21 , z 22 , · · · , z 2T , · · · , z L1 , z L2 , · · · , z LT } denote the set of all Gaussian noise used in this procedure, where z ij is the noise injected at the j-th iteration of Langevin dynamics corresponding to the i-th noise level. Next, suppose we have two samples x <ref type="bibr" target="#b0">(1)</ref> and x <ref type="bibr" target="#b1">(2)</ref> with the same initialization x 0 , and denote the corresponding set of Gaussian noise as {z <ref type="bibr" target="#b0">(1)</ref> ij } 1≤i≤L,1≤j≤T and {z <ref type="bibr" target="#b1">(2)</ref> ij } 1≤i≤L,1≤j≤T respectively. We can generate N interpolated samples between x (1) and x <ref type="bibr" target="#b1">(2)</ref> , where for the k-th interpolated sample we use Gaussian noise {cos</p><formula xml:id="formula_33">kπ 2(N +1) z (1) ij + sin kπ 2(N +1) z (2)</formula><p>ij } 1≤i≤L,1≤j≤T and initialization x 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Additional results without the denoising step</head><p>We further demonstrate the stabilizing effect of EMA in <ref type="figure">Fig. 8</ref>, where FIDs are computed without the denoising step. As indicated by <ref type="figure">Figs. 4 and 8</ref>, EMA can stabilize training and remove sample artifacts regardless of whether denoising is used or not.</p><p>FID scores should be interpreted with caution because they may not align well with human judgement. For example, the samples from NCSNv2 as demonstrated in <ref type="figure" target="#fig_0">Fig. 10b</ref> have an FID score of 28.9 (without denoising), worse than NCSN <ref type="figure" target="#fig_0">(Fig. 10a</ref>) whose FID is 26.9 (without denoising), but arguably  produce much more visually appealing samples. To investigate whether FID scores align well with human ratings, we use the HYPE ∞ [29] score (higher is better), a metric of sample quality based on human evaluation, to compare the two models that generated samples in <ref type="figure" target="#fig_0">Figs. 10a and 10b</ref>. We provide full results in <ref type="table" target="#tab_4">Table 5</ref>, where all numbers except those for NCSN and NCSNv2 are directly taken from <ref type="bibr" target="#b28">[29]</ref>. As <ref type="table" target="#tab_4">Table 5</ref> shows, our NCSNv2 achieves 37.3 on CelebA 64 × 64 which is comparable to ProgressiveGAN <ref type="bibr" target="#b31">[32]</ref>, whereas NCSN achieves 19.8. This is completely different from the ranking indicated by FIDs. Finally, we provide ablation results without the denoising step in <ref type="figure">Fig. 9</ref>. It is qualitatively similar to <ref type="figure">Fig. 5</ref> where results are computed with denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Training and sampling speed</head><p>In <ref type="table" target="#tab_5">Table 6</ref>, we provide the time cost for training and sampling from NCSNv2 models on various datasets considered in our experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Additional results on ablation studies</head><p>As discussed in Section 6, we partition all techniques into three groups: (i) Technique 5, (ii) Technique 1,2,4, and (iii) Technique 3, and investigate the performance of models after successively removing (iii), (ii), and (i) from NCSNv2. Aside from the FID curves in Figs. 5 and 9, we also provide samples from different models for visual inspection in <ref type="figure" target="#fig_0">Figs. 13 and 14</ref>. To generate these samples, we compute the FID scores on 1000 samples every 5000 training iterations for each considered model, and sample from the checkpoint of the smallest FID (the same setting as in <ref type="bibr" target="#b0">[1]</ref>). From samples in <ref type="figure" target="#fig_0">Figs. 13 and 14</ref>, we easily observe that removing any group of techniques leads to worse samples.   <ref type="figure" target="#fig_0">Figure 15</ref>: Training vs. test loss curves of NCSNv2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5.2 Nearest neighbors</head><p>Starting from this section, all samples are from NCSNv2 at the last training iteration. For each generated sample, we show the nearest neighbors from the training dataset, measured by 2 distance in the feature space of a pre-trained InceptionV3 network. Since we apply random horizontal flip when training, we also take this into consideration when computing nearest neighbors, so that we can detect cases in which NCSNv2 memorizes a flipped training data point.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5.3 Additional interpolation results</head><p>We generate samples from NCSNv2 and interpolate between them using the method described in Appendix B.2.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Generated samples on datasets of decreasing resolutions. From left to right: FFHQ 256 × 256, LSUN bedroom 128 × 128, LSUN tower 128 × 128, LSUN church_outdoor 96 × 96, and CelebA 64 × 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Running annealed Langevin dynamics to sample from a mixture of Gaussian centered at images in the CIFAR-10 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 ,</head><label>2</label><figDesc>samples inFig. 2c(using Technique 1) exhibit comparable diversity to ground-truth images (Fig. 2a), and have better variety thanFig. 2b (σ 1 = 1). Quantitatively, the average pairwise distance of samples inFig. 2cis 18.65, comparable to data (17.78) but much higher than that ofFig. 2b (10.12).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Training loss curves of two noise conditioning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>FIDs and color artifacts over the course of training (best viewed in color). The FIDs of NCSN have much higher volatility compared to NCSN with EMA. Samples from the vanilla NCSN often have obvious color shifts. All FIDs are computed with the denoising step. (a) CIFAR-10 FIDs (b) CelebA FIDs FIDs for different groups of techniques. Subscripts of "NCSN" are IDs of techniques in effect. "NCSNv2" uses all techniques. Results are computed with the denoising step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>From top to bottom: FFHQ 256 2 , LSUN bedroom 128 2 , LSUN tower 128 2 , and LSUN church_outdoor 96 2 . Within each group of images: the first row shows uncurated samples from NCSNv2, and the second shows the interpolation results between the leftmost and rightmost samples with NCSNv2. You may zoom in to view more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>NCSN vs. NCSNv2 samples on LSUN church_outdoor (a)(b) and LSUN bedroom (c)(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 2 /σ 2 ∼ χ 2 D</head><label>22</label><figDesc>, i.e., p s (s) = 1 2 D/2 Γ(D/2) s D/2−1 e −s/2 . Because r = x 2 = σ √ s, we can use the change of variables formula to get p(r) = 2r σ 2 p s (s) = 2r σ 2 p s r 2 σ 2 = 1 2 D/2−1 Γ(D/2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>FIDs and color artifacts over the course of training (best viewed in color). The FIDs of NCSN have much higher volatility compared to NCSN with EMA. Samples from the vanilla NCSN often have obvious color shifts. All FIDs are computed without the denoising step. (a) CIFAR-10 FIDs (b) CelebA FIDs FIDs for different groups of techniques. Subscripts of "NCSN" are IDs of techniques in effect. "NCSNv2" uses all techniques. Results are computed without the denoising step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Uncurated samples from NCSN (a) and NCSNv2 (b) on CelebA 64 × 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>(Figure 11 :Figure 12 :</head><label>1112</label><figDesc>a) NCSN (Iter. = 50k) (b) NCSN (Iter. = 100k) (c) NCSN (Iter. = 200k) (d) NCSN w/ EMA (Iter. = 50k) (e) NCSN w/ EMA (Iter. = 100k) (f) NCSN w/ EMA (Iter. = 200k) EMA reduces undesirable color shifts on CIFAR-10. We show samples from NCSN and NCSN with EMA at the 50k/100k/200k-th training iteration.(a) NCSN (Iter. = 50k) (b) NCSN (Iter. = 100k) (c) NCSN (Iter. = 150k) (d) NCSN w/ EMA (Iter. = 50k) (e) NCSN w/ EMA (Iter. = 100k) (f) NCSN w/ EMA (Iter. = 150k) EMA reduces undesirable color shifts on CelebA-10. We show samples from NCSN and NCSN with EMA at the 50k/100k/150k-th training iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Samples from models with different groups of techniques applied. NCSN is the original model in<ref type="bibr" target="#b0">[1]</ref> and does not use any of the newly proposed techniques. Subscripts of "NCSN" denote the IDs of techniques in effect. NCSN 5 only applies EMA. NCSN 1,2,4,5 applies both EMA and technique group (ii). NCSNv2 is the result of all techniques combined. Checkpoints are selected according to the lowest FID (with denoising) over the course of training.(a) NCSN on CIFAR-10 (b) NCSN on CelebA (c) NCSN5 on CIFAR-10 (d) NCSN5 on CelebA (e) NCSN1,2,4,5 on CIFAR-10 (f) NCSN1,2,4,5 on CelebA (g) NCSNv2 on CIFAR-10 (h) NCSNv2 on CelebA Samples from models with different groups of techniques applied. NCSN is the original model in [1] and does not use any of the newly proposed techniques. Subscripts of "NCSN" denote the IDs of techniques in effect. NCSN 5 only applies EMA. NCSN 1,2,4,5 applies both EMA and technique group (ii). NCSNv2 is the result of all techniques combined. Checkpoints are selected according to the lowest FID (without denoising) over the course of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Nearest neighbors on CIFAR-10. NCSNv2 samples are on the left side of the red vertical line. Corresponding nearest neighbors are on the right side in the same row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 :</head><label>17</label><figDesc>Nearest neighbors on CelebA 64 × 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 :</head><label>18</label><figDesc>Nearest neighbors on LSUN church_outdoor 96 × 96.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 19 :</head><label>19</label><figDesc>Nearest neighbors on FFHQ 256 × 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 20 :</head><label>20</label><figDesc>NCSNv2 interpolation results on CelebA 64 × 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 21 :</head><label>21</label><figDesc>NCSNv2 interpolation results on LSUN church_outdoor 96 × 96.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 22 :</head><label>22</label><figDesc>NCSNv2 interpolation results on LSUN bedroom 128 × 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 23 :</head><label>23</label><figDesc>NCSNv2 interpolation results on LSUN tower 128 × 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 24 :Figure 25 :</head><label>2425</label><figDesc>NCSNv2 interpolation results on FFHQ 256 × 256. C.6 Additional uncurated samples Uncurated CIFAR-10 32 × 32 samples from NCSNv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 26 :</head><label>26</label><figDesc>Uncurated CelebA 64 × 64 samples from NCSNv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 27 :</head><label>27</label><figDesc>Uncurated LSUN church_outdoor 96 × 96 samples from NCSNv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 28 :</head><label>28</label><figDesc>Uncurated LSUN bedroom 128 × 128 samples from NCSNv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 29 :</head><label>29</label><figDesc>Uncurated LSUN tower 128 × 128 samples from NCSNv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 30 :</head><label>30</label><figDesc>Uncurated FFHQ 256 × 256 samples from NCSNv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Inception and FID scores.</figDesc><table><row><cell>Model</cell><cell cols="2">Inception ↑ FID ↓</cell></row><row><cell>CIFAR-10 Unconditional</cell><cell></cell><cell></cell></row><row><cell>PixelCNN [17]</cell><cell>4.60</cell><cell>65.93</cell></row><row><cell>IGEBM [18]</cell><cell>6.02</cell><cell>40.58</cell></row><row><cell>WGAN-GP [19]</cell><cell>7.86 ± .07</cell><cell>36.4</cell></row><row><cell>SNGAN [20]</cell><cell>8.22 ± .05</cell><cell>21.7</cell></row><row><cell>NCSN [1]</cell><cell cols="2">8.87 ± .12 25.32</cell></row><row><cell>NCSN (w/ denoising)</cell><cell>7.32 ± .12</cell><cell>29.8</cell></row><row><cell cols="2">NCSNv2 (w/o denoising) 8.73 ± .13</cell><cell>31.75</cell></row><row><cell>NCSNv2 (w/ denoising)</cell><cell cols="2">8.40 ± .07 10.87</cell></row><row><cell>CelebA 64 × 64</cell><cell></cell><cell></cell></row><row><cell>NCSN (w/o denoising)</cell><cell>-</cell><cell>26.89</cell></row><row><cell>NCSN (w/ denoising)</cell><cell>-</cell><cell>25.30</cell></row><row><cell>NCSNv2 (w/o denoising)</cell><cell>-</cell><cell>28.86</cell></row><row><cell>NCSNv2 (w/ denoising)</cell><cell>-</cell><cell>10.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The architectures of NCSN for images of various resolutions.</figDesc><table><row><cell>(a) NCSN 32 2 -64 2</cell><cell>(b) NCSN 96 2 -128 2</cell></row><row><cell>3x3 Conv2D, 128</cell><cell>3x3 Conv2D, 128</cell></row><row><cell>CondResBlock, 128</cell><cell>CondResBlock, 128</cell></row><row><cell>CondResBlock, 128</cell><cell>CondResBlock, 128</cell></row><row><cell>CondResBlock down, 256</cell><cell>CondResBlock down, 256</cell></row><row><cell>CondResBlock, 256</cell><cell>CondResBlock, 256</cell></row><row><cell>CondResBlock down, 256</cell><cell>CondResBlock down, 256</cell></row><row><cell>dilation 2</cell><cell>CondResBlock, 256</cell></row><row><cell>CondResBlock, 256 dilation 2</cell><cell>CondResBlock down, 512 dilation 2</cell></row><row><cell>CondResBlock down, 256 dilation 4</cell><cell>CondResBlock, 512 dilation 2</cell></row><row><cell>CondResBlock, 256 dilation 4</cell><cell>CondResBlock down, 512 dilation 4</cell></row><row><cell>CondRefineBlock, 256</cell><cell>CondResBlock, 512</cell></row><row><cell>CondRefineBlock, 256</cell><cell>dilation 4</cell></row><row><cell>CondRefineBlock, 128</cell><cell>CondRefineBlock, 512</cell></row><row><cell>CondRefineBlock, 128</cell><cell>CondRefineBlock, 256</cell></row><row><cell>3x3 Conv2D, 3</cell><cell>CondRefineBlock, 256</cell></row><row><cell></cell><cell>CondRefineBlock, 128</cell></row><row><cell></cell><cell>CondRefineBlock, 128</cell></row><row><cell></cell><cell>3x3 Conv2D, 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The architectures of NCSNv2 for images of various resolutions.</figDesc><table><row><cell>RefineBlock, 128</cell></row><row><cell>RefineBlock, 128</cell></row><row><cell>3x3 Conv2D, 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters of NCSN/NCSNv2. The latter is configured according to Technique 1-4. σ 1 and L determine the set of noise levels. T and are parameters of annealed Langevin dynamics. We use the following datasets in our experiments: CIFAR-10<ref type="bibr" target="#b1">[2]</ref>, CelebA<ref type="bibr" target="#b15">[16]</ref>, LSUN<ref type="bibr" target="#b26">[27]</ref>, and FFHQ<ref type="bibr" target="#b27">[28]</ref>. CIFAR-10 contains 50000 training images and 10000 test images, all of resolution 32 × 32. CelebA contains 162770 training images and 19962 test images with various resolutions. For preprocessing, we first center crop them to size 140 × 140, and then resize them to 64 × 64. We choose the church_outdoor, bedroom and tower categories in the LSUN dataset. They contain 126227, 3033042, and 708264 training images respectively, and all have 300 validation images. For preprocessing, we first resize them so that the smallest dimension of images is 96 (for church_outdoor) or 128 (for bedroom and tower), and then center crop them to equalize their lengths and heights. Finally, the FFHQ dataset consists of 70000 high-quality facial images at resolution 1024 × 1024. We resize them to 256 × 256 in our experiments. Because FFHQ does not have an official test dataset, we randomly select 63000 images for training and the remaining 7000 as the test dataset. In addition, we apply random horizontal flip as data augmentation in all cases.Metrics: We use FID<ref type="bibr" target="#b12">[13]</ref> and HYPE ∞<ref type="bibr" target="#b28">[29]</ref> scores for quantitative comparison of results. When computing FIDs on CIFAR-10 32 × 32, we measure the distance between the statistics of samples and training data. When computing FIDs on CelebA 64 × 64, we follow the settings in<ref type="bibr" target="#b29">[30]</ref> where the distance is measured between 10000 samples and the test dataset. We use the official website https://hype.stanford.edu for computing HYPE ∞ scores. Regarding model selection, we follow the settings in<ref type="bibr" target="#b0">[1]</ref>, where we compute FID scores on 1000 samples every 5000 training iterations and choose the checkpoint with the smallest FID for computing both full FID scores (with more samples from it) and the HYPE ∞ scores.</figDesc><table><row><cell>Model</cell><cell>Dataset</cell><cell>σ 1</cell><cell>L</cell><cell>T</cell><cell></cell><cell cols="2">Batch size Training iterations</cell></row><row><cell>NCSN</cell><cell>CIFAR-10 32 2</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>2e-5</cell><cell>128</cell><cell>300k</cell></row><row><cell>NCSN</cell><cell>CelebA 64 2</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>2e-5</cell><cell>128</cell><cell>210k</cell></row><row><cell>NCSN</cell><cell>LSUN church_outdoor 96 2</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>2e-5</cell><cell>128</cell><cell>200k</cell></row><row><cell>NCSN</cell><cell>LSUN bedroom 128 2</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>2e-5</cell><cell>64</cell><cell>150k</cell></row><row><cell>NCSNv2</cell><cell>CIFAR-10 32 2</cell><cell>50</cell><cell>232</cell><cell>5</cell><cell>6.2e-6</cell><cell>128</cell><cell>300k</cell></row><row><cell>NCSNv2</cell><cell>CelebA 64 2</cell><cell>90</cell><cell>500</cell><cell>5</cell><cell>3.3e-6</cell><cell>128</cell><cell>210k</cell></row><row><cell cols="4">NCSNv2 LSUN church_outdoor 96 2 140 788</cell><cell>4</cell><cell>4.9e-6</cell><cell>128</cell><cell>200k</cell></row><row><cell cols="4">NCSNv2 LSUN bedroom/tower 128 2 190 1086</cell><cell>3</cell><cell>1.8e-6</cell><cell>128</cell><cell>150k</cell></row><row><cell>NCSNv2</cell><cell>FFHQ 256 2</cell><cell cols="2">348 2311</cell><cell>3</cell><cell>0.9e-7</cell><cell>32</cell><cell>80k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>HYPE ∞ scores on CelebA 64 × 64.</figDesc><table><row><cell>Model</cell><cell cols="4">HYPE ∞ (%) Fakes Error(%) Reals Error(%) Std.</cell></row><row><cell>StyleGAN  *  [28]</cell><cell>50.7</cell><cell>62.2</cell><cell>39.3</cell><cell>1.3</cell></row><row><cell>ProgressiveGAN [32]</cell><cell>40.3</cell><cell>46.2</cell><cell>34.4</cell><cell>0.9</cell></row><row><cell>BEGAN [33]</cell><cell>10</cell><cell>6.2</cell><cell>13.8</cell><cell>1.6</cell></row><row><cell>WGAN-GP [19]</cell><cell>3.8</cell><cell>1.7</cell><cell>5.9</cell><cell>0.6</cell></row><row><cell>NCSN</cell><cell>19.8</cell><cell>22.3</cell><cell>17.3</cell><cell>0.4</cell></row><row><cell>NCSNv2</cell><cell>37.3</cell><cell>49.8</cell><cell>24.8</cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* With truncation tricks.* with truncation tricks</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Training and sampling speed of NCSNv2 on various datasets.</figDesc><table><row><cell>Dataset</cell><cell>Device</cell><cell cols="2">Sampling time Training time</cell></row><row><cell cols="2">CIFAR-10 2x V100</cell><cell>2 min</cell><cell>22 h</cell></row><row><cell>CelebA</cell><cell>4x V100</cell><cell>7 min</cell><cell>29 h</cell></row><row><cell>Church</cell><cell>8x V100</cell><cell>17 min</cell><cell>52 h</cell></row><row><cell cols="2">Bedroom 8x V100</cell><cell>19 min</cell><cell>52 h</cell></row><row><cell>Tower</cell><cell>8x V100</cell><cell>19 min</cell><cell>52 h</cell></row><row><cell>FFHQ</cell><cell>8x V100</cell><cell>50 min</cell><cell>41 h</cell></row><row><cell>C.3 Color shifts</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>First, we demonstrate that our NCSNv2 does not overfit to the training dataset by showing the curves of training/test loss inFig. 15. Since the loss on the test dataset is always close to the loss on the training dataset during the course of training, this indicates that our model does not simply memorize training data.</figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell></row><row><cell>(d)</cell><cell>(e)</cell><cell>(f)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>The authors would like to thank Aditya Grover, Rui Shu and Shengjia Zhao for reviewing an early draft of this paper, as well as Gabby Wright and Sharon Zhou for resolving technical issues in computing HYPE ∞ scores. This research was supported by NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), AFOSR (FA9550-19-1-0024), and Amazon AWS.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11895" to="11907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="695" to="709" />
			<date type="published" when="2005-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Least squares estimation without priors or supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raphan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="374" to="420" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exponential convergence of langevin distributions and their discrete approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tweedie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="363" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sliced score matching: A scalable approach to density and score estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahaj</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019</title>
		<meeting>the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019<address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">204</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Piché-Taillefer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05475</idno>
		<title level="m">Ioannis Mitliagkas, and Rémi Tachet des Combes. Adversarial score matching and improved sampling for image generation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural empirical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning energy-based models in highdimensional spaces with multi-scale denoising score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich T</forename><surname>Sommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1910</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Solving linear inverse problems using the prior implicit in a denoiser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Kadkhodaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13640</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tweedie&apos;s formula and selection bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">496</biblScope>
			<biblScope unit="page" from="1602" to="1614" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Linear and nonlinear models: fixed effects, random effects, and mixed models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grafarend</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>de Gruyter</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Implicit generation and generalization in energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08689</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5228" to="5237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14837" to="14847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hype: A benchmark for human eye perceptual evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Narcomey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3444" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09779</idno>
		<title level="m">Bridging the gap between f -gans and wasserstein gans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09237</idno>
		<title level="m">On the convergence of adam and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Began</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<title level="m">Boundary equilibrium generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
