<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
							<email>‡yaoquanming@4paradigm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
							<email>leichen@cse.ust.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hong Kong University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hong Kong SAR</orgName>
								<orgName type="institution" key="instit3">China ‡ 4Paradigm Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph (KG) embedding is a fundamental problem in data mining research with many real-world applications. It aims to encode the entities and relations in the graph into low dimensional vector space, which can be used for subsequent algorithms. Negative sampling, which samples negative triplets from non-observed ones in the training data, is an important step in KG embedding. Recently, generative adversarial network (GAN), has been introduced in negative sampling. By sampling negative triplets with large scores, these methods avoid the problem of vanishing gradient and thus obtain better performance. However, using GAN makes the original model more complex and hard to train, where reinforcement learning must be used. In this paper, motivated by the observation that negative triplets with large scores are important but rare, we propose to directly keep track of them with cache. However, how to sample from and update the cache are two important questions. We carefully design the solutions, which are not only efficient but also achieve good balance between exploration and exploitation. In this way, our method acts as a "distilled" version of previous GANbased methods, which does not waste training time on additional parameters to fit the full distribution of negative triplets. The extensive experiments show that our method can gain significant improvement on various KG embedding models, and outperform the state-of-the-arts negative sampling methods based on GAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Knowledge graph (KG) is a special kind of graph structure, with entities as nodes, and relations as directed edges. Each edge (also called a fact) is represented as a triplet with the form (head entity, relation, tail entity), which is denoted as (h, r, t), indicating that two entities are connected by a specific relation, e.g. (Shakespeare, isAuthorOf, Hamlet) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b35">[36]</ref>. KG is very general and useful, and it has been used as fundamental building blocks for many applications, e.g., structured search <ref type="bibr" target="#b33">[34]</ref>, question answering <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and intelligent virtual assistant <ref type="bibr" target="#b42">[43]</ref>. Such importance has also inspired many famous KG projects, e.g., FreeBase <ref type="bibr" target="#b3">[4]</ref>, DBpedia <ref type="bibr" target="#b1">[2]</ref>, and YAGO <ref type="bibr" target="#b35">[36]</ref>.</p><p>However, as these triplets are hard to manipulate, one fundamental problem is how to find a good representation for entities and relations in the KG <ref type="bibr" target="#b29">[30]</ref>. Early works towards this goal lie in statistical relational learning using the symbolic triplet data <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>. However, these methods neither lead to good generalization performance, nor can they be applied for large scale knowledge graphs. Recently, graph This work is partially done when Y. Zhang was an intern in 4Paradigm Inc, and Q. Yao is the correspondence author. embedding techniques <ref type="bibr" target="#b39">[40]</ref> have been introduced in KG. These methods attempt to encode entities and relations in KG into a low-dimensional vector space while capturing nodes' and edges' connection properties. They are scalable and have also shown a promising performance in basic KG tasks, such as link prediction and triplet classification <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p><p>Besides, based on the learned entity and relation embeddings, downstream tasks, such as entity classification <ref type="bibr" target="#b31">[32]</ref> and entity linking <ref type="bibr" target="#b5">[6]</ref>, can also be benefited. Given that the relation encoding entity types (denoted as IsA) or the relation encoding equivalent entities (denoted as EqualTo) is contained in the KG, and has been included into the learning process, entity classification can be treated as the link prediction task (x, IsA, ?), and entity linking treated as triplet classification task (a, EqualT o, b). A more direct entity linking method proposed in <ref type="bibr" target="#b31">[32]</ref> is to check the similarity score between embeddings of two entities.</p><p>In recent years, constructing new scoring functions which can better model the complex interactions between entities and relations have been the main focus for improving KG embedding's performance <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b45">[46]</ref>. However, another very important perspective of KG embedding, i.e., negative sampling, is not sufficiently emphasized. The need of negative sampling comes from the fact that there are only positive triplets in KG <ref type="bibr" target="#b9">[10]</ref>. To avoid trivial solutions of the embedding, for each positive triplet, a set that contains its all possible negative samples, needs to be hand-made. Then, for the effectiveness and efficiency of stochastic updates in the KG embedding, once we have picked up a positive triplet, we also need to sample a negative triplet from its corresponding negative sample set. Unfortunately, the quality of these negative triplets does matter.</p><p>Due to its simplicity and efficiency, uniform sampling is broadly used in KG embedding <ref type="bibr" target="#b39">[40]</ref>. However, it is a fixed scheme and ignores changes on the distribution of negative triplets during the training. Thus, it suffers seriously from the vanishing gradient problem. Specifically, as observed in <ref type="bibr" target="#b38">[39]</ref>, most negative triplets in the sampling set are easily classified ones. Since scoring functions tend to give observed (positive) triplets large values, as training goes, scores (evaluated from scoring functions) for most non-observed (probably negative) triplets become smaller. Thus, when negative triplets are uniformly sampled, it is very likely that we pick up one with zero gradient. As a result, the training process of KG embedding will be impeded by such vanishing gradients rather than by the optimization algorithm. Such problem prevents KG embedding getting desired performance. A better sampling scheme, i.e., Bernoulli sampling, is introduced in <ref type="bibr" target="#b41">[42]</ref>. It improves uniform sampling by considering one-to-many, manyto-many, and many-to-one mapping in relation between head and tail. However, it is still a fixed sampling scheme, which suffers from vanishing gradients.</p><p>Therefore, high-quality negative triplets should have large scores. To efficiently capture them during training, we have two main challenges for the negative sampling: (i). How to capture and model negative triplets' dynamic distribution? and (ii). How can we sample negative triplets in an efficient way? Recently, there are two pioneered works, i.e., IGAN <ref type="bibr" target="#b38">[39]</ref> and KBGAN <ref type="bibr" target="#b8">[9]</ref>, attempting to address these challenges. Their ideas are both replacing the fixed sampling scheme with a generative adversarial network (GAN) <ref type="bibr" target="#b15">[16]</ref>. However, the GAN-based solutions still have many problems. First, GAN increases the number of training parameters because an extra generator is introduced. Second, GAN training can suffer from instability and degeneracy <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>, and the REINFORCE gradient <ref type="bibr" target="#b43">[44]</ref> used in IGAN and KBGAN is known to have high variance. These drawbacks lead to instable performance for different scoring functions, and hence pretrain becomes a must for both IGAN and KBGAN.</p><p>In this paper, to address the challenges of high-quality negative sampling while avoiding the problems from using GAN, we propose a new negative sampling method based on cache, called NSCaching. With empirically studying the score distribution of negative samples, we find that the score distribution is highly skew, i.e., there are only a few negative triplets with large scores and the rest are useless. This observation motivates to only maintain high-quality negative triplets during the training, and dynamically update the maintained triplets. First, we store the high-quality negative triplets in cache, and then design importance sampling (IS) strategy to update the cache. The IS strategy can not only capture the dynamic characteristic of the distribution, but also benefit the efficiency of NSCaching. Furthermore, we also take good care of "exploration and exploitation", which balances exploring all possible high-quality negative triplets and sampling from a few large score negative triplets in cache. Contributions of our work are summarized as follows:</p><p>• We propose a simple and efficient negative sampling scheme, NSCaching. It is a general negative sampling scheme, which can be injected into all popularly used KG embedding models. NSCaching has fewer parameters than both IGAN and KBGAN, and can be trained with gradient descent as the original KG embedding models. • We propose the uniform strategy to sample from the cache and IS strategy to update the cache in NSCaching with good care of "exploration and exploitation". • We analyze the connection between NSCaching and the self-paced learning <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref>. We show NSCaching can firstly learn easily classified samples, and then gradually switch to harder samples. <ref type="bibr">•</ref> We conduct experiments on four popular data sets, i.e., WN18 and FB15K, and their variants WN18RR and FB15K237. Experimental results demonstrate that our method is very efficient and is more effective than the state-of-the-arts, i.e., IGAN and KBGAN, as well. Notation. We denote the set of entities as E and set of relations as R. A fact (edge) in KG is represented by a triplet, i.e., (h, r, t), where h ∈ E is the head entity, t ∈ E is the tail entity, and r ∈ R is the relationship. Observed facts in a KG are represented by a set S ≡ {(h, r, t)}. Finally, we denote the embedding vectors of h, r and t by its corresponding boldface character, i.e. h, r and t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARY: FRAMEWORK OF KG EMBEDDING</head><p>Here, we first introduce the general framework for training KG embedding models in Section II-A. Then, we describe its two key components, i.e., negative sampling and scoring function in Section II-B and II-C respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The General Framework</head><p>To build a KG embedding model, the most important thing is to design a scoring function f , which captures the interactions between two entities based on a relation <ref type="bibr" target="#b39">[40]</ref>. Different scoring functions have their own weaknesses and strengths in capturing the underneath interactions. Besides, the observed facts in KG are supposed to have larger scores than non-observed ones. With the factual information, the embeddings are learned by solving the optimization problem that maximizes the scoring function for positive triplets and minimizes for non-observed triplets at the same time. Based on the properties of scoring functions, KG embedding models are generally divided into two categories. The first one is translational distance model, i.e.,</p><formula xml:id="formula_0">L(E, R) = (h,r,t)∈S γ − f (h, r, t) + f (h, r,t) + ,<label>(1)</label></formula><p>and the second one is semantic matching model, i.e.,</p><formula xml:id="formula_1">L(E, R) = (h,r,t)∈S (+1, f (h, r, t)) + −1, f (h, r,t) ,<label>(2)</label></formula><p>where (h, r,t) ∈ S is the hand-made negative triplet for (h, r, t) and (α, β) = log (1 + exp(−αβ)) is the logistic loss. The above two objectives can be optimized by using stochastic gradient descent in an unified framework (Algorithm 1). In each iteration, a mini-batch S batch of size m is firstly sampled from S at step 3. In step 5, since there are no negative triplets in S, a setS (h,r,t) , i.e.,</p><formula xml:id="formula_2">S (h,r,t) = (h, r, t) / ∈ S |h ∈ E ∪ {(h, r,t) / ∈ S |t ∈ E},<label>(5)</label></formula><p>which contains negative triplets for (h, r, t), is made, and one negative triplet (h, r,t) is sampled fromS (h,r,t) . Finally, embedding parameters are updated in step 6. Thus, in optimization, the most important problem is how to do negative sampling, i.e. generate and sample negative triplet fromS (h,r,t) .</p><p>Algorithm 1 General framework of KG embedding. Input: training set S = {(h, r, t)}, embedding dimension d and scoring function f ; 1: initialize the embeddings for each e ∈ E and r ∈ R. 2: for i = 1, · · · , T do <ref type="bibr">3:</ref> sample a mini-batch S batch ∈ S of size m; <ref type="bibr">4:</ref> for each (h, r, t) ∈ S batch do <ref type="bibr">5:</ref> sample a negative triplet (h, r,t) ∈S (h,r,t) ; // negative sampling <ref type="bibr">6:</ref> update parameters of embeddings w.r.t. the gradients using (i). translational distance models:</p><formula xml:id="formula_3">∇ γ − f (h, r, t) + f h , r,t + ,<label>(3)</label></formula><p>or (ii). semantic matching models:</p><formula xml:id="formula_4">∇ (+1, f (h, r, t)) + ∇ −1, f (h, r,t) ;<label>(4)</label></formula><p>7:</p><p>end for 8: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Negative Sampling</head><p>Existing works on negative sampling can be divided into two categories, i.e., sample from fixed and sample from dynamic distributions.</p><p>1) Sample from fixed distributions: In the early works <ref type="bibr" target="#b6">[7]</ref>, negative triplets are uniformly sampled from the setS (h,r,t) . Such strategy is simple and efficient. Later, a better sampling scheme, i.e., Bernoulli sampling, is introduced in <ref type="bibr" target="#b41">[42]</ref>. It improves uniform sampling by reducing the appearance of false negative triplets existing in one-to-many, many-to-many, and many-to-one relations between head and tail entities. However, as mentioned in the introduction, they still sample from fixed distributions, which can neither model the dynamic changes in distributions of negative triplets nor sample triplets with large scores. Thus, they seriously suffer from vanishing gradient.</p><p>2) Sample from dynamic distributions: More recently, two pioneered works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref> made a more dedicated analysis of problems with fixed sampling scheme. They observed that most of the negative triplets are easy ones, of which scores quickly go small during the training. This leads to the vanishing gradient problem if a fixed sampling scheme is used. Motivated by the success of Generative Adversarial Network (GAN) <ref type="bibr" target="#b15">[16]</ref> and its ability to model dynamic distribution, IGAN and KBGAN introduce GAN for negative sampling in KG.</p><p>When GAN is applied to negative sampling, a jointly trained generator serves as a sampler that can not only generate high-quality triplets by confusing the discriminator, but also dynamically adapt to the new distributions by keeping training. The discriminator, i.e., the KG embedding model, learns to distinguish between the positive triplets and the negative triplets selected by the generator. Under an alternating training procedure, the generator dynamically approximates the negative sample distribution, and the KG embedding model is improved by high-quality negative samples.</p><p>Specifically, given a positive triplet (h, r, t), IGAN models the distributionh,t ∼ p(e|(h, r, t)) over all entities to form a negative triplet (h, r,t). The quality of (h, r,t) is measured by the scoring function of the discriminator, i.e. the target KG embedding model. By joint training, IGAN can dynamically sample negative triplets with high quality. KBGAN operates in a different way. Instead of modeling a distribution over the whole entity set, KBGAN learns to sample from a subset of random entities. Namely, it first uniformly samples a set of entities to form a candidate set N eg = {(h, r,t)}, and then picks up one triplet from it. Under the framework of GAN, generator in KBGAN can approximate the score distribution of triplets in the set N eg, and sample a triplet with relatively high quality.</p><p>Even though GAN provides a solution to model the dynamic negative sample distribution, it is famous for suffering from instability and degeneracy <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Besides, REINFORCE gradient <ref type="bibr" target="#b43">[44]</ref> has to be used, which is known to have high variance. Thus, pretrain is a must for both IGAN and KBGAN. Finally, it increases the number of model's parameters and brings extra costs on training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Scoring Functions</head><p>The design of scoring function has been the main power source for improving embedding performance in recent years. Depending on the property of scoring functions, they are used in either translational distance or semantic matching models.</p><p>1) Translational distance model: The simplest and most representative translational distance model is TransE <ref type="bibr" target="#b6">[7]</ref>. Inspired from the word representation learning area <ref type="bibr" target="#b27">[28]</ref>, if a triplet (h, r, t) is true, the entity embeddings h, t should be connected by the relational vector r, i.e. h + r ≈ t. Under this assumption for example, two facts (China, Capital, Beijing) and (UK, Capital, London) will enjoy a relation that China − U K ≈ Beijing − London in the embedding space. Thus in TransE, the scoring function is defined as the negative translational distance of h and t connected by relation r, i.e., f (h, r, t) = h + r − t 1 .</p><p>Despite the simplicity of TransE, it faces the problem when dealing with one-to-many, many-to-one and many-to-many relations. Take one-to-many relation for example, TransE enforces h + r ≈ t i for different tail entity t i , thus resulting in very similar embeddings for these different entities. To solve this problem, variants like TransH <ref type="bibr" target="#b41">[42]</ref>, TransR <ref type="bibr" target="#b25">[26]</ref>, TransD <ref type="bibr" target="#b19">[20]</ref> are introduced to project embeddings of head/tail entity h, t into various spaces. By maximizing the scoring function for all positive triplets, the distance between h + r and t in corresponding space can be reduced.</p><p>2) Semantic matching model: Another group of scoring functions operate without the assumption that h + r ≈ t. Instead, they use similarity to measure the plausibility of triplets (h, r, t). RESCAL <ref type="bibr" target="#b31">[32]</ref> is the most original model. The entity embeddings h, t are also continuous vectors in R d . But for each relation, it is represented as a matrix which models the pairwise interaction between every dimension in entity embedding space R d . Namely, the scoring function of a triplet (h, r, t) is defined as f (h, r, t) = h M r t, where the relation is represented as a matrix M r ∈ R d×d . This scoring function captures pairwise interactions between all components of h and t, which needs O(d 2 ) parameters per relation.</p><p>Some simple and effective variants of RESCAL are Dist-Mult <ref type="bibr" target="#b45">[46]</ref>, HolE <ref type="bibr" target="#b30">[31]</ref> and ComplEx <ref type="bibr" target="#b37">[38]</ref>. DistMult simplifies RESCAL by restricting the interaction matrix M r into a diagonal matrix, which can reduce the number of parameters per relation from O(d 2 ) to O(d). HolE and ComplEx improves DistMult by modeling asymmetric relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED MODEL</head><p>In this section, we first describe our key observations in Section III-A, which are ignored by existing works but are the main motivations of our work. The proposed method is described in Section III-B, where we show how challenges in negative sampling are addressed by cache. Finally, we show an interesting connection between NSCaching and self-pace learning <ref type="bibr" target="#b23">[24]</ref> in Section III-C, which further explains the good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Closer Look at Distribution of Negative Triplets</head><p>Recall that, in Equation <ref type="formula" target="#formula_2">(5)</ref>, the negative triplet (h, r,t) ∈ S is formed by replacing either the head or tail entity of a positive triplet (h, r, t) ∈ S with any other entities in E. Before introducing the proposed method, we analyze the distribution of scores for (h, r,t) ∈S (h,r,t) .</p><p>(a) Different epochs.</p><p>(b) Different triplets.  Note that once the distance is larger than the margin γ, i.e., the red vertical line, the gradient of corresponding negative triplets will vanish to zero. Indeed, we can see the distribution changes during the training process; and negative triplets with large scores are rare. These observations are consistent with those ones in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>, which further explain the vanishing gradient problem of uniform sampling, as most sampled negative triplets will have small scores.</p><p>Although, the necessity of finding negative triplets with large scores from a dynamic distribution is mentioned by above works, they do not deeply study these distributions. Key Observations. The more important observations are:</p><p>• The score distribution of negative triplets is highly skew. • Regardless of the training <ref type="figure" target="#fig_0">(Figure 1(a)</ref>) and the choice of positive triplets <ref type="figure" target="#fig_0">(Figure 1(b)</ref>), only a few negative triplets have large scores. Thus, while GAN has strong ability to monitor the full generation process of negative triplets, it wastes a lot of parameters and training time on learning how negative triplets with small scores are distributed. This is obviously not necessary. Besides, reinforcement learning has been used once GAN is applied, which increases the difficulties on training. As a result, is it possible to directly keep track of those negative triplets with large scores?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. NSCaching: the Proposed Method</head><p>In this section, we describe the proposed method, which addresses the aforementioned question. The basic idea is very simple and intuitive. Recall that the challenges in negative sampling are (i) how to model the dynamic distribution of negative triplets and (ii) how to sample negative triplets in an efficient way. By considering the key observations, we are motivated to use a small amount of memory, which caches negative samples with large scores for each triplet in S, and sample the negative triplet directly from the cache. Algorithm 2 shows the KG embedding framework based on our cachebased negative sampling scheme. Note that the proposed sampling scheme does not depend on the choice of scoring functions, all ones previously mentioned in Section II-C can be used here. sample a mini-batch S batch ∈ S of size m; <ref type="bibr">4:</ref> for each (h, r, t) ∈ S batch do <ref type="bibr">5:</ref> index the cache H by (r, t) and T by (h, r) to get the candidate sets of heads H (r,t) and tails T (h,r) . <ref type="bibr">6:</ref> core step: sampleh ∈ H (r,t) andt ∈ T (h,r) . <ref type="bibr">7:</ref> uniformly pick up (h, r,t) ∈ {(h, r, t), (h, r,t)}. <ref type="bibr">8:</ref> core step: update the cache H (r,t) and T (h,r) using Algorithm 3; <ref type="bibr">9:</ref> update embeddings using Equation (3) or (4); <ref type="bibr">10:</ref> end for 11: end for Basically, as a negative triplet can be constructed by either replacing the head or tail entity, we maintain a head-cache H </p><formula xml:id="formula_5">O( m n+1 (N 1 + N 2 )d) O(m(N 1 + N 2 )d) (|E| + |R|)d</formula><p>(indexed by (r, t)) and a tail-cache T (indexed by (h, r)), which storeh ∈ E andt ∈ E respectively. Each pair (h, r) or (r, t) corresponds to a unique index. First, when a positive triplet is received, the corresponding cache containing candidates for negative triplets, i.e., H (r,t) and T (h,r) , are indexed in step 5. A negative triplet is generated from H (r,t) and T (h,r) at step 6-7, and then the cache is updated in step 8. Finally, the embeddings are updated based on the choice of scoring functions. An overview of the proposed method with state-of-thearts are in <ref type="table" target="#tab_1">Table I</ref>. The main difference with general KG embedding framework in Algorithm 1 is step 5-8 in Algorithm 2, where the sampling scheme is based on the cache instead. Besides, compared with previous complex GANbased works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>, our method in Algorithm 2 acts like a discriminative and distilled model of GAN, which only cares about negative triplets with large scores during the training. Thus, the proposed method, i.e., NSCaching, not only has fewer parameters, but also can be easily trained from randomly initialized models (from the scratch). Moreover, experimental results in Section IV show that NSCaching achieves the best performance.</p><p>However, in order to achieve best performance, we need to carefully design how to sample from the cache (step 6) and update the cache (step 8). In the sequel, we will describe the "exploration and exploitation" inside these steps and how they are balanced in detail. Then, we give a time and space analysis of Algorithm 2, which further explain its efficiency and memory saving. Note that, we only discuss operations and designs for the head-cache H here, as designs are the same for the tail-cache T .</p><p>1) Uniform sampling strategy from the cache (step 6): Recall that only headh in negative triplets with large scores are in cache H (r,t) , thus picking up anyh ∈ H (r,t) probably avoids the vanishing gradient problem. As larger scores also lead to bigger gradients, a very natural scheme is to always sample the negative triplet with the largest score.</p><p>However, as the distribution can change during the iterations of the algorithm, the negative triplets in the cache may not be accurate enough for the sampling in the latest iteration. Besides, there are false negative triplets in the negative sample sets, of which scores can also be very high <ref type="bibr" target="#b39">[40]</ref>. As a consequence, we also need to consider other triplets except the one with largest score in the cache.</p><p>This raises the question that how to keep the balance Algorithm 3 Updating head-cache (step 8).</p><formula xml:id="formula_6">Input: head cache H (r,t) of size N 1 , triplet (h, r, t) ∈ S. 1: initializeH (r,t) ← ∅; 2: uniformly sample a subset R m ⊂ E with N 2 entities; 3:Ĥ (r,t) ← H (r,t) ∪ R m ; 4:</formula><p>compute the score f (h, r, t) for allh ∈Ĥ (r,t) ; 5: for i = 1, · · · , N 1 do 6:</p><p>sampleh ∈Ĥ (r,t) with probability in Equation <ref type="formula" target="#formula_7">(6)</ref>; <ref type="bibr">7:</ref> removeh fromĤ (r,t) ; 8:H (r,t) ←H (r,t) ∪h; 9: end for 10: update by H (r,t) ←H (r,t) .</p><p>between exploration (i.e., explore all the possible high-quality negative samples) and exploitation (i.e., sample the largest score negative triplet in cache).</p><p>These motivate us to use uniformly random sampling scheme in step 6. It is simple, efficient, and does not introduce any bias into the selection process. Indeed, a stronger scheme can be sampling based on triplets' scores, where larger score indicates higher probability to be sampled. However, it has extra memory costs as scores needs to be stored as well. Moreover, it introduces bias causing by dynamic changing distribution and false negative triplets, which leads to inferior performance as shown in Section IV-C1.</p><p>2) Importance sampling strategy to update the cache (step 8): As mentioned in Section II-A, the cache needs to be dynamically changed during the iterations of the algorithm. Otherwise, while negative triplets are kept in H (r,t) , sampling from cache is still a scheme with fixed distribution, which eventually suffers from vanishing gradient problem. Thus, we need to refresh the cache in each iteration. Moreover, the cache needs to be updated in an efficient way.</p><p>The proposed importance sampling (IS) strategy is presented in Algorithm 3. First, we uniformly sample a subset R m ⊂ E of size N 2 (step 2), then union it with H (r,t) and obtainĤ <ref type="bibr">(r,t)</ref> . The scores for all triplets inĤ (r,t) are evaluated in step 4. After that, we construct a subsetH (r,t) fromĤ (r,t) by sampling entries inĤ (r,t) without replacement N 1 times following probability p(h|(t, r)) = exp(f (h, r, t)) hi∈Ĥ (r,t) exp(f (h i , r, t))</p><p>.</p><p>Finally,H (r,t) is returned as the updated head-cache. Note that exploration and exploitation also need to be carefully balanced in Algorithm 3. As the cache needs to be updated, we have to sample from E, and uniform sampling is chosen due to its efficiency. Thus, a bigger N 1 implies more exploitation, while a larger N 2 leads to more exploration. In step 6, indeed, uniform sampling or keeping triplets with top N 1 scores can be alternative choices. However, both of them are inappropriate. First, uniformly sampling is obviously not proper, as triplets in H (r,t) have much larger scores than those in R m . Then, deterministically sampling top N 1 is not appropriate as well, which again dues to the existence of false negative triplets (Section III-B1). All above concerns will also be empirically studied in experiments Section IV.</p><p>3) Space and time complexities: Here, we analyze the space and time complexities of NSCaching (Algorithm 2). Comparing with basic Algorithm 1, the main additional cost by introducing cache comes from Algorithm 3 in step 8. In Algorithm 3, the time complexity of computing the score of</p><formula xml:id="formula_8">N 1 + N 2 candidate triplets f (h, r, t) is O((N 1 + N 2 )d).</formula><p>The cost of step 6 contains two parts, i.e., normalization of the score and uniform sampling, they take O(N 1 + N 2 ) and O(N 1 ) respectively, which are very small. Thus, the total cost of introducing cache is O((N 1 + N 2 )d) for one triplet. We can lazily update the cache n epochs later rather than immediately updating, which can further reduce update complexity to O((N 1 + N 2 )d/(n + 1)).</p><p>As for space complexity, evaluating the scores for N 1 + N 2 candidate triplets takes O((N 1 + N 2 )d) space. Since we only store indices in the cache, it takes O(|S|N 1 ) space to store these indices for negative triplets. However, since there are many one-to-many, many-to-one and many-to-many relations, the cost will be smaller than O(|S|N 1 ) and the cache does not need to be stored in memory. In our experiments, values of N 1 and N 2 used on WN18 and FB15K are both 50, which is much smaller than the number of entities.</p><p>In comparison, to generate one negative triplet, the generator in IGAN <ref type="bibr" target="#b38">[39]</ref> costs O(|E|d) time since it needs to compute the distribution over all entities. KBGAN <ref type="bibr" target="#b8">[9]</ref> needs O(N 1 d) cost for measuring a candidate set of N 1 triplets. The additional space cost for IGAN and KBGAN is also O(|E|d) and O <ref type="figure" target="#fig_0">(N 1 d)</ref> respectively. Finally, the comparisons are summarized in <ref type="table" target="#tab_1">Table I</ref> with TransE as the scoring function.</p><p>4) Discussion on the Convergence: Both the baseline KG embedding models <ref type="bibr" target="#b39">[40]</ref> and NSCaching use stochastic gradient descent (SGD) for model training. While there is no theoretical guarantee, SGD has been applied on many nonconvex and complex models <ref type="bibr" target="#b21">[22]</ref>, where the convergence is empirically observed, including the baseline KG embedding model <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>. The only difference of NSCaching to that baseline model is how to sample negative triplets.</p><p>Besides, since NSCaching samples negative triplets with larger scores, its gradients have larger magnitude than that of baseline approach. This also prevents NSCaching from being early stopped by the sampling process and helps to converge with higher testing performance that of baseline models. The above are all empirically shown and studied in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Connection to Self-Pace Learning</head><p>The main idea of self-paced (or curriculum) learning <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref> is to pick up easy samples first, and then gradually switch to hard ones. In this way, the classifier can first identify the rough position where the decision boundary should locate, and then the boundary can be further refined near hard examples. It is very effective for complex and noncovex models.</p><p>Recently, it is also introduced into network embedding and a big improvement on embedding's quality has been reported <ref type="bibr" target="#b11">[12]</ref>. Besides, GAN is also used to monitor the distribution of edges in the network, and negative edges with scores above one threshold are sampled from the generator in GAN. Selfpaced learning is achieved by increasing the threshold during the training of embedding <ref type="bibr" target="#b11">[12]</ref>. Thus, we can see neither KBGAN nor IGAN has benefited from self-paced learning.</p><p>In contrast, our caching scheme can explicitly benefit from it. The reason is that the embedding model only has weak discriminative ability in the beginning of the training. Thus, while there are still a lot of negative triplets with large scores, it is more likely that they are easy ones as most of negative samples are easy. However, as training goes on, those easy samples will gradually have small scores and are removed from the cache. These mean NSCaching will learn from easy samples first, but then gradually focus on hard ones, which is exactly the principle of self-paced learning. The above explanations are also verified by experiments, where we can see the negative triplets in the cache change from easy to hard ones (Section IV-F) and NSCaching training from scratch can already achieve better performance than IGAN and KBGAN with pre-training (Section IV-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we carry empirical study of our method. All algorithms are written in Python with PyTorch framework <ref type="bibr" target="#b32">[33]</ref> and run on a TITAN Xp GPU.</p><p>A. Experiment Setup 1) Datasets: Four datasets are used here, i.e., WN18, FB15K and their variants WN18RR, FB15K237. WN18 and FB15K are firstly introduced in <ref type="bibr" target="#b6">[7]</ref>. They are widely tested among the most famous Knowledge Graph embedding learning works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. WN18RR and FB15K237 are variants that remove near-duplicate or inverse-duplicate relations from WN18 and FB15K, and are introduced by <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b36">[37]</ref> respectively. The two variants are harder and more realistic. Their statistics are shown in <ref type="table" target="#tab_1">Table II</ref>. Specifically, WN18 and WN18RR are subsets of Wordnet <ref type="bibr" target="#b28">[29]</ref>, which is a large lexical database of English. The entities correspond to word senses, and relations mean the lexical relation between them. FB15K and FB15K237 are subsets of Freebase dataset <ref type="bibr" target="#b3">[4]</ref> which contains general facts of the world. Freebase keeps growing until January 2014 and it now contains approximately 44 million topics and 2.4 billion triplets.</p><p>2) Tasks: Following previous KG embedding works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>, and the GAN-based works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>, we test the performance on link prediction task. This is also the testbed to measure KG embedding models. Link prediction aims to predict the missing entity h or t for a positive triplet (h, r, t). In this task, we measure the rank of head entity h and tail entity t among all the entity sets. Thus, link prediction emphasizes the rank of the correct entity rather than their concrete scores.</p><p>3) Performance measurements: As in previous works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> , we evaluate different models based on the following metrics: To avoid underestimating the performance of different models, we report the performance in a "Filtered" setting, i.e., all the corrupted triplets that exist in train, valid and test set are filtered out <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Note that, MR is not a good metric, as it is easily influenced by false positive samples. We report it here to keep consistency with existing literatures <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>. 4) Choices of the scoring function: A large amount of scoring functions have been proposed in literature, including translational distance models TransE <ref type="bibr" target="#b6">[7]</ref>, TransH <ref type="bibr" target="#b41">[42]</ref>, TransR <ref type="bibr" target="#b25">[26]</ref>, TransD <ref type="bibr" target="#b19">[20]</ref>, TranSparse <ref type="bibr" target="#b20">[21]</ref>, TransM <ref type="bibr" target="#b10">[11]</ref>, ManifoldE <ref type="bibr" target="#b44">[45]</ref>, etc., and semantic matching models RESCAL <ref type="bibr" target="#b31">[32]</ref>, DistMult <ref type="bibr" target="#b45">[46]</ref>, HolE <ref type="bibr" target="#b30">[31]</ref>, ComplEx <ref type="bibr" target="#b37">[38]</ref>, ANALOGY <ref type="bibr" target="#b26">[27]</ref>, etc. All these methods are summarized in a recent survey <ref type="bibr" target="#b39">[40]</ref>. Follow <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>, in the sequel, TransE, TransH, TransD, DistMult and ComplEx will be used as scoring functions for comparison (see their definitions in <ref type="table" target="#tab_1">Table III</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with State-of-the-arts</head><p>In this section, we focus on the comparison with state-ofthe-arts methods. Hyper-parameters of NSCaching are studied in Section IV-C.</p><p>1) Compared methods: Following methods for negative sampling are compared:  TransH <ref type="bibr" target="#b41">[42]</ref> h−w r hwr +r−(t−w r twr) 1 TransD <ref type="bibr" target="#b19">[20]</ref> h+wrw h h+r−(t+wrw t t) 1 semantic DistMult <ref type="bibr" target="#b45">[46]</ref> h · diag(r) · t matching ComplEx <ref type="bibr" target="#b37">[38]</ref> Re(h · diag(r) · t ) aims at reducing false negative labels by replacing the head or tail with different probability for one-to-many, many-to-one and many-to-many relations. Specifically, it samples (h, r, t) or (h, r,t) under a predefined Bernoulli distribution. Since it is shown to be better than uniform sampling, we choose it as the basic random sampling scheme; • KBGAN [9]: This model firstly samples a set N eg uniformly from the whole entity set E. Then head or tail entity is replaced with the entities in N eg to form a set of candidate (h, r, t) and (h, r,t). The generator in KBGAN tries to pick up one triplet among them. As proposed in <ref type="bibr" target="#b8">[9]</ref>, we choose the simplest model TransE as the generator. For fair comparison, the size of set N eg is same as our cache size N 1 . We use the published code 1 and change the configure same as ours for fair comparison; • NSCaching (Algorithm 2): As in Section III, the negative samples are formed by replacing the head entity h or tail entity t with one uniformly sampled from head cache H or tail cache T . The cache is updated as in Algorithm 3. Note that we can also lazily update the cache several iterations later, which can further save time. However, we just report the result of immediate update, which is shown to be both effective and efficient. We use N 1 = N 2 = 50 and lazy-update with n = 0 unless otherwise specified. As the source code of IGAN <ref type="bibr" target="#b38">[39]</ref> is not available, we do not compare with it here. Instead, we directly use the reported performance in the sequel. Finally, we also use Bernoulli sampling to choose between (h, r, t) and (h, r,t) for KBGAN and NSCaching.</p><p>Besides, as suggested in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>, two training strategies are used for KBGAN and NSCaching, i.e.,</p><p>• From scratch: The embedding of relations and entities are initialized by the Xavier uniform initializer <ref type="bibr" target="#b13">[14]</ref>, and the models (denoted as KBGAN + scratch and NSCaching + scratch) are directly applied to train the given KG; • With pretrain: Same as <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>, we firstly pretrain each scoring function under the baseline model, i.e. Bernoulli sampling, several epochs on both data sets. We denote it as pretrained. Then the obtained parameters are used    to warm-start the given KG rather than from scratch. We keep training based on the warm-started KG embedding and evaluate the performance under different models, i.e., Bernoulli, KBGAN + pretrain and NSCaching + pretrain. Besides, the generator in KBGAN is warm-started with corresponding TransE model.</p><formula xml:id="formula_9">IGAN pretrain -- 240 91.3 -- -- -- -- 81 74.0 -- -- -- scratch -- 244 92.7 -- -- -- -- 90 73.1 -- -- --</formula><formula xml:id="formula_10">IGAN pretrain -- 258 94.0 -- -- -- -- 81 77.0 -- -- -- scratch -- 276 86.9 -- -- -- -- 90 73.3 -- -- --</formula><formula xml:id="formula_11">.3 -- -- -- -- 79 77.6 -- -- -- scratch -- 221 93.0 -- -- -- -- 89 74.0 -- -- --</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Hyper-parameter settings:</head><p>We use grid search to select the following hyper-parameters: hidden dimension d ∈ {20, 50, 100, 200}, learning rate η ∈ {0.0001, 0.001, 0.01, 0.1}. For translational distance models, we tune the margin value γ ∈ {1, 2, 3, 4}. And for semantic matching models, we tune the penalty value λ ∈ {0.001, 0.01, 0.1} <ref type="bibr" target="#b37">[38]</ref>. We use Adam <ref type="bibr" target="#b21">[22]</ref>, which is a popular variant of SGD algorithm for the training, and adopt its default settings, except for the learning rate. The best hyper-parameter is tuned under Bernoulli sampling scheme and evaluated by the MRR metric on validation set. We keep them fixed for the baseline methods Bernoulli, KBGAN and our proposed NSCaching. Following <ref type="bibr" target="#b8">[9]</ref>, we save and record the pretrained model after several initial training epochs. Then, Bernoulli method keeps training until 3000 epochs; and the results of KBGAN and NSCaching algorithm are evaluated within 1000 epochs, either from scratch or with pretrain. All the recorded results are tested based on the best parameters chosen by the MRR value on validation set.</p><p>3) Results on translational distance models: The performance on link prediction is compared in <ref type="table" target="#tab_1">Table IV</ref>. First, we can see that, for the translational distance models (TransE, TransH, TransD), KBGAN, NSCaching and IGAN (both with pretrain and from scratch) gain significant improvement upon the baseline scheme Bernoulli, especially for the gaining on MRR, which is mainly influenced by top rankings. This verifies the needs of using high-quality negative triplets during negative sampling and these methods can effectively pick up these negative triplets.</p><p>Then, IGAN and KBGAN with pretrain can perform better, indicated by MRR and Hit@10, than from scratch. This shows pretrain is helpful for GAN based models. In comparison, the proposed NSCaching trained from either state (pretrain or scratch) can outperform IGAN and KBGAN. Finally, we find that MR is not an appropriate metric, as many of the pretrained models, which is not converged yet, show even smaller MR than the Bernoulli.</p><p>Convergence of testing performance for various algorithms are shown in <ref type="figure">Figure 2</ref> and 3. We use TransD as it offers the best performance among the three translational distance models. As can be seen, all algorithms will converge to a stable testing MRR and Hit@10, which verifies the empirical convergence of Adam optimizer. Then, while pretrain is a must for KBGAN to achieve good performance, NSCaching can obtain good performance either from scratch or using pretrain. Finally, in all cases, NSCaching converges much faster and is more stable than both Bernoulli and KBGAN. KBGAN from scratch even performs much worse than with pretrian. This observation further verifies the fact that GAN based methods usually suffer from instability and degeneracy. This method needs careful balance between the generator and the target KG embedding model. However, NSCaching works consistently and performs the best among various settings.</p><p>Convergence of testing performance for various algorithms are shown in <ref type="figure">Figure 4</ref> and 5. We use ComplEx as the representative since it is much better than DistMult. As can be seen, both Bernoulli and the proposed NSCaching will converge to a stable state. In the contrast, KBGAN will turn down and overfit after several epochs. However, NSCaching, either with pretrain or from scratch, leads the performance and is well adopted on the semantic matching models without further tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Results on triplets classification:</head><p>To further verify the quality of learned embedding, we test the learned embeddings on triplet classification task on WN18RR and FB15K237 datasets. This task is to confirm whether a given triplet (h, r, t) is correct or not, i.e., binary classification on triplet <ref type="bibr" target="#b41">[42]</ref>. In practice, it can help us to quickly answer the truth-or-false questions. The WN18RR 2 and FB15K237 3 dataset released a set of positive and negative triplets, which can be used to evaluate the performance on the classification task. The decision rule of classification is as follows: for each (h, r, t), if its score is no less than the relation-specific threshold σ r , then predict positive. Otherwise, negative. The threshold σ r is determined according to maximizing the classification accuracy on the validation set. As shown in <ref type="table" target="#tab_8">Table V</ref>, NSCaching still outperforms the baselines. The new experiment further justifies that our proposed NSCaching can help learn a better embedding of the KG. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cache Update and Sampling Scheme</head><p>In Section IV-B, we have shown that NSCaching achieves the best performance on four benchmark datasets. Here, we analyze design concerns on "exploration and exploitation" at step 6 and 8 in Algorithm 2. TransD and WN18 are used here.</p><p>1) Uniform sampling from the cache (step 6): Given a cache, which stores high-quality negative samples, how to sample from it is the first question we care about. Recall that we discussed three strategies in Section III-B1, i.e., (i) uniform sampling from the cache (dented as "uniform sampling"); (ii) importance sampling according to the score of each sample in cache (denoted as "IS sampling"); and (iii) top sampling, by choosing the sample with largest score (denoted as "top sampling"). Testing performance of MRR on WN18 trained by TransD are compared in <ref type="figure" target="#fig_6">Figure 6</ref>.(a). As can be seen, top sampling has the worst performance, and uniform sampling is the best.</p><p>To show how exploration and exploitation are balanced here, we further compute two criterion to show the difference between these strategies. (i) Repeat ratio (denoted as "RR"), which measures the percentage of repeated negative triplets (h, r,t) within 20 epochs; and (ii) non-zero loss ratio (denoted as "NZL"), which is the percentage of non-zero losses in same range. The value of RR is related to exploration, if the number of repeated negative triplets is high, the negative samples only explore a small part of the sample spaces, thus result in worse exploration. NZL ratio measures exploitation, a larger NZL means higher quality of picked negative samples. The RR is shown in <ref type="figure" target="#fig_7">Figure 7</ref>(a). The Bernoulli sampling method has almost zero repeat triplets since the number of explored negatives is extremely large, it has the best exploration. Among the schemes based on NSCaching, uniform sampling has better exploration than IS, then followed by top sampling. NZL ratio is shown in <ref type="figure" target="#fig_7">Figure 7</ref>(b). As training going on, the baseline Bernoulli model suffers the zero loss problem severely, thus leading to vanishing gradient. All of the three schemes have more than half non-zero losses, thus achieves exploitation. To sum up, uniform sampling is the most balanced strategy among the three schemes, thus NSCaching + uniform achieving the best performance. 2) Importance sampling strategy to update the cache (step 8): As discussed in Section III-B2, we have two choices on updating the cache: (i) importance sampling based method, which samples N 1 entities from N 1 +N 2 candidates according to the probability in (6) without replacement, (IS update). (ii) top sampling method, which directly select N 1 entities with top scores in the candidates, (top update). Again, let us first look at performance comparison in <ref type="figure" target="#fig_6">Figure 6</ref>.(b). We can see that IS update outperforms top update by a large margin.</p><p>Then, to explain the exploration and exploitation here, we add two extra measurements for comparison. They are (i). the number of changed elements in cache (denoted as "CE") and (ii) the ratio of non-zero losses, i.e., NZL. More changed elements leads to larger exploration, and more nonzero losses means more exploitation.</p><p>The value of CE measures the different elements in the cache in a period of epochs. As shown in <ref type="figure" target="#fig_8">Figure 8.(a)</ref>, the number of changed elements in top update scheme is much smaller than that of the importance sampling update. As a result, the cache is updated quite slow and the model mainly focuses on these highly scored negative triplets, which may contain many false positive triplets. As a comparison, the importance sampling based update scheme can keep the cache fresh and keep track of dynamic changes of the negative sampling distribution. It not only provides enough qualified negative triplets for the KG embedding model to avoid zero loss, but also explore the large negative sample space well. In summary, we choose the importance sampling strategy to update the cache. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sensitivity Analysis: Cache Size</head><p>Comparing with the baseline KG embedding models (i.e., Bernoulli <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b41">[42]</ref>), the only extra hyper-parameters here are N 1 and N 2 . Basically, N 1 is the size of cache. Then, N 2 is the size of randomly sampled negative triplets fromS (h,r,t) , which will be later used to update the cache. Here, we show their impact on NSCaching's performance. <ref type="figure">Figure 9</ref>.(a) shows how performance changes by varying the cache size N 1 among {10, 30, 50, 70, 90}, with fixed N 2 = 50. When the cache size is small, average score of entities stored in cache should be larger than those in larger cache. Thus, false negative samples will be more likely to be sampled, which will influence the boundary to a bad location. As for the others values of N 1 , NSCaching performs quite stable. The convergence speed is similar, as well as the values in converged state. Thus, when finding appropriate cache size, the value of N 1 can be searched from smaller value until the performance is stable. Different performance of the random candidate subset size N 2 is shown in <ref type="figure">Figure 9.(b)</ref>. Obviously, the entities in cache will be updated more frequently when N 2 gets larger, which lead to better exploration. But the trade-off is that larger value of N 2 costs more. As shown by the colored lines in <ref type="figure">Figure 9</ref>.(b), NSCaching performs consistently when N 2 is larger than 10. However, if the random subset is small, the content in cache will be harder to be updated, thus lead to poor performance as the yellow dashed line (N 2 = 10).</p><p>By combining together the influence of cache size N 1 in <ref type="figure">Figure 9</ref> and random subset size N 2 in <ref type="figure">Figure 9</ref>.(b), we find (a) Diff. N 1 (b) Diff. N 2 <ref type="figure">Fig. 9</ref>. Comparison of different N 1 when random subset size N 2 is fixed to 50, and different N 2 when cache size N 1 is fixed to 50. Evaluated by TransD model on WN18. that (i) NSCaching is not sensitive to the two sizes; (ii) both sizes can not be too small; (iii) N 1 = N 2 is a good balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Illustration of Vanishing Gradient</head><p>To further clarity the vanishing gradient problem, we plot average 2 -norm of gradients v.s. number of epochs in <ref type="figure" target="#fig_0">Figure 10</ref>. Note that Adam <ref type="bibr" target="#b21">[22]</ref>, which is a stochastic gradient descent algorithm, is used as the optimizer. First, we can see that while norms of gradients for both NSCaching and Bernoulli become smaller, they will not decrease to zero since the sampling process of the mini-batch will introduce noisy into gradients. However, the norm from NSCaching is larger than that from Bernoulli, which dues to the usage of cachingbased negative sampling scheme. Thus, we can see NSCaching can successfully avoid the problem of vanishing gradient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Explanation of the connection to Self-Paced Learning</head><p>Finally, we visualize the changes of entities in the cache, which verifies the effects of self-paced learning introduced in Section III-C. Following <ref type="bibr" target="#b38">[39]</ref>, we also use FB13 here since its triplets are more interpretable than the four evaluated datasets. We pick up (manorama, prof ession, actor) as the positive triplets, and the changes in its tail-cache are show in <ref type="table" target="#tab_1">Table VI</ref>. As can be seen, entities are firstly meaninglessness, e.g., ostrava and ben lilly, then they gradually changes to human jobs, e.g., artist and sex worker. V. RELATED WORK 1) Generative Adversarial Network: Generative Adversarial Network (GAN) is originally introduced as a powerful model for plausible image generation. The GAN contains two modules: a generator that serves as a complex distribution sampler, and a discriminator that measures the quality of generated samples. Under elaborately control on the training procedure of generator and discriminator <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>, GAN achieved significant success computer vision field <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b46">[47]</ref>. It has been shown to sample high-quality negative samples for knowledge graph embedding <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>.</p><p>2) Negative Sampling: Negative sampling is originally introduced as an alternative to the hierarchical softmax, which aims at reducing complexity of softmax on large scale dataset <ref type="bibr" target="#b18">[19]</ref>. It then becomes popular in embedding learning, especially for word embedding <ref type="bibr" target="#b14">[15]</ref>, graph embedding <ref type="bibr" target="#b16">[17]</ref>, and KG embedding <ref type="bibr" target="#b39">[40]</ref>. More recently, there have been interests in applying the GAN to negative sampling, e.g., IGAN <ref type="bibr" target="#b38">[39]</ref> and KBGAN <ref type="bibr" target="#b8">[9]</ref> for KG embedding and self-paced GAN <ref type="bibr" target="#b11">[12]</ref> for network embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We proposed NSCaching as a novel negative sampling method for knowledge graph embedding learning. The negative samples are from a cache that can dynamically hold high-quality negative samples. We analyze the designing of NSCaching through the balance of exploration and exploitation. Experimentally, we empirically test NSCaching on two datasets and five scoring functions. Results show that the method can generalize well under various settings and achieves state-of-the-arts performance on FB15K dataset. When dealing with millions scale KG, memory of storing the cache becomes a problem. Using distributed computation or hashing will be pursued as future works. Besides, the theoretical convergence of NSCaching is also an important and interesting future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Distribution of negative triplets on WN18 trained by Bernoulli-TransD (see Section IV-B1). For a given triplet (h, r, t), we fix the head entity h and relation r, and compute the distanceD (h,r,t) = f (h, r,t) − f (h, r, t).We measure the complementary cumulative distribution function (CCDF) F D (x) = P (D ≥ x) to show the proportion of negative triplets that satisfy D ≥ x. The red dashed line shows where the margin γ lies. (a) is the distribution of negative triplets in 6 timestamp of a certain triplet (h, r, t). (b) is the negative sample distribution of 5 different triplets (h, r, t) after the pretraining stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 (</head><label>1</label><figDesc>a) shows the changes in the distribution of negative samples for one positive triplet; and Figure 1(b) shows distributions of negative samples from different positive triplets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>Mean reciprocal ranking (MRR): It is computed by average of the reciprocal ranks 1/|S| |S| i=1 1 ranki where rank i , i ∈ {1, . . . , |S|} is a set of ranking results; • Hit@10: It is the percentage of appearance in top-k: 1/|S| |S| i=1 I(rank i &lt; k), where I(·) is the indicator function; • Mean rank (MR): It is computed by 1 |S| |S| i=1 rank i . Smaller value of MR tends to infer better results. MRR and Hit@10 measure the top rankings of positive entity in different level. Hit@10 cares about general top rankings, and the top 1 samples contribute most to MRR. The larger value of MRR and Hit@10 indicates better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>Bernoulli [42]: As a basic extension of the uniform sampling scheme used in TransE, Bernoulli sampling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .Fig. 3 .Fig. 4 .</head><label>234</label><figDesc>Testing MRR performance v.s. clock time (in seconds) based on TransD. Testing Hit@10 performance v.s. clock time (in seconds) based on TransD. Testing MRR performance v.s. clock time (in seconds) based on ComplEx.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 )Fig. 5 .</head><label>45</label><figDesc>Results on semantic matching models: The performance is shown in the bottom rows of Table IV. Same as the performance on translational distance models, NSCaching outperforms baseline scheme Bernoulli significantly, as indicated by the bold and underline numbers. However, KBGAN does not show consistent performance. It performs even worse than the Bernoulli sampling scheme on WN18, WN18RR and FB15K, Testing Hit@10 performance v.s. clock time (in seconds) based on ComplEx.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>(a) Diff. sampling strategies (b) Diff. cache updating strategies Comparison on testing MRR v.s. epoch of different sampling strategies and cache updating strategies. Evaluated by TransD model on WN18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>(a) RR v.s. epoch. (b) NZL v.s. epoch. Balancing on exploration (left) and exploitation (right) of different sampling strategies. Evaluated by TransD model on WN18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>(a) CE v.s. epoch. (b) NZL v.s. epoch. Balancing on exploration (left) and exploitation (right) of different strategies for updating the cache. Evaluated by TransD model on WN18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Mini-batch's average 2 -norm of gradients in one epoch v.s. number of epochs for Bernoulli and NSCaching on WN18RR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 2</head><label>2</label><figDesc>NSCaching: Cache-based KG embedding. Input: training set S = {(h, r, t)}, embedding dimension d, scoring function f , and head cache H, tail cache T . 1: initialize embeddings for each e ∈ E and r ∈ R, headcache H and tail-cache T ; 2: for i = 1, · · · , T do</figDesc><table><row><cell>3:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF THE PROPOSED APPROACH WITH STATE-OF-THE-ARTS, WHICH ADDRESS THE NEGATIVE SAMPLE. MODEL PARAMETERS ARE BASED ON TRANSE, m IS THE SIZE OF MINI-BATCH, n IS THE EPOCH OF LAZY-UPDATE.</figDesc><table><row><cell></cell><cell></cell><cell>strategy</cell><cell cols="2">minibatch computation</cell><cell>model</cell></row><row><cell></cell><cell>negative sample</cell><cell>training</cell><cell>time</cell><cell>space</cell><cell>parameters</cell></row><row><cell>baseline</cell><cell>uniform random</cell><cell>gradient descent (from scratch)</cell><cell>O(md)</cell><cell>O(md)</cell><cell>(|E| + |R|)d</cell></row><row><cell>IGAN [39]</cell><cell>GAN</cell><cell>reinforce learning (with pre-train)</cell><cell>O(m|E|d)</cell><cell>O(m|E|d)</cell><cell>3(|E| + |R|)d</cell></row><row><cell>KBGAN [9]</cell><cell>GAN</cell><cell>reinforce learning (with pre-train)</cell><cell>O(mN 1 d)</cell><cell>O(mN 1 d)</cell><cell>2(|E| + |R|)d</cell></row><row><cell>NSCaching</cell><cell>using cache</cell><cell>gradient descent (from scratch)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II DETAILED</head><label>II</label><figDesc></figDesc><table><row><cell cols="6">INFORMATION OF THE DATASETS USED IN EXPERIMENTS</cell></row><row><cell>Dataset</cell><cell cols="2">#entity #relation</cell><cell>#train</cell><cell>#valid</cell><cell>#test</cell></row><row><cell>WN18</cell><cell>40,943</cell><cell>18</cell><cell>141,442</cell><cell>5,000</cell><cell>5,000</cell></row><row><cell>WN18RR</cell><cell>93,003</cell><cell>11</cell><cell>86,835</cell><cell>3,034</cell><cell>3,134</cell></row><row><cell>FB15K</cell><cell>14,951</cell><cell>1,345</cell><cell cols="3">484,142 50,000 59,071</cell></row><row><cell>FB15K237</cell><cell>14,541</cell><cell>237</cell><cell cols="3">272,115 17,535 20,466</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III DEFINITIONS</head><label>III</label><figDesc>OF DIFFERENT SCORING FUNCTIONS. ALL MODEL PARAMETER ARE REAL VALUES, EXCEPT COMPLEX ARE COMPLEX VALUES. RE(·) TAKES THE REAL PART OUT OF COMPLEX NUMBERS, DIAG(r) CONSTRUCTS A DIAGONAL MATRIX WITH r.</figDesc><table><row><cell>model</cell><cell>scoring function</cell><cell>definition</cell></row><row><cell>translational</cell><cell>TransE [7]</cell><cell>h + r − t 1</cell></row><row><cell>distance</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF VARIOUS ALGORITHMS ON THE FOUR DATASET. PERFORMANCE OF THE predefined MODEL IS INCLUDED AS REFERENCE. AS CODE OF IGAN IS NOT AVAILABLE, ITS PERFORMANCE IS DIRECTLY COPIED FROM<ref type="bibr" target="#b38">[39]</ref>. NOTE THAT MRR, AND THOSE ON WN18RR, FB15K237 DATASETS ARE NOT REPORTED AS THEY ARE NOT SHOWN<ref type="bibr" target="#b38">[39]</ref>. BOLD NUMBER MEANS THE BEST PERFORMANCE, AND UNDERLINE MEANS THE SECOND BEST.</figDesc><table><row><cell>scoring</cell><cell>Dataset</cell><cell></cell><cell>WN18</cell><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell><cell>FB15K</cell><cell></cell><cell></cell><cell>FB15K237</cell><cell></cell></row><row><cell>functions</cell><cell>Metrics</cell><cell>MRR</cell><cell cols="3">MR Hit@10 MRR</cell><cell cols="3">MR Hit@10 MRR</cell><cell cols="3">MR Hit@10 MRR</cell><cell cols="2">MR Hit@10</cell></row><row><cell></cell><cell>pretrained</cell><cell>0.4213</cell><cell>217</cell><cell>91.50</cell><cell>0.1753</cell><cell>4038</cell><cell>44.48</cell><cell>0.4679</cell><cell>60</cell><cell>74.70</cell><cell>0.2262</cell><cell>237</cell><cell>38.64</cell></row><row><cell></cell><cell>Bernoulli</cell><cell>0.5001</cell><cell>249</cell><cell>94.13</cell><cell>0.1784</cell><cell>3924</cell><cell>45.09</cell><cell>0.4951</cell><cell>65</cell><cell>77.37</cell><cell>0.2556</cell><cell>197</cell><cell>41.89</cell></row><row><cell></cell><cell>KBGAN pretrain</cell><cell>0.6880</cell><cell>293</cell><cell>94.92</cell><cell>0.1864</cell><cell>4420</cell><cell>45.39</cell><cell>0.4858</cell><cell>82</cell><cell>77.02</cell><cell>0.2938</cell><cell>628</cell><cell>46.69</cell></row><row><cell>TransE</cell><cell>scratch NSCaching pretrain</cell><cell>0.6606 0.7867</cell><cell>301 271</cell><cell>94.80 66.62</cell><cell>0.1808 0.2048</cell><cell>5356 4404</cell><cell>43.24 47.38</cell><cell>0.3771 0.6475</cell><cell>335 62</cell><cell>72.67 81.54</cell><cell>0.2926 0.3004</cell><cell>722 188</cell><cell>46.59 47.36</cell></row><row><cell></cell><cell>scratch</cell><cell>0.7818</cell><cell>249</cell><cell>94.63</cell><cell>0.2002</cell><cell>4472</cell><cell>47.83</cell><cell>0.6391</cell><cell>62</cell><cell>80.95</cell><cell>0.2993</cell><cell>186</cell><cell>47.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell cols="5">COMPARISON OF VARIOUS ALGORITHMS ON TASKS OF RELATION</cell></row><row><cell cols="4">PREDICTION AND TRIPLET CLASSIFICATION.</cell></row><row><cell>scoring function</cell><cell cols="2">Dataset</cell><cell>WN18RR</cell><cell>FB15K237</cell></row><row><cell></cell><cell cols="2">Bernoulli</cell><cell>86.81</cell><cell>78.24</cell></row><row><cell></cell><cell>KBGAN</cell><cell>pretrained</cell><cell>85.93</cell><cell>79.03</cell></row><row><cell>TransD</cell><cell></cell><cell>scratch</cell><cell>86.01</cell><cell>79.05</cell></row><row><cell></cell><cell cols="2">NSCaching pretrained</cell><cell>87.84</cell><cell>80.63</cell></row><row><cell></cell><cell></cell><cell>scratch</cell><cell>87.64</cell><cell>80.69</cell></row><row><cell></cell><cell cols="2">Bernoulli</cell><cell>84.48</cell><cell>77.64</cell></row><row><cell></cell><cell>KBGAN</cell><cell>pretrained</cell><cell>79.87</cell><cell>74.11</cell></row><row><cell>ComplEx</cell><cell></cell><cell>scratch</cell><cell>71.73</cell><cell>72.61</cell></row><row><cell></cell><cell cols="2">NSCaching pretrained</cell><cell>84.96</cell><cell>79.88</cell></row><row><cell></cell><cell></cell><cell>scratch</cell><cell>84.83</cell><cell>80.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI EXAMPLES</head><label>VI</label><figDesc>OF NEGATIVE ENTITIES IN CACHE ON FB13. EACH LINE DISPLAYS 5 RANDOM SAMPLED NEGATIVE ENTITIES FROM TAIL CACHE OF A POSITIVE FACT (manorama, profession, actor) IN DIFFERENT EPOCHS. epoch entities in cache 0 allen clarke, jose gola, ostrava, ben lilly, hans zinsser 20 accountant, frank pais, laura marx, como, domitia lepida 100 artist, , aviator, hans zinsse, john h cough 200 physician, artist, raich carter, coach, mark shivas 500 artist, physician, cavan, sex worker, attorney at law</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/cai-lw/KBGAN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/thunlp/OpenKE/blob/master/benchmarks/WN18RR/ valid neg.txt 3 https://github.com/thunlp/OpenKE/blob/master/benchmarks/FB15K237/ valid neg.txt</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DBpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kbgan: Adversarial learning for knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting rdf triples in incomplete knowledge bases with tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="326" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transition-based knowledge graph embedding with relational mapping properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACLIC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-paced network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>The MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">word2vec explained: deriving mikolov et al.&apos;s negative-sampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge graph completion with adaptive sparse transfer matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statistical predicate invention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analogical inference for multi-relational embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2168" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Introducing the knowledge graph: things, not strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Official Google blog</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Continuous Vector Space Models and their Compositionality</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Incorporating GAN for negative sampling in knowledge representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TKDE</publisher>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">On evaluating embedding models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ainer Gemulla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07180</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The knowledge-based software assistant: A program summary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICKBSE</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">From one point to a manifold: knowledge graph embedding for precise link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1315" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
