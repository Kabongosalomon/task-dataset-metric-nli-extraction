<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">META-LEARNING EXTRACTORS FOR MUSIC SOURCE SEPARATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Samuel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ganeshan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">META-LEARNING EXTRACTORS FOR MUSIC SOURCE SEPARATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-music source separation, meta-learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a hierarchical meta-learning-inspired model for music source separation (Meta-TasNet) in which a generator model is used to predict the weights of individual extractor models. This enables efficient parameter-sharing, while still allowing for instrument-specific parameterization. Meta-TasNet is shown to be more effective than the models trained independently or in a multi-task setting, and achieve performance comparable with state-of-the-art methods. In comparison to the latter, our extractors contain fewer parameters and have faster run-time performance. We discuss important architectural considerations, and explore the costs and benefits of this approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Mankind's enduring and nearly universal appreciation of music has inspired the creation of thousand of instruments, each with its own unique timbral qualities. Yet there are strong similarities in the sonic characteristics of many instruments. A saxophone and a clarinet utilize similar methods for producing sound and thus exhibit similar timbral characteristics across time. A soprano singer and trumpet may differ categorically, but occupy similar frequency bands. If our models are aware of such relationships, there is potential to tailor specific separation strategies to each instrument, while making better use of the training data as a whole.</p><p>In this work we explore the application of ideas from meta-learning and AutoML to the problem of source separation. Our goal is to generate instrument-specific highprecision separation models, each of which is finely-tuned for dealing with the nuances of a particular instrument. However, rather than train each of these models directly, we train a separate generator network to predict their parameters. Thus the generator network is able to understand the relationships between instruments, and take them into account when generating specific separation networks (here, the masking subnetwork of a ConvTasNet <ref type="bibr" target="#b0">[1]</ref>). This functions as a form of parameter sharing, allowing the training data for one in- * Work completed during an internship at Preferred Networks. D×T ' D×T ' <ref type="figure">Fig. 1</ref>: The overall architecture. The blue area depicts the parameter generator, a network which predicts the weights of the extractor's masking subnetwork specific to each instrument. The extractor network then uses these weights when separating the instrument source from the mixture. strument to benefit another. The resulting extractors achieve greater performance with fewer parameters.</p><p>Our contributions are the following:</p><p>1. To our knowledge we are the first to apply the networkgenerating network approach to the problem of source separation, where we show it outperforms naive training of instrument-specific separator networks. 2. In comparison to a single multi-task model, our models perform better, and are smaller and faster. 3. We describe a number of improvements for Conv-TasNet, and our final architecture achieves state-of-theart performance on a number of MUSDB18 tasks, a first for waveform-based separation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">GENERATING EXTRACTOR MODELS</head><p>The key idea is to utilize a tiered architecture where a generator network "supervises" the training of the individual extractors by generating some of their parameters directly. This allows the generator to develop a dense representation of how instruments relate to each other as it pertains to the task, and to utilize their commonalities when generating each extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Extractor Model</head><p>Our model is based on Conv-TasNet <ref type="bibr" target="#b0">[1]</ref>, a time domain-based approach to speech separation comprising three parts: (1) an encoder which applies a 1-D convolutional transform to a segment of the mixture waveform to produce a high-dimensional representation, (2) a masking function which calculates a multiplicative function which identifies a targeted area in the learned representation, and (3) a decoder (1-D inverse convolutional layer) which reconstructs the separated waveform for the target source. The use of an intermediate representation overcomes the difficulty of working with high resolution in the time dimension (tens of thousands samples per second) without the disadvantages of compressing the data via an unlearned transformation, such as a mel spectrogram. Given an input mixture s = i∈I s i ∈ R T of all the sources I, the encoder maps it into a latent representation h = encoder(s) ∈ R D×T via a 1-D convolution. This is forwarded to the masking subnetwork, a temporal convolutional network (TCN), whose outputs are the separation masks m i = mask i (h) ∈ [0, 1] D×T . The separated latentsh i are then obtained by the element-wise product h i = h m i . After masking, the final extracted signals i is returned by a transposed 1-D convolution of the decoder:</p><formula xml:id="formula_0">s i = decoder(h i ) ∈ R T .</formula><p>The masking network is of particular interest, as it contains the source-specific masking information; the encoder and decoder are source-agnostic and remain the same for separation of all sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Meta-learning Extractor Parameters</head><p>The generator is a network that predicts the parameters of a secondary network, the baseline model which we refer to as an extractor, conditioned on additional information. As it pertains to this work, additional information is the identity of the instrument to be separated, provided as a one-hot vector i. This vector is projected into e i ∈ R M where the generator can encode the attributes -similarities and dissimilarities -of the instruments along multiple axes.</p><p>As the encoder and decoder are defined as instrumentagnostic components, we focus solely on using the generator to predict the parameters of the masking subnetwork. As described in Sec. 2.1, this consists primarily of a series of TCN layers. The generator function defines the the weights and biases of the k-th layer in the extractor network as:</p><formula xml:id="formula_1">θ k := W k P k e i<label>(1)</label></formula><p>where e i ∈ R M is the learned embedding of an instrument i and P k ∈ R M ×M , W k ∈ R |θ k |×M are learnable linear functions. The definition is further constrained with M &lt; M so that the mapping by P k extracts the most relevant information from e i which is then transformed with W k to yield all the parameters of the k-th layer (similar to <ref type="bibr" target="#b1">[2]</ref>). Other aspects of the model remain unchanged.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ADAPTATIONS FOR TIME-DOMAIN MUSIC SOURCE SEPARATION</head><p>Additionally, we observe better performance by modifying the Conv-TasNet architecture as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-stage Architecture</head><p>We observe that models trained using lower sampling rates perform better on 44kHz separation, despite the loss in resolution. We therefore propose a multi-stage architecture <ref type="figure" target="#fig_1">(Fig.  2</ref>) which begins by predicting low resolution audio, and iteratively upsamples at each stage (similar to <ref type="bibr" target="#b2">[3]</ref>). After computing a mask m i and applying it on h,h i is forwarded into the next stage (apart from being decoded into the time domain). At each stage the sampling rate is increased and the encoder's kernel width, stride size, and output size are increased proportionally. Note that this preserves the number of time-steps T in all stages. We use three stages with 8, 16 and 32kHz sampling rates; the mask in the last 32kHz stage is calculated as m 32 i = mask(concat(h 16 i , h 32 )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Auxiliary Loss Functions</head><p>Working in time-domain allows us to optimize scale-invariant signal-to-noise ratio (SI-SNR) in an end-to-end manner. This is an approximation of the signal-to-distortion (SDR) ratio metric used for the final evaluation <ref type="bibr" target="#b3">[4]</ref>. We additionally utilize three auxiliary loss functions for improved training, each a component in a weighted sum which constitutes the final loss. We denote s b,i ∈ R T as the ground-truth separated signal for an instrument i ∈ I, </p><formula xml:id="formula_2">L diss,b := |I| 2 −1 i =j∈I abs(h b,i ) · abs(h b,j ) h b,i h b,j<label>(2)</label></formula><p>2. Similarity Loss: For each instrument, this loss maximizes the similarity between instrument representations in different training samples:</p><formula xml:id="formula_3">L sim,i := − |B| 2 −1 b =b ∈B h b,i · h b ,i h b,i h b ,i<label>(3)</label></formula><p>3. Reconstruction Loss: This loss increases the SI-SNR between the mixture signal s, and signal as processed without any masking,ŝ = decoder(encoder(s)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stronger Encoder</head><p>The original formulation of the (Conv-)TasNet uses a single 1-D convolutional layer encoder as a learnable replacement of the STFT. We utilize a more complex encoder capable of capturing more distinct features from the input signal ( <ref type="figure">Fig. 3</ref>). We concatenate the information gathered by K 1-D convolutional layers where the k-th layer has kernel size 1 /2 k W and output dimension 2 k /2 K D. In this way, the multiple kernels are able to capture a wider frequency range with more fidelity.</p><p>We also include features from the classical STFT spectrogram of the input mixture, normalizing it, and projecting it down with one linear transformation (as a learnable replacement for a mel filter). These two branches are merged and run through Conv-ReLU-Conv to produce the latent representation.</p><p>The decoder uses a similar architecture to match the capacity of the encoder: after a transformation by Conv-ReLU, the vector is split and put to multiple transposed 1-D convolutions with the same kernel sizes as in the encoder. Finally, the estimated signals i is the sum of these outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>For quantitative comparison against existing systems, we use the MUSDB18 <ref type="bibr" target="#b4">[5]</ref> dataset consisting of 86 train, 14 validation and 50 test tracks of multi-genre 44.1kHz music. Each track is annotated with vocals, drums, bass, and other.</p><p>We augment the data using standard techniques <ref type="bibr" target="#b5">[6]</ref>: random cut of the 8-second training sample, random amplification of each source from (0.75, 1.25), random selection of the left or the right channel and shuffling of the sources between different tracks in half of the training batch. We train using RAdam <ref type="bibr" target="#b6">[7]</ref> with the Lookahead optimizer <ref type="bibr" target="#b7">[8]</ref> for a max of 250 iterations. Further hyperparameter settings and training details are available together with the code 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We examine the effectiveness of meta-learning when compared to various other forms of parameter-sharing, and also report the importance of modifications to the Conv-TasNet architecture <ref type="table">(Table 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-TasNet Modifications</head><p>We find that each modification introduced in Sec. 3 improves performance over vanilla Conv-TasNet, with the multi-stage model resulting in the largest gains (a ∼.3 increase in average SI-SNR over all instruments). The combination of all such improvements is denoted as the Baseline system, and improves average SI-SNR by nearly a entire point. This makes it comparable with other state-of-theart approaches that instead use spectrogram representations. Parameter Sharing Shifting focus to parameter sharing, we compare three different architectures, each with a different approach to sharing. The Baseline model is independent, training a separate masking model for each instrument. Finding inspiration from multitask models <ref type="bibr" target="#b8">[9]</ref>, we also experiment with tying the TCN layer parameters, sharing them across instruments. This requires other layers in the masking network to learn instrument-specific projections after the TCN. Finally we present a meta-learning model (Ours).</p><p>Unfortunately we were not able to train a competitive multi-task model, and our shared TCN performs significantly  <ref type="table">Table 1</ref>: Ablations of our improvements to Conv-TasNet, and comparisons between our method and other alternatives to parameter sharing (Baseline as no sharing). We report the SI-SNR value on MUSDB18 dev dataset.</p><p>below our improved Baseline system. The Baseline is a strong model, and we find it even outperforms meta-learning on two instruments (vocals and drums). In terms of overall performance, the meta-learning model achieves a modest improvement of SI-SNR by 0.8 averaged across all instruments. We also compare against previously published and stateof-the-art systems for MUSDB18 (  <ref type="bibr" target="#b14">[15]</ref>), and we show the median of frames, median of tracks, evaluated with BSSEval v4 <ref type="bibr" target="#b3">[4]</ref>. Since our loss is scale-invariant and the BSSEval v4 is scale-dependent, we scale the estimationss i by α = argmin α (s − i α isi ) 2 .</p><p>The combined improvements of our proposed modification and meta-learning yields results comparable to state-ofthe-art (MMDenseLSTM), and new best scores on bass and other categories. Notably the previous best performance for a time domain-based model was from Wave-U-Net, and we improve upon this type of model by a large margin.</p><p>Meta-learning in this domain presents interesting tradeoffs. In situations where maximum performance and model size are important (such as on device applications), our metalearned model achieves slightly higher performance and has 4x fewer 2 masking parameters than our baseline. If training is constrained in terms of time or memory, our Conv-TasNet Baseline is a model that reaches convergence quickly, and provides approximately state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>A major design choice in music source separation models is whether to (1) train a separate model for each instrument <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b1">(2)</ref> to use a single class-conditional model, or (3) to use an instrument agnostic approach <ref type="bibr" target="#b16">[16]</ref>. Our approach aims to combine the advantages of the first two; the high-precision of independent models, with improved optimization via parameter  sharing in single models. It is also an effort to incorporate prior source knowledge into TasNet-type models. Parameter sharing is an important design decision in many neural architectures, and many methods exist, ranging from the standard multi-task formulation, to conditioning on an embedded representations in a similar manner as we do (query-based networks <ref type="bibr" target="#b17">[17]</ref>), or generating parameters of a mixture model used during mask creation <ref type="bibr" target="#b18">[18]</ref>. We pursue these goals through other means (end-to-end training parameter generation models), and achieve significantly better performance than their reported results.</p><p>In terms of methodology, our work is directly inspired by meta-learning, especially for sequence modeling <ref type="bibr" target="#b19">[19]</ref>, and where a generator network predicts the parameters of a second network. This fits the paradigm set forth by HyperNEAT <ref type="bibr" target="#b20">[20]</ref> and hyper-networks <ref type="bibr" target="#b21">[21]</ref>. As in the latter, our architecture is learned end-to-end, making perhaps the largest disadvantage one of training speed: smaller, better extractors at test time come at the cost of a many-fold increase in training time.</p><p>Such approaches have also been applied to machine translation (referred to as context parameter generation <ref type="bibr" target="#b1">[2]</ref>). Similarly, we focus solely on parameter generation, but view learning additional optimization parameters (learning rate, layer architecture) as promising future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We have shown that meta-learning extractor parameters can yield many benefits, including better performance and smaller model sizes. In future work we wish to apply this approach to larger, more diverse sets of instruments, where such benefits should be more pronounced. And because instrument envelope durations vary, a fixed-width window for all representation learning is not optimal, and meta-learning more significant architectural choices may yield further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>We would like to thank Motoki Abe, Huachun Zhu, and Daiki Higurashi for helpful feedback and discussions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of the multi-stage architecture. The resolution of the estimated signal is progressively enhanced by utilizing information from previous stages. The encoders increase the stride s to preserve the same time dimension T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 : 1 .</head><label>31</label><figDesc>Illustration of our encoder architecture. The input (mixture) signal is transformed with multiple convolutional heads (red) and standard SFTF (yellow). These two branches are then merged and mapped onto the latent space (blue) where the outcome is later separated. b ∈ B as the index of bth training sample in a batch, and h b,i = encoder(s b,i ) ∈ R D×T as the separated latent space. The auxiliary losses can now be defined as: Dissimilarity Loss: For each training sample, this loss minimizes the similarity between the different instrument representations h b,i :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>). Here SDR was</cell></row><row><cell>either taken from the respective papers ([10][11][12]) or from</cell></row><row><cell>the SiSEC18 [4] evaluation scores ([13][14]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>A comparison SDR scores of our proposed approach with other systems on the test section of MUSDB18 dataset (*indicates the system works directly in the time domain).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/pfnet-research/meta-tasnet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The encoder has roughly 9.17M parameters, the decoder 3.83M and the parameter generator 32.54M.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech and Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual parameter generation for universal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Emmanouil Antonios Platanios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="425" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobutaka</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Latent Variable Analysis and Signal Separation: 14th International Conference</title>
		<meeting><address><addrLine>Surrey, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="293" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The MUSDB18 corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafar</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>Stöter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stylianos Ioannis Mimilakis, and Rachel Bittner</title>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep convolutional neural networks and data augmentation for acoustic event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beat</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07160</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Michael R Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.08610</idno>
		<title level="m">Lookahead optimizer: k steps forward, 1 step back</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1603.01249</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Monoaural audio source separation using deep convolutional neural networks,&quot; in International conference on latent variable analysis and signal separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritish</forename><surname>Chandna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Janer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilia</forename><surname>Gómez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="258" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">End-toend music source separation: is it possible in the waveform domain?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Lluis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Open-unmix -a reference implementation for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>Stoter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Wave-u-net: A multi-scale neural network for endto-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03185</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising autoencoder with recurrent skip connections and residual regression for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="773" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mmdenselstm: An efficient combination of convolutional and recurrent neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabarun</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 16th International Workshop on Acoustic Signal Enhancement (IWAENC)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="106" to="110" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recursive speech separation for unknown number of speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudarsanam</forename><surname>Parthasaarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabarun</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Audio query-based music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeong-Seok</forename><surname>Kyogu Lee Jie Hwan Lee1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Society for Music Information Retrieval Conference (ISMIR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Classconditional embeddings for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikant</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meta-learning a dynamical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop Track, International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A hypercube-based encoding for evolving largescale neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>D&amp;apos;ambrosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gauci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Life</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="212" />
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
