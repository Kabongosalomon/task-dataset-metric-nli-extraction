<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Classification with Channel-Separated Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
							<email>trandu@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
							<email>hengwang@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
							<email>torresani@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Video Classification with Channel-Separated Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Group convolution has been shown to offer great computational savings in various 2D convolutional architectures for image classification. It is natural to ask: 1) if group convolution can help to alleviate the high computational cost of video classification networks; 2) what factors matter the most in 3D group convolutional networks; and 3) what are good computation/accuracy trade-offs with 3D group convolutional networks.</p><p>This paper studies the effects of different design choices in 3D group convolutional networks for video classification. We empirically demonstrate that the amount of channel interactions plays an important role in the accuracy of 3D group convolutional networks. Our experiments suggest two main findings. First, it is a good practice to factorize 3D convolutions by separating channel interactions and spatiotemporal interactions as this leads to improved accuracy and lower computational cost. Second, 3D channel-separated convolutions provide a form of regularization, yielding lower training accuracy but higher test accuracy compared to 3D convolutions. These two empirical findings lead us to design an architecture -Channel-Separated Convolutional Network (CSN) -which is simple, efficient, yet accurate. On Sports1M, Kinetics, and Something-Something, our CSNs are comparable with or better than the state-of-the-art while being 2-3 times more efficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video classification has witnessed much good progress in recent years. Most of the accuracy gains have come from the introduction of new powerful architectures <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b39">37]</ref>. However, many of these architectures are built on expensive 3D spatiotemporal convolutions. Furthermore, these convolutions are typically computed across all the channels in each layer. 3D CNNs have complexity O(CT HW ) as opposed to the cost of O(CHW ) of 2D CNNs, where T denotes the number of frames, H, W the spatial dimensions and C the number of channels. For both foundational and practical reasons, it is natural to ask which parameters in these large 4D kernels matter the most.</p><p>Kernel factorizations have been applied in several settings to reduce compute and improve accuracy. For example, several recent video architectures factor 3D convolution in space and time: examples include P3D <ref type="bibr" target="#b25">[25]</ref>, R(2+1)D <ref type="bibr" target="#b32">[32]</ref>, and S3D <ref type="bibr" target="#b42">[40]</ref>. In these architectures, a 3D convolution is replaced with a 2D convolution (in space) followed by a 1D convolution (in time). This factorization can be leveraged to increase accuracy and/or to reduce computation. In the still-image domain, separable convolution <ref type="bibr" target="#b7">[7]</ref> is used to factorize the convolution of 2D k × k filters into a pointwise 1 × 1 convolution followed by a depthwise k × k convolution. When the number of channels is large compared to k 2 , which is usually the case, this reduces FLOPs by ∼ k 2 for images. For the case of 3D video kernels, the FLOP reduction is even more dramatic: ∼ k 3 .</p><p>Inspired by the accuracy gains and good computational savings demonstrated by 2D separable convolutions in image classification <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b44">42]</ref>, this paper proposes a set of architectures for video classification -3D Channel-Separated Networks (CSN) -in which all convolutional operations are separated into either pointwise 1×1×1 or depthwise 3×3×3 convolutions. Our experiments reveal the importance of channel interaction in the design of CSNs. In particular, we show that excellent accuracy/computational cost balances can be obtained with CSNs by leveraging channel separation to reduce FLOPs and parameters as long as high values of channel interaction are retained. We propose two factorizations, which we call interaction-reduced and interaction-preserved. Compared to 3D CNNs, both our interaction-reduced and interaction-preserved CSNs provide higher accuracy and FLOP savings of about 2.5-3× when there is enough channel interaction. We experimentally show that the channel factorization in CSNs acts as a regularizer, leading to a higher training error but better generalization. Finally, we show that our proposed CSNs outperform or are comparable with the current state-of-the art methods on Sports1M, Kinetics, and Something-Something while being 2-3 times faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Group convolution. Group convolution was adopted in AlexNet <ref type="bibr" target="#b21">[21]</ref> as a way to overcome GPU memory limitations. Depthwise convolution was introduced in Mo-bileNet <ref type="bibr" target="#b18">[18]</ref> as an attempt to optimize model size and computational cost for mobile applications. Chollet <ref type="bibr" target="#b7">[7]</ref> built an extreme version of Inception <ref type="bibr" target="#b30">[30]</ref> based on 2D depthwise convolution, named Xception, where the Inception block was redesigned to include multiple separable convolutions. Concurrently, Xie et al. proposed ResNeXt <ref type="bibr" target="#b41">[39]</ref> by equipping ResNet <ref type="bibr" target="#b17">[17]</ref> bottleneck blocks with groupwise convolution. Further architecture improvements have also been made for mobile applications. ShuffleNet <ref type="bibr" target="#b44">[42]</ref> further reduced the computational cost of the bottleneck block with both depthwise and group convolution. MobileNetV2 <ref type="bibr" target="#b27">[27]</ref> improved MobileNet <ref type="bibr" target="#b18">[18]</ref> by switching from a VGG-style to a ResNet-style network, and introducing a "reverted bottleneck" block. All of these architectures are based on 2D CNNs and are applied to image classification while our work focuses on 3D group CNNs for video classification. Video classification. In the last few years, video classification has seen a major paradigm shift, which involved moving from hand-designed features <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b33">33]</ref> to deep network approaches that learn features and classify end-to-end <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b38">36,</ref><ref type="bibr" target="#b12">12]</ref>. This transformation was enabled by the introduction of large-scale video datasets <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref> and massively parallel computing hardware, i.e., GPU. Carreira and Zisserman <ref type="bibr" target="#b3">[3]</ref> recently proposed to inflate 2D convolutional networks pre-trained on images to 3D for video classification. Wang et al. <ref type="bibr" target="#b39">[37]</ref> proposed non-local neural networks to capture long-range dependencies in videos. ARTNet <ref type="bibr" target="#b34">[34]</ref> decouples spatial and temporal modeling into two parallel branches. Similarly, 3D convolutions can also be decomposed into a Pseudo-3D convolutional block as in P3D <ref type="bibr" target="#b25">[25]</ref> or factorized convolutions as in R(2+1)D <ref type="bibr" target="#b32">[32]</ref> or S3D <ref type="bibr" target="#b42">[40]</ref>. 3D group convolution was also applied to video classification in ResNeXt <ref type="bibr" target="#b16">[16]</ref> and Multi-Fiber Networks <ref type="bibr" target="#b5">[5]</ref> (MFNet).</p><p>Among previous approaches, our work is most closely related to the following architectures. First, our CSNs are similar to Xception <ref type="bibr" target="#b7">[7]</ref> in the idea of using channelseparated convolutions. Xception factorizes 2D convolution in channel and space for object classification, while our CSNs factorize 3D convolution in channel and space-time for action recognition. In addition, Xception uses simple blocks, while our CSNs use bottleneck blocks. The variant ir-CSN of our model shares similarities with ResNeXt <ref type="bibr" target="#b41">[39]</ref> and its 3D version <ref type="bibr" target="#b16">[16]</ref> in the use of bottleneck block with group/depthwise convolution. The main difference is that ResNext <ref type="bibr" target="#b41">[39,</ref><ref type="bibr" target="#b16">16]</ref> uses group convolution in its 3×3×3 layers with a fixed group size (e.g., G = 32), while our ir-CSN uses depthwise convolutions in all 3×3×3 layers which makes our architecture fully channel-separated. As we will a) conv b) group conv c) depthwise conv input channel output channel <ref type="figure">Figure 1</ref>. Group convolution. Convolutional filters can be partitioned into groups with each filter receiving input from channels only within its group. (a) A conventional convolution, which has only one group. (b) A group convolution with 2 groups. (c) A depthwise convolution where the number of groups matches the number of input/output filters, i.e., each group contains only one channel.</p><p>show in section 4.2, making our network fully channelseparated helps not only to reduce a significant amount of compute, but also to improve model accuracy by better regularization. We emphasize that our contribution includes not only the design of CSN architectures, but also a systematic empirical study of the role of channel interactions in the accuracy of CSNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Channel-Separated Convolutional Networks</head><p>In this section, we discuss the concept of 3D channelseparated networks. Since channel-separated networks use group convolution as their main building block, we first provide some background about group convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>Group convolution. Conventional convolution is implemented with dense connections, i.e., each convolutional filter receives input from all channels of its previous layer, as in <ref type="figure">Figure 1</ref>(a). However, in order to reduce the computational cost and model size, these connections can be sparsified by grouping convolutional filters into subsets. Filters in a subset receive signal from only channels within its group (see <ref type="figure">Figure 1</ref>(b)). Depthwise convolution is the extreme version of group convolution where the number of groups is equal to the number of input and output channels (see <ref type="figure">figure 1</ref>(c)). Xception <ref type="bibr" target="#b7">[7]</ref> and MobileNet <ref type="bibr" target="#b18">[18]</ref> were among the first networks to use depthwise convolutions. <ref type="figure">Figure 1</ref> presents an illustration of conventional, group, and depthwise convolutional layers for the case of 4 input channels and 4 output channels. Counting FLOPs, parameters, and interactions. Dividing a conventional convolutional filter into G groups reduces compute and parameter count by a factor of G. These reductions occur because each filter in a group receives input from only a fraction 1/G of the channels from the previous layer. In other words, channel grouping restricts feature interaction: only channels within a group can interact. If multiple group convolutional layers are stacked directly on top of each other, this feature segregation is further amplified since each channel becomes a function of small channel-subsets in all preceding layers. So, while group convolution saves compute and parameters, it also reduces feature interactions.</p><p>We propose to quantify the amount of channel interaction as the number of pairs of two input channels that are connected through any output filter. If the convolutional layer has C in channels and G groups, each filter is connected to C in /G input channels. Therefore each filter will have C in G 2 interacting feature pairs. According to this definition, the example convolutions in <ref type="figure">Figure 1</ref>(a)-(c) will have 24, 4, and 0 channel interaction pairs, respectively.</p><p>Consider a 3D convolutional layer with spatiotemporal convolutional filters of size k×k×k and G groups, C in input channels, and C out output channels. Let T HW be the total number of voxels in the spatiotemporal tensor provided as input to the layer. Then, the number of parameters, FLOPs (floating-point operations), and number of channel interactions can be measured as:</p><formula xml:id="formula_0">#parameters = C out · C in G · k 3 (1) #FLOPs = C out · C in G · k 3 · T HW (2) #interactions = C out · Cin G 2<label>(3)</label></formula><p>Recall that n 2 = n(n−1) 2 = O n 2 . We note that while FLOPs and number of parameters are commonly used to characterize a layer, the "amount" of channel interaction is typically overlooked. Our study will reveal the importance of this factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Channel Separation</head><p>We define channel-separated convolutional networks (CSN) as 3D CNNs in which all convolutional layers (except for conv1) are either 1×1×1 conventional convolutions or k×k×k depthwise convolutions (where, typically, k = 3). Conventional convolutional networks model channel interactions and local interactions (i.e., spatial or spatiotemporal) jointly in their 3D convolutions. Instead, channel-separated networks decompose these two types of interactions into two distinct layers: 1×1×1 conventional convolutions for channel interaction (but no local interaction) and k×k×k depthwise convolutions for local spatiotemporal interactions (but not channel interaction). Channel separation may be applied to any k×k×k traditional convolution by decomposing it into a 1×1×1 convolution and a depthwise k×k×k convolution.</p><p>We introduce the term "channel-separated" to highlight the importance of channel interaction; we also point out that the existing term "depth-separable" is only a good description when applied to tensors with two spatial dimen-sions and one channel dimension. We note that channelseparated networks have been proposed in Xception <ref type="bibr" target="#b7">[7]</ref> and MobileNet <ref type="bibr" target="#b18">[18]</ref> for image classification. In video classification, separated convolutions have been used in P3D <ref type="bibr" target="#b25">[25]</ref>, R(2+1)D <ref type="bibr" target="#b32">[32]</ref>, and S3D <ref type="bibr" target="#b42">[40]</ref>, but to decompose 3D convolutions into separate temporal and spatial convolutions. The network architectures presented in this work are designed to separate channel interactions from spatiotemporal interactions.  <ref type="figure" target="#fig_0">Figure 2</ref>(b)). This block reduces parameters and FLOPs of the traditional 3×3×3 convolution significantly, but preserves all channel interactions via a newly-added 1×1×1 convolution. We call this an interaction-preserved channel-separated bottleneck block and the resulting architecture an interaction-preserved channel-separated network (ip-CSN). Interaction-reduced channel-separated bottleneck block is derived from the preserved bottleneck block by removing the extra 1×1×1 convolution. This yields the depthwise bottleneck block shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c). Note that the initial and final 1×1×1 convolutions (usually interpreted respectively as projecting into a lower-dimensional subspace and then projecting back to the original dimensionality) are now the only mechanism left for channel interactions. This implies that the complete block shown in (c) has a reduced number of channel interactions compared with those shown in (a) or (b). We call this design an interaction-reduced channel-separated bottleneck block and the resulting architecture an interaction-reduced channel-separated network (ir-CSN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Example: Channel-Separated Bottleneck Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Channel Interactions in Convolutional Blocks</head><p>The interaction-preserving and interaction-reducing blocks in section 3.3 are just two architectures in a large spectrum. In this subsection we present a number of convolutional block designs, obtained by progressively increasing the amount of grouping. The blocks differ in terms of compute cost, parameter count and, more importantly, channel interactions. Group convolution applied to ResNet blocks.  presents a ResNet <ref type="bibr" target="#b17">[17]</ref> simple block consisting of two 3×3×3 convolutional layers. <ref type="figure" target="#fig_2">Figure 3(b)</ref> shows the simple-G block, where the 3×3×3 layers now use grouped convolution. Likewise, <ref type="figure" target="#fig_2">Figure 3</ref>(c) presents simple-D, with two depthwise layers. Because depthwise convolution requires the same number of input and output channels, we optionally add a 1×1×1 convolutional layer (shown in the dashed rectangle) in blocks that change the number of channels.   a bottleneck-DG, as illustrated in <ref type="figure" target="#fig_3">Figure 4(d)</ref>. In all cases, the 3×3×3 convolutional layers always have the same number of input and output channels.</p><p>There are some deliberate analogies to existing architectures here. First, bottleneck-G <ref type="figure" target="#fig_3">(Figure 4(b)</ref>) is exactly a ResNeXt block <ref type="bibr" target="#b41">[39]</ref>, and bottleneck-D is its depthwise variant. Bottleneck-DG ( <ref type="figure" target="#fig_3">Figure 4(d)</ref>) resembles the Shuf-fleNet block <ref type="bibr" target="#b44">[42]</ref>, without the channel shuffle and without the downsampling projection by average pooling and concatenation. The progression from simple to simple-D is similar to moving from ResNet to Xception (though Xception has many more 1×1×1 convolutions). We omit certain architecture-specific features in order to better understand the role of grouping and channel interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Ablation Experiment</head><p>This empirical study will allow us to cast some light on the important factors in the performance of channelseparated network and will lead us to two main findings:</p><p>1. We will empirically demonstrate that within the family of architectures we consider, similar depth and similar amount of channel interaction implies similar accuracy. In particular, the interaction-preserving blocks reduce compute significantly but preserve channel interactions, with only a slight loss in accuracy for shallow networks and an increase in accuracy for deeper networks. that this causes overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Dataset. We use Kinetics-400 <ref type="bibr" target="#b20">[20]</ref> for the ablation experiments in this section. Kinetics is a standard benchmark for action recognition in videos. It contains about 260K videos of 400 different human action categories. We use the training split (240K videos) for training and the validation split (20K videos) for evaluating different models. Base architecture. We use ResNet3D, presented in <ref type="table">Table 1</ref>, as our base architecture for most of our ablation experiments in this section. More specifically, our model takes clips with a size of T×224×224 where T = 8 is the number of frames, 224 is the height and width of the cropped frame. Two spatial downsampling layers (1×2×2) are applied at conv1 and at pool1, and three spatiotemporal downsampling (2×2×2) are applied at conv3 1, conv4 1 and conv5 1 via convolutional striding. A global spatiotemporal average pooling with kernel size T 8 ×7×7 is applied to the final convolutional tensor, followed by a fullyconnected (fc) layer performing the final classification. Data augmentation. We use both spatial and temporal jittering for augmentation. Specifically, video frames are scaled such that the shorter edge of the frames becomes s while we maintain the frame original aspect ratio. During training, s is uniformly sampled between 256 and 320. Each clip is then generated by randomly cropping windows of size 224×224. Temporal jittering is also applied during training by randomly selecting a starting frame and decod-ing T frames. For the ablation experiments in this section we train and evaluate models with clips of 8 frames (T = 8) by skipping every other frame (all videos are pre-processed to 30fps, so the newly-formed clips are effectively at 15fps). Training. We train our models with synchronous distributed SGD on GPU clusters using caffe2 <ref type="bibr" target="#b2">[2]</ref> (with 16 machines, each having 4 GPUs). We use a mini-batch of 8 clips per GPU, thus making a total mini-batch of 512 clips. Following <ref type="bibr" target="#b32">[32]</ref>, we set epoch size to 1M clips due to temporal jitterring augmentation even though the number of training examples is only about 240K. Training is done in 45 epochs where we use model warming-up <ref type="bibr" target="#b14">[14]</ref> in the first 10 epochs and the remaining 35 epochs will follow the half-cosine period learning rate schedule as in <ref type="bibr" target="#b10">[10]</ref>. The initial learning rate is set to 0.01 per GPU (equivalent to 0.64 for 64 GPUs). Testing. We report clip top-1 accuracy and video top-1 accuracy. For video top-1, we use center crops of 10 clips uniformly sampled from the video and average these 10 clippredictions to obtain the final video prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Reducing FLOPs, preserving interactions</head><p>In this ablation, we use CSNs to vary both FLOPs and channel interactions. Within this architectural family, channel interactions are a good predictor of performance, whereas FLOPs are not. In particular, FLOPs can be reduced significantly while preserving interaction count. <ref type="table">Table 2</ref> presents results of our interaction-reduced CSNs (ir-CSNs) and interaction-preserved CSNs (ip-CSNs) and compare them with the ResNet3D baseline using different number of layers. In the shallow network setting (with 26 layers), both the ir-CSN and the ip-CSN have lower accuracy than ResNet3D. The ir-CSN provides a computational savings of 3.6x but causes a 2.9% drop in accuracy. The ip-CSN yields a saving of 2.9x in FLOPs with a much smaller drop in accuracy (0.7%). We note that all of the shallow models have very low count of channel interactions: ResNet3D and ip-CSN have about 0.42 giga-pairs (0.42 × 10 9 pairs), while ir-CSN has only 0.27 giga-pairs (about 64% of the original). This observation suggests that shallow instances of ResNet3D benefit from their extra parameters, but the preservation of channel interactions reduces the gap for ip-CSN.</p><p>Conversely, in deeper settings both ir-CSNs and ip-CSNs outperform ResNet3D (by about 0.9 − 1.4%). Furthermore, the accuracy gap between ir-CSN and ip-CSN becomes smaller. We attribute this gap shrinking to the fact that, in the 50-layer and 101-layer configurations, ir-CSN has nearly the same number of channel interactions as ip-CSN since most interactions stem from the 1×1×1 layers. One may hypothesize that ip-CSN outperforms ResNet3D and ir-CSN because it has more nonlinearities (ReLU). To answer this question, we trained ip-CSNs without ReLUs between the 1×1×1 and the 3×3×3 layers and we observed no notable difference in accuracy. We can conclude that traditional 3×3×3 convolutions contain many parameters which can be removed without an accuracy penalty in the deeper models. We further investigate this next. We also experimented with a space-time decomposition of the 3D filters <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b42">40]</ref> in ir-CSN-50. This model obtains 69.7% on Kinetics validation (vs. 70.3% of vanilla ir-CSN-50) while requiring more memory and having roughly the same GFLOPs as ir-CSN. The small accuracy drop may be due to the fact that CSN 3D filters are already channelfactorized and the space-time decomposition may limit excessively their already constrained modeling ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">What makes CSNs outperform ResNet3D?</head><p>In section 4.2 we found that both ir-CSNs and ip-CSNs outperform the ResNet3D baseline when there are enough channel interactions, while having fewer parameters and greatly reducing FLOPs. It is natural to ask: what makes CSNs more accurate? <ref type="figure" target="#fig_7">Figure 5</ref> provides a useful insight to answer this question. The plot shows the evolution of the training errors of ip-CSN and ResNet3D, both with 101 layers. Compared to ResNet3D, ip-CSN has higher training errors but lower testing error (see validation accuracy shown in <ref type="table">Table 2</ref>). This suggests that the channel separation in CSN regularizes the model and prevents overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The effects of different blocks in group convolutional networks</head><p>Here we start from our base architecture (shown in Table 1) then ablatively replace the convolutional blocks with those presented in section 3.4. Again we find that channel interaction plays a critical role in understanding the results. Naming convention. Since the ablation in this section will be considering several different convolutional blocks,   <ref type="table">Table 2</ref>). This suggests that the channel separation provides a beneficial regularization, combatting overfitting.  <ref type="table">Table 3</ref>. Naming convention. We name architectures by block name followed by the total number of blocks (see last column). Only two block names are given in this table. More blocks are presented in section <ref type="bibr">3.4.</ref> to simplify the presentation, we name each architecture by block type (as presented in section 3.4) and total number of blocks, as shown in the last column of <ref type="table">Table 3</ref>. <ref type="figure" target="#fig_8">Figure 6</ref> presents the results of our ablation on convolutional blocks. It shows the video top-1 accuracy on the Kinetics validation set vs the model computational cost (# FLOPs). We note that, in this experiment, we use our base architecture with two different numbers of blocks (8 and 16) and just vary the type of convolutional block and number of groups to study the tradeoffs. <ref type="figure" target="#fig_8">Figure 6</ref>(a) presents our ablation experiment with simple-X-8 and bottleneck-X-8 architectures (where X can be none, G, or D, or even DG in the case of bottleneck block). Similarly, <ref type="figure" target="#fig_8">Figure 6</ref>(b) presents our ablation experiment with simple-X-16 and bottleneck-X-16 architectures. We can observe the computation/accuracy effects of the group convolution transformation. Reading each curve from right to left (i.e. in decreasing accuracy), we see simple-X transforming from simple block to simple-G (with increasing number of groups), then to simple-D block. For bottleneck-X, reading right to left shows bottleneck block, then transforms to bottleneck-G (with increasing groups), bottleneck-D, then finally to bottleneck-DG (again with increasing groups).</p><p>While the general downward trend is expected as we decrease parameters and FLOPs, the shape of the simple and bottleneck curves is quite different. The simple-X models degrade smoothly, whereas bottleneck-X stays relatively flat (particularly bottleneck-16, which actually increases slightly as we decrease FLOPs) before dropping sharply.</p><p>In order to better understand the different behaviors of the simple-X-Y and bottleneck-X-Y models (blue vs. red curves) in <ref type="figure" target="#fig_8">Figure 6</ref> and the reasons behind the turning points of bottleneck-D block (green start markers in <ref type="figure" target="#fig_8">Figure 6</ref>), we plot the performance of all these models according to another view: accuracy vs channel interactions <ref type="figure" target="#fig_10">(Figure 7)</ref>. As shown in <ref type="figure" target="#fig_10">Figure 7</ref>, the number of channel interactions in simple-X-Y models (blue squares and red diamonds) drops quadratically when group convolution is applied to their 3×3×3 layers. In contrast, the number of channel interactions in bottleneck-X-Y models (green circles and purple triangles) drops marginally when group convolution is applied to their 3×3×3 since they still have many 1×1×1 layers (this can be seen in the presence of two marker clusters which are circled in red: the first cluster includes purple triangles near the top-right corner and the other one includes green circles near the center of the <ref type="figure">figure)</ref>. The channel interaction in bottleneck-X-Y starts to drop significantly when group convolution is applied to their 1×1×1 layers, and causes the model sharp drop in accuracy. This fact explains well why there is no turning point in simple-X-Y curves and also why there are turning points in bottleneck-X-Y curves. It also confirms the important role of channel interactions in group convolutional networks. Bottleneck-D block (also known as ir-CSN) provides the best computation/accuracy tradeoff. For simple blocks, increasing the number of groups causes a continuous drop in accuracy. However, in the case of the bottleneck block (i.e. bottleneck-X-Y) the accuracy curve remains almost flat as we increase the number of groups until arriving at the bottleneck-D block, at which point the accuracy degrades dramatically when the block is turned into a bottleneck-DG (group convolution applied to 1×1×1 layers). We conclude that a bottleneck-D block (or ir-CSN) gives the best computation/accuracy tradeoff in this family of ResNet-style blocks, due to its high channel-interaction count.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparison with the State-of-the-Art</head><p>Datasets. We evaluate our CSNs on Sports1M <ref type="bibr" target="#b19">[19]</ref>, Kinetics-400 <ref type="bibr" target="#b20">[20]</ref>, and Something-Something-v1 <ref type="bibr" target="#b15">[15]</ref>. Sports1M is a large-scale action recognition dataset containing 1.1 million videos from 487 sport action classes. Kinetics contains about 260K videos of 400 different human action categories. Something 2 -v1 consists of 100K videos of 174 different object-interaction actions. For Sports1M, we use the public train and test splits provided with the dataset. For Kinetics and Something 2 -v1, we use the train split for training and the validation set for testing. Training. Differently from our ablation experiments, here we train our CSNs with 32-frame clip inputs (T = 32) with a sampling rate of 2 (skipping every other frame) following the practice described in <ref type="bibr" target="#b32">[32]</ref>. All the other training settings such as data augmentation and optimization parameters are the same as those described in our previous section. Testing. For Sports1M and Something 2 -v1, we uniformly sample 10 clips per video, scale the shorter edge to 256 (keeping aspect ratio), and use only the center crop of 224×224 per clip for inference. We average the softmax predictions of these 10 crops for video prediction. On Kinetics, since the 30 crops evaluation in <ref type="bibr" target="#b39">[37]</ref> is widely adopted, we follow this setup for a fair comparison with previous approaches. Results on Sports1M.  <ref type="bibr" target="#b32">[32]</ref> by 2.2% on video top-1 accuracy while being 2-4x faster than R(2+1)D. Our ir-CSN-101, even with a smaller number of FLOPs, still outperforms all previous work by good margins. On large-scale benchmarks like Sports1M, the difference between ir-CSN and ip-CSN is very small. The added benefit of ir-CSN is that it has smaller GFLOPs, especially in deeper settings where the number of channel interactions is similar to that of ip-CSN. This is consistent with the observation from our ablation. Results on Kinetics. We train our CSN models on Kinetics and compare them with current state-of-the-art methods. In addition to training from scratch, we also finetune our CSNs with weights initialized from models pre-trained on Sports1M. For a fair comparison, we compare our CSNs with the methods that use only RGB as input. <ref type="table" target="#tab_5">Table 5</ref> presents the results. Our ip-CSN-152, even when trained from scratch, outperforms all of the previous models, except for SlowFast <ref type="bibr" target="#b10">[10]</ref>. Our ip-CSN-152, pre-trained on Sports1M outperforms I3D <ref type="bibr" target="#b3">[3]</ref>, R(2+1)D <ref type="bibr" target="#b32">[32]</ref>, and S3D-G [40] by 8.1%, 4.9%, and 4.5%, respectively. It also outperforms recent work: A 2 -Net [4] by 4.6%, Globalreasoning networks <ref type="bibr" target="#b6">[6]</ref> by 3.1%. We note that our ip-CSN-152 achieves higher accuracy than both I3D with Non-local Networks (NL) <ref type="bibr" target="#b39">[37]</ref> and SlowFast <ref type="bibr" target="#b10">[10]</ref> (+1.5% and +0.3%) while being also faster (3.3x and 2x, respectively). Our ip-CSN-152 is still 0.6% lower than SlowFast augmented with Non-Local Networks. Finally, recent work <ref type="bibr" target="#b13">[13]</ref> has shown that R(2+1)D can achieve strong performance when pre-trained on a large-scale weakly-supervised dataset. We pre-train/finetune ir-and ip-CSN-152 on the same dataset and compare it with R(2+1)D-152 (the last three rows of Table 5). In this large-scale setup, ip-and ir-CSN-152 outperform R(2+1)D-152 by 1.2 and 1.3%, respectively, in video top-1 accuracy while being 3.0-3.4 times faster.</p><p>Results on Something-Something. <ref type="table">Table 6</ref> compares our CSNs with state-of-the-art methods on Something 2 -v1 validation set. Our ir-CSN-152, even when trained from scratch, outperforms all of previous methods. Our ip-CSN-152, when pretrained on IG-65M <ref type="bibr" target="#b13">[13]</ref>, achieves 53.3% top-1 accuracy, the new state-of-the-art record for this benchmark. On the same pretraining and finetuning setting, our ir-CSN-152 and ip-CSN-152 outperform R(2+1)D-152 by 0.5% and 1.7%, respectively.  <ref type="table">Table 6</ref>. Comparisons with state-of-the-art methods on Something 2 -V1. Our CSNs outperform all previous methods by good margins when restricting all models to use only RGB as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented Channel-Separated Convolutional Networks (CSN) as a way of factorizing 3D convolutions. The proposed CSN-based factorization not only helps to significantly reduce the computational cost, but also improves the accuracy when there are enough channel interactions in the networks. Our proposed architecture, ir-and ip-CSN, significantly outperform existing methods and obtains state-of-the-art accuracy on three major benchmarks: Sports1M, Kinetics, and Something-Something. The model is also multiple times faster than current competing networks. We have made code and pre-trained models publicly available <ref type="bibr" target="#b9">[9]</ref>. Acknowledgements. We thank Kaiming He for insightful discussions and Haoqi Fan for help in improving our training framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Visualization of CSN filters</head><p>Because all 3×3×3 convolution layers in CSN (except for conv 1) are depthwise, it is easy to visualize these learned filters. <ref type="figure" target="#fig_11">Figure 8-11</ref> present some of the filters learned by our ir-CSN-152. Our ir-CSN-152 has the first convolution layer, i.e. conv 1 is a normal 3D convolution and 50 bottleneck-D convolutional blocks. These blocks are grouped into four groups by the output size (see <ref type="table">Table 1</ref>). The ir-CSN-152 conv 1 filters are presented in <ref type="figure" target="#fig_11">Figure 8</ref>. The learned 3×3×3 depthwise convolution filters of the first 3 blocks (named comp 0, comp 1, and comp 2) are presented <ref type="figure" target="#fig_12">Figure 9</ref>. <ref type="figure">Figure 10</ref> and 11 present the learned 3×3×3 depthwise convolution filters in a random chosen blocks in the 2nd and 3rd convolutional group, respectively. It is interesting to observe that conv 1 captures texture, motion, color changing pattern since it has all cross-channel filters. The depthwise convolution filters learn the motion and/or texture patterns within the same feature channel given by its previous layer. Full images and GIF animations can be downloaded at https://bit.ly/2KuEsKY.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>presents two ways of factorizing a 3D bottleneck block using channel-separated convolutional networks.Figure 2(a) presents a standard 3D bottleneck block, while Figure 2(b) and 2(c) present interaction-preserved and interaction-reduced channel-separated bottleneck blocks, respectively. Interaction-preserved channel-separated bottleneck block is obtained from the standard bottleneck block (Figure 2(a) by replacing the 3×3×3 convolution in (a) with a 1×1×1 traditional convolution and a 3×3×3 depthwise convolution (shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 Figure 2 .</head><label>32</label><figDesc>Standard vs. channel-separated convolutional blocks. (a) A standard ResNet bottleneck block. (b) An interactionpreserved bottleneck block: a bottleneck block where the 3×3×3 convolution in (a) is replaced by a 1×1×1 standard convolution and a 3×3×3 depthwise convolution (shown in dashed box). (c)An interaction-reduced bottleneck block, a bottleneck block where the 3×3×3 convolution in (a) is replaced with a depthwise convolution (shown in dashed box). We note that channel interaction is preserved in (b) by the 1×1×1 convolution, while (c) lost all of the channel interaction in its 3×3×3 convolution after factorization. Batch norm and ReLU are used after each convolution layer. For simplicity, we omit the skip connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>ResNet simple block transformed by group convolution. (a) Simple block: a standard ResNet simple block with two 3×3×3 convolutional layers. (b) Simple-G block: a ResNet simple block with two 3×3×3 group convolutional layers. (c) Simple-D block: a ResNet simple block with two 3×3×3 depthwise convolutional layers with an optional 1×1×1 convolutional layer (shown in dashed box) added when increasing number of filters is needed. Batch norm and ReLU are used after each convolution layer. For simplicity, we omit the skip connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 (</head><label>4</label><figDesc>a) presents a ResNet bottleneck block consisting of two 1×1×1 and one 3×3×3 convolutional layers. Figures 4(b-c) present bottleneck-G and bottleneck-D where the 3×3×3 convolutions are grouped and depthwise, respectively. If we further apply group convolution to the two 1×1×1 convolutional layers, the block becomes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>-G c) bottleneck-D d) bottleneck-DG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>ResNet bottleneck block transformed by group convolution. (a) A standard ResNet bottleneck block. (b) Bottleneck-G: a ResNet bottleneck block with a 3×3×3 group convolutional layer. (c) Bottleneck-D: a bottleneck block with a 3×3×3 depthwise convolution (previously named as ir-CSN, the new name of Bottleneck-D is used here for simplicity and analogy with other blocks). (d) Bottleneck-DG: a ResNet bottleneckblock with a 3×3×3 depthwise convolution and two 1×1×1 group convolutions. We note that from (a) to (d), we gradually apply group convolution to the 3×3×3 convolutional layer and then the two 1×1×1 convolutional layers. Batch norm and ReLU are used after each convolution layer. For simplicity, in the illustration we omit to show skip connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Training error as a function of training iterations for ip-CSN-101 and ResNet3D-101 on Kinetics. ip-CSN has higher training error, but lower testing error (compare validation accuracies in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>ResNet3D accuracy/computation tradeoff by transforming group convolutional blocks. Video top-1 accuracy on the Kinetics validation set against computation cost (# FLOPs) for a ResNet3D with different convolutional block designs. (a) Group convolution transformation applied to simple and bottleneck blocks with shallow architectures with 8 blocks. (b) Group convolution transformation applied to simple and bottleneck blocks with deep architectures with 16 blocks. The bottleneck-D block (marked with green starts) gives the best accuracy tradeoff among the tested block designs. Base architectures are marked with black hexagrams. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Accuracy vs. channel interactions. Plotting the Kinetics validation accuracy of different models with respect to their total number of channel interactions. Channel interactions are presented on a log scale for better viewing. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Visualization of ir-CSN-152 conv 1 filters. The layer includes 64 convolutional filters, each one of size 3×7×7. Filters are scaled-up by x5 for better presentation. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Visualization of the 3×3×3 channel-separated convolutional filters in the first three blocks of ir-CSN-152 after conv 1 (and pool 1): (a) comp 0, (b) comp 1, and (c) comp 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>compares results of our CSNs with those of previous methods on Sports1M. Our ir-CSN-152 and ip-CSN-152 outperform C3D [31] by 14.4%, P3D [25] by 9.1%, Conv Pool [41] by 3.8%, and R(2+1)D</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison with state-of-the-art architectures on Kinetics. Accuracy is measured on the Kinetics validation set. For fair evaluation, the comparison is restricted to models trained on RGB input. Our ir-CSN-152 is better than or comparable with previous models while being multiple times faster. *Models leveraging large-scale pre-training, thus not comparable with others.</figDesc><table><row><cell>Method</cell><cell cols="3">pretrain vi@1 vi@5 GFLOPs×crops</cell></row><row><cell>ResNeXt [16]</cell><cell>none</cell><cell>65.1 85.7</cell><cell>N/A</cell></row><row><cell>ARTNet(d) [34]</cell><cell>none</cell><cell>69.2 88.3</cell><cell>24×250</cell></row><row><cell>I3D [3]</cell><cell>ImageNet</cell><cell>71.1 89.3</cell><cell>108×N/A</cell></row><row><cell>TSM [24]</cell><cell>ImageNet</cell><cell>72.5 90.7</cell><cell>65×N/A</cell></row><row><cell>MFNet [5]</cell><cell>ImageNet</cell><cell>72.8 90.4</cell><cell>11×N/A</cell></row><row><cell cols="3">Inception-ResNet [1] ImageNet 73.0 90.9</cell><cell>N/A</cell></row><row><cell cols="3">R(2+1)D-34 [32] Sports1M 74.3 91.4</cell><cell>152×N/A</cell></row><row><cell>A 2 -Net [4]</cell><cell>ImageNet</cell><cell>74.6 91.5</cell><cell>41×N/A</cell></row><row><cell>S3D-G [40]</cell><cell>ImageNet</cell><cell>74.7 93.4</cell><cell>71×N/A</cell></row><row><cell>D3D [29]</cell><cell>ImageNet</cell><cell>75.9 N/A</cell><cell>N/A</cell></row><row><cell>GloRe [6]</cell><cell>ImageNet</cell><cell>76.1 N/A</cell><cell>55×N/A</cell></row><row><cell>I3D+NL [37]</cell><cell>ImageNet</cell><cell>77.7 93.3</cell><cell>359×30</cell></row><row><cell>SlowFast [10]</cell><cell>none</cell><cell>78.9 93.5</cell><cell>213×30</cell></row><row><cell>SlowFast+NL [10]</cell><cell>none</cell><cell>79.8 93.9</cell><cell>234×30</cell></row><row><cell>ir-CSN-101</cell><cell>none</cell><cell>76.2 92.2</cell><cell>73.8×30</cell></row><row><cell>ip-CSN-101</cell><cell>none</cell><cell>76.7 92.3</cell><cell>83.0×30</cell></row><row><cell>ir-CSN-152</cell><cell>none</cell><cell>76.8 92.5</cell><cell>96.7×30</cell></row><row><cell>ip-CSN-152</cell><cell>none</cell><cell>77.8 92.8</cell><cell>108.8×30</cell></row><row><cell>ir-CSN-101</cell><cell>Sports1M</cell><cell>78.1 93.4</cell><cell>73.8×30</cell></row><row><cell>ip-CSN-101</cell><cell>Sports1M</cell><cell>78.5 93.5</cell><cell>83.0×30</cell></row><row><cell>ir-CSN-152</cell><cell>Sports1M</cell><cell>79.0 93.5</cell><cell>96.7×30</cell></row><row><cell>ip-CSN-152</cell><cell>Sports1M</cell><cell>79.2 93.8</cell><cell>108.8×30</cell></row><row><cell cols="3">R(2+1)D-152* [13] IG-65M 81.3 95.1</cell><cell>329×30</cell></row><row><cell>ir-CSN-152*</cell><cell cols="2">IG-65M 82.6 95.3</cell><cell>96.7×30</cell></row><row><cell>ip-CSN-152*</cell><cell cols="2">IG-65M 82.5 95.3</cell><cell>108.8×30</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Revisiting the effectiveness of off-the-shelf temporal modeling approaches for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1708.03805</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Caffe2: A new lightweight, modular, and scalable deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe2-Team</surname></persName>
		</author>
		<ptr target="https://caffe2.ai/.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aˆ2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeuIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2017. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV VS-PETS</title>
		<meeting>ICCV VS-PETS</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Facebook</surname></persName>
		</author>
		<idno>2018. 9</idno>
		<ptr target="https://github.com/facebookresearch/VMZ" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale weaklysupervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fründ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04261</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunggi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Sungjoon Son, Gyutae Park, and Nojun Kwak</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1811.08383</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreemanananth</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1801.04381</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">D3D: distilled 3d networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<idno>abs/1812.08249</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Visualization of the 128 3×3×3 channel-separated convolutional filters in comp 10 of ir-CSN-152</title>
		<imprint/>
	</monogr>
	<note>Figure 10. comp 10 is an arbitrarily chosen block in the second convolutional group</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Visualization of the 256 3×3×3 channel-separated convolutional filters in comp 12 of ir-CSN-152</title>
		<imprint/>
	</monogr>
	<note>Figure 11. comp 12 is an arbitrarily chosen block in the third convolutional group</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Actionst ransformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ECO: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
