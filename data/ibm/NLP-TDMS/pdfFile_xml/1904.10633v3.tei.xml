<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LFFD: A Light and Fast Face Detector for Edge Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghao</forename><surname>He</surname></persName>
							<email>yonghao.he@aliyun.com</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhong</forename><surname>Xu</surname></persName>
							<email>xudezhong@emails.bjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifang</forename><surname>Wu</surname></persName>
							<email>lfwu@bjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jian</surname></persName>
							<email>jianmeng648@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
							<email>chpan@nlpr.ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LFFD: A Light and Fast Face Detector for Edge Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face detection, as a fundamental technology for various applications, is always deployed on edge devices which have limited memory storage and low computing power. This paper introduces a Light and Fast Face Detector (LFFD) for edge devices. The proposed method is anchorfree and belongs to the one-stage category. Specifically, we rethink the importance of receptive field (RF) and effective receptive field (ERF) in the background of face detection. Essentially, the RFs of neurons in a certain layer are distributed regularly in the input image and theses RFs are natural "anchors". Combining RF "anchors" and appropriate RF strides, the proposed method can detect a large range of continuous face scales with 100% coverage in theory. The insightful understanding of relations between ERF and face scales motivates an efficient backbone for onestage detection. The backbone is characterized by eight detection branches and common layers, resulting in efficient computation. Comprehensive and extensive experiments on popular benchmarks: WIDER FACE and FDDB are conducted. A new evaluation schema is proposed for application-oriented scenarios. Under the new schema, the proposed method can achieve superior accuracy (WIDER FACE Val/Test -Easy: 0.910/0.896, Medium: 0.881/0.865, Hard: 0.780/0.770; FDDB -discontinuous: 0.973, continuous: 0.724). Multiple hardware platforms are introduced to evaluate the running efficiency. The proposed method can obtain fast inference speed ( NVIDIA TITAN Xp: 131.45 FPS at 640×480; NVIDIA TX2: 136.99 PFS at 160×120; Raspberry Pi 3 Model B+: 8.44 FPS at 160×120) with model size of 9 MB.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mAP(%)</head><p>Subset Easy Medium Hard ISRN <ref type="bibr" target="#b35">[36]</ref> 0.967 0.958 0.909 VIM-FD <ref type="bibr" target="#b38">[39]</ref> 0.967 0.957 0.907 DSFD <ref type="bibr" target="#b15">[16]</ref> 0.966 0.957 0.904 SRN <ref type="bibr" target="#b2">[3]</ref> 0.964 0.952 0.901 PyramidBox <ref type="bibr" target="#b27">[28]</ref> 0.961 0.950 0.889 <ref type="table">Table 1</ref>. Accuracy of the top-5 methods on validation set of WIDER FACE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face detection is a long-standing problem in computer vision. In practice, it is the prerequisite to some face-related applications, such as face alignment <ref type="bibr" target="#b13">[14]</ref> and face recognition <ref type="bibr" target="#b30">[31]</ref>. Besides, face detectors are always deployed on edge devices, such as mobile phones, IP cameras and IoT (Internet of Things) sensors. These devices have limited memory storage and low computing power. Under such condition, face detectors that have high accuracy and fast running speed are in demand.</p><p>Current state of the art face detectors have achieved fairly high accuracy on convictive benchmark WIDER FACE <ref type="bibr" target="#b32">[33]</ref> by leveraging pre-trained heavy backbones like VGG16 <ref type="bibr" target="#b26">[27]</ref>, Resnet50/152 <ref type="bibr" target="#b6">[7]</ref> and Densenet121 <ref type="bibr" target="#b9">[10]</ref>. We investigate the top-5 methods on WIDER FACE and present their accuracy in <ref type="table">Table 1</ref>. It can be observed that these methods have similar accuracy with marginal gaps which are hardly perceived in practical applications. It is difficult and unpractical to further boost the accuracy by using more complex and heavier backbones. In our view, to better balance accuracy and latency is crucial for applying face detection to more applicable areas.</p><p>Face detection is a fast-growing branch of general object detection in the past decade. The early work of Viola-Jones face detector <ref type="bibr" target="#b28">[29]</ref> proposes a classic detection framework -cascade classifiers with hand-crafted features. One of its well-known followers is aggregate channel features (ACF) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref> which can take advantages of channel features effectively. Although the methods mentioned above can achieve fast running speed, they rely on hand-crafted features and are not trained end-to-end, resulting in not robust detection accuracy.</p><p>Recently, convolutional neural network (CNN) based face detectors <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref> show great progress partially owing to the success of WIDER FACE benchmark. These methods can be roughly divided into two categories: two-stage methods and one-stage methods. Two-stage methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref> consist of proposal selection and localization regression, which are mainly originated from R-CNN series <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26]</ref>. Whereas, one-stage methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36]</ref> coherently combine classification and bounding box (bbox) regression, always achieving anchor-based and multi-scale detection simultaneously. For most one-stage methods, anchor design and matching strategy is one of the essential components. In order to improve the accuracy, these methods propose more complex modules based on heavy backbones. Although the above methods can achieve state of the art results, they may not properly balance accuracy and latency.</p><p>In this paper, we propose a Light and Fast Face Detector (LFFD) for edge devices, considerably balancing both accuracy and running efficiency. The proposed method is inspired by the one-stage and multi-scale object detection method SSD <ref type="bibr" target="#b16">[17]</ref> which also enlightens some other face detectors <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref>. One of the characteristics of SSD is that pre-defined anchor boxes are manually designed for each detection branch. These boxes always have different sizes and aspect ratios to cover objects with different scales and shapes. Therefore, anchors play an important role in most one-stage detection methods. For some face detectors <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref>, sophisticated anchor strategies are crucial parts of the contributions. However, anchor based methods may face three challenges: 1) anchor matching is unable to sufficiently cover all face scales. Although this can be relieved, it remains a problem; 2) matching anchors to groundtruth bboxes is determined by thresholding IOU (Intersection over Union). The threshold is set empirically and it is difficult to make a solid investigation of its impact; 3) setting the number of anchors for different scales depends on experiences, which may induce sample imbalance and redundant computation.</p><p>In our point of view, RF of neurons in feature maps are inherent and natural "anchors". RF can easily handle above challenges. Firstly, continuous scales of faces can be predicted within a certain RF size, rather than discrete scales in anchor-based methods. Secondly, matching strategy is clear, namely a RF is matched to a groundtruth bbox if and only if its center falls in the groundtruth bbox . Thirdly, the number of RFs is naturally fixed and they are regularly distributed in the input image. What's more, we make a qualitative analysis on pairing face scales and RF sizes by understanding the insights of ERF, resulting in an efficient backbone with eight detection branches. The backbone only consists of common layers (conv3×3, conv1×1, ReLU and residual connection), which is much lighter than VGG16 <ref type="bibr" target="#b26">[27]</ref>, Resnet50 <ref type="bibr" target="#b6">[7]</ref> and Densenet121 <ref type="bibr" target="#b9">[10]</ref>. Consequently, the final model has only 2.1M parameters ( versus VGG16-138.3M and Resnet50-25.5M ) and achieves superior accuracy and running speed, which makes it appropriate for edge devices.</p><p>In summary, the main contributions of this paper include:</p><p>• We study the relations of RF, ERF and face detection. The relevant understanding motivates the network design. • We introduce the RF to overcome the drawbacks of the previous anchor-based strategies, resulting in a anchorfree method. • We proposed a new backbone with common layers for accurate and fast face detection. • Extensive and comprehensive experiments on multiple hardware platforms are conducted on benchmarks WIDER FACE and FDDB to firmly demonstrate the superiority of the proposed method for edge devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Face detection has attracted a lot of attention since a decade ago.</p><p>Early works Early face detectors leverage hand-crafted features and cascade classifiers to detect faces in forms of sliding window. Viola-Jones face detector <ref type="bibr" target="#b28">[29]</ref> uses Adaboost with Haar-like features to train face classifiers discriminatively. Subsequently, utilizing more effective handcrafted features <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b31">32]</ref> and more powerful classifiers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref> becomes the mainstream. These methods are not trained end-to-end, treating feature learning and classifier training separately. Although achieving fast running speed, they can not obtain satisfied accuracy.</p><p>CNN-based methods Current CNN-based face detectors benefit from two-stage <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26]</ref> and one-stage <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> general object detection. Both <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b29">[30]</ref> are based on faster R-CNN <ref type="bibr" target="#b25">[26]</ref>, adapting the original faster R-CNN to face detection. Zhang et al. <ref type="bibr" target="#b34">[35]</ref> proposes a cascaded CNN for coarse-to-fine face detection with inside cascaded structure. Recently, one-stage face detectors are dominant. MTCNN <ref type="bibr" target="#b33">[34]</ref> performs face detection in a sliding window manner and relies on image pyramid. HR <ref type="bibr" target="#b8">[9]</ref> is an advanced version of MTCNN to some extent, also requiring image pyramid. Image pyramid has some drawbacks like slow speed and high memory cost. S3FD <ref type="bibr" target="#b37">[38]</ref> takes RF into consideration for detection branch design and proposes an anchor matching strategy to improve hit rate. In <ref type="bibr" target="#b39">[40]</ref>, Zhu et al. focuses on detecting small faces by proposing a robust anchor generating and matching strategy. It can be concluded that anchor related strategies are crucial for face detection. Following S3FD <ref type="bibr" target="#b37">[38]</ref>, PyramidBox <ref type="bibr" target="#b27">[28]</ref> enhances the backbone with low-level feature pyramid layers (LFPN) for better multi-scale detection. SSH <ref type="bibr" target="#b19">[20]</ref> constructs three detection modules cooperating with context modules for scale-invariant face detection. DSFD <ref type="bibr" target="#b15">[16]</ref> is characterized by feature enhance modules, early layer supervision and an improved anchor matching strategy for better initialization. S3FD, PyramidBox, SSH and DSFD use VGG16 as backbones, leading to big model size and inefficient computation. FaceBoxes <ref type="bibr" target="#b36">[37]</ref> aims to make the face detector run in real-time by rapidly reducing the size of input images. In detail, it reaches a large stride size 32 after four layers: two convolution layers and two pooling layers. Although the running speed of FaceBoxes is fast, it abandons the detection of small faces, resulting in relatively low accuracy on WIDER FACE. Different from FaceBoxes, our method handles the detection of small faces delicately, achieving fast running speed and large scale coverage in the meantime. It can be observed that the networks used by recent state of the art methods tend to become more complex and heavier. In our view, to gain marginal improvement in accuracy at the cost of running speed is not appropriate for practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Light and Fast Face Detector</head><p>In this section, we first revisit the concept of RF and its relation to face detection in Sec. 3.1. Then Sec. 3.2 describes the rationality and advantages of using RFs as natural "anchors". Subsequently, the details of the proposed network is depicted in Sec. 3.3. Finally, we present the specifications of network training in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisit RF in the Background of Face Detection</head><p>In the beginning, we make a brief description of RF and its properties. RF is a definite area of the input image, which affects the activation of the corresponding neuron. RF determines the range that a neuron can see in the original input. Intuitively, the target object can be well detected with high probabilities if it is enclosed by a certain RF. In general, the neurons in shallow layers have small RFs and those in deeper layers have large RFs. One of the important properties of RF is that each input pixel contributes differently for the neuron's activation <ref type="bibr" target="#b17">[18]</ref>. Specifically, the pixels locating around the center of RF have larger impact. And the impact decreases gradually when the pixels are far away from the center. This phenomenon is named as effective receptive field (ERF). ERFs inherently exist in neural networks and present a Gaussian-like distribution. Thus, mak-ing the target object in the middle of the RF is also important. The proposed LFFD benefits from the above observations.</p><p>Face detection is a well-known branch of general object detection and it has some characteristics. First, big faces are approximately rigid due to their unmovable components, such as eyes, noses and mouths. Although there are facial expression changes, hair occlusion and other unconstrained situations, big faces are still distinguishable. Second, tiny or small faces have to be treated differently compared to big faces. Tiny faces always have unrecognizable appearances (an example is shown in <ref type="figure">Fig. 1</ref>). It is even difficult for humans to make a face/non-face decision by only seeing the facial area of a tiny face, and the same goes for CNN based classifiers <ref type="bibr" target="#b8">[9]</ref>. With more context information including necks and shoulders, tiny faces become easier to recognize. Detailed discussion can be referred to <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure">Figure 1</ref>. Tiny faces detection. The top-left image only contains a face, and the top-right image depicts a face with sufficient context information. It is easy to see that the face becomes more distinguishable with the context information gradually increasing. The lower part describes the relation between RF and ERF for detecting the tiny face.</p><p>Based on above understandings, faces with different sizes need various RF strategies:</p><p>• for tiny/small faces, ERFs have to cover the faces as well as sufficient context information;</p><p>• for medium faces, ERFs only have to contain the faces with little context information;</p><p>• for large faces, only keeping them in RFs is enough.</p><p>These strategies guide us to design an effective backbone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RFs as Natural "Anchor"</head><p>One-stage detectors are mostly characterized by predefined bbox anchors. In order to detect different objects, anchors are in multiple aspect ratios and sizes. These anchors are always redundantly defined. In terms of face detection, it is rational to use 1:1 aspect ratio anchors since faces are approximately square, which is also mentioned in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref>. The shapes of RFs are also square if the width and height of the kernel are equal. The proposed method regards RFs as natural "anchors". For the neurons in the same layer, their RFs are regularly tiled in the input image. The number and size of RFs are inherently determined once the network is built.</p><p>As for matching strategy, the proposed method uses a straight and concise way -the RF is matched to a groundtruth bbox if and only if its center falls in the groundtruth bbox, other than thresholding IOU. In the typical anchor-based method S3FD <ref type="bibr" target="#b37">[38]</ref>, Zhang et al. also analyses the influence of ERFs and designs anchor augmentation for tiny faces in particular. In spite of improving the anchor hit rate, S3FD induces the anchor imbalance problem (too many anchors for tiny faces) which has to be addressed by additional means. However, the proposed method can achieve 100% face coverage in theory by controlling the RF stride. Besides, RF with our matching strategy can naturally handle continuous face scales. For an instance, RFs of 100 pixels are able to predict faces between 20 pixels to 40 pixels. In this way, anchor imbalance problem is greatly relieved and faces from each scale are equally treated.</p><p>Based on the above discussion, we do not create any anchors and the proposed method do not really match anchors to groundtruth bboxes. Therefore, the proposed method is anchor-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>According to above analyses, we can design a specialised backbone for face detection. There are two factors that determine the placement of loss branches -the size and stride of RFs. The size of RFs guarantees that the learned features of faces are robust and distinguishable, whereas the stride ensures the 100% coverage. The overall architecture of the proposed network is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The proposed method can detect faces that are lager than 10 pixels (the size of a face is indicated by the longer side), since WIDER FACE benchmark dataset requires faces more than 10 pixels to be detected. It can be observed that the proposed backbone is one-stage with four parts. The concrete information about loss branches can be found in <ref type="table" target="#tab_0">Table 2</ref>.</p><p>The tiny part has 10 convolution layers. The first two lay-ers downsample the input with stride 4, stride 2 from each. Therefore, RFs of other convolution layers in this part are in stride 4. One crucial principle is: downsample the input as quick as possible while keeping the 100% face coverage. This part has two loss branches. The loss branch 1 stems from c8 whose RF size is 55 for continuous face scale 10-15. Similarly, the loss branch 2 is from c10 with RF size 71 for continuous face scale 15-20. Obviously, we can make sure that centers of at least two RFs can fall in the smallest face, thus achieving 100% coverage. There is a special case that one center may fall in more than two faces at the same time, in which the corresponding RF is ignored directly. As we have discussed in Sec. 3.1, tiny faces need more context information and ERFs are smaller than RFs. To this end, we use much larger RFs than average face scales. The ratios of RFs and average face scales are 4.4 and 4.0 for branch 1 and branch 2, respectively. In <ref type="table" target="#tab_0">Table 2</ref>, such ratios are gradually decreased from 4.4 to 1.3, because larger faces need less context information. In the backbone, all convolution layers have the kernel size of 3×3. Nevertheless, the kernel size of convolution layers in branches is 1×1 which does not change the size of RFs. In each branch, there are two sub-branches, one for face classification and the other one for bbox regression. The small part is in charge of two continuous face scales 20-40 and 40-70. The first convolution layer c11 in this part downsamples the feature maps by 2×. For the subsequent parts, their first convolution layers accomplish the same function. In small part, the RF increasing speed becomes 16 compared to that of tiny part 8. So it takes less convolution layers to reach the targeted RF sizes. The medium part is similar to the small part, having only one branch.</p><p>At the end of the backbone, the large part has seven convolution layers. These layers easily enlarge the detection scale without too much computation gain due to small feature maps. Three branches are from this part. Since big faces are much easier to detect, the ratios of RFs and average face scales are relatively small.</p><p>The proposed method can detect a large range of faces from 10 pixels to 560 pixels within one inference. The overall backbone only consists of conv 3×3, conv 1×1, ReLU and residual connection. The main reason is that conv 3×3 and conv 1×1 are highly optimized by inference libraries, such as cuDNN * , ncnn † , mace ‡ and paddle-mobile § , since they are most widely used. We do not adopt BN <ref type="bibr" target="#b10">[11]</ref> as components due to slow inference speed, although it has become the standard configuration of many networks. We compare the speed between the original backbone and the one with BN: the original one can achieve 7.6 ms and the * https://developer.nvidia.com/cudnn † https://github.com/Tencent/ncnn ‡ https://github.com/XiaoMi/mace § https://github.com/PaddlePaddle/paddle-mobile one with BN only has 8.9 ms, resulting in 17% slower (resolution: 640×480, hardware: TITAN X (Pascal)) . In stead of using BN, we train much more iterations for better convergence. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, in each part, residual connections are placed side by side for easily training the deep backbone. The number of filters of all convolution layers in the first two parts is 64. We do not increase the filters, since the first two parts have relatively large feature maps which are computationally expensive. However, the number of filters in the last two parts can be increased to 128 without too much additional computation. More details can be found in <ref type="table" target="#tab_0">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Details</head><p>In this subsection, we describe the training related details in several aspects.</p><p>Dataset and data augmentation. The proposed method is trained on the training set of WIDER FACE benchmark <ref type="bibr" target="#b32">[33]</ref>, including 12,880 images with more than 150,000 valid faces. Faces less than 10 pixels are discarded directly. Data augmentation is important for improving the robustness. The detailed strategies are listed as follows:</p><p>• Color distort, such as random lighting noise, random contrast, random brightness, et al. More information can refer to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. • Random sampling for each scale. In the proposed network, there are eight loss branches, each in charge of a certain continuous scale. Thus, we have to guarantee that: 1) the number of faces for each branch is approximately the same; 2) each face can be sampled for each branch with the same probability. To this end, we first randomly select an image, and then randomly select a face in the image. Second, a continuous face scale is selected and the face is randomly resized within the scale as well as the entire image and other face bboxes. Finally, we crop a sub-image of 640×640 at the center of the selected face, filling the outer space with black pixels. • Randomly horizontal flip. We flip the cropped image with probability of 0.5.</p><p>Loss function. In each loss branch, there are two sub-branches for face classification and bbox regression. For face classification, we use softmax with cross-entropy loss over two classes. The matched RF anchors are positive and the others are negative. <ref type="bibr">Those</ref>   <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref> and <ref type="bibr" target="#b39">[40,</ref><ref type="bibr">44]</ref>. Faces that fall in gray scales are also ignored by the corresponding branch. For bbox regression, we adopt L2 loss directly. The regression groundtruth is defined as:</p><formula xml:id="formula_0">RF x − b tl x RF s /2 , RF y − b tl y RF s /2 , RF x − b br x RF s /2 , RF y − b br y RF s /2 ,<label>(1)</label></formula><p>where RF Hard negative mining. For each branch, negative RF anchors are usually more than positive ones. For stable and better training, only a fractional negative RF anchors are used for back-propagation: we sort the loss values of all negative anchors and only select the top ones for learning. The ratio between the positive and negative anchors is at most 1:10. Empirically, hard negative mining can bring faster and stable convergence.</p><p>Training parameters. We initialize all parameters with xavier method and train the network from scratch. The inputs first minus 127.5, and then divided by 127.5. The optimization method is SGD with 0.9 momentum, zero weight decay and batch size 32. The reason for zero weight decay is that the number of parameters in the proposed network is much less than that of VGG16. Thus, there is no need to punish. The initial learning rate is 0.1. We train 1,500,000 iterations and reduce the learning rate by multiplying 0.1 at iteration 600,000, 1,000,000, 1,200,000 and 1,400,000. The training time is about 5 days with two NVIDIA GTX1080TI. Our method is implemented using MXNet <ref type="bibr" target="#b1">[2]</ref> and the source code is released ¶ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, comprehensive and extensive experiments are conducted. Firstly, a new evaluation schema is proposed and the evaluation results on benchmarks are presented. Secondly, we analyse the running efficiency on multiple platforms. Thirdly, we further investigate the amount of computation and storage memory cost, introducing the computation efficiency rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on Benchmarks</head><p>In this subsection, a new evaluation schema is described at the beginning. The new schema is named as Single Inference on the Original (SIO). SIO is proposed to reform the evaluation procedure for real-world applications. We notice ¶ https://github.com/YonghaoHe/A-Light-and-Fast-Face-Detector-for-Edge-Devices  that the latency in some practical scenarios has the same importance as the accuracy. The conventional evaluation procedure involves some tricky means, such as flips and image pyramids, for achieving higher accuracy. However, the time consumption is not acceptable by doing that. To this end, SIO can be easily operated in the following way: 1) keep the image in its original size as the net input; 2) the net does only one inference with the original image. The outputs of SIO are fed to the subsequent metrics.</p><p>In the experiments, we have to reproduce the results according to SIO schema. Therefore, we collect the compared methods which have released codes and models. Finally, the following methods are taken for comparison: DSFD <ref type="bibr" target="#b15">[16]</ref> ( Resnet152 backbone ), PyramidBox <ref type="bibr" target="#b27">[28]</ref> ( VGG16 backbone ), S3FD <ref type="bibr" target="#b37">[38]</ref> ( VGG16 backbone ), SSH <ref type="bibr" target="#b19">[20]</ref> ( VGG16 backbone ) and FaceBoxes <ref type="bibr" target="#b36">[37]</ref>. DSFD and PyramidBox are state of the art methods. The proposed method is named <ref type="table">Table 3</ref>. Performance results on the validation set of WIDER FACE. The values in () are results from the original papers. as LFFD. LFFD and FaceBoxes do not rely on existing pretrained backbones and are trained from scratch. We evaluate all methods on two benchmarks: FDDB <ref type="bibr" target="#b11">[12]</ref> and WDIER FACE <ref type="bibr" target="#b32">[33]</ref>.</p><p>FDDB dataset. FDDB contains 2845 images with 5171 unconstrained faces. There are two types of scoring: discrete score and continuous score. The first scoring criterion is obtained by thresholding IOU. And the second criterion directly uses IOU ratios. We show final evaluation results of LFFD on FDDB against above five methods in <ref type="figure" target="#fig_4">Fig. 3</ref>. The overall performance on both scoring types shows the similar trends. DSFD, PyramidBox, S3FD and SSH can achieve high accuracy with marginal gaps. The proposed LFFD gains slightly lower accuracy than the first four methods, but outperforms FaceBoxes evidently. The results indicate that LFFD is superior for detecting unconstrained faces.</p><p>WIDER FACE dataset. In WIDER FACE, there are 32,203 images and 393,703 labelled faces. These faces are in a high degree of variability in scale, pose and occlusion. Until now, WIDER FACE is the most widely used benchmark for face detection. All images are randomly divided into three subsets: training set (40%), validation set (10%) and testing set(50%). Furthermore, images in each subset are graded to three levels (Easy, Medium and Hard) according to the difficulties for detection. Roughly speaking, a large number of tiny/small faces are in Medium and Hard parts. The groundtruth annotations are available only for training and validation sets. All the compared methods are trained on training set. We report the results on the validation and testing sets in <ref type="table">Table 3</ref> and 4, respectively.</p><p>Some observations can be made. Firstly, performance drop is evident for DSFD, PyramidBox, S3FD and SSH compared to their original results. On the one hand, achieving high accuracy through only one inference is relatively difficult. On the other hand, the tricks can indeed improve the accuracy impressively. Secondly, PyramidBox obtains the best results on Hard parts, whereas the performance of SSH on Hard parts is decreased dramatically mainly due to the neglect of some tiny faces. Thirdly, FaceBoxes does not get desirable results on Medium and Hard parts. Since Face-Boxes produces large stride 32 rapidly, which means that faces smaller than 32 pixels are hardly detected. To make it clearer, we conduct additional experiments for FaceBoxes, named as FaceBoxes3.2×, in which the both sides of input images are enlarged 3.2×. We can see that the results on Medium and Hard parts are improved remarkably. The performance drop on Easy parts is attributed to that some faces are resized too large to be detected. To some extent, the results of FaceBoxes and FaceBoxes3.2× indicate that FaceBoxes can not cover faces with large range. Fourthly, the proposed method LFFD consistently outperforms Face-Boxes, although having gaps with state of the art methods. Additionally, LFFD is better than SSH that uses VGG16 as the backbone on Hard parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Running Efficiency</head><p>In this subsection, we analyse the running speed of all methods on three different platforms. The information of each platform and related libraries are listed in <ref type="table">Table 5</ref>. We use batchsize 1 and a few common resolutions for testing. For fair comparison, FaceBoxes3.2× is used here instead of FaceBoxes. The running speed is measured in ms and the corresponding FPS. The final results are presented in <ref type="table">Table 6</ref>, 7 and 8.</p><p>In <ref type="table">Table 6</ref>, we also add VGG16 and Resnet50 for sufficient comparison. SSH and S3FD are based on VGG16, having similar speed with VGG16. Whereas, Pyramid-Box is much slower due to additional complex modules, although based on VGG16 as well. DSFD can achieve state of the art accuracy, but it has the slowest running speed. The proposed LFFD runs the fastest at 3840×2160, and FaceBoxes3.2× obtains the highest speed at other three resolutions. Both LFFD and FaceBoxes3.2× can reach or even exceed the real-time running speed (&gt; 30 FPS) at the first <ref type="table">Table 5</ref>. Information of hardware platforms and related running libraries. <ref type="table">Table 6</ref>. Running efficiency on TITAN Xp. <ref type="table">Table 7</ref>. Running efficiency on TX2. three resolutions. The aforementioned trend that state of the art methods pursue higher accuracy at the cost of running speed is clearly verified. TX2 and Raspberry Pi 3 are edge devices with low computation power. DSFD, PyramidBox, S3FD and SSH are either too slow or failed to run on these two platforms. Thus, we only evaluate the proposed LFFD and FaceBoxes3.2× at lower resolutions in <ref type="table">Table 7</ref> and 8. The overall results show that LFFD is faster than FaceBoxes3.2× except for the case at 640×480 on Raspberry Pi 3. LFFD can better benefit from optimizations of ncnn than FaceBoxes3.2× at low resolutions 160×120 and 320×240. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Parameter, Computation and Model Size</head><p>We investigate the compared methods from the perspective of parameter, computation and model size in this subsection. The edge devices always have constrained storage memories. It is necessary to consider the memory usage of face detectors. The number of parameters is highly related to the model size. However, less parameters do not mean less computation. Following <ref type="bibr" target="#b18">[19]</ref>, we use FLOPs to measure the computation at resolution 640×480. All the information is presented in <ref type="table" target="#tab_4">Table 9</ref>.</p><p>For state of the art methods DSFD and PyramidBox, they have large amounts of parameters and FLOPs. The proposed LFFD and FaceBoxes3.2× have light networks which are appropriate to deploy on edge devices. To further demonstrate the efficiency of the proposed network, we define a new metric:</p><formula xml:id="formula_1">E net = F LOP s/t,<label>(2)</label></formula><p>where t indicates the running time. E net reflects the computation efficiency of networks (the larger, the more efficient) and can be calculated at a certain resolution on a specific platform. We compute this metric for LFFD and FaceBoxes3.2× at 640×480 on three platforms (LFFD vs. FaceBoxes3.2×):</p><p>• 1.22G/ms vs. 0.42G/ms on TITAN Xp; • 0.14G/ms vs. 0.04G/ms on TX2;</p><p>• 0.0022G/ms vs. 0.00088G/ms on Raspberry Pi 3;</p><p>Evidently, the proposed network has much more efficient computation, which demonstrates the superiority of the concise network design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduces a light and fast face detector that properly balances accuracy and latency. By deeply rethinking the RF in the background of face detection, we propose an anchor-free method to overcome the drawbacks of anchor-based methods. The proposed method regards the RFs as natural "anchors" which can cover continuous face scales and reach nearly 100% hit rate. After investigating the essential relations between ERFs and face scales, we delicately design an simple but efficient network with eight detecting branches. The proposed network consists of common building blocks with less filters, resulting in fast inference speed. Comprehensive and extensive experiments are conducted to fully analyse the proposed method. The final results demonstrate that our method can achieve superior accuracy with small model size and efficient computation, which makes it an excellent candidate for edge devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Authors contributed equally.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of the proposed network. The backbone has 25 convolution layers and is divided into four parts: tiny part, small part, medium part and large part. Along the backbone, there are eight loss branches which are in charge of detecting faces with different scales. The entire backbone only consists of conv 3×3, conv 1×1, ReLU and residual connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>x and RF y are center coordinates of the RF, b tl x and b tl y are coordinates of top-left corner of the bbox, b br x and b br y are coordinates of bottom-right corner of the bbox and the normalization constant is RF s /2, RF s is the RF size. The L2 loss is only activated for positive RF anchors without being ignored. In the final loss function, the two losses have the same weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Evaluation results on FDDB. Many other published methods are not displayed here for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Detailed information about the proposed network.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>RF anchors with more than one matched faces are ignored. Besides, gray scale is set for each continuous scale. Let {SL i } 8 i=1 be lower bounds of continuous scales and {SU i } 8 i=1 for upper bounds. The lower and upper gray bounds are calculated as { SL i * 0.9 } 8 i=1 and { SU i * 1.1 } 8 i=1 . For each continuous scale i, the relevant gray scales are [ SL i * 0.9 , SL i ] and [SU i , SU i * 1.1 ]. For example, branch 3 is for face scale 20-40, the corresponding gray scales are</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Performance results on the testing set of WIDER FACE. The values in () are results from the original papers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 8 .</head><label>8</label><figDesc>Running efficiency on Raspberry Pi 3 Model B+.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 9 .</head><label>9</label><figDesc>Number of parameters, FLOPs and model size. The model size may vary slightly with different libraries.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the design of cascades of boosted ensembles for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Mullin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="86" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Selective refinement network for high performance face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02693</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Some improvements on deep convolutional neural network based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5402</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>University of Massachusetts, Amherst</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face detection with the faster r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face &amp; Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="650" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Face alignment in-the-wild: A survey. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dsfd: dual shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06440</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssh: Single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiresolution grayscale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Menp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast training and selection of haar features using statistics in boosting-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pyramidbox: A context-assisted single shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="797" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01061</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Face r-cnn</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06655</idno>
		<title level="m">Deep face recognition: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Joint Conference on Biometrics</title>
		<meeting>IEEE International Joint Conference on Biometrics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting faces using inside cascaded contextual cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3171" to="3179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Improved selective refinement network for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06651</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faceboxes: A cpu real-time face detector with high accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Joint Conference on Biometrics</title>
		<meeting>IEEE International Joint Conference on Biometrics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Robust and high performance face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02350</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Seeing small faces from robust anchor&apos;s perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast human detection using a cascade of histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1491" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
