<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning CNN for Single Image De-Raining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Yasarla</surname></persName>
							<email>ryasarl1@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
							<email>vpatel36@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning CNN for Single Image De-Raining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image de-raining is an extremely challenging problem since the rainy image may contain rain streaks which may vary in size, direction and density. Previous approaches have attempted to address this problem by leveraging some prior information to remove rain streaks from a single image. One of the major limitations of these approaches is that they do not consider the location information of rain drops in the image. The proposed Uncertainty guided Multi-scale Residual Learning (UMRL) network attempts to address this issue by learning the rain content at different scales and using them to estimate the final de-rained output. In addition, we introduce a technique which guides the network to learn the network weights based on the confidence measure about the estimate. Furthermore, we introduce a new training and testing procedure based on the notion of cycle spinning to improve the final de-raining performance. Extensive experiments on synthetic and real datasets to demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods. Code is available at: https://github.com/rajeevyasarla/ UMRL--using-Cycle-Spinning</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many practical computer vision-based systems such as surveillance and autonomous driving often require processing and analysis of videos and images captured under adverse weather conditions such as rain, snow, haze etc. These weather-based conditions adversely affect the visual quality of images and as a result often degrade the performance of vision systems. Hence, it is important to develop algorithms that can automatically remove these artifacts before they are fed to a vision-based system for further processing.</p><p>In this paper, we address the problem of removing rain streaks from a single rainy image. Rain streak removal or  <ref type="bibr" target="#b34">[35]</ref> where zoomed in part shows the blurry effects on face and various rain streaks near the elbow. (c) De-rained using UMRL. (d) Rainy image. (e) De-rained using Fu et al. <ref type="bibr" target="#b6">[7]</ref> where zoomed in part shows under de-raining of the image. (f) De-rained using UMRL, zoomed in highlighted parts show the clear differences between UMRL and other compared methods.</p><p>image de-raining is a difficult problem since a rainy image may contain rain streaks which may vary in size, direction and density. A number of different techniques have been developed in the literature to address this problem. These algorithms can be clustered into two main groups -(i) video based algorithms <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16]</ref>, and (ii) single imagebased algorithms <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>. Algorithms corresponding to the first category assume temporal consistency among the image frames, and use this assumption for deraining. On the other hand, single image de-raining methods attempt to use some prior information to remove rain components from a single image <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35]</ref>. Priors such as sparsity <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21]</ref> and low-rank representation <ref type="bibr" target="#b3">[4]</ref> have been used in the literature. In particular, the method proposed by Fu et al. <ref type="bibr" target="#b6">[7]</ref> uses a priori image domain knowledge by focusing on high frequency details during training to improve the de-raining performance. However, it was shown in <ref type="bibr" target="#b34">[35]</ref>, that this method tends to remove some important parts in the de-rained image (see <ref type="figure" target="#fig_0">Figure 1</ref>(e)). Similarly, a recent work by Zhang and Patel <ref type="bibr" target="#b34">[35]</ref> uses the imagelevel priors to estimate the rain density information which is then used for de-raining. Although their approach provides the state-of-the-art results, they estimate image level priors which do not consider the location information of rain drops in the image. As a result, their algorithm tends to introduce some artifacts in the final de-rained images. These artifacts can be clearly seen from the de-rained results shown in <ref type="figure" target="#fig_0">Figure 1</ref></p><formula xml:id="formula_0">(b).</formula><p>In this paper we take a different approach to image deraining where we make use of the observation that rain streak density and direction does not change drastically with different scales. Rather than relying on the rain density information (i.e. heavy, medium or light) present in the rainy image <ref type="bibr" target="#b34">[35]</ref>, we develop a method in which the rain streak location information is taken in to consideration in a multiscale fashion to improve the de-raining performance. While providing the estimated rain content (i.e. residual map) to the subsequent layers of the network, we may end-up propagating the errors in estimations. To block the flow of incorrect estimation in rain streaks, we estimate an uncertainty metric along with the rain streak information. We use an Unet architecture with skip connections <ref type="bibr" target="#b23">[24]</ref> as our base network. The proposed network learns the residue at each level in the decoder of Unet with an uncertainty map, which indicates how confident the network is about the rain content it learned. Say there are L layers in the decoder network, the uncertainty map generated at layer "l" is given to layer "l + 1" so that the subsequent layers of "l" can discard the rain content learned by layer "l" if the confidence value is low in the uncertainty map.</p><p>Another important contribution of our work is that we propose to incorporate the cycle spinning framework of Coifman and Donoho <ref type="bibr" target="#b4">[5]</ref> into our de-raining method. Cycle spinning was originally proposed to remove the artifacts introduced by orthogonal wavelets in image de-noising. Similar to wavelets, deep learning-based methods also introduce some artifacts near the edges of the de-rained images (see <ref type="figure" target="#fig_0">Figure 1</ref>). In cycle spinning, the data is first shifted by some amount, the shifted data are then de-noised, the de-noised data are then un-shifted, and finally the un-shifted data are averaged to obtain the final de-noised result. Cycle spinning has been successfully applied to reduce the artifacts introduced near the edges in many applications including image de-blurring <ref type="bibr" target="#b5">[6]</ref> and de-noising <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Hence, we adopt it in our de-raining framework. In fact, we show that cycle spinning is a generic method that can be used to improve the performance of any deep learning-based image de-raining method. <ref type="figure" target="#fig_0">Figure 1</ref> (c) and (f) present sample results from our Un-certainty guided Multi-scale Residual Learning using cycle spinning (UMRL) network, where one can clearly see that UMRL is able to remove the noise artifacts and provides better results as compared to <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b7">[8]</ref>.</p><p>To summarize, this paper makes the following contributions:</p><p>• A novel method called UMRL is proposed which generates the rain streak content at each location of the image along with the uncertainty map that guides the subsequent layers about the rain streak information at each location.</p><p>• We incorporate cycle spinning in both training and testing phases of our network to improve the final deraining performance.</p><p>• We run extensive experiments to show the performance of UMRL against the several recent state-of the-art approaches on both synthetic and real rainy images. Furthermore, an ablation study is conducted to demonstrate the effectiveness of different parts of the proposed UMRL network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>An observed rainy image y can be modeled as the superposition of a rain component (i.e. residual map) r with a clean image x as follows y = x + r.</p><p>(1) Given y the goal of image de-raining is to estimate x. This can be done by first estimating the residual map r and then subtracting it from the observed image y. Various methods have been proposed in the literature for image de-raining <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref> including dictionary learning-based <ref type="bibr" target="#b0">[1]</ref>, Gaussian mixture-model (GMM) based <ref type="bibr" target="#b22">[23]</ref>, and low-rank representation based <ref type="bibr" target="#b18">[19]</ref> methods. In recent years, deep learning-based single image de-raining methods have also been proposed in the literature. Fu et al. <ref type="bibr" target="#b6">[7]</ref> proposed a convolutional neural network (CNN) based approach in which they directly learn the mapping relationship between rainy and clean image detail layers from data. Zhang et al. <ref type="bibr" target="#b33">[34]</ref> proposed a generative adversarial network (GAN) based method for image de-raining. Furthermore, to minimize the artifacts introduced by GANs and ensure better visual quality, a new refined loss function was also introduced in <ref type="bibr" target="#b33">[34]</ref>. Fu et al. <ref type="bibr" target="#b7">[8]</ref> presented an end-to-end deep learning framework for removing rain from individual images using a deep detail network which directly reduces the mapping range from input to output. Zhang and Patel <ref type="bibr" target="#b34">[35]</ref> proposed a density-aware multi-stream densely connected CNN for joint rain density estimation and de-raining. Their network automatically determines the rain-density information and <ref type="figure">Figure 2</ref>: An overview of the proposed UMRL network. The aim of the UMRL network is to estimate the clean image given the corresponding rainy image. To address that, UMRL learns the residual maps and computes the confidence maps to guide the network. To achieve this, we introduce RN and CN networks and feed their outputs to the subsequent layers.</p><p>then efficiently removes the corresponding rain-streaks using the estimated rain-density label. Note the methods proposed in <ref type="bibr" target="#b7">[8]</ref>, and <ref type="bibr" target="#b34">[35]</ref> showed the benefits of using multiscale networks for image de-raining. Recently, Wang et al. <ref type="bibr" target="#b27">[28]</ref> proposed a hierarchical approach based on estimating different frequency details of an image to get the de-rained image. The method proposed by Qian et al. <ref type="bibr" target="#b21">[22]</ref> generates attentive maps using the recurrent neural networks, and then uses the features from different scales to compute the loss for removing the rain drops on glasses. Note that this method was specifically designed for removing rain drops from a glass rather than removing rain streaks from an image. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17]</ref> illustrated the importance of attention based methods in low-level vision tasks. In a recent work <ref type="bibr" target="#b16">[17]</ref>, Li et al. proposed a convolutional and recurrent neural network-based method for single image de-raining which makes use of the contextual information for rain removal. It was observed in <ref type="bibr" target="#b34">[35]</ref>, that some of the recent deep learningbased methods tend to under de-rain or over de-rain the image if the rain condition present in the rainy image is not properly considered during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Unlike many deep learning-based methods that directly estimate the de-rained image from the noisy observation, we take a different approach in which we first estimate the rain streak componentr (i.e. residual map) and then use it to estimate the de-rained image asx = y −r. We define c as the confidence score which is an uncertainty map about the estimation ofr. The confidence score at each pixel is a measure of how much the network is certain about the residual value computed at each pixel. Qian et al. <ref type="bibr" target="#b21">[22]</ref> estimate an attentive map based on the rainy image using a recurrent network and then use it as a location-based information to the de-raining network. In contrast, our method combines the residual and confidence information judiciously and uses them as input to subsequent layers at higher scales. In this way, it passes the location-based rain information to the rest of the network. We estimate the residual map and its corresponding uncertainty map at three different scales, <ref type="figure">Figure 3</ref>: (a) Input rainy image, y. (b), (c), and (d) are the residual maps r ×1 , r ×2 , r ×4 at scales 1.0, 0.5 and 0.25, respectively. Note that the residual maps at different scales have the same direction and density.</p><formula xml:id="formula_1">{r ×1 , c ×1 } (at the original input size), {r ×2 , c ×2 } (at 0.5 scale of input size), and {r ×4 , c ×4 } (at 0.25 scale of input size). (a) (b) (c) (d)</formula><p>Let r ×2 (0.5 scale size of r) and r ×4 (0.25 scale size of r) be the residual maps at different scales. As can be seen from <ref type="figure">Figure 3</ref>, the residual maps r ×1 , r ×2 , and r ×4 have the same direction and density at each location of the image. To estimate these residual maps, we start with the Unet architecture <ref type="bibr" target="#b23">[24]</ref> as the base network. We use the convolutional block (ConvBlock as shown in <ref type="figure">Figure  4</ref>(a)) as the building block of our base network. The base network can be described as follows: </p><formula xml:id="formula_2">ConvBlock(3,</formula><formula xml:id="formula_3">-Conv2d(3 × 3),</formula><p>where AvgPool is the average pooling layer, UpSample is the upsampling convolution layer, and ConvBlock(i, j) indicates ConvBlock with i input channels and j output channels. A Refinement Network(RFN) is used at the end of Unet to produce de-rained image. The Refinement Network(RFN) consists of the following blocks Conv2d(7 × 7)-Conv2d(3 × 3)-tanh(), which takes y −r i as the input and generatesx i (i.e. derained image) as the output. Here, Conv2d(m × m) represents 2D convolution using the kernel of size m × m. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">UMRL Network</head><p>Rainy streaks are high frequency components and existing de-raining methods either tend to remove high frequencies that are not rain streaks or do not remove the rain near high frequency components of the clean image like edges as shown in the <ref type="figure">Figure 5</ref>. To address this issue, one can use the information about the location in image where network might go wrong in estimating the residual value. This can be done by estimating a confidence value corresponding to the estimated residual value and guide the network to remove the artifacts, especially near the edges. For example, we can observe clearly from <ref type="figure">Figure 5</ref> that the residual map and its corresponding confidence map were able to capture the regions where there is high probability of incorrect estimates. We estimate the residual value and its corresponding confidence map at different scales (1.0(×1), 0.5(×2) and 0.25(×4)) of the input size. This information is then fed back to the subsequent layers so that the network can learn the residual value at each location, given the computed residual value and confidence value at a lower scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Residual and Confidence Map Networks</head><p>Feature maps at different scales such as ×2 and ×4 are given as input to the Residual Network (RN) to estimate the residual map at the corresponding scale as shown in the <ref type="figure">Figure 2</ref>. RN consists of the following sequence of convolutional layers, Convblock(64,32)-Convblock(32,32)-Convblock <ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b2">3)</ref> as shown in <ref type="figure">Figure 4(b)</ref>. We use the estimated residual map and the feature maps as input to the Confidence map Network (CN) to compute the confidence measure at every pixel, which indicates how sure the network is about the residual value at each pixel. CN consists of the following sequence of convolutional layers, Convblock(67,16)-Convblock(16,16)-Convblock <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b2">3)</ref> as shown in the <ref type="figure">Figure 4(c)</ref>. Given the estimated residual map and the corresponding feature maps as input to the confidence map network, it estimates c ×4 and c ×2 . The element wise product ofr i and c i is computed, and up-sampled to pass it as an input to the subsequent layer of the UMRL network as shown in <ref type="figure">Figure 2</ref> for i ∈ {×2, ×4}. Given the output residual map r ×1 and the feature maps of the final layer of UMRL as input to CN, we get c ×1 . We compute the de-rained image at different scale aŝ</p><formula xml:id="formula_4">x i = RFN(y i −r i ),<label>(2)</label></formula><p>where RFN is the Refinement Network, y i andx i are the input rainy image and the output de-rained image at scales, i ∈ {×1(1.0), ×2(0.5), ×4(0.25)}. We use the confidence guided loss and the preceptual loss to train our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Loss for UMRL</head><p>We use the confidence to guide the residual learning in the training stage of UMRL network. We define the confidence guided loss as,</p><formula xml:id="formula_5">L l = i∈{×1,×2,×4} (c i x i ) − (c i x i ) 1 , L c = i∈{×1,×2,×4} j k log(c i jk ) , L u = L l − λ 1 L c ,<label>(3)</label></formula><p>where is the element wise product. Here, L l tries to minimize the L1-norm betweenx i and x i and also the value of c i jk . On the other hand, L c tries to increase c i jk by making it close to 1. A trivial solution for L l can be seen as c i jk = 0 ∀ i, j, k. To avoid this, we construct L u as a linear combination of L l and L c , where L c acts as a regularizer to avoid the trivial solution. Similar loss has been used for classification and regression tasks in methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. However, to the best of our knowledge ours is the first attempt to use this kind of loss in image restoration tasks. Inspired by the importance of the perceptual loss in many image restoration tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>, we use it to further improve the visual quality of the de-rained images. The perceptual loss is feature based loss, and in our case, extracted features from layer relu1 2 of pretrained network VGG-16 <ref type="bibr" target="#b25">[26]</ref>, and computed perceptual loss similar to method proposed in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref>. Let F (.) denote the features obtained using the VGG16 model <ref type="bibr" target="#b25">[26]</ref>, then the perceptual loss is defined as follows</p><formula xml:id="formula_6">L p = 1 N HW i j k F (x 1 ) i,j,k − F (x 1 ) i,j,k 2 2 ,<label>(4)</label></formula><p>where N is the number of channels of F (.), H is the height and W is the width of feature maps. The overall loss used to train the UMRL network is,  </p><formula xml:id="formula_7">L = L l − λ 1 L c + λ 2 L p ,<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cycle Spinning</head><p>As discussed earlier, cycle spinning was originally proposed to minimize the artifacts near the edges introduced by the orthogonal wavelets when de-noising images <ref type="bibr" target="#b4">[5]</ref>. In this work, we adapt this idea to further improve the de-raining performance of UMRL. <ref type="figure" target="#fig_3">Figure 6</ref> gives an overview of cycle spinning using UMRL. Let T cs (., p, q) be the function to shift an image cyclically by p rows and q columns. Given an image of size m × n, we shift the image cyclically in steps of p rows and q columns to get the shifted images as shown in the <ref type="figure" target="#fig_3">Figure 6</ref>. We then de-rain the shifted images using the UMRL network, inverse shift and average them to get the final de-rained image during testing. <ref type="figure">Figure 7</ref> shows an example of cyclically spun input images and the corresponding de-rained images. By applying cycle spinning to our method, we are able to remove some artifacts introduced by the original UMRL network. In particular, as will be shown later, cycle spinning can be applied to any CNN-based de-raining method to further improve its performance. <ref type="figure">Figure 7</ref>: Cyclically spinned images with (a) p = 100, q = 200, (b) p = 0, q = 200, and (c) p = 300, q = 400. (d),(e),(f) are the corresponding de-rained images using UMRL.</p><formula xml:id="formula_8">(a) (b) (c) (d) (e) (f)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we evaluate the performance of our method on both synthetic and real images. Peak-Signalto-Noise Ratio (PSNR) and Structural Similarity index (SSIM) <ref type="bibr" target="#b28">[29]</ref> measures are used to compare the performance of different methods on synthetic images. We visually inspect the performance of different methods on real images, as we don't have the ground truth clean images. The performance of the proposed UMRL method is compared against several recent state-of-the-art algorithms such as (a) Gaussian mixture model (GMM) based <ref type="bibr" target="#b17">[18]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training and Testing Details</head><p>The UMRL network is trained using the synthetic image datasets created by the authors of <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref>. The dataset in <ref type="bibr" target="#b34">[35]</ref> consists of 12000 images with different rain levels like low, medium and high. The dataset in <ref type="bibr" target="#b33">[34]</ref> contains 700 training images. The (y, x) rainy-clean image pairs are shifted randomly p rows and q columns using T cs (., p, q) to obtain y s , x s , respectively. The shifted pairs (y s , x s ) are used to train UMRL using the loss L. The Adam optimizer with the batch size of 1 is used to train the network. Learning rate is set to 0.001 for first 10 epochs and 0.0001 for the remaining epochs. During training initially λ 1 and λ 2 are set equal to 0.1 and 1.0, respectively, but when the mean of all values in the confidences maps c ×1 , c ×2 and c ×4 is greater than 0.8 then λ 1 is set equal to 0.03. UMRL is trained for 30 epochs that is a total of 30 × 12700 iterations.</p><p>Similar to the previous approaches <ref type="bibr" target="#b34">[35]</ref>, we evaluate the performance of UMRL using the datasets Test-1 containing 1200 images from <ref type="bibr" target="#b34">[35]</ref>, and Test-2 containing 1000 images from <ref type="bibr" target="#b6">[7]</ref>. We use the real-world rainy images provided by Zhang et al. <ref type="bibr" target="#b33">[34]</ref> and Yang et al. <ref type="bibr" target="#b30">[31]</ref> for testing UMRL based cycle spinning method. The testing images are cyclically shifted in steps of 50 rows and 50 columns using T cs (., ., .) and fed as input to UMRL for de-raining, further these de-rained images are inverse shifted and averaged to get the final output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We study the performance of each block's contribution to the UMRL network by conducting extensive experiments on the test datasets. We start with the Unet-based base network (BN) and then add one component at a time to see the significance each component brings to the network in estimating the final de-rained image. <ref type="table" target="#tab_1">Table 1</ref>, shows the contribution of each block on the UMRL network. Note that BN and BN+RN are trained using a linear combination of L1-norm and L p as loss (L1+L p ). The UMRL is trained using the overall loss, L. It can be seen from <ref type="table" target="#tab_1">Table 1</ref> that as more components (i.e RN and CN) are being added to the base network, the performance improves significantly. The base network, BN itself produces poor results. However, when RN is added to BN, the performance improves significantly. In particular, BN+RN is already able to produce results that are comparable to DID-MDN <ref type="bibr" target="#b34">[35]</ref>. The combination of BN, RN and CN (i.e UMRL) produces the best results. Furthermore, by comparing the last two columns of <ref type="table" target="#tab_1">Table 1</ref> we see that cycle spinning further improves the performance of UMRL. Using cycle spinning, we are able to gain the performance improvement of approximately 0.3 dB on both datasets as it was able to remove the artifacts near edges. From the <ref type="figure" target="#fig_6">Figure 9</ref> by zooming-in, we can clearly observe the cycle spinning is helping the method to remove small rain streaks in the sky and on the edges of building. We preformed similar experiments to see how much improvement cycle spinning brings over DDN <ref type="bibr" target="#b7">[8]</ref> and DID-MDN <ref type="bibr" target="#b34">[35]</ref>. In general, we observe approximately 0.25 dB gain in the performance with cycle spinning compared to without cycle spinning as shown in <ref type="table" target="#tab_2">Table 2</ref>.  <ref type="figure">Figure 8</ref> illustrates that confidence map is guiding the network to learn the rain content at the edges and texture regions clearly by imposing low confidence values. From <ref type="figure">Figure 8</ref> by looking at the histograms of confidence maps at different scales, we can observe that as the scale is increasing the confidence values are approaching 1 at most of the pixels. This behavior is expected since at lower scales, the rain streaks will be blurry (see <ref type="figure">Figure 3</ref>) and the network is less confident about the values it estimates. This explains why UMRL tries to increase the confidence value by estimating accurate residual maps, in return CN is computing and feeding back the possible areas where UMRL goes wrong. Ground Truth <ref type="figure" target="#fig_0">Figure 10</ref>: De-rained results on synthetic datasets Test-1 and Test-2 consisting different rain levels (low, medium and heavy) and different directions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Synthethic Test Images</head><p>The proposed UMRL method based on cycle spinning is compared against the state-of-the-art algorithms qualitatively and quantitatively. <ref type="table" target="#tab_3">Table 3</ref> shows the quantitative performance of our method. As it can be seen from this table, our method clearly out-performs the present state-of-the-art algorithms. Furthermore, we compare our method against a recent ECCV'18 method called REcurrent SE Context Aggregation Net (RESCAN) <ref type="bibr" target="#b16">[17]</ref> using the Rain800 dataset containing 100 images from <ref type="bibr" target="#b33">[34]</ref>. The PSNR and SSIM values achieved by RESCAN <ref type="bibr" target="#b16">[17]</ref>    <ref type="figure" target="#fig_0">Figure 10</ref> shows the qualitative performance of different methods on three sample images from Test-1 and Test-2 datasets. Though Fu et al. (TIP'17) <ref type="bibr" target="#b6">[7]</ref> is able to remove some rain streaks, it is unable to remove all the rain components. DDN <ref type="bibr" target="#b7">[8]</ref> is over de-raining on some images and on others it is slightly under de-raining as shown in the third column of <ref type="figure" target="#fig_0">Figure 10</ref>. DID-MDN <ref type="bibr" target="#b34">[35]</ref> is over de-raining as shown in the fourth column of <ref type="figure" target="#fig_0">Figure 10</ref> where it removes the texture on wooden wall, edges of the building in second image. Furthermore, it blurs the edges of water tank in the fourth image. By comparing third and fourth images of the fourth column, we see that the outputs of DID-MDN <ref type="bibr" target="#b34">[35]</ref> has a small blurred version of the residual streaks in the sky of those images. Visually we can see in the fifth column of <ref type="figure" target="#fig_0">Figure 10</ref>, our method produces images without any artifacts. For example in (i) it is able to recover the texture on wooden wall, in (ii) it is able to produce images with clear sky in the third and fourth images of fifth column, and in (iii) it is able to produce the sharp edges in second and fourth images.</p><p>To de-rain an image of size 512×512, on average UMRL takes about 0.05 seconds, and UMRL with cycle spinning takes about 5.1 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Real-World Rainy Images</head><p>We conducted experiments on the real-world images provided by <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref>. Results are shown in <ref type="figure" target="#fig_0">Figure 11</ref>. Similar to the results obtained on synthetic images, we observe the same trend of either over de-raining or under de-raining by the other methods. On the other hand, our method is able to remove rain streaks while preserving details of objects in the resultant output images. For example, the background and man's face in the first image of the fifth column is more clear than the outputs from other methods. Also, Trees and plants in the second image of the fifth column, front man's face and t-shirt collar in the third image are visually more clear than the results from other method. All of these experiments clearly show that our method can handle different levels of rain (low, medium and high) with different shapes and scales. More results on synthetic and real-world images are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel UMRL method based on cycle spinning to address the single image de-raining problem. In our approach, we introduced uncertainty guided residual learning where the network tries to learn the residual maps and the corresponding confidence maps at different scales which were then fed back to the subsequent layers to guide the network. In addition to UMRL, we analyzed the benefits of using cycle spinning in de-raining using various recently proposed deep de-raining networks. Extensive experiments showed that UMRL is robust enough to handle different levels of rain content for both synthetic and real-world rainy images. and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sample image de-raining results. (a) Rainy image. (b) De-rained using DID-MDN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>(a) Convolutional block (ConvBlock). BN -batchnormalization, ReLU -Rectified Linear Units, Conv2d(m × m) -convolutional layer with kernel of size m × m. (b) Residual Network (RN). (c) Confidence map Network (CN). (a) Input rainy image, y. (b) De-rained image using the base network. (c) De-rained using [35]. (d) Derained using the proposed UMRL method. (e) The residual map. (f) The confidence map at scale 1.0(×1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>where λ 1</head><label>1</label><figDesc>and λ 2 are two parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>The idea behind cycle spinning using the UMRL network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(CVPR16) (b) Fu et al.[7] CNN method (TIP'17), (c) Joint Rain Detection and Removal (JORDER) [31](CVPR17), (d) Deep detailed Network (DDN)[8] (CVPR'17) (e) Zhu et al. [37] (JBO) (ICCV17) (f) Density-aware Image Deraining method using a Multistream Dense Network (DID-MDN) [35] (CVPR'18).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>PSNR: 15 Figure 8 :</head><label>158</label><figDesc>.3 SSIM: 0.71 PSNR:24.5 SSIM: 0.87 PSNR:26.9 SSIM: 0.92 (a) Input rainy image. (b) De-rained image using BN + RN. (c) De-rained using BN + RN + CN (UMRL). (d),(e), and (f) are the corresponding confidence maps at scales ×4, ×2, ×1. (g),(h), and (i) are the corresponding normalized histograms, that is sum of all bin values is equal to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>De-rained images using (a) DDN [8], (b) DID-MDN [35], (c) UMRL, and (d) UMRL + cycle spinning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>PSNR: 28</head><label>28</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>De-rained results on sample real-world images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>32)-AvgPool-ConvBlock(32,32)-AvgPool-Convblock(32,32)-AvgPool-ConvBlock(32,32)-AvgPool-ConvBlock(32,32)-UpSample-ConvBlock(64,32)-UpSample-ConvBlock(67,32)-UpSample-ConvBlock(67,16)-ConvBlock<ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b15">16)</ref></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>PSNR and SSIM (PSNR|SSIM) results corresponding to the ablation study.</figDesc><table><row><cell>Dataset</cell><cell>Rainy Image</cell><cell>DID-MDN [35]</cell><cell>BN</cell><cell>BN+RN</cell><cell>BN+RN+CN (UMRL)</cell><cell>UMRL+ cycle spinning</cell></row><row><cell cols="2">Test-1 21.15|0.77</cell><cell>27.95|0.91</cell><cell cols="2">24.25|0.83 27.65|0.87</cell><cell>29.42|0.91</cell><cell>29.77|0.92</cell></row><row><cell cols="2">Test-2 19.31|0.77</cell><cell>26.08|0.90</cell><cell cols="2">23.32|0.83 25.88|0.87</cell><cell>26.47|0.91</cell><cell>26.67|0.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>PSNR and SSIM (PSNR|SSIM) results corresponding to the ablation study regarding the use of cycle spinning.</figDesc><table><row><cell>Dataset</cell><cell>Rainy Image</cell><cell>DDN [8]</cell><cell>DDN [8] + cycle spinning</cell><cell>DID-MDN [35]</cell><cell>DID-MDN [35] + cycle spinning</cell><cell>UMRL</cell><cell>UMRL+ cycle spinning</cell></row><row><cell cols="3">Test-1 21.15|0.77 27.33|0.90</cell><cell>27.52|0.91</cell><cell>27.95|0.91</cell><cell>28.19|0.91</cell><cell>29.42|0.91</cell><cell>29.77|0.92</cell></row><row><cell cols="3">Test-2 19.31|0.77 25.63|0.88</cell><cell>25.90|0.89</cell><cell>26.08|0.90</cell><cell>26.37|0.91</cell><cell>26.47|0.91</cell><cell>26.67|0.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>PSNR and SSIM comparison of UMRL against state-of-art methods (PSNR|SSIM))</figDesc><table><row><cell>Dataset</cell><cell>Rainy Image</cell><cell>GMM based [18](CVPR'16)</cell><cell>Fu et al. [7](TIP'17)</cell><cell>JORDER [31](CVPR'17)</cell><cell>DDN [8](CVPR'17)</cell><cell>JBO [37](ICCV'17)</cell><cell>DID-MDN [35](CVPR'18)</cell><cell>UMRL+ cycle spinning</cell></row><row><cell cols="2">Test-1 21.15|0.77</cell><cell>22.75|0.84</cell><cell>22.07|0.84</cell><cell>24.32|0.86</cell><cell>27.33|0.90</cell><cell>23.05|0.85</cell><cell>27.95|0.91</cell><cell>29.77|0.92</cell></row><row><cell cols="2">Test-2 19.31|0.77</cell><cell>22.60|0.81</cell><cell>19.73|0.83</cell><cell>22.26|0.84</cell><cell>25.63|0.88</cell><cell>22.45|0.84</cell><cell>26.08|0.90</cell><cell>26.67|0.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>are 24.37 and 0.84, whereas our method achieved 24.59 and 0.87, respectively.</figDesc><table><row><cell>Rainy Image</cell><cell>Fu et al.</cell><cell>DDN</cell><cell>DID-MDN</cell><cell>Ours</cell></row><row><cell></cell><cell>[7](TIP'17)</cell><cell>[8](CVPR'17)</cell><cell>[35](CVPR'18)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H S</forename><surname>Bhadauria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M L</forename><surname>Dewal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning(ICML)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analysis of effect of cycle spinning on wavelet-and curvelet-based denoising methods on brain ct images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H S</forename><surname>Bhadauria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M L</forename><surname>Dewal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selflearning based image decomposition with applications to single image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duan-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien</forename><surname>Cheng Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Wei</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1430" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A generalized low-rank appearance model for spatio-temporally correlated rain streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="1968" to="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translation-invariant denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D L</forename><surname>R R Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets and Statistics</title>
		<editor>Antoniadis A., Oppenheim G.</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="125" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast wavelet algorithm for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Raimondo</surname></persName>
		</author>
		<ptr target="http://anziamj.austms.org.au/V46/CTAC2004/Dono" />
	</analytic>
	<monogr>
		<title level="m">Proc. of 12th Computational Techniques and Applications Conference CTAC-2004</title>
		<editor>Rob May and A. J. Roberts</editor>
		<meeting>of 12th Computational Techniques and Applications Conference CTAC-2004</meeting>
		<imprint>
			<date type="published" when="2005-03" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="29" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clearing the skies a deep network architecture for single-image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2944" to="2956" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Removing rain from single images via a deep detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1715" to="1723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision and rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selflearning based image decomposition with applications to single image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y C F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="83" to="93" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for realtime style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision(ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic single-imagebased rain streaks removal via image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1742" to="1755" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video rain streak removal by multiscale convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent squeeze-and-excitation context aggregation net for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision(ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="262" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rain streak removal using layer priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Erase or fill? deep joint recurrent rain removal and reconstruction in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Removing rain from a single image via discriminative sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision(ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3397" to="3405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attentive generative adversarial network for raindrop removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speaker verification using adapted gaussian mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert B</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="19" to="41" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)&quot;, volume &quot;9351&quot; of &quot;LNCS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Utilizing local phase information to remove rain from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Santhaseelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayan</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A hierarchical approach for rain or snow removing in a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3936" to="3950" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attngan: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep joint rain detection and removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1357" to="1366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-style generative network for real-time transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06953</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<title level="m">Convolutional sparse and lowrank coding-based rain streak removal. 7 IEEE Winter Conference In Applications of Computer Vision(WACV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1259" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Image de-raining using a conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Density-aware single image de-raining using a multi-stream dense network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<idno>abs/1802.07412</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wee Kheng Leow, and Teck Khim Ng. Rain removal in video by combining temporal and chromatic properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyi</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="461" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint bilayer optimization for single-image rain streak removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2536" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
