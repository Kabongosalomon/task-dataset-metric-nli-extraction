<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Video Instance Segmentation with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
							<email>wangyuqing06@meituan.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
							<email>shenhao04@meituan.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meituan</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Video Instance Segmentation with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: https://git.io/VisTR</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CNN transformer encoder-decoder no object sequence of images sequence of multiple image features sequence of object predictions Figure 1 -Overall pipeline of VisTR. The model takes a sequence of images as input and outputs a sequence of instance predictions.</p><p>Here same shapes represent predictions in one image, and same colors represent predictions of the same object instance. Note that the overall predictions follow the input frame order, and the order of object predictions for different images keeps the same (Best viewed on screen).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Video instance segmentation (VIS) is the task that requires simultaneously classifying, segmenting and tracking object instances of interest in video. Recent methods typically develop sophisticated pipelines to tackle this task. Here, we propose a new video instance segmentation framework built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel sequence decoding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly. At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspective of similarity learning, thus considerably simplifying the overall pipeline and is significantly different from existing approaches.</p><p>Without bells and whistles, VisTR achieves the highest * Corresponding author.</p><p>speed among all existing VIS models, and achieves the best result among methods using single model on the YouTube-VIS dataset. For the first time, we demonstrate a much simpler and faster video instance segmentation framework built upon Transformers, achieving competitive accuracy. We hope that VisTR can motivate future research for more video understanding tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref> <p>-Overall pipeline of VisTR. The model takes a sequence of images as input and outputs a sequence of instance predictions. Here same shapes represent predictions in one image, and same colors represent predictions of the same object instance. Note that the overall predictions follow the input frame order, and the order of object predictions for different images keeps the same (Best viewed on screen).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Video instance segmentation (VIS) is the task that requires simultaneously classifying, segmenting and tracking object instances of interest in video. Recent methods typically develop sophisticated pipelines to tackle this task. Here, we propose a new video instance segmentation framework built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel sequence decoding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly. At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspective of similarity learning, thus considerably simplifying the overall pipeline and is significantly different from existing approaches.</p><p>Without bells and whistles, VisTR achieves the highest</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance segmentation is one of the fundamental tasks in computer vision. While significant progress has been witnessed in instance segmentation of images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>, much less effort was spent on segmenting instances in videos. Here we propose a new video instance segmentation framework built upon Transformers. Video instance segmentation (VIS), recently proposed in <ref type="bibr" target="#b29">[30]</ref>, requires one to simultaneously classify, segment and track object instances of interest in a video sequence. It is more challenging in that one needs to perform instance segmentation for each indi-vidual frame and at the same time to establish data association of instances across consecutive frames, a.k.a., tracking.</p><p>State-of-the-art methods typically develop sophisticated pipelines to tackle this task. Top-down approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref> follow the tracking-by-detection paradigm, relying heavily on image-level instance segmentation models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> and complex human-designed rules to associate the instances. Bottom-up approaches <ref type="bibr" target="#b0">[1]</ref> separate object instances by clustering learned pixel embeddings. Due to heavy reliance on the dense prediction quality, these methods often need multiple steps to generate the masks iteratively, which makes them slow. Thus, a simple, end-to-end trainable VIS framework is highly desirable.</p><p>Here, we take a deeper look at the video instance segmentation task. Video frames contain richer information than single images such as motion patterns and temporal consistency of instances, offering useful cues for instance segmentation, and classification. At the same time, the better learned instance features can help tracking of instances. In essence, the instance segmentation and instance tracking are both concerned with similarity learning: instance segmentation is to learn the pixel-level similarity and instance tracking is to learn the similarity between instances. Thus, it is natural to solve these two sub-tasks in a single framework and benefit each other. Here we aim to develop such an end-to-end VIS framework. The framework needs to be simple and achieves strong performance without whistles and bells. To this end, we propose to employ the Transformers <ref type="bibr" target="#b22">[23]</ref>. Importantly, for the first time we demonstrate that, as the Transformers provide building blocks, it enables one to design a simple and clean framework for VIS, and possibly for a much wider range of video processing tasks in computer vision. Thus potentially, it is possible to unify most vision tasks of different input modalitiessuch as image, video and point clouds processing-into the Transformer framework. Transformers are widely used for sequence to sequence learning in NLP <ref type="bibr" target="#b22">[23]</ref>, and start to show promises in vision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. Transformers are capable of modeling long-range dependencies, and thus can be naturally applied to video for learning temporal information across multiple frames. In particular, the core mechanism of Transformers, self-attention, is designed to learn and update the features based on all pairwise similarities between them. The above characteristics of Transformers make them great candidates for the VIS task.</p><p>In this paper, we propose the Video Instance Segmentation TRansformer (VisTR), which views the VIS task as a parallel sequence decoding/prediction problem. Given a video clip that consists of multiple image frames as input, the VisTR outputs the sequence of masks for each instance in the video in order directly. The output sequence for each instance is referred to as instance sequence in this paper. The overall VisTR pipeline is illustrated in <ref type="figure">Fig. 1</ref>. In the first stage, given a sequence of video frames, a standard CNN module extracts features of individual image frames, then the multiple image features are concatenated in the frame order to form the clip-level feature sequence. In the second stage, the Transformer takes the clip-level feature sequence as input, and outputs a sequence of object predictions in order. In <ref type="figure">Fig. 1</ref> same shapes represent predictions for the same image, and the same colors represent the same instance of different images. The sequence of predictions follow the order of input images, and the predictions of each image follows the same instance order. Thus, instance tracking is achieved seamlessly and naturally in the same framework of instance segmentation.</p><p>To achieve this goal, there are two main challenges: 1) how to maintain the order of outputs and 2) how to obtain the mask sequence for each instance out of the Transformer network. Correspondingly, we introduce the instance sequence matching strategy and the instance sequence segmentation module. The instance sequence matching performs bipartite graph matching between the output instance sequence and the ground-truth instance sequence, and supervises the sequence as a whole. Thus, the order can be maintained directly. The instance sequence segmentation accumulates the mask features for each instance across multiple frames through self-attention and segments the mask sequence for each instance through 3D convolutions.</p><p>Our main contributions are summarized as follows.</p><p>• We propose a new video instance segmentation framework built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel sequence decoding/prediction problem. The framework is significantly different from existing approaches, considerably simplifying the overall pipeline. • VisTR solves the VIS from a new perspective of similarity learning. Instance segmentation is to learn the pixel-level similarity and instance tracking is to learn the similarity between instances. Thus, instance tracking is achieved seamlessly and naturally in the same framework of instance segmentation. • The key to the success of VisTR is a new strategy for instance sequence matching and segmentation, which is tailored for our framework. This carefully-designed strategy enables us to supervise and segment instances at the sequence level as a whole. • VisTR achieves strong results on the YouTube-VIS dataset, achieving 40.1% in mask mAP at the speed of 57.7 FPS , which is the best and fastest among methods that use a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Video object segmentation. VOS <ref type="bibr" target="#b17">[18]</ref> is closely related to VIS. Analogue to object tracking, which is detecting boxes of foreground objects in a class-agnostic fashion, VOS is segmenting masks of foreground class-agnostic objects. Same as in tracking, usually one is allowed to use only the first few frames' annotations for training. In contrast, VIS requires to segment and track all instance masks of a fixed category set of objects in a video sequence.</p><p>Video instance segmentation. The VIS task <ref type="bibr" target="#b29">[30]</ref> requires classifying, segmenting instances in each frame and linking the same instance across frames. State-of-the-art methods typically develop sophisticated pipelines to tackle it. Mask-Track R-CNN <ref type="bibr" target="#b29">[30]</ref> extends the Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> with a tracking branch and external memory that saves the features of instances across multiple frames. Maskprop <ref type="bibr" target="#b1">[2]</ref> builds on the Hybrid Task Cascade Network <ref type="bibr" target="#b5">[6]</ref>, and re-uses the predicted masks to crop the extracted features, then propagates them temporally to improve the segmentation and tracking. STEm-Seg <ref type="bibr" target="#b0">[1]</ref> proposes to model video clips as 3D spacetime volumes and then separates object instances by clustering learned embeddings. Note that the above approaches either rely on complex heuristic rules to associate the instances or require multiple steps to generate and optimize the masks iteratively. In contrast, here we aim to build a simple and end-to-end trainable VIS framework.</p><p>Transformers. Transformers were first proposed in <ref type="bibr" target="#b22">[23]</ref> for the sequence-to-sequence machine translation task, and since then have become the de facto method in most NLP tasks. The core mechanism of Transformers, self-attention, makes it particularly suitable for modeling long-range dependencies. Very recently, Transformers start to show promises in solving computer vision tasks. DETR <ref type="bibr" target="#b3">[4]</ref> builds an object detection systems based on Transformers, which largely simplifies the traditional detection pipeline, and achieves on par performances compared with highlyoptimized CNN based detectors <ref type="bibr" target="#b18">[19]</ref>. Our work here is inspired by DETR. ViT <ref type="bibr" target="#b7">[8]</ref> introduces the Transformer to image recognition and models an image as a sequence of patches, which attains excellent results compared to stateof-the-art convolutional networks. The above works show the effectiveness of Transformers in image understanding tasks. To our knowledge, thus far there are no prior applications of Transformers to video instance segmentation. It is intuitive to see that the Transformers' advantage of modeling long-range dependencies makes it an ideal candidate for learning temporal information across multiple frames for video understanding tasks. Here, we propose the VisTR method and provide an affirmative answer to that. As the original Transformers are auto-regressive models, which generate output tokens one by one, for efficiency, VisTR employs a non-auto-regressive variant of the Transformer to achieve parallel sequence generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method: VisTR</head><p>We tackle the video instance segmentation task by modeling it as a direct sequence prediction problem. Given a video clip that consists of multiple image frames as input, the VisTR outputs the sequence of masks for each instance in the video in order. To achieve this goal, we introduce the instance sequence matching and segmentation strategy to supervise and segment the instances at the sequence level as a whole. In this section, we first introduce the overall architecture of the proposed VisTR in Sec. 3.1, then the details of the instance sequence matching and segmentation module will be described in Sec. 3.2 and Sec. 3.3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">VisTR Architecture</head><p>The overall VisTR architecture is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. It contains four main components: a CNN backbone to extract compact feature representations of multiple frames, an encoder-decoder Transformer to model the similarity of pixel-level and instance-level features, an instance sequence matching module for supervising the model, and an instance sequence segmentation module. Backbone. The backbone extracts the original pixel-level feature sequence of the input video clip. Assume that the initial video clip with T frames of resolution H 0 × W 0 is denoted by x clip ∈ R T ×3×H0×W0 . First, a standard CNN backbone generates a lower-resolution activation map for each frame, then the features for each frame are concatenated to form the clip level feature map f 0 ∈ R T ×C×H×W . Transformer encoder. The Transformer encoder is employed to model the similarities among all the pixel level features in the clip. First, a 1×1 convolution is applied to the above feature map, reducing the dimension from C to d (d &lt; C), resulting in a new feature map f 1 ∈ R T ×d×H×W . To form a clip level feature sequence that can be fed into the Transformer encoder, we flatten the spatial and temporal dimensions of f 1 into one dimension, resulting in a 2D feature map of size d × (T · H · W ). Note that the temporal order is always in accordance with that of the initial input. Each encoder layer has a standard architecture that consists of a multi-head self-attention module and a fully connected feed forward network (FFN). Temporal and spatial positional encoding. The Transformer architecture is permutation-invariant, while the segmentation task requires precise position information. To compensate for this, we supplement the features with fixed positional encodings information that contains the three dimensional (temporal, horizontal and vertical) positional information in the clip. Here we adapt the positional encoding in the original Transformer <ref type="bibr" target="#b22">[23]</ref> for our 3D case. Specifically, for the coordinates of each dimension we independently use d /3 sine and cosine functions with different fre-  quencies:</p><formula xml:id="formula_0">PE(pos, i) = sin pos · ω k , for i = 2k, cos pos · ω k , for i = 2k + 1;<label>(1)</label></formula><p>where ω k = 1/10000 2k/ d 3 ; 'pos' is the position in the corresponding dimension. Note that the d should be divisible by 3, as the positional encodings of the three dimensions should be concatenated to form the final d channel positional encoding. These encodings are added to the input of each attention layer. Transformer decoder. The Transformer decoder aims to decode the top pixel features that can represent the instances of each frame, which is called instance level features. Motivated by DETR <ref type="bibr" target="#b3">[4]</ref>, we also introduce a fixed number of input embeddings to query the instance features from pixel features, termed as instance queries. Suppose that the model decodes n instances each frame, then for T frames the instance query number is N = n · T . The instance queries are learned by the model and have the same dimension with the pixel features. Taking the output of encoder E and N instance queries Q as input, the Transformer decoder outputs N instance features, denoted by O in <ref type="figure" target="#fig_0">Fig. 2</ref>. The overall predictions follow the input frame order, and the order of instance predictions for different images is the same. Thus, the tracking of instances in different frames could be realized by linking the items of the corresponding indices directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Instance Sequence Matching</head><p>VisTR infers a fixed-size sequence of N predictions, in a single pass through the decoder. One of the main challenges for this framework is to maintain the relative position of predictions for the same instance in different images, a.k.a., in-stance sequence. In order to find the corresponding ground truth and supervise the instance sequence as a whole, we introduce the instance sequence matching strategy.</p><p>As the VisTR decode n instances each frame, the number of instance sequence is also n. Let us denote byŷ = {ŷ i } n i=1 the predicted instance sequences, and y the ground truth set of instance sequences. Assuming n is larger than the number of instances in the video clip, we consider y also as a set of size n padded with ∅. In order to find a bipartite graph matching between the two sets, we search for a permutation of n elements σ ∈ S n with the lowest cost:</p><formula xml:id="formula_1">σ = arg min σ∈Sn n i L match y i ,ŷ σ(i)<label>(2)</label></formula><p>where L match y i ,ŷ σ(i) is a pair-wise matching cost between ground truth y i and an instance sequence prediction with index σ(i). The optimal assignment could be computed efficiently by the Hungarian algorithm <ref type="bibr" target="#b10">[11]</ref>, following prior work (e.g., <ref type="bibr" target="#b20">[21]</ref>). As computing the mask sequence similarity directly is computationally intensive, we find a surrogate, the box sequence to perform the matching. To obtain the box predictions, we apply a 3-layer feed forward network (FFN) with ReLU activation function and a linear projection layer to the object predictions O of Transformer decoder. Following the same practice of DETR <ref type="bibr" target="#b3">[4]</ref>, the FFN predicts the normalized center coordinates, height and width of the box w.r.t. input image, and the linear layer predicts the class label using a softmax function. We also add a "background" class to represent that no object is detected.</p><p>Given the N = n · T bounding box predictions for the object predictions sequence, we could associate n box sequences for each instance by their indices, referred to as ins1 box seq...ins4 box seq in <ref type="figure" target="#fig_0">Fig. 2</ref>. The matching loss takes both the class predictions and the similarity of predicted and ground truth boxes into account. Each element i of the ground truth set can be seen as</p><formula xml:id="formula_2">y i = {(c i , c i ..., c i ), (b i,0 , b i,1 ..., b i,T )}<label>(3)</label></formula><p>where c i is the target class label (which may be ∅) for this instance, and b i,t ∈ [0, 1] 4 is a vector that defines ground truth box center coordinates and its relative height and width in the frame t. T represent the number of input frames. Thus, for the predictions of instance with index σ(i) we denoted the probability of class c i aŝ</p><formula xml:id="formula_3">p (σ(i)) (c i ) = {p (σ(i),0) (c i )...,p (σ(i),T ) (c i )}<label>(4)</label></formula><p>and the predicted box sequence aŝ</p><formula xml:id="formula_4">b σ(i) = b (σ(i),0) ,b (σ(i),1) ...,b (σ(i),T )<label>(5)</label></formula><p>With the above notation, we define</p><formula xml:id="formula_5">L match y i ,ŷ σ(i) = −p σ(i) (c i ) + L box b i ,b σ(i) ,<label>(6)</label></formula><p>where c i = ∅. Based on the above criterion, we could find the one-to-one matching of the sequences by the Hungarian algorithm. Given the optimal assignment, we could compute the loss function, the Hungarian loss for all pairs matched in the previous step. The loss is a linear combination of a negative log-likelihood for class prediction, a box loss and mask loss for the instance sequences:</p><formula xml:id="formula_6">L Hung (y,ŷ) = N i=1 (− logpσ (i) (c i )) + L box (b i ,bσ(i)) + L mask (m i ,mσ(i)) .<label>(7)</label></formula><p>Here c i = ∅, andσ is the optimal assignment computed in Eq. (2). The Hungarian loss is used to train the whole framework.</p><p>The second part of the matching cost and the Hungarian loss is L box that scores the bounding boxes. We use a linear combination of the sequence level L 1 loss and the generalized IOU loss <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_7">L box b i ,b σ(i) = 1 T T t=1 λ iou · L iou b i,t ,b σ(i),t + λ L1 b i,t −b σ(i),t 1 .<label>(8)</label></formula><p>Here λ iou , λ L1 ∈ R are hyper-parameters. These two losses are normalized by the number of instances inside the batch.</p><p>In the sequel, we present the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Instance Sequence Segmentation</head><p>The instance sequence segmentation module aims to predict the mask sequence for each instance. To realize that, the model needs to accumulate the mask features of multiple frames for each instance firstly, then the mask sequence segmentation is performed on the accumulated features.</p><p>The mask features are obtained by computing the similarity map between the object predictions O and the Transformer encoded features E. To simplify the calculation, we only compute with the features of its corresponding frame for each object prediction. For each frame, the object predictions O and the corresponding encoded feature maps E are fed into the self-attention module to obtain the initial attention maps. Then the attention maps will be fused with the initial backbone features B and the transformed encoded features E of the corresponding frames, following a similar practice with the DETR <ref type="bibr" target="#b3">[4]</ref>. The last layer of the fusion is a deformable convolution layer <ref type="bibr" target="#b6">[7]</ref>. In this way, the mask features for each instance of different frames are obtained.</p><p>Following the same spirit of taking the instance sequence as a whole, the mask features of the same instance in different frames should be propagated and reinforce each other. We propose to utilize the 3D convolution to realize that. Assume that the mask feature for instance i of frame t is g i,t ∈ R 1×a× H 0/4× W 0/4 , where a is the channel number, then we concatenate the features of T frames to form the G i ∈ R 1×a×T × H 0/4× W 0/4 . The instance sequence segmentation module takes the instance sequence mask feature G i as input, and output the mask sequence m i ∈ R 1×1×T × H 0/4× W 0/4 for the instance directly. This module contains three 3D convolutional layers and Group Normalization <ref type="bibr" target="#b28">[29]</ref> layers with ReLU activation function. No normalization or activation is performed after the last convolution layer, and the output channel number of the last layer is 1. In this way, the masks of the instance for T frames are obtained. The mask loss for supervising the predictions in Eq. <ref type="formula" target="#formula_6">(7)</ref> is defined as a combination of the Dice <ref type="bibr" target="#b15">[16]</ref> and Focal loss <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_8">L mask m i ,m σ(i) = λ mask 1 T T t=1 L Dice (m i,t ,m σ(i),t ) + L Focal (m i,t ,m σ(i),t ) .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct experiments on the YouTube-VIS <ref type="bibr" target="#b29">[30]</ref> dataset, which contains 2238 training, 302 validation and 343 test video clips. Each video of the dataset is annotated with per pixel segmentation mask, category and instance labels. The object category number is 40. As the test set evaluation is closed, we evaluate our method in the validation set. The evaluation metrics are average precision (AP) and average recall (AR), with the video Intersection over Union (IoU) of the mask sequences as the threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Model settings. As the largest number of the annotated video length for YouTube-VIS <ref type="bibr" target="#b29">[30]</ref> is 36, we take this value as the default input video clip length T . Thus, no postprocessing is needed to associate different clips from one video, which makes our model totally end-to-end trainable. The model predicts 10 objects for each frame, thus the total object query number is 360. For the Transformer we use 6 encoder, 6 decoder layers of width 384 with 8 attention heads. Unless otherwise specified, ResNet-50 <ref type="bibr" target="#b9">[10]</ref> is used as our backbone networks and the same hyper-parameters of DETR <ref type="bibr" target="#b3">[4]</ref> are used. Training. The model is implemented with PyTorch-1.6 <ref type="bibr" target="#b16">[17]</ref>, trained with AdamW <ref type="bibr" target="#b14">[15]</ref> of initial Transformer's learning rate being 10 −4 , the backbone's learning rate being 10 −5 . The models are trained for 18 epochs, with the learning rate decays by 10x at 12 epochs. We initialize our backbone networks with the weights of DETR pretrained on COCO <ref type="bibr" target="#b13">[14]</ref>. The models are trained on 8 V100 GPUs of 32G RAM, with 1 video clip per GPU. The frame sizes are downsampled to 300×540 to fit the GPU memory. Inference. During inference, we follow the same scale setting as training. No post-processing is needed for associating instances. Instances with scores larger than 0.001 are kept. The mean score for all the frames is used as the instance score. For instances that have been classified to different categories in different frames, we use the most frequently predicted category as the final instance category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>In this section we conduct extensive ablation experiments to study the core factors of VisTR. Comparison results are reported in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>The main difference between video and image is that video contains temporal information. How to effectively learn and exploit temporal information is the key to video understanding. Firstly, we study the importance of temporal information to VisTR in two dimensions: the amount and the order. Video sequence length. To evaluate the importance of the amount of temporal information to VisTR, we experiment with models trained with different input video sequence lengths. As reported in <ref type="table" target="#tab_2">Table 1a</ref>, with the length varying from 18 to 36, the AP increases monotonically from 29.7% to 33.3%. This result shows that more temporal information indeed helps the model learn better. As the largest video length of the dataset is 36, we argue that, if with a larger dataset, VisTR can achieve even better results. Note that for this experiment, if the clip length is less than the video length, instance matching in overlapping frames is used for associating them from different clips. Video sequence order. As the movement of objects in real scenes are continuous, we believe that the order of temporal information is also important. To evaluate, we perform a comparison of the model trained with input video sequence in random order vs. time order. Results in <ref type="table" target="#tab_2">Table 1c</ref> show that the model learned according to the time order information achieves 1 point higher, which verifies the importance of the temporal order. Positional encoding. Position information is important for the dense prediction problem of VIS. As the original feature sequence contains no positional information, we supplement with the spatial and temporal positional encodings, which indicate the relative positions in the video sequence. Experiments of models with and without positional encoding are presented in <ref type="table" target="#tab_2">Table 1d</ref>. The model without positional encoding manages to achieve 28.4% AP. Our explanation is that the ordered format of the sequence supervision and the correspondence between the input and output order of the Transformer provide some relative positional information implicitly. In the second experiment, the performance improves by about 5 points, which verifies the necessity of explicit positional encoding. Instance queries. The instance queries are learned embeddings for decoding the representative instance predictions. In this experiment, we study the effect of instance queries and attempt to exploit the inner connections among them by varying the embedding number. Suppose the model decode n instances each frame, and the frame number is T . The input instance query number should be n × T to decode the same number for predictions. In the default setting, one embedding is responsible for one prediction, the model directly learns n × T unique embeddings, termed as 'prediction level' in Table 1b. In the 'video level setting', one embedding is learned for all the instance predictions, i.e., the same embedding is repeated n × T times as the input of decoder. In the 'frame-level' setting, the model only learns T unique embeddings and repeats them by n times. In the 'instance level' setting, the model only learns n unique embeddings and repeats them by T times. The n and T corresponds to the value of 10 and 36 in the table respectively. The result is 8.4% AP and 13.7% AP for 'video level' and 'frame level' settings respectively. Surprisingly, the 'instance level' queries can achieve 32.0% AP, which is only 1.3 points lower than the default setting. The result shows that the queries for one instance can be shared for the VisTR model, which makes the tracking natural. But the queries for one frame can not be shared. Transformers for feature encoding. As illustrated in the 'instance sequence segmentation' module of <ref type="figure" target="#fig_0">Fig. 2</ref>   <ref type="table" target="#tab_2">Table 1e</ref>, the CNN-encoded features achieves 32.0% AP, and the Transformer-encoded features achieve 1.3 points higher. This demonstrates that features are learned better after the Transformer updates them based on all pairwise similarities between them through self-attention. The result also shows the superiority of modeling the spatial and temporal features as a whole.</p><p>Instance sequence segmentation. The segmentation process contains both the instance mask feature accumulation and instance sequence segmentation modules. The instance sequence segmentation module takes the instance sequence as a whole. We expect that it can strengthen the mask prediction by learning the temporal information through 3D convolutions. Thus, when objects are in challenging situations such as occlusions or motion blurs, the module can learn to propagate information from other frames to help the segmentation. Besides, the features of the same instance from multiple frames could help the network recognize the instance better. In this experiment, we perform a study of models with or without the 3D instance sequence segmentation module. For the former case, we apply a 2D convolutional layer with the output channel being 1 to the mask features for each instance of each frame to obtain the masks. The comparison is shown in <ref type="table" target="#tab_2">Table 1f</ref>. The instance sequence segmentation module improves the result by 1.1 points, which verifies the effectiveness of the proposed module.</p><p>With these ablation studies, we conclude that in VisTR design: the temporal information, positional encodings, in-stance queries, global self-attention in the encoder and the instance sequence segmentation module, all play important roles w.r.t. the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main Results</head><p>We compare VisTR against some state-of-the-art methods in video instance segmentation in <ref type="table">Table 2</ref>. The comparison is performed in terms of both accuracy and speed. The methods in the first three rows are originally proposed for tracking or VOS. We have cited the results reported by the re-implementations in <ref type="bibr" target="#b29">[30]</ref> for VIS. Other methods including the MaskTrack RCNN, MaskProp <ref type="bibr" target="#b1">[2]</ref> and STEm-Seg <ref type="bibr" target="#b0">[1]</ref> are originally proposed for the VIS task in the temporal order.</p><p>For the accuracy measured by AP, VisTR achieves the best result among methods using a single model without any bells and whistles. Using the same backbone of ResNet-50 <ref type="bibr" target="#b9">[10]</ref>, VisTR achieves about 6 points higher in AP than the MaskTrack R-CNN and the recently proposed STEm-Seg method. Besides, we argue the AP gap between VisTR and MaskProp mainly comes from its combination of multiple networks, i.e., Spatiotemporal Sampling Network <ref type="bibr" target="#b2">[3]</ref>, Feature Pyramid Network <ref type="bibr" target="#b11">[12]</ref>, Hybrid Task Cascade Network <ref type="bibr" target="#b5">[6]</ref> and the High-Resolution Mask Refinement postprocessing. Since our aim is to design a conceptually simple and end-to-end framework, many improvements methods, such as complex video data augmentation and multistage mask refinement are beyond the scope of this work. For the speed measured by FPS (frames per second), VisTR shows a significant advantage among all the reported results, achieving 27.7 FPS with the ResNet-101 backbone. If excluding the data loading process of multiple images, the speed can achieve 57.7 FPS. Note that, as we load the  <ref type="table">Table 2</ref> -Video instance segmentation AP (%) on the YouTube-VIS <ref type="bibr" target="#b29">[30]</ref> validation dataset. Note that, for the first three methods, we have cited the results reported by the re-implementations in <ref type="bibr" target="#b29">[30]</ref> for VIS. Other results are adopted from their original paper. For the speed of VisTR we report the FPS results with and without the data loading process. Here we naively load the images serially, taking unnecessarily long time. The data loading process can be much faster by parallelizing. images in serial, the data loading process can be easily parallelized. The fast speed of VisTR owes to its design of parallel decoding and no post-processing.</p><p>The visualization of VisTR on the YouTube-VIS [30] validation dataset is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, with each row containing images sampled from the same video. VisTR can track and segment instances well in challenging situations such as: (a) instances overlapping, (b) changes of relative positions between instance, (c) confusion by the same category instances that are close together and (d) instances in various poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a new video instance segmentation framework built upon Transformers, which views the VIS task as a direct end-to-end parallel sequence decoding/prediction problem. In this way, instance tracking is achieved seamlessly and naturally in the same framework of instance segmentation, which is significantly different from and simpler than existing approaches, considerably simplifying the overall pipeline. Without bells and whistles, VisTR achieves the best result and the highest speed among methods using a single model on the YouTube-VIS dataset. To our knowledge, our work is the first one that applies the Transformer to video instance segmentation. We hope that similar approaches can be applied to many more video understanding tasks in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 -</head><label>2</label><figDesc>The overall architecture of VisTR. It contains four main components: 1) a CNN backbone that extracts feature representation of multiple images; 2) an encoder-decoder Transformer that models the relations of pixel-level features and decodes the instance-level features; 3) an instance sequence matching module that supervises the model; and 4) an instance sequence segmentation module that outputs the final mask sequences (Best viewed on screen).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 -</head><label>3</label><figDesc>Visualization of VisTR on the YouTube-VIS<ref type="bibr" target="#b29">[30]</ref> validation dataset. Each row contains images from the same video. For each video, here the same colors depict the mask sequences of the same instances (Best viewed on screen).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>AP AP50 AP75 AR1 AR10 video level 1 8.4 13.2 9.5 20.0 20.8 frame level 36 13.7 23.3 14.5 30.4 35.1 ins. level 10 32.0 52.8 34.0 31.6 37.2 pred. level 360 33.3 53.4 35.1 33.1 38.5(b) Instance query embedding. Instance-level query is only 1.3% lower in AP than the prediction-level query with 36× fewer embeddings.</figDesc><table><row><cell>Length</cell><cell>AP</cell><cell cols="4">AP50 AP75 AR1 AR10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>18</cell><cell>29.7</cell><cell>50.4</cell><cell>31.1</cell><cell>29.5</cell><cell>34.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>24</cell><cell>30.5</cell><cell>47.8</cell><cell>33.0</cell><cell>29.5</cell><cell>34.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell>31.7</cell><cell>53.2</cell><cell>32.8</cell><cell>31.3</cell><cell>36.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>36</cell><cell>33.3</cell><cell>53.4</cell><cell>35.1</cell><cell>33.1</cell><cell>38.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">(a) Video sequence length. The performance improves as the se-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">quence length increases.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>time order</cell><cell>AP</cell><cell cols="4">AP50 AP75 AR1 AR10</cell><cell></cell><cell>AP</cell><cell cols="4">AP50 AP75 AR1 AR10</cell></row><row><cell>random</cell><cell>32.3</cell><cell>52.1</cell><cell>34.3</cell><cell>33.8</cell><cell>37.3</cell><cell cols="2">w/o 28.4</cell><cell>50.1</cell><cell>29.5</cell><cell>29.6</cell><cell>33.3</cell></row><row><cell>in order</cell><cell>33.3</cell><cell>53.4</cell><cell>35.1</cell><cell>33.1</cell><cell>38.5</cell><cell>w</cell><cell>33.3</cell><cell>53.4</cell><cell>35.1</cell><cell>33.1</cell><cell>38.5</cell></row><row><cell cols="6">(c) Video sequence order. Sequence in time order is 1.0% better in</cell><cell cols="6">(d) Position encoding. Position encoding brings about 5% AP gains</cell></row><row><cell cols="3">AP than sequence in random order.</cell><cell></cell><cell></cell><cell></cell><cell>to VisTR.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>AP</cell><cell cols="4">AP50 AP75 AR1 AR10</cell><cell></cell><cell>AP</cell><cell cols="4">AP50 AP75 AR1 AR10</cell></row><row><cell>CNN</cell><cell>32.0</cell><cell>54.5</cell><cell>31.5</cell><cell>31.6</cell><cell>37.7</cell><cell cols="2">w/o 33.3</cell><cell>53.4</cell><cell>35.1</cell><cell>33.1</cell><cell>38.5</cell></row><row><cell cols="2">Transformer 33.3</cell><cell>53.4</cell><cell>35.1</cell><cell>33.1</cell><cell>38.5</cell><cell>w</cell><cell>34.4</cell><cell>55.7</cell><cell>36.5</cell><cell>33.5</cell><cell>38.9</cell></row><row><cell cols="6">(e) CNN-encoded feature vs. Transformer-encoded feature for</cell><cell cols="6">(f) Instance sequence segmentation module. The module with 3D</cell></row><row><cell cols="6">mask prediction. The transformer improves the feature quality.</cell><cell cols="3">convolutions brings 1.1% AP gains.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>. The</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">module takes three types of features as input: the features</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">'B' from the backbone, the feature 'E' from the encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">and the attention map computed by the feature 'E' and</cell></row></table><note>#</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 -</head><label>1</label><figDesc>Ablation experiments for VisTR. All models are trained on YouTubeVIS train for 10 epochs and tested on YouTubeVIS val, using the ResNet-50 backbone. 'O'. To show the superiority of Transformers in feature encoding, we compare the results of using the original input 'O' vs. output 'E' of the encoder for the second feature, a.k.a., CNN-encoded features vs. Transformer-encoded features. As reported in</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was in part supported by Beijing Science and Technology Project (No. Z181100008-918018). CS and his employer received no financial support for the research, authorship, and/or publication of this article.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatio-temporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljoša</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9739" to="9748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="331" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blendmask: Top-down meets bottom-up for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyang</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8573" to="8581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. 3D Vis</title>
		<meeting>Int. Conf. 3D Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9481" to="9490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SOLO: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SOLOv2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Centermask: single shot instance segmentation with point representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
