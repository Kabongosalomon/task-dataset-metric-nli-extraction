<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mix Dimension in Poincaré Geometry for 3D Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Vision and Signal Analysis</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoqiang</forename><surname>Xia</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Vision and Signal Analysis</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mix Dimension in Poincaré Geometry for 3D Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) have already demonstrated their powerful ability to model the irregular data, e.g., skeletal data in human action recognition, providing an exciting new way to fuse rich structural information for nodes residing in different parts of a graph. In human action recognition, current works introduce a dynamic graph generation mechanism to better capture the underlying semantic skeleton connections and thus improves the performance. In this paper, we provide an orthogonal way to explore the underlying connections. Instead of introducing an expensive dynamic graph generation paradigm, we build a more efficient GCN on a Riemann manifold, which we think is a more suitable space to model the graph data, to make the extracted representations fit the embedding matrix. Specifically, we present a novel spatial-temporal GCN (ST-GCN) architecture which is defined via the Poincaré geometry such that it is able to better model the latent anatomy of the structure data. To further explore the optimal projection dimension in the Riemann space, we mix different dimensions on the manifold and provide an efficient way to explore the dimension for each ST-GCN layer. With the final resulted architecture, we evaluate our method on two current largest scale 3D datasets, i.e., NTU RGB+D and NTU RGB+D 120. The comparison results show that the model could achieve a superior performance under any given evaluation metrics with only 40% model size when compared with the previous best GCN method, which proves the effectiveness of our model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Human action recognition is one of the most important topics in computer vision studies. It could contribute to many potential applications such as human behavior analysis, video understanding, and virtual reality. In general, several different kinds of modalities, e.g., appearance, depth, optical flow, and skeletal data, are utilized in the action recognize tasks. Recently, skeleton-based human action recognition has attracted considerable attention, since the compact skeleton data make the models more efficient and more robust to the variations of viewpoints and environments.</p><p>In this paper, we focus on the problem of 3D skeletonbased action recognition and expect to provide a more robust neural network for this task. Recently, graph convolutional networks (GCNs) <ref type="bibr" target="#b15">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b26">Kipf and Welling 2016)</ref>, which have been designed well to preserve the natural topology structure of the skeleton, followed by a temporal convolutional network, i.e., ST-GCNs, have been successfully adopted in skeletonbased action recognition <ref type="bibr" target="#b41">Peng et al. 2020;</ref><ref type="bibr" target="#b45">Shi et al. 2019</ref>). Yan et al. first proposed spatialtemporal graph convolutional network  for this task, which decouples the neural architecture into a GCN to capture the spatial information and an 1D convolutional filter to model the dynamic information. At the GCN part, most current ST-GCN approaches provide a pre-defined graph embedding matrix to encode the skeleton topology. This matrix and the skeleton sequence data together are fed into the ST-GCNs to extract high-level representations. However, as mentioned in work <ref type="bibr" target="#b41">(Peng et al. 2020</ref>), a fixed graph embedding matrix will introduce the constraints into the feature learning progress, which may be harmful for the representation at higher layers, leading to a negative influence to final classification. Therefore, works in <ref type="bibr" target="#b45">(Shi et al. 2019;</ref><ref type="bibr" target="#b41">Peng et al. 2020</ref>) present either a global or a layer-wise dynamic graph generation paradigm to break the learning constraint. Experiments <ref type="bibr" target="#b45">(Shi et al. 2019;</ref><ref type="bibr" target="#b41">Peng et al. 2020)</ref> prove that the the dynamic graph generation mechanism could further improve the performance for this task.</p><p>This paper aims to deal with the skeleton-based human action recognition tasks from another perspective. Instead of providing a dynamic graph embedding, we turn to explore a better modelling space for the skeleton graph sequences. Despite the success of current feature representation with deep neural networks in Euclidean space, graph data is proved to exhibit a highly non-Euclidean latent anatomy . However, as far as we know, all the previous ST-GCNs <ref type="bibr" target="#b45">Shi et al. 2019;</ref><ref type="bibr" target="#b41">Peng et al. 2020</ref>) are defined in the Euclidean space, which may be not the optimal choice for modelling the hierarchical graph data. We argue that neural network operations directly defined in a data-related space, e.g., Hyperbolic man-ifold <ref type="bibr" target="#b11">(Benedetti and Petronio 2012)</ref>, can benefit the learning processing.</p><p>To this effect, in this paper, we present a spatial-temporal graph convolutional networks on a particular model of hyperbolic geometry, i.e., the Poincar model <ref type="bibr" target="#b11">(Benedetti and Petronio 2012)</ref>. Hyperbolic geometry, which is a non-Euclidean geometry with a constant negative Gaussian curvature, has recently increasingly gained momentum in the context of deep neural networks <ref type="bibr" target="#b40">(Nickel and Kiela 2017;</ref><ref type="bibr" target="#b50">Tifrea, Bécigneul, and Ganea 2018)</ref> due to their high efficient capacity and tree-likeliness properties. Building a ST-GCN on hyperbolic geometry could benefit from the hyperbolic distance since the distance between irrelevant samples will be exponentially greater than the distance between similar samples. Our method is orthogonal to the dynamic graph generation ones. Instead of generating a dynamic graph embedding by computing the node embedding similarity, we explore a more reasonable manifold projection such that the projected feature is more suitable for the given embedding matrix. The relationship between samples represented in the hyperbolic space can emphasize similar samples and suppress irrelevant samples. Besides, our method is also more general for graph sequence data since they naturally lie in a non-Euclidean space.</p><p>However, adopting deep neural networks with the non-Euclidean settings is challenging since the non-trivial of the principled generalizations of basic operations such as convolutions. Inspired by work <ref type="bibr" target="#b19">(Gulcehre et al. 2018)</ref>, we get help from the projection between the hyperbolic space and the tangent space. Since there is a bijection between them, the convolutional operations will be conducted on the tangent spaces and then the extracted features will be projected back as a trajectory on the manifold. In this way, we can get the graph embedding on the hyperbolic space by projecting the feature back to the manifold. To further explore the optimal projection dimension in this non-Euclidean space, we mix different dimensions in the hyperbolic space and provide an efficient way to explore the dimension for each graph neural network layer. Finally, with the resulted ST-GCN, we evaluate our method on two currently most challenging 3D skeleton-based datasets, i.e., NTU RGB+D  and NTU RGB+D 120 <ref type="bibr" target="#b34">(Liu et al. 2019a)</ref>. Compared with various state-of-the-art approaches,our method gets the best results under any given evaluation protocols. Moreover, in terms of the model size, current best model <ref type="bibr" target="#b41">(Peng et al. 2020</ref>) is even 2.5 times bigger than our model, which proves the effectiveness of our method. Our contribution can be summarized as follows:</p><p>• We present a novel spatial temporal graph convolutional network via Poincaré geometry, which gives a brand-new ST-GCN to model the graph sequences on the Riemann manifold. • The approach learns a multidimensional structural embedding for each graph based on a Poincaré model. To provide a more distinguished representation, an efficient way is provided to explore a better projection space by mixing the dimensions on the Poincaré model. • To evaluate the effectiveness of our method, comprehen-sive experiments are conducted on two current most challenging 3D skeleton-based action recognition tasks. Results shows that our model obtains the best classification accuracy under any given metrics with an efficient fashion. The rest of this paper is organized as follows. Section reviews related approaches and discusses their relationships to the present work. Section gives a detailed description about the methodology and the corresponding neural architecture. We conduct the experiments in Section and report the results obtained on the various datasets. In this section, experiments include an ablation experiment performed on the NTU RGB+D dataset and the performance comparisons with the state-of-the-art approaches. Finally, a conclusion is drawn in Section .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works Graph Convolutional Networks (GCNs)</head><p>Generalizing convolutional neural networks from regular data, e.g., images, to irregular data, e.g.,graph data, has been an active topic in recent years. As one of the most successful representative, Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b39">(Monti et al. 2017;</ref><ref type="bibr" target="#b52">Veličković et al. 2018;</ref><ref type="bibr" target="#b26">Kipf and Welling 2016;</ref><ref type="bibr" target="#b15">Defferrard, Bresson, and Vandergheynst 2016)</ref>, including spatial-temporal graph convolutional networks (ST-GCNs), increasingly attract attention and get promising results in many research fields.</p><p>Mainly, according to how the graph convolutions are defined, the GCNs can be divided into two categories, i.e., the spectral-domain and the spatial-domain methods. The spectral one converts graph data into its spectrum and applies a filter in spectral domain. There are many representative works in this stream <ref type="bibr" target="#b26">(Kipf and Welling 2016;</ref><ref type="bibr" target="#b15">Defferrard, Bresson, and Vandergheynst 2016)</ref>, and it could deal with the whole graph at the one time. Meanwhile, it is timeconsuming especially for large graphs. Another limitation is that the spectral construction is limited to a single domain since the spectral filter coefficients are basis dependent. If one could construct compatible orthogonal bases across different domains, this problem can be solved. However, such construction requires expert knowledge of the correspondence between domains, which is extremely difficult in most cases. On the contrary, spatial based methods directly design convolution operation in spatial domain <ref type="bibr" target="#b39">(Monti et al. 2017;</ref><ref type="bibr" target="#b52">Veličković et al. 2018)</ref>, which resembles the traditional convolutional filter as it is for images. The spatial-domain methods are more scalable to large graphs as they directly perform the convolution in the graph domain via information aggregation. However, the disadvantage is that it is difficult to model the global structure. To model the dynamic information for graph sequences, e.g., skeletal clips for action recognition, many ST-GCNs <ref type="bibr" target="#b45">Shi et al. 2019;</ref><ref type="bibr" target="#b41">Peng et al. 2020</ref>) are proposed. However, as far as our knowledge, even the skeletal data lies in a Non-Euclidean space, all of the ST-GCNs are defined in the Euclidean space. Instead, in this paper, we propose a brandnew GCN model, which models the human action in a Non-Euclidean space. We notice that there are also works <ref type="bibr" target="#b40">(Nickel and Kiela 2017;</ref><ref type="bibr" target="#b17">Ganea, Bécigneul, and Hofmann 2018;</ref><ref type="bibr" target="#b37">Liu, Nickel, and Kiela 2019)</ref> define neural networks in Riemannian space, but they are either based on conventional forward networks or just design transforming and aggregating functions for a network with only two or three layers. Instead, we provide a deep spatial temporal graph convolutional network to deal with dynamic graph sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Action Recognition</head><p>Action recognition <ref type="bibr" target="#b42">Peng, Hong, and Zhao 2019;</ref><ref type="bibr" target="#b41">Peng et al. 2020</ref>) is one of the most important areas in both industry and academia. We can find numbers of previous action recognition works based on RGB images or videos as the RGB data is available everywhere in real life. However, one of their disadvantage is that the learnt representations are prone to being distracted since entire areas of video frames are exploited to learn the representations.</p><p>Currently, the skeleton data is more easy to access and become more popular for this task. Compared with RGB based action recognition, skeleton based ones are more compact and more robust to the complicated and changing background. The works on this topic mainly follows three streams: 1) Hand-crafted features, in which works leverage the dynamics of joint motion by using handcrafted features, including utilizing LOP feature to overcome intra-class variance issue <ref type="bibr" target="#b54">(Wang et al. 2012)</ref>, building histograms of 3D joint locations <ref type="bibr" target="#b55">(Xia, Chen, and Aggarwal 2012)</ref>, and modelling 3D geometric relationships in a Lie group (Vemulapalli, Arrate, and Chellappa 2014). These methods require much expert knowledge. 2) Conventional deep learning approaches, which provide an automatic feature learning strategy and have become the mainstream methods. Work <ref type="bibr" target="#b25">(Kim and Reiter 2017)</ref> rearranges the graph-structure skeleton data and models it as a pseudo-image based on the manually designed transformation rules, such that the constructed grid data could directly benefited from CNNs. Since the input is time series data, there are also many attempts to utilize RNN and LSTM to model the dynamic information. Representative works include <ref type="bibr" target="#b16">(Du, Wang, and Wang 2015;</ref><ref type="bibr" target="#b44">Shahroudy et al. 2016;</ref><ref type="bibr" target="#b47">Song et al. 2017;</ref><ref type="bibr" target="#b58">Zhang et al. 2017;</ref><ref type="bibr" target="#b46">Si et al. 2018</ref>), where they model the skeleton sequences by either extending RNN to spatio-temporal domains, dividing human skeleton into parts or providing a fully connected deep LSTM for this task. Nevertheless, the performance is hard to be further improved since the physical structure and topology of the graph data is not well considered. 3) Methods based on spatio-temporal graph convolutional networks, which are more popular and suitable for this task. Intuitively, skeleton-features can be represented as a graph structure since their components are homeomorphic. Therefore, joints and bones in skeletons can be defined as the vertices and connections of the graph. Yan et al. developed a spatio-temporal graph convolutional network (ST-GCN)  to model the skeleton data as the graph structure, which leverages the powerful ability to model the irregular data and thus achieves better performance than previous methods. ST-GCN becomes a general framework to deal with the skeleton-based action recognition task. Based on this, work in <ref type="bibr" target="#b45">(Shi et al. 2019)</ref> explores an global adaptive embedding matrix generation method, which further improves the performance. Peng et al. introduced the neural architecture search and automatically designed a GCN <ref type="bibr" target="#b41">(Peng et al. 2020)</ref> architecture for this task. The work <ref type="bibr" target="#b41">(Peng et al. 2020</ref>) gets the current best result. Our work is also based on ST-GCN, but we provide a much more efficient way to model the graph sequences thus even no need to construct a dynamic graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Networks on Riemannian Manifold</head><p>Popular deep learning methods typically embed data into a low dimensional Euclidean vector space using a strategy such as functional similarity to capture semantic representation. This is straightforward since our intuition for the reallife world is highly related to the Euclidean space. However, in many fields, e.g., genomics, social networks, and skeleton-based action recognition, the latent anatomy of the data is well defined by non-Euclidean spaces such as Riemannian manifolds. Most of previous deep neural networks are directly applied to such data and also get promising performance due to the powerful ability of deep learning to capture patterns from data.</p><p>However, the capability of an optimal modelling space should not only reduce the computational burden but also further improve the task performances. Recently, embedding hierarchical data in Riemannian space has achieved attractive results and gained popularity in deep learning. For instance, by constructing in Riemannian space, Mathieu et al. proposed a Poincaré variational Auto-Encoders <ref type="bibr" target="#b38">(Mathieu et al. 2019</ref>) and shows a better generalisation for the hierarchical structures. Cho et al.provided a Riemannian approach to batch normalization <ref type="bibr" target="#b14">(Cho and Lee 2017)</ref> and also achieved superior performance. Here we focus on Hyperbolic geometry, which is a non-Euclidean geometry with a constant negative Gaussian curvature. One significant intrinsic property in this geometry is the exponential growth. There have already been some attempts on designing neural networks in this space. Specifically, Nickel and Kiela <ref type="bibr" target="#b40">(Nickel and Kiela 2017)</ref> reported pioneering research on learning representation in hyperbolic spaces. Then, work in <ref type="bibr" target="#b17">(Ganea, Bécigneul, and Hofmann 2018)</ref> introduced Hyperbolic Neural Networks, connecting hyperbolic geometry with deep learning. After that, works also provide hyperbolic analogues of conventional operations, in which several other algorithms have been developed, such as Poincaré GloVe <ref type="bibr" target="#b50">(Tifrea, Bécigneul, and Ganea 2018)</ref> and Hyperbolic Attention Networks <ref type="bibr" target="#b19">(Gulcehre et al. 2018)</ref>. We also find that works <ref type="bibr" target="#b37">(Liu, Nickel, and Kiela 2019;</ref><ref type="bibr" target="#b13">Chami et al. 2019</ref>) construct graph neural networks using hyperbolic geometry, which are similar with our work. However, our model is different from these works since we are dealing with the dynamic graph sequences while they just focus on static graph. Besides, we provide an efficient way to explore the influence of the projection dimensions for the network while none of them touch this problem. At the first stage, we utilize GCN filters to capture the graph representation for each frame and temporal filters are then used to capture the dynamic information. With this output from the first stage, we mix different dimensions and then project them to the tangent space, where an ST-GCN is used to extract higher-level graph representations. The feature then is mapped back to the manifold. In this way, we model the graph on the Riemannian manifold. Note here, the ST-GCN is based on the same module in stage (a) and the manifold space here is on Poincaré model. We stack several modules from stage (b) to capture higher-level semantic representations. After that, as shown in stage (c), the graph feature is projected back to Euclidean space such that a Euclidean loss function can be used to optimize this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHODOLOGY</head><p>In this section, we describe our ST-GCN defined on Poincaré model. The framework is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The basic module of our network consists of a GCN followed by a temporal convolutional filter. Such block is stacked for multiple layers to capture the high-level representations for the graph sequences. The spatial temporal model is defined by the Poincaré geometry. Finally, the learned features are projected back to Euclidean space for the prediction. We will detail the important components of our framework in the following parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial-Temporal Graph Convolutional Networks</head><p>Graph data is a very useful data structure to model complex structures such as social network, and bio-information, while it is not easy to model this kind of irregular data using neural networks. Define G as one frame of the skeleton sequence. Assume the skeleton is composed of N nodes and the node connections are encoded in the adjacency matrix A ∈ R N ×N . X ∈ R N ×3 denotes the input representations for the graph (skeleton) with N nodes. To extract the feature representation of G, a Fourier transform is conducted on it such that the transformed signal could then be processed with formulation of fundamental operations such as filtering. As a result, a normalized graph Laplacian L = I n − D −1/2 AD −1/2 is used for Fourier transform. Here, the diagonal degree matrix D is constructed with elements D ii = j A ij . Then a graph filtered by operator g θ , parameterized by θ, can be formulated as</p><formula xml:id="formula_0">Y = g θ (L)X = U g θ (Λ)U T X,<label>(1)</label></formula><p>where Y is the extracted graph feature and U is the Fourier basis subjected to L = U ΛU T with the Λ as its corresponding eigenvalues. Suggested by <ref type="bibr" target="#b20">(Hammond, Vandergheynst, and Gribonval 2011)</ref>, the filter g θ can be further approximated by a Chebyshev polynomials with K-th order such that the computational burden is significantly reduced. That is</p><formula xml:id="formula_1">Y = K k=0 θ k T k (L)X,<label>(2)</label></formula><p>where θ k denotes Chebyshev coefficients. For k &gt; 1, Chebyshev polynomial T k (L) is recursively defined as</p><formula xml:id="formula_2">T k (L) = 2LT k−1 (L) − T k−2 (L)<label>(3)</label></formula><p>with T 0 = 1 and T 1 =L. HereL = 2L/λ max − I n is normalized to <ref type="bibr">[-1,1]</ref>. For Eq.</p><p>(2), work in (Kipf and Welling 2016) sets K = 1, λ max = 2. In this way, a first-order approximation of spectral graph convolutions is formed. Therefore,</p><formula xml:id="formula_3">Y = θ 0 X + θ 1 (L − I n )X = θ 0 X − θ 1 (D −1/2 AD −1/2 )X.</formula><p>(4) This equation can be further simplified by using an unified parameter θ, which means setting θ = θ 0 = −θ 1 and making the training process adapt the approximation error, then Y = θ(I n + D −1/2 AD −1/2 )X.</p><p>We set L = I n + D −1/2 AD −1/2 in the following parts for simplicity. An ST-GCN is then built by succeeding a temporal filter after each GCN. One can stack multiple GCN layers to get high-level graph features. Generally, at middle GCN layers, X ∈ R n×C is with multi-channels. Thus, we get</p><formula xml:id="formula_5">Y = LXθ.<label>(6)</label></formula><p>To further improve the robustness, Works in <ref type="bibr" target="#b45">(Shi et al. 2019;</ref><ref type="bibr" target="#b41">Peng et al. 2020</ref>) present an ST-GCN block which generates dynamic embedding matrix based on the node correlations. Instead of providing a dynamic one, we design our ST-GCN block in Poincaré model, making the node representations fit with original graph structure. In this way, we capture the graph features with much less parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Poincaré Manifold</head><p>In this section, we will then discuss how to define an ST-GCN on a Riemannian manifold. Here, we focus on the Poincaré model in the Hyperbolic space <ref type="bibr" target="#b43">(Reynolds 1993)</ref>, which is a maximally symmetric, simply connected Riemannian manifold with constant negative sectional curvature. Hyperbolic space is analogous to the n-dimensional sphere, which has constant positive curvature. As a special case, the Poincaré model can be derived using a stereoscopic projection of the hyperboloid model onto the unit circle of the z=0 plane. It is hard to visualize as it is hard to imagine in a curved space. We can embed models of 2D hyperbolic geometry into a pseudo-Euclidean space called Minkowski space <ref type="bibr" target="#b49">(Tataru 2001)</ref>. Here, a n-dimensional Minkowski space is a real vector space of real dimension n, in which there is a constant Minkowski metric. As illustrated in the <ref type="figure">Figure 2</ref>, the Poincaré disk i.e., a 2D Poincaré model, is constructed as a projection from the upper half hyperboloid onto the unit disk at z=0. Poincaré disk breaks the rule in Euclidean space. For instance, like in the <ref type="figure" target="#fig_1">Figure 3</ref>, given a line ← → AB and a point C / ∈ ← → AB, then we can draw at least two lines cross through C that do not intersect line ← → AB. The two lines through C, denoted as lines l 1 and l 2. Different from the Euclidean 2D space, we have that ← → AB parallels both to l 1 and l 2, but at the same time l 1 and l 2 are not paralleled. Note also that what different from the Euclidean space is that l 2 intersects one of a pair of parallel lines (l 1), but does not intersect the other parallel line ( ← → AB). In hyperbolic geometry, a significant intrinsic property is the exponential growth, instead of the polynomial growth as in Euclidean geometry. This means that the distance between irrelevant samples will be exponentially greater than the distance between similar samples. Thus, the relationship between samples represented in the hyperbolic space can emphasize similar samples and suppress irrelevant samples. As a result, hyperbolic geometry outperforms Euclidean geometry in some special tasks, such as learning hierarchical embedding <ref type="bibr" target="#b17">(Ganea, Bécigneul, and Hofmann 2018)</ref>.</p><p>Here, we formally define this manifold. Let M be a ndimensional manifold. There are three basic components which count much important for the manifold M, namely, geodesic, tangent space, and the Riemannian metric.</p><p>A geodesic is the generalization of a straight line to curved space, defined to be a curve where you can parallel transport a tangent vector without deformation. In our hyperboloid model, as illustrated in <ref type="figure">Fig. 2</ref>, the geodesic (or our hyperbolic line) is defined to be the curve created by intersecting the plane defined by two points and the origin (i.e., coordinate (0,0,0)) with the hyperboloid. So one point end has to go down first then back up to reach another point. This distance is not directly towards it using the shortest path on the surface in Euclidean space, but will be around the circumference. Formally, the distance for x, y ∈ M are defined as:</p><formula xml:id="formula_6">d(x, y) = arcosh(1 + 2 ||x − y|| 2 (1 − ||x|| 2 )(1 − ||y|| 2 )</formula><p>). <ref type="formula">(7)</ref> A tangent space T x M at point x is defined as the first order linear approximation of M around x. A Riemannian metric g is a collection of inner-products g x : T x × T x → R varying smoothly with x. For the Poincaré model, the open unit ball equipped with the Riemannian metric tensor, which is defined with</p><formula xml:id="formula_7">g x = ( 2 1 − ||x|| 2 ) 2 g E<label>(8)</label></formula><p>where g E = I n denotes the Euclidean metric tensor, which is conformal to the Euclidean one. Then, a Riemannian manifold (M, g) is a manifold M with a group Riemannian metric g. Now, we build our spatial-temporal graph convolutional networks on the Poincaré geometry to provide a more robustness representation for the temporal graph sequences. One advantage of the hyperbolic space is that it provides a bijection between the hyperbolic space and the tangent space at a point such that the operations for points on the hyperboloid manifold can be performed in tangent space and then mapped back and vice-versa. The bijection is done by the exponential map, which maps the points on the tangent space to the manifold and is defined as exp x : T x M → M. The logarithmic map, which, as the inverse step, maps points on the tangent space back to the manifold, is defined as</p><formula xml:id="formula_8">log x : M → T x M. Mathematically, exp x (v) = x ⊕ (tanh( λ x ||v|| 2 ) v ||v|| ) (9) log x (y) = 2 λ x archtanh(|| − x ⊕ y||) −x ⊕ y || − x ⊕ y||<label>(10)</label></formula><p>where v is a tangent vector, and λ x = 2 1−||x|| 2 is a conformal factor. ⊕ is the Mbius addition for any x, y ∈ M:</p><formula xml:id="formula_9">x ⊕ y = (1 + 2 x, y + ||y|| 2 )x + (1 − ||x|| 2 )y 1 + 2 x, y + ||x|| 2 ||y|| 2 .<label>(11)</label></formula><p>With the aforementioned projection functions, we perform the GCN operations on the Poincaré models. Since there is no definition of vector space in the Riemannian space, inspired by <ref type="bibr" target="#b17">(Ganea, Bécigneul, and Hofmann 2018)</ref>, we conduct the feature extraction on the tangent space by projecting the graph embeddings with the logarithmic map. In this way, the Euclidean transformation conducted by the neural operations can be utilized for the feature in hyperbolic space. Specifically, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, there are three <ref type="figure">Figure 2</ref>: Illustration of the 2D Poincaré model. For any point on the hyperboloid, we form a line by extending it out to a focal point (0,0,-1). Then, the intersection point on the z=0 plane will be its projection point in the Poincaré model. stages in our framework. The input raw data is first encoded by a feature embedding network. Then, the captured feature is projected into a tangent space and filtered by a GCN and a temporal filter, which are</p><formula xml:id="formula_10">Y i = L log x (X i )Θ,<label>(12)</label></formula><formula xml:id="formula_11">Y = Θ t×1 {Y i } T ,<label>(13)</label></formula><p>Here, X i represents the i-th frame of the inputs and {Y i } T are the representations of the entire T frames after the GCN. {Y i } T are then fed into a temporal filter with kernel t × 1. There will also be an activation function to perform a nonlinear projection on Y . Finally, we map the representation back to the a Euclidean space with log x function and thus optimize the network with Euclidean loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mix-Dimensions on Manifold</head><p>With aforementioned method, we can build an ST-GCN on Riemannian manifold. However, it is still not easy to manually determine the projection dimension for each layer on the manifold. There will be thousands of combinations for a ten layers ST-GCN, which definitely is not possible to manually evaluate each setting and find out the best one. Here, we provide an efficient way to explore the optimal settings for each layer. Inspired by the slimmable networks <ref type="bibr" target="#b57">(Yu et al. 2018)</ref>, where they trained a slimmable network to perform the network pruning, we mix different dimensions at each projection point to efficiently explore the best projection dimension for each layer. Specifically, we provide a set of projection dimensions on the Poincaré model. Thus, there will be a group of corresponding ST-GCN blocks on the tangent space. Instead of constructing a set of ST-GCN blocks there, we make the higher dimension projection share the operations of the lower ones. In this way, we can build a super-model and the exploration for the higher dimension would not need training from scratch since they could benefit from the training for the lower dimension. To this end, we construct corresponding switchable batch normalization <ref type="bibr" target="#b57">(Yu et al. 2018</ref>) and a slimmable network on the tangent space. In fact, this mix-dimension method provides thousands of combinations of the ST-GCNs on the Poincaré model at one time. Instead of evaluating the performance of all these combinations, we compute the relative modeling ability for this task. We assume that the super-model based on the mixdimension method could provide an estimation of each individual dimension setting. As a result, we build the ST-GCN model by dividing each layer into specific group, and we try to figure out what is the best combination for this network. For instance, at a layer, we project the graph representation into Poincaré model with 64 dimensions. Instead of only providing this projection,we project the graph to a set of dimensions, <ref type="bibr">[32,</ref><ref type="bibr">48,</ref><ref type="bibr">64,</ref><ref type="bibr">80,</ref><ref type="bibr">96]</ref>, at the same time. We conduct the same processing for the other layers. Therefore, there will be five different ST-GCN blocks for this single layer. Thus, However, it will cost expensive computation for a deep GCN models. So instead of building all the models, we construct network with a slimmbale biggest one. In each iteration, we randomly sample a combination of projection dimension and the corresponding ST-GCN will be activated and trained. After the training phase is finished, we randomly choose a batch of projection combinations. Based on their relative prediction accuracy on this task, we choose the best one as our ST-GCN on the Poincaré model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>This section describes the experiments in terms of datasets, the architecture, the training details, the comparison results and the corresponding analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>In the experiments, we evaluate our method on two current largest scale 3D skeleton-based action recognition datasets, i.e., NTU RGB+D ) and NTU RGB+D 120 <ref type="bibr" target="#b34">(Liu et al. 2019a)</ref>.</p><p>NTU RGB+D NTU RGB+D ) is currently one of the most widely used action recognition datasets. There are four modalities, including RGB videos, depth sequences, infrared videos and 3D skeleton data. Here, we only conduct experiments on the skeleton data. There are totally 56,880 video clips involving 60 human action classes <ref type="figure">Figure 4</ref>: Illustration of ST-GCN block. There are two inputs, including the L and the X in Eq. (6). Here, GCN denotes the spatial graph convolutional network, and Conv-T represents the temporal filter. Both of them are followed by batch normalization (BN) layer and a activation layer (ReLU). Moreover, a residual connection is added for each block. The output and the original L are fed into next block. and the max number of frames in each sample is 300. They are captured from three cameras with different settings. In this dataset, each actor skeleton contains 25 3D joints coordinates and there are at most two actors in each video clip. The original benchmark provided two evaluations, which are Cross-Subject (X-subject) and the Cross-View (X-view) evaluations. In the X-subject evaluation, the training set contains 40,320 videos from 20 subjects, and the rest 16,560 video clips are used for testing. In the X-view evaluation, 37,920 videos captured from cameras No.2 and No.3 are used in the training and the rest 18,960 videos from camera No.1 are used for testing. We follow the original two benchmarks and report the Top-1 accuracy. For inputs with more than one stream, e.g., bones, a score-level fusion result is reported.</p><p>NTU RGB+D 120 NTU RGB+D 120 <ref type="bibr" target="#b34">(Liu et al. 2019a</ref>) is the extended version of the aforementioned NTU RGB+D dataset. It is a more challenging dataset with high variability since it involves more subjects and action categories. There are totally 114,480 samples which cover 120 classes. They are captured from 106 distinct subjects in a wide range of age distribution. There are two different evaluation protocols in this new released dataset: Cross-Subject (X-subject), like in the previous dataset, which splits subjects in half to training and testing parts; Cross-Setup (X-setup), which divides samples by 32 camera setup IDs. The even setup IDs is utilized for training (16 setups) and odd setup IDs for testing (16 setups). Here, 32 IDs are based on different camera setups, e.g., different heights with different horizontal angles. In consistent with the original benchmark method, Top-1 accuracy is reported in the two benchmarks.</p><p>Before the data was fed into the networks, there should be some pre-processing such that the data structure for each video clip is unified. We keep it consistent with the previous best methods <ref type="bibr" target="#b45">(Shi et al. 2019;</ref><ref type="bibr" target="#b41">Peng et al. 2020)</ref> for fair comparison. All the skeleton sequences are unified to 300 frames with two actors. For the single-actor data, the second body will be padded with all zeros. Each dimension of the 3D coordinates is put into three channels as inputs. Like <ref type="bibr" target="#b45">(Shi et al. 2019;</ref><ref type="bibr" target="#b41">Peng et al. 2020)</ref>, we also compute the second-order information of joint, i.e., bones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture and Training</head><p>The basic framework is as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Firstly, we build an embedding model with the original ST-GCN module (like in <ref type="figure">Fig. 4)</ref>, in which the graph convolutional layer is followed by a temporal convolution with a kernel size 9 × 1 to capture the dynamic information. The three dimensional node coordinates are projected into a space with 64D. Then, as shown in the second stage of the framework, we build networks on manifold. A group of manifold projection, <ref type="bibr">[32,</ref><ref type="bibr">48,</ref><ref type="bibr">64,</ref><ref type="bibr">80,</ref><ref type="bibr">96]</ref>, is constructed. The corresponding ST-GCN layers are also built on Poincaré model. We empirically stack six layers on this manifold space and divide them into three levels. For each level, we insert two ST-GCN layers and double the projection group setting based on the previous level, which means the mixed dimensions at second and third level are <ref type="bibr">[64, 96, 128, 160, 192], and [128, 192, 256, 320, 384]</ref>, respectively. Finally, as shown in the third stages of <ref type="figure" target="#fig_0">Fig. 1</ref>, the graph representations modeled from the manifold space is projected back to the Euclidean space. The resulted node embeddings are averaged to a feature vector. Then a fully connected layer followed by a softmax function is utilized to predict a class prediction.</p><p>There are mainly two steps to perform the training. Firstly, we mix dimensions and efficiently explore the best combination on the NTU RGB+D data, X-subject evaluation, for 50 epochs. Then we randomly sample 100 combinations and choose the best one based on the prediction accuracy. We find that one of the best projection combination for the six layers is <ref type="bibr">[64,</ref><ref type="bibr">64,</ref><ref type="bibr">160,</ref><ref type="bibr">128,</ref><ref type="bibr">320,</ref><ref type="bibr">256]</ref>. Then, based on this resulted combination, we remove the mix-dimension process and build our ST-GCN on Poincaré model. During the training process, the cross-entropy loss is utilized as the classification loss. The learning rate is set as 0.1 and is decreased based on a cosine function. A stochastic gradient descent (SGD) with Nesterov momentum (0.9) is applied as the optimization algorithm for the network. We set the weight decay to 0.0005 as regularization. This model will be trained for 50 epochs and compared to other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In this section, we evaluate the effectiveness of our method on the NTU RGB+D dataset under the given two evaluation metrics. Here, current state-of-the-art ST-GCN (Yan, Xiong, and Lin 2018) works as a baseline since we share the same ST-GCN module. To further evaluate that how much we can gain from the Poincaré geometry, we implement a smaller ST-GCN (seven layers with output channels <ref type="bibr">[64,</ref><ref type="bibr">64,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">256]</ref> for each one) by keeping its network architecture totally consistent with Ours. This model is referred as ST-GCN-s. Finally, we employ the optimal projection dimensions on the manifold explored by our mix-dimension method. This network is represented by Ours-M.</p><p>The comparison results are listed in <ref type="table" target="#tab_0">Table 1</ref>. It can be seen from this table that for the given two evaluations, our networks can largely improve the performance on both joint Comparison with the State-Of-The-Art methods</p><p>First, we conduct experiments on NTU RGB+D dataset, and like <ref type="bibr" target="#b45">(Shi et al. 2019;</ref><ref type="bibr" target="#b41">Peng et al. 2020)</ref>, we report the best results after performing the score-level fusion on joints and bones. Here, we compared with 14 State-Of-The-Art (SOTA) approaches under the given evaluation metrics. There is one hand-crafted method <ref type="bibr" target="#b21">(Hu et al. 2015)</ref>, but most of them are deep learning methods, including CNNbased method <ref type="bibr" target="#b25">(Kim and Reiter 2017)</ref>, RNN-based methods <ref type="bibr" target="#b47">Song et al. 2017;</ref><ref type="bibr" target="#b58">Zhang et al. 2017;</ref><ref type="bibr" target="#b46">Si et al. 2018</ref>) and reinforcement learning based method <ref type="bibr" target="#b48">(Tang et al. 2018)</ref>. Besides, we also compare with various ST-GCNs <ref type="bibr" target="#b56">Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b27">Li et al. 2019a;</ref><ref type="bibr" target="#b18">Gao et al. 2019;</ref><ref type="bibr" target="#b28">Li et al. 2019b;</ref><ref type="bibr" target="#b45">Shi et al. 2019;</ref><ref type="bibr" target="#b41">Peng et al. 2020)</ref>, which get very promising results on this task. It is also worth mentioning that Peng et al. <ref type="bibr" target="#b41">(Peng et al. 2020</ref>) introduced a neural architecture search (NAS) method into this task and automatically design an ST-GCN, which obtains the current best results. We also compare with this method. All the comparison results are listed in <ref type="table" target="#tab_1">Table 2</ref>. It can be seen from <ref type="table" target="#tab_1">Table 2</ref> that under both X-subject and X-view evaluations, our model achieves the current best classification accuracy. Specifically, compared with current best method NAS-GCN <ref type="bibr" target="#b41">(Peng et al. 2020)</ref>, our model can achieve better results and also decrease the model size by 2.5 times. On NTU RGB+D 120 dataset, we compared to 14 skeleton-based action recognition approaches under Xsubject and X-setup evaluation metrics. The methods includes hand-crafted method <ref type="bibr" target="#b21">(Hu et al. 2015)</ref>, conventional deep learning methods, including Part-Aware </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>X-subject X-view Dynamic Skeleton <ref type="bibr" target="#b21">(Hu et al. 2015)</ref> 60.2% 65.2% Part-Aware LSTM  62.9% 70.3% STA-LSTM <ref type="bibr" target="#b47">(Song et al. 2017)</ref> 73.4% 81.2% TCN <ref type="bibr" target="#b25">(Kim and Reiter 2017)</ref> 74.3% 83.1% VA-LSTM <ref type="bibr" target="#b58">(Zhang et al. 2017)</ref> 79.2% 87.7% Deep STGCK  74.9% 86.3% ST-GCN  81.5% 88.3% DPRL <ref type="bibr" target="#b48">(Tang et al. 2018)</ref> 83.5% 89.8% SR-TSL <ref type="bibr" target="#b46">(Si et al. 2018)</ref> 84.8% 92.4% STGR-GCN <ref type="bibr" target="#b27">(Li et al. 2019a)</ref> 86.9% 92.3% GR-GCN <ref type="bibr" target="#b18">(Gao et al. 2019)</ref> 87.5% 94.3% AS-GCN  86.8% 94.2% 2S-AGCN <ref type="bibr" target="#b45">(Shi et al. 2019)</ref> 88.5% 95.1% NAS-GCN <ref type="bibr" target="#b41">(Peng et al. 2020)</ref> 89.4% 95.7%  <ref type="bibr" target="#b29">(Liu and Yuan 2018)</ref>. We also compare with the promising GCN-based models <ref type="bibr" target="#b28">Li et al. 2019b</ref>). This task is more challenging than the above one, thus all the baselines are lower than 80%. Here, we report the best result on joint data. All the comparison results are listed in <ref type="table" target="#tab_2">Table 3</ref>. We can see from <ref type="table" target="#tab_2">Table 3</ref> that graph convolutional networks are much suitable for this task when comparing the last two methods to previous hand-crafted method and conventional deep learning methods. Under the X-subject protocol, our model improves previous best CNN-based method <ref type="bibr" target="#b29">(Liu and Yuan 2018)</ref> by more than 15%. Compared to the GCN methods, we can still get 4.3% and 2.8% improvements under the X-setup and X-subject evaluation metrics, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we address the task of video action recognition from skeleton sequences. To better modelling the graphstructure data, we propose a novel spatial-temporal graph convolutional network by applying Poincaré geometry. The graph embedding is projected into a Poincaré model where we further learn and extract a high-level of graph representation. To further explore the optimal projection dimension on the manifold space and provide a better setting for different layers, we provide an efficient exploration strategy by mixing a group of dimensions on the Poincaré model. With the resulted dimension setting for each layer, we construct our ST-GCN on this manifold space. Then, we conduct extensive comparison experiments to verify the effectiveness of our method. Comparison results on two current most challenging datasets show that the features modelled by our model are much more distinguishable than the model with the same setting in Euclidean space. Besides, our model can also get the best results for the given tasks with only 40% of the previous best model in term of the model size. All of these prove the effectiveness of our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of our framework. There are mainly three stages in our framework, including (a) Graph feature embedding, (b) Extracting graph in Poincaré model, and (c) Classification in the Euclidean space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the property of Poincaré model. Go through point C, one can draw more than one line (l 1 and l 2 ) which is paralleled to line AB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>et al. 2016), Soft RNN (Hu et al. 2018), Spatio-Temporal LSTM (Liu et al. 2016), Internal Feature Fusion (Liu et al. 2017a), GCA-LSTM (Liu et al. 2017c), Multi-Task Learning Network (Ke et al. 2017), FSNet (Liu et al. 2019b) , Skeleton Visualization (Liu, Liu, and Chen 2017), Two-Stream Attention LSTM (Liu et al. 2017b), Multi-Task CNN with RotClips (Ke et al. 2018), and Body Pose Evolution Map</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation Study. Performance comparison on NTU RGB+D dataset under two given evaluations. Here we compare with ST-GCN (Yan, Xiong, and Lin 2018) and a small ST-GCN (seven layers), denoted as ST-GCN-s, by employing same architecture with Ours. Here, Ours-M is the architecture resulted from Mix-Dimension. The big improvements achieved by our method prove the effectiveness of our model.</figDesc><table><row><cell></cell><cell>Methods</cell><cell>Joint</cell><cell>Bone</cell></row><row><cell></cell><cell cols="2">ST-GCN (Yan, Xiong, and Lin 2018) 80.2%</cell><cell>77.6%</cell></row><row><cell>X-subject</cell><cell>ST-GCN-s</cell><cell>79.2%</cell><cell>76.8%</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">86.2% 85.7%</cell></row><row><cell></cell><cell>Ours-M</cell><cell cols="2">87.8% 87.2%</cell></row><row><cell></cell><cell cols="2">ST-GCN (Yan, Xiong, and Lin 2018) 91.1%</cell><cell>90.5%</cell></row><row><cell>X-view</cell><cell>ST-GCN-s</cell><cell>89.7%</cell><cell>88.3%</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">93.9% 93.7%</cell></row><row><cell></cell><cell>Ours-M</cell><cell cols="2">95.0% 94.7%</cell></row><row><cell cols="4">and bone data. And with the helps of our Mix-dimension,</cell></row><row><cell cols="4">the performance of the proposed model can be further im-</cell></row><row><cell cols="4">proved. Specifically, under the X-subject evaluations, our</cell></row><row><cell cols="4">model (Ours-M) can overcome the baseline from joint and</cell></row><row><cell cols="4">bone data by 7.6% and 9.6%, respectively. When compared</cell></row><row><cell cols="4">to ST-GCN-s, which is with a similar model size to Ours,</cell></row><row><cell cols="4">the proposed model defined with Poincaré model could even</cell></row><row><cell cols="4">outperform it by 8.6% and 10.4%, respectively. All of them</cell></row><row><cell cols="4">proves that defining ST-GCN on manifold space could ben-</cell></row><row><cell>efit greatly.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on NTU RGB+D with 14 stateof-the-art methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on NTU RGB+D 120 with 13 state-of-the-art methods. * means implemented by us.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by ICT 2023 project (grant 328115), the Academy of Finland for project MiGA (grant 316765), and Infotech Oulu. As well, the authors wish to acknowledge CSC-IT Center for Science, Finland, for computational resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Methods X-setup X-subject Part-Aware LSTM (Shahroudy et al. 2016) 25.5% 26</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rnn (</forename><surname>Soft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>3% 44.9% Dynamic Skeleton (Hu et al. 2015) 50.8% 54</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lstm (</forename><surname>Spatio-Temporal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<title level="m">7% 57.9% Internal Feature Fusion (Liu et al. 2017a) 58.2% 60</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gca-Lstm (</forename><surname>Liu</surname></persName>
		</author>
		<idno>58.3% 59.2%</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<idno>2017) 58.4% 57.9%</idno>
		<title level="m">Multi-Task Learning Network</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Fsnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno>59.9% 62.4%</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<idno>2017) 60.3% 63.2%</idno>
		<title level="m">Skeleton Visualization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<idno>2% 63.3%</idno>
		<title level="m">Stream Attention LSTM (Liu et al. 2017b)</title>
		<imprint>
			<biblScope unit="page">61</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Body Pose Evolution Map</title>
		<idno>64.6% 66.9%</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>St-Gcn (yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<idno>71.3% 72.4%</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">As-Gcn (</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Lectures on hyperbolic geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benedetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Petronio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4869" to="4880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Riemannian approach to batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5225" to="5235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hyperbolic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5345" to="5355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimized skeleton-based action recognition via sparsified graph regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM International Conference on Multimedia</title>
		<meeting>the 2019 ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09786</idno>
		<title level="m">Hyperbolic attention networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Early action prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatiotemporal graph convolution for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Semi-supervised classification with graph convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Spatio-temporal graph routing for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatiotemporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition using spatiotemporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3007" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Skeleton-based human action recognition with global context-aware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ntu rgb+ d 120: A largescale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Skeleton-based online action prediction using scale selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hyperbolic graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8228" to="8239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Continuous hierarchical representations with poincaré variational auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Le Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12544" to="12555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Poincaré embeddings for learning hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning graph convolutional network for skeleton-based human action recognition by neural searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">26692676</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video action recognition via neural architecture searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry on a hyperboloid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American mathematical monthly</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="442" to="455" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Strichartz estimates in the hyperbolic space and global existence for the semilinear wave equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tataru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical society</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="795" to="807" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tifrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poincar\&amp;apos;e Glove</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06546</idno>
		<title level="m">Hyperbolic word embeddings</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08928</idno>
		<title level="m">Slimmable neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
