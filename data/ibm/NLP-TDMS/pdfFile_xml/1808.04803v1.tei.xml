<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical binary CNNs for landmark localization with limited resources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Hierarchical binary CNNs for landmark localization with limited resources</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Binary Convolutional Neural Networks</term>
					<term>Residual learning</term>
					<term>Landmark localization</term>
					<term>Human pose estimation</term>
					<term>Face alignment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our goal is to design architectures that retain the groundbreaking performance of Convolutional Neural Networks (CNNs) for landmark localization and at the same time are lightweight, compact and suitable for applications with limited computational resources. To this end, we make the following contributions: (a) we are the first to study the effect of neural network binarization on localization tasks, namely human pose estimation and face alignment. We exhaustively evaluate various design choices, identify performance bottlenecks, and more importantly propose multiple orthogonal ways to boost performance. (b) Based on our analysis, we propose a novel hierarchical, parallel and multi-scale residual architecture that yields large performance improvement over the standard bottleneck block while having the same number of parameters, thus bridging the gap between the original network and its binarized counterpart. (c) We perform a large number of ablation studies that shed light on the properties and the performance of the proposed block. (d) We present results for experiments on the most challenging datasets for human pose estimation and face alignment, reporting in many cases state-of-the-art performance. (e) We further provide additional results for the problem of facial part segmentation. Code can be downloaded from https://www.adrianbulat.com/binary-cnn-landmarks</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HIS work is on localizing a predefined set of fiducial points on objects of interest which can typically undergo non-rigid deformations like the human body or face. Very recently, work based on Convolutional Neural Networks (CNNs) has revolutionized landmark localization, demonstrating results of remarkable accuracy even on the most challenging datasets for human pose estimation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and face alignment <ref type="bibr" target="#b3">[4]</ref>. However, deploying (and training) such methods is computationally expensive, requiring one or more high-end GPUs, while the learned models typically require hundreds of MBs, thus rendering them completely unsuitable for real-time or mobile applications. This work is on highly accurate and robust yet efficient and lightweight landmark localization using binarized CNNs.</p><p>Our work is inspired by recent results of binarized CNN architectures on image classification <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Contrary to these works, we are the first to study the effect of neural network binarization on fine-grained tasks like landmark localization. Similarly to <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, we find that binarization results in performance drop, however to address this we opted to investigate and propose several architectural innovations which led to the introduction of a novel hierarchical, parallel and multi-scale residual block, as opposed to investigating ways to improve the binarization process as proposed in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In summary, our main methodological contributions are: <ref type="bibr">•</ref> We are the first to study the effect of binarization on state-of-the-art CNN architectures for the problem of localization, namely human pose estimation and Manuscript received <ref type="bibr">April 19, 2005</ref>; revised August 26, 2015.   <ref type="bibr" target="#b6">[7]</ref>. <ref type="bibr">(b)</ref> The proposed hierarchical parallel &amp; multi-scale structure: our block increases the receptive field size, improves gradient flow, is specifically designed to have (almost) the same number of parameters as the original bottleneck, does not contain 1 × 1 convolutions, and in general is derived from the perspective of improving the performance and efficiency for binary networks. Note: a layer is depicted as a rectangular block containing: its filter size, the number of input and output channels; "C" -denotes concatenation and "+" an element-wise sum. face alignment. To this end, we exhaustively evaluate various design choices, and identify performance bottlenecks. More importantly, we describe multiple orthogonal ways to boost performance; see Subsections 4.2, 4.3 and 4.4.</p><p>• Based on our analysis, we propose a new hierarchical, parallel and multi-scale residual architecture (see Subsection 4.5) specifically designed to work well for the binary case. Our block results in large performance improvement over the baseline binary arXiv:1808.04803v1 [cs.CV] 14 Aug 2018 residual block of <ref type="bibr" target="#b6">[7]</ref> (about 6% in absolute terms when the same number of parameters are used (see Subsection 4.6.1, <ref type="table" target="#tab_1">Table 2)</ref>). <ref type="figure" target="#fig_1">Fig. 1</ref> provides a comparison between the baseline residual block of <ref type="bibr" target="#b6">[7]</ref> and the one proposed in this work. <ref type="bibr">•</ref> We investigate the effectiveness of more advanced extensions of the proposed block (see <ref type="bibr">Section 7)</ref> and improved network architectures including network stacking (see <ref type="bibr">Section 8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further experimental contributions include:</head><p>• While our newly proposed block was developed with the goal of improving the performance of binary networks, we also show that the performance boost offered by the proposed architecture also generalizes to some extent for the case of real-valued networks (see Subsection 4.6.2). <ref type="bibr">•</ref> We perform a large number of ablation studies that shed light on the properties and the performance of the proposed block (see Sections 4.6 and 6). <ref type="bibr">•</ref> We present results for experiments on the most challenging datasets for human pose estimation and face alignment, reporting in many cases state-of-the-art performance (see <ref type="bibr">Section 6)</ref>. <ref type="bibr">•</ref> We further provide additional results for the problem of facial part segmentation (see <ref type="bibr">Section 9)</ref>.</p><p>Compared to our previous work in <ref type="bibr" target="#b7">[8]</ref>, this paper investigates the effectiveness of more advanced binary architectures (both at block and network level), provides a more in-depth analysis of the proposed methods and results (including more qualitative ones) and additionally includes the aforementioned experiment on facial part segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CLOSELY RELATED WORK</head><p>This Section reviews related work on network quantization, network design, and gives an overview of the state-of-theart on human pose estimation and face alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network quantization</head><p>Prior work <ref type="bibr" target="#b8">[9]</ref> suggests that high precision parameters are not essential for obtaining top results for image classification. In light of this, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> propose 16-and 8-bit quantization, showing negligible performance drop on a few small datasets <ref type="bibr" target="#b11">[12]</ref>. <ref type="bibr" target="#b12">[13]</ref> proposes a technique which allocates different numbers of bits (1-2-6) for the network parameters, activations and gradients.</p><p>Binarization (i.e. the extreme case of quantization) was long considered to be impractical due to the destructive property of such a representation <ref type="bibr" target="#b9">[10]</ref>. Recently <ref type="bibr" target="#b13">[14]</ref> showed this not to be the case and that by quantizing to {−1, 1} good results can be actually obtained. <ref type="bibr" target="#b14">[15]</ref> introduces a new technique for training CNNs that uses binary weights for both forward and backward passes, however, the real parameters are still required during training. The work of <ref type="bibr" target="#b5">[6]</ref> goes one step further and binarizes both parameters and activations. In this case multiplications can be replaced with elementary binary operations <ref type="bibr" target="#b5">[6]</ref>. By estimating the binary weights with the help of a scaling factor, <ref type="bibr" target="#b4">[5]</ref> is the first work to report good results on a large dataset (ImageNet).</p><p>Notably, our method makes use of the recent findings from <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref> using the same way of quantizing the weights and replacing multiplications with bit-wise xor operations.</p><p>Our method differs from all aforementioned works in two key respects: (a) instead of focusing on image classification, we are the first to study neural network binarization in the context of a fine-grained computer vision task namely landmark localization (human pose estimation and facial alignment) by predicting a dense output (heatmaps) in a fully convolutional manner, and (b) instead of enhancing the results by improving the quantization method, we follow a completely different path, by enhancing the performance via proposing a novel architectural design for a hierarchical, parallel and multi-scale residual block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Block design</head><p>The proposed method uses a residual-based architecture and hence the starting point of our work is the bottleneck block described in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>. More recently, <ref type="bibr" target="#b16">[17]</ref> explores the idea of increasing the cardinality of the residual block by splitting it into a series of c parallel (and much smaller so that the number of parameters remains roughly the same) sub-blocks with the same topology which behave as an ensemble. Beyond bottleneck layers, Szegedy et. al. <ref type="bibr" target="#b17">[18]</ref> propose the inception block which introduces parallel paths with different receptive field sizes and various ways of lowering the number of parameters by factorizing convolutional layers with large filters into smaller ones. In a follow-up paper <ref type="bibr" target="#b18">[19]</ref>, the authors introduce a number of inceptionresidual architectures. The latter work is the most related one to the proposed method.</p><p>Our method is different from the aforementioned architectures in the following ways (see <ref type="figure" target="#fig_1">Fig. 1b</ref>): we create a hierarchical, parallel and multi-scale structure that (a) increases the receptive field size inside the block and (b) improves gradient flow, (c) is specifically designed to have (almost) the same number of parameters as the original bottleneck, (d) our block does not contain 1 × 1 convolutions, and (e) our block is derived from the perspective of improving the performance and efficiency of binary networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network design</head><p>Our target was not to propose a new network architecture for landmark localization; hence we used the state-of-theart Hour-Glass (HG) network of <ref type="bibr" target="#b1">[2]</ref> which makes use of the bottleneck block of <ref type="bibr" target="#b15">[16]</ref>. Because we are interested in efficiency, most of our experiments are conducted using a single network. Our baseline was the single binary HG obtained by directly quantizing it using <ref type="bibr" target="#b4">[5]</ref>. As <ref type="table" target="#tab_0">Table 1</ref> shows, there is a significant performance gap between the binary and the real valued HGs. We bridge this gap by replacing the bottleneck block used in the original HG with the proposed block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Human Pose Estimation</head><p>Traditionally, human pose estimation methods relied on tree structured graphical models <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> to represent the spatial relationships between body parts and were usually built using hand crafted features. More recently, methods based on CNNs have shown remarkable results outperforming traditional methods by large margin <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Because learning a direct mapping from the image to the location of the body parts is a highly non-linear problem that is difficult to learn, most methods represent each landmark as a confidence map encoded as a 2D Gaussian centered at the landmark's location and adopt the fully convolutional framework of <ref type="bibr" target="#b30">[31]</ref>. Furthermore, instead of making single-shot predictions, almost all methods follow a cascaded approach making a number of intermediate predictions, refined in a sequential manner <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Notably, to further reduce the number of parameters of the cascaded approaches the method introduced in <ref type="bibr" target="#b29">[30]</ref> uses a recurrent neural network.</p><p>While achieving remarkable performance, all the aforementioned deep learning methods are computationally demanding, requiring at least one high-end GPU. In contrast, our network uses binary weights and activations and as such it is intended to run on systems with limited resources (e.g. embedded devices, smartphones).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Face Alignment</head><p>Current state-of-the-art for large pose 2D and 3D face alignment is also based on CNNs <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. However, despite their accuracy, these methods are computationally demanding. Our network produces stateof-the-art results for this task, yet it is designed to run on devices with limited computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heatmaps</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binarized</head><p>Real</p><formula xml:id="formula_0">+ + + + Fig. 2.</formula><p>The architecture of a single Hour-Glass (HG) network <ref type="bibr" target="#b1">[2]</ref>. Following <ref type="bibr" target="#b4">[5]</ref>, the first and last layers (brown colour) are left real while all the remaining layers are binarized.</p><p>The ResNet consists of two types of blocks: basic and bottleneck. We are interested only in the latter one which was designed to reduce the number of parameters and keep the network memory footprint under control. We use the "preactivation" version of <ref type="bibr" target="#b6">[7]</ref>, in which batch normalization <ref type="bibr" target="#b36">[37]</ref> and the activation function precede the convolutional layer. Note that we used the version of bottleneck defined in <ref type="bibr" target="#b1">[2]</ref> the middle layer of which has 128 channels (vs 64 used in <ref type="bibr" target="#b6">[7]</ref>).</p><p>The residual block is the main building block of the Hourglass (HG) network, shown in <ref type="figure">Fig. 2</ref>, which is a stateof-the-art architecture for landmark localization that predicts a set of heatmaps (one for each landmark) in a fully convolutional fashion. The HG network is an extension of <ref type="bibr" target="#b30">[31]</ref> allowing however for a more symmetric top-down and bottom-up processing. See also <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>Herein, we describe how we derive the proposed binary hierarchical, parallel and multi-scale block of <ref type="figure">Fig. 7e</ref>. In Section 4.6.1, by reducing the number of its parameters to match the ones of the original bottleneck, we further derive the block of <ref type="figure" target="#fig_1">Fig. 1b</ref>. This Section is organized as follows:</p><p>• We start by analyzing the performance of the binarized HG in Subsection 4.1 which provides the motivation as well as the baseline for our method.  We continue, by combining ideas from these architectures, we propose the binary hierarchical, parallel and multi-scale block of <ref type="figure">Fig. 7e</ref>. Note that the proposed block is not a trivial combination of the aforementioned architectures but a completely new structure.</p><p>• Finally, we attempt to make a fair comparison between the performance of the proposed block against that of the original bottleneck module for both real and binary cases.</p><p>We note that all results for this Section were generated for the task of human pose estimation using the standard training-validation partition of MPII <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Binarized HG</head><p>The binarization is accomplished using:</p><formula xml:id="formula_1">I * W ≈ (sign(I) sign(W)) * α,<label>(1)</label></formula><p>where I is the input tensor, W represents the layer weights, α ∈ R + is a scaling factor computed as the average of the absolute weight values and denotes the binary convolution operation which can be efficiently implemented with XNOR. We start from the original bottleneck blocks of the HG network and, following <ref type="bibr" target="#b4">[5]</ref>, we binarize them keeping only the first and last layers of the network real. See also <ref type="figure">Fig.  2</ref>. This is crucial, especially for the very last layer where higher precision is required for producing a dense output (heatmaps). Note that these layers account for less than 0.01% of the total number of parameters.</p><p>The performance of the original (real-valued) and the binarized HG networks can be seen in <ref type="figure" target="#fig_4">Fig. 3</ref> and <ref type="table" target="#tab_0">Table 1</ref>. We observe that binarization results in significant performance drop. As we may notice, for almost all parts, there is a large difference in performance which clearly indicates that the binary network has significant less representational power. Some failure cases are shown in <ref type="figure">Fig. 4</ref> illustrating that the binary network was not able to learn some difficult poses. We address this with a better architecture as detailed in the next four Subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">On the Width of Residual Blocks</head><p>The original bottleneck block of <ref type="figure">Fig. 7a</ref> is composed of 3 convolutional layers with a filter size of 1 × 1, 3 × 3 and  1 × 1, with the first layer having the role of limiting the width (i.e. the number of channels) of the second layer, thus greatly reducing the number of parameters inside the module. However, it is unclear whether the idea of having a bottleneck structure will be also successful for the binary case, too. Due to the limited representational power of the binary layers, greatly reducing the number of channels might reduce the amount of information that can be passed from one layer to another, leading to lower performance.</p><p>To investigate this, we modify the bottleneck block by increasing the number of channels in the thin 3 × 3 layer from 128 to 256. By doing so, we match the number of channels from the first and last layer, effectively removing the "bottleneck", and increasing the amount of information that can be passed from one block to another. The resulting wider block is shown in <ref type="figure">Fig. 7b</ref>. Here, "wider" 1 refers to the increased number of channels over the initial thin layer.</p><p>As <ref type="table" target="#tab_1">Table 2</ref> illustrates, while this improves performance against the baseline, it also raises the memory requirements. Conclusion: Widening the thin layer offers tangible performance improvement, however at a high computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">On Multi-Scale Filtering</head><p>Small filters have been shown both effective and efficient <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b38">[39]</ref> with models being solely made up by a combination of convolutional layers with 3 × 3 and/or 1 × 1 filters <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b38">[39]</ref>. For the case of real-valued networks, a large number of kernels can be learned. However, for the binary case, the number of possible unique convolutional kernels is limited to 2 k states only, where k is the size of the filter. Examples of such 3 × 3 learned filters are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. To address the limited representation power of 3 × 3 filters for the binary case, and similarly to <ref type="bibr" target="#b18">[19]</ref>, we largely depart from the block of <ref type="figure">Fig. 7b</ref> by proposing the multi-scale structure of <ref type="figure">Fig. 7c</ref>. Note that we implement our multi-scale approach using both larger filter sizes and max-pooling, which greatly increase the effective receptive field within the block. Also, because our goal is to analyze the impact of a multi-scale approach alone, we intentionally keep the number of parameters to a similar level to that of the original bottleneck block of <ref type="figure">Fig. 7a</ref>. To this end, we avoid a leap in the number of parameters, by (a) decomposing the 5 × 5 filters into two layers of 3 × 3 filters, and (b) by preserving the presence of thin layer(s) in the middle of the block.</p><p>Given the above, we split the input into two branches. The first (left) branch works at the same scale as the original bottleneck of <ref type="figure">Fig. 7a</ref> but has a 1 × 1 layer that projects the 256 channels into 64 (instead of 128) before going to the 3 × 3 one. The second (right) branch performs a multi-scale analysis by firstly passing the input through a max-pooling layer and then creating two branches, one using a 3 × 3 filter and a second one using a 5 × 5 decomposed into two 3 × 3. By concatenating the outputs of these two sub-branches, we obtain the remaining 64 channels (out of the 128 of the original bottleneck block). Finally, the two main branches are concatenated adding up to 128 channels, which are again back-projected to 256 with the help of a convolutional layer with 1 × 1 filters. <ref type="bibr" target="#b0">1</ref>. The term wider here strictly refers to a "moderate" increase in the number of channels in the thin layer (up to 256), effectively removing the "bottleneck". Except for the naming there is no other resemblance with <ref type="bibr" target="#b37">[38]</ref> which performs a study of wide vs deep, using a different building block alongside a much higher number of channels (up to 2048) and without any form of quantization. A similar study falls outside the scope of our work.</p><p>The accuracy of the proposed structure can be found in <ref type="table" target="#tab_1">Table 2</ref>. We can observe a healthy performance improvement at little additional cost and similar computational requirements to the original bottleneck of <ref type="figure">Fig. 7a</ref>. Conclusion: When designing binarized networks, multiscale filters should be preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">On 1 × 1 Convolutions</head><p>In the previously proposed block of <ref type="figure">Fig. 7c</ref>, we opted to avoid an increase in the number of parameters, by retaining the two convolutional layers with 1 × 1 filters. In this Subsection, by relaxing this restriction, we analyze the influence of 1 × 1 filters on the overall network performance.</p><p>In particular, we remove all convolutional layers with 1 × 1 filters from the multi-scale block of <ref type="figure">Fig. 7c</ref>, leading to the structure of <ref type="figure">Fig. 7d</ref>. Our motivation to remove 1 × 1 convolutions for the binary case is the following: because 1 × 1 filters are limited to two states only (either 1 or -1) they have a very limited learning power. Due to their nature, they behave as simple filters deciding when a certain value should be passed or not. In practice, this allows the input to pass through the layer with little modifications, sometimes actually blocking "good features" and hurting the overall performance by a noticeable amount. This is particularly problematic for the task of landmark localization, where a high level of detail is required for successful localization. Examples of this problem are shown in <ref type="figure" target="#fig_6">Fig. 6</ref>.</p><p>Results reported in <ref type="table" target="#tab_1">Table 2</ref> show that by removing 1 × 1 convolutions, performance over the baseline is increased by more than 8%. Even more interestingly, the newly introduced block outperforms the one of Subsection 4.2, while having less parameters, which shows that the presence of 1 × 1 filters limits the performance of binarized CNNs. Conclusion: The use of 1 × 1 convolutional filters on binarized CNNs has a detrimental effect on performance and should be avoided. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">On Hierarchical, Parallel &amp; Multi-Scale</head><p>Binary networks are even more sensitive to the problem of fading gradients <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and for our network we found that the gradients are up to 10 times smaller than those corresponding to its real-valued counterpart. To alleviate this, we design a new module which has the form of a hierarchical, parallel multi-scale structure allowing, for each resolution, the gradients to have 2 different paths to follow, the shortest of them being always 1. The proposed block is depicted in <ref type="figure">Fig. 7e</ref>. Note that, in addition to better gradient flow, our design encompasses all the findings from the previous Subsections: (a) no convolutional layers with 1 × 1 filters should be used, <ref type="bibr">(b)</ref> the block should preserve its width as much as possible (avoiding large drops in the number of channels), and (c) multi-scale filters should be used.</p><p>Contrary to the blocks described in Subsections 4.2 -4.4, where the gradients may need to pass through two more layers before reaching the output of the block, in the newly proposed module, each convolutional layer has a direct path that links it to the output, so that at any given time and for all the layers within the module the shortest possible path is equal to 1. The presence of a hierarchical structure inside the module efficiently accommodates larger filters (up to 7× 7), decomposed into convolutional layers with 3 × 3 filters. This allows for the information to be analysed at different scales because of the different filter sizes used (hence the term "multi-scale"). We opted not to use pooling because it results in loss of information. Furthermore, our design avoids the use of an element-wise summation layer as for example in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, further improving the gradient flow and keeping the complexity under control.</p><p>As we can see in <ref type="table" target="#tab_1">Table 2</ref>, the proposed block matches and even outperforms the block proposed in Section 4.3 having far less parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block type # params PCKh</head><p>Bottleneck (original) <ref type="figure">(Fig. 7a)</ref> 3.5M 67.2% Wider <ref type="figure">(Fig. 7b</ref>) 11.3M 70.7% Multi-Scale (MS) <ref type="figure">(Fig. 7c)</ref> 4.0M 69.3% MS without 1x1 filters <ref type="figure">(Fig. 7d</ref> Conclusion: Good gradient flow and hierarchical multiscale filtering are crucial for high performance without excessive increase in the parameters of the binarized network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Proposed vs Bottleneck</head><p>In this Section, we attempt to make a fair comparison between the performance of the proposed block (Ours, Final, as in <ref type="figure">Fig. 7e</ref>) against that of the original bottleneck module <ref type="figure">(Fig. 7a</ref>) by taking two important factors into account:</p><p>• Both blocks should have the same number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The two blocks should be compared for the case of binary but also real-valued networks.</p><p>With this in mind, in the following Sections, we show that:</p><p>• The proposed block largely outperforms a bottleneck with the same number of parameters for the binary case.   <ref type="figure">Fig. 7</ref>. Different types of blocks described and evaluated. Our best performing block is shown in figure (e). A layer is depicted as a rectangular block containing: its filter size, number of input channels and the number of output channels). "C" -denotes concatenation operation, "+" an element-wise sum and "UP" a bilinearly upsample layer.</p><p>• The proposed block also outperforms a bottleneck with the same number of parameters for the real case but in this case the performance difference is smaller.</p><p>We conclude that, for the real case, increasing the number of parameters (by increasing width) results in performance increase; however this is not the case for binary networks where a tailored design as the one proposed here is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Binary</head><p>To match the number of parameters between the proposed and bottleneck block, we follow two paths. Firstly, we increase the number of parameters of the bottleneck: (a) a first way to do this is to make the block wider as described in Section 4.2. Note that in order to keep the number or input-output channels equal to 256, the resulting block of <ref type="figure">Fig. 7b</ref> has a far higher number of parameters than the proposed block. Despite this, the performance gain is only moderate (see Section 4.2 and <ref type="table" target="#tab_1">Table 2</ref>). (b) Because we found that the 1 × 1 convolutional layers have detrimental effect to the performance of the Multi-Scale block of <ref type="figure">Fig. 7c</ref>, we opted to remove them from the bottleneck block, too. To this end, we modified the Wider module by (a) removing the 1 × 1 convolutions and (b) halving the number of parameters in order to match the number of parameters of the proposed block. The results in <ref type="table" target="#tab_1">Table 2</ref> clearly show that this modification is helpful but far from being close to the performance achieved by the proposed block.</p><p>Secondly, we decrease the number of parameters in the proposed block to match the number of parameters of the original bottleneck. This block is shown in <ref type="figure" target="#fig_1">Fig. 1b</ref>. To this end, we reduced the number of input-output channels of the proposed block from 256 to 192 so that the number of channels in the first layer are modified from [256 → 128, 3 × 3] to [192→96, 3×3], in the second layer from [128→64, 3×3] to [96→48, 3 × 3] and in the third layer from [64→64, 3 × 3] to [48→48, 3×3]. Notice, that even in this case, the proposed binarized module outperforms the original bottleneck block by more than 5% (in absolute terms) while both have very similar number of parameters (see <ref type="table" target="#tab_1">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Real</head><p>While the proposed block was derived from a binary perspective, <ref type="table" target="#tab_4">Table 3</ref> shows that a significant performance gain is also observed for the case of real-valued networks. In order to quantify this performance improvement and to allow for a fair comparison, we increase the number of channels inside the original bottleneck block so that both networks have the same depth and a similar number of parameters. For our binary block, in order to bring it back to the real valued domain, we simply replace the "sign" function with ReLU activations while keeping all the weights real. Even in this case, our block outperforms the original block although the gain is smaller than that observed for the binary case. We conclude that for real-valued networks performance increase can be more easily obtained by simply increasing the number of parameters, but for the binary case a better design is needed as proposed in this work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ABLATION STUDIES</head><p>In this Section, we present a series of other architectural variations and their effect on the performance of our binary network. All reported results are obtained using the proposed block of <ref type="figure">Fig. 7e</ref> coined Ours, Final. We focus on the effect of augmentation and different losses which are novel experiments not reported in <ref type="bibr" target="#b4">[5]</ref>, and then comment on the effect of pooling, ReLUs and performance speed-up. Is Augmentation required? Recent works have suggested that binarization is an extreme case of regularization <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b41">[40]</ref>. In light of this, one might wonder whether data augmentation is still required. <ref type="table">Table 4</ref> shows that in order to accommodate the presence of new poses and/or scale variations, data augmentation is very helpful providing a large increase (4%) in performance. See Section 6.1 for more details on how augmentation was performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4</head><p>The effect of using: augmentation, different losses (Sigmoid vs L2), different pooling methods and of adding a ReLU after the conv layer, when training our binary network in terms of PCKh-based performance on MPII validation set. We note that "(Ours, Final)" was trained using a Sigmoid Loss, Maxpooling and applying augmentation. The additional text after it denotes the change made. The effect of loss. We trained our binary network to predict a set of heatmaps, one for each landmark <ref type="bibr" target="#b26">[27]</ref>. To this end, we experimented with two types of losses: the first one places a Gaussian around the correct location of each landmark and trains using a pixel-wise L2 loss <ref type="bibr" target="#b26">[27]</ref>.</p><p>However, the gradients generated by this loss are usually small even for the case of a real-valued network. Because binarized networks tend to amplify this problem, as an alternative, we also experimented with the Sigmoid crossentropy pixel-wise loss typically used for detection tasks <ref type="bibr" target="#b42">[41]</ref>. We found that the use of the Sigmoid cross-entropy pixel-wise loss increased the gradients by 10-15x (when compared to the L2 loss), offering a 2% improvement (see <ref type="table">Table 4</ref>), after being trained for the same number of epochs.</p><p>Pooling type. In the context of binary networks, and because the output is restricted to 1 and -1, max-pooling might result in outputs full of 1s only. To limit this effect, we placed the activation function before the convolutional layers as proposed in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Additionally, we opted to replace max-pooling with average pooling. However, this leads to slightly worse results (see <ref type="table">Table 4</ref>). In practice, we found that the use of blocks with pre-activation suffices and that the ratio of 1 and -1 is close to 50% even after maxpooling.</p><p>With or without ReLU. Because during the binarization process all ReLU layers are replaced with the Sign function, one might wonder if ReLUs are still useful for the binary case. Our findings are in line with the ones reported in <ref type="bibr" target="#b4">[5]</ref>. By adding a ReLU activation after each convolutional layer, we observe a 2% performance improvement (see <ref type="table">Table 4</ref>), which can be attributed to the added non-linearity, particularly useful for training very deep architectures.</p><p>Performance. In theory, by replacing all floating-point multiplications with bitwise XOR and making use of the SWAR (Single instruction, multiple data within a register) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, the number of operations can be reduced up to 32x when compared against the multiplication-based convolution. However, in our tests, we observed speedups of up to 3.5x, when compared against cuBLAS, for matrix multiplications, a result being in accordance with those reported in <ref type="bibr" target="#b5">[6]</ref>. We note that we did not conduct experiments on CPUs. However, given the fact that we used the same method for binarization as in <ref type="bibr" target="#b4">[5]</ref>, similar improvements in terms of speed, of the order of 58x, are to be expected: as the realvalued network takes 0.67 seconds to do a forward pass on a i7-3820 using a single core, a speedup close to x58 will allow the system to run in real-time.</p><p>In terms of memory compression, by removing the biases, which have minimum impact (or no impact at all) on performance, and by grouping and storing every 32 weights in one variable, we can achieve a compression rate of 39x when compared against the single precision counterpart of Torch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">COMPARISON WITH STATE-OF-THE-ART</head><p>In this Section, we compare our method against the current state-of-the-art for human pose estimation and 3D face alignment. Our final system comprises a single HG network but replaces the real-valued bottleneck block used in <ref type="bibr" target="#b1">[2]</ref> with the proposed binary, parallel, multi-scale block trained with the improvements detailed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Training</head><p>All human pose estimation and 3D face alignment models were trained from scratch following the algorithm described in <ref type="bibr" target="#b4">[5]</ref> and using rmsprop <ref type="bibr" target="#b43">[42]</ref>. The initialization was done as in <ref type="bibr" target="#b15">[16]</ref>. For human pose estimation, we randomly augmented the data with rotation (between -40 o and 40 o degrees), flipping and scale jittering (between 0.7 and 1.3). We trained the network for 100 epochs, dropping the learning rate four times, from 2.5e-4 to 5e-5. A similar procedure was applied to the models for 3D face alignment, with the difference that the training was done for 55 epochs only. The input was normalized between 0 and 1 and all described networks were trained using the binary cross-entropy loss, defined as:</p><formula xml:id="formula_2">l = 1 N N n=1 W i=1 H j=1 p n ij log p n ij + (1 − p n ij ) log (1 − p n ij ),<label>(2)</label></formula><p>where p n ij denotes the ground truth confidence map of the n−th part at the output pixel location (i, j) and p n ij is the corresponding predicted output at the same location.</p><p>The models were implemented with Torch7 <ref type="bibr" target="#b44">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Human Pose Estimation.</head><p>As in all previous experiments, we used the standard training-validation partition of MPII <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. We report the performance of (a) the proposed binary block, (b) the proposed block when implemented and trained with real values, (c) the real-valued stacked HG network consisting of 8 stacked single real-valued HG networks trained with intermediate supervision (state-of-the-art on MPII <ref type="bibr" target="#b1">[2]</ref>) and, finally, (d) the same real-valued network as in (c) where the bottleneck block is replaced by our proposed block. The results are shown in <ref type="table" target="#tab_6">Table 5</ref>. We observe that when a single HG network with the proposed block is trained with real weights, its performance reaches that of <ref type="bibr" target="#b1">[2]</ref>. This result clearly illustrates the enhanced learning capacity of the proposed block. Moreover, there is still a gap between the binary and real-valued version of the proposed block indicating that margin for further improvement is possible. We also observe that a full-sized model (with 8 HG networks) based on the proposed block performs slightly better than the original network from <ref type="bibr" target="#b1">[2]</ref>, indicating that, for the real-valued case, the new block is more effective than the original one when a smaller computational budget is used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Face alignment.</head><p>We used three very challenging datasets for large pose face alignment, namely AFLW <ref type="bibr" target="#b45">[44]</ref>, AFLW-PIFA <ref type="bibr" target="#b46">[45]</ref>, and AFLW2000-3D <ref type="bibr" target="#b47">[46]</ref>. The evaluation metric is the Normalized Mean Error (NME) <ref type="bibr" target="#b46">[45]</ref>. AFLW is a large-scale face alignment dataset consisting of 25,993 faces annotated with up to 21 landmarks. The images are captured in arbitrary conditions exhibiting a large variety of poses and expressions. As <ref type="table" target="#tab_7">Table 6</ref> shows, our binarized network outperforms the state-of-the-art methods of <ref type="bibr" target="#b48">[47]</ref> and <ref type="bibr" target="#b33">[34]</ref>, both of which use large real-valued CNNs.  <ref type="figure" target="#fig_8">Fig. 8a</ref> and <ref type="table" target="#tab_9">Tables 7 and 8</ref> show our results on AFLW-PIFA. When evaluated on both visible and occluded points, our method improves upon the current best result of <ref type="bibr" target="#b32">[33]</ref> (which uses real weights) by more than 10%.</p><p>AFLW2000-3D is a subset of AFLW re-annotated by [46] from a 3D perspective with 68 points. We used this dataset only for evaluation. The training was done using the first 40,000 images from 300W-LP <ref type="bibr" target="#b47">[46]</ref>. As <ref type="figure" target="#fig_8">Fig. 8b</ref> shows, on AFLW2000-3D, the improvement over the state-of-the-art method of <ref type="bibr" target="#b47">[46]</ref> (real-valued) is even larger. As further results in <ref type="figure" target="#fig_10">Fig. 9</ref> show, while our method improves over the entire range of poses, the gain is noticeably higher for large poses ([60 • − 90 • ]), where we outperform <ref type="bibr" target="#b47">[46]</ref> by more than 40%. (see also <ref type="bibr" target="#b47">[46]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ADVANCED BLOCK ARCHITECTURES</head><p>In this section, we explore the effectiveness of two architectural changes applied to our best performing block (Ours, final), namely varying its depth and its cardinality. Again, we used the standard training-validation partition of MPII.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">On the depth of the proposed block</head><p>To further explore the importance of the multi-scale component in the overall structure of the proposed block, we gradually increase its depth and as a result, the number of its layers, as shown in <ref type="figure" target="#fig_10">Fig. 9b</ref>. The advantage of doing this is twofold: (a) it increases the receptive field within the block, and (b) it analyses the input simultaneously at multiple scales. We ensure that by doing so the number of parameters remains (approximately) constant. To this end, we halve the number of channels of the last layer at each stage. In the most extreme case, the last layer will have a single channel. Because, the representational power of such a small layer is insignificant, in practice we stop at a minimum of 4, which corresponds to a depth equal to 8. The results, reported in <ref type="figure" target="#fig_10">Fig. 9b</ref>, show that the performance gradually improves up to 76.5% for a depth equal to 6, and then, further on, it saturates and eventually gradually degrades as the depth increases.</p><p>Conclusion: The depth of the multi-scale component is an important factor on the overall module performance. Increasing it, up to a certain point, is beneficial and can further improve the performance at no additional cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">On the cardinality of the proposed block</head><p>Inspired by the recent innovations of <ref type="bibr" target="#b16">[17]</ref> for real-valued networks, in this section we explore the behavior of an increased cardinality (defined as in <ref type="bibr" target="#b16">[17]</ref> as the size of the set of transformations) when applied to our binary hierarchical, parallel &amp; multi-scale block.</p><p>Starting again from our block of <ref type="figure">Fig. 7e</ref>, we replicate its structure C times making the following adjustments in the process: (1) While the number of input channels of the first layer remains the same, the output and the input of the subsequent layers are reduced by a factor of C, and (2) the output of the replicated blocks is recombined via concatenation. The final module structure is depicted in <ref type="figure" target="#fig_1">Fig. 10b</ref>.</p><p>The full results with respect to the network size and the block cardinality (ranging from 1 to 16) are shown in <ref type="figure" target="#fig_1">Fig. 10b</ref>. Our findings are that increasing the block cardinality, while shown to provide good improvement on image classification using real-value networks, for the case of binary networks, given their significantly smaller size, depth and representational power, the same observation does not hold. In particular, when incorporated into the structure of our block with a similar number of parameters,    the module under-performs by 1% compared to the original block (having a cardinality equal to one). Conclusion: For the binary case, further increasing the block cardinality hurts performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">IMPROVED NETWORK ARCHITECTURES</head><p>In all previous sections, we investigated the performance of the various blocks by incorporating them into a single hourglass network, i.e. by keeping the network architecture fixed. In this section, we explore a series of architectural changes applied to the overall network structure. First, inspired by <ref type="bibr" target="#b52">[51]</ref>, we simplify the HG model, improving its performance without sacrificing accuracy for the binary case. Then, we study the effect of stacking multiple networks together and analyze their behavior.</p><p>Heatmaps C C C C <ref type="figure" target="#fig_1">Fig. 11</ref>. Improved, U-Net inspired, HG architecture. The dark-green modules were left unchanged, while for the light-green ones we doubled the number of their input channels from 256 to 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Improved HG architecture</head><p>Motivated by the findings of Subsection 4.5 that shed light on the importance of the gradient flow and suggested that skip connections with shorter paths should be used where possible, we adopt a similar approach to the overall HG architecture.</p><p>In particular, to improve the overall gradient flow, we removed the residual blocks in the upsampling branches that are tasked with the "injection" of high resolution information into the later stages of the network. To adjust to that change, the number of input channels of the first layer from the modules that are immediately after the point where the branch is merged via concatenation is increased by two times (to accommodate to the increase in the number of channels). The resulting architecture, depicted in <ref type="figure" target="#fig_1">Fig. 11</ref>, is a modified U-net architecture <ref type="bibr" target="#b52">[51]</ref> which was binarized in the same way as the HG model.</p><p>The results, reported in <ref type="table" target="#tab_0">Table 10</ref>, show that by removing the residual blocks from the upsampling branches, the performance, over the baseline HG is increased by 0.5%, further solidifying the importance of the gradient flow in the performance of binary networks. Furthermore, due to the decrease in the number of layers and parameters, an up to 20% speedup is observed. The network is trained using the same procedure described previously, for 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 10</head><p>Comparison between HG and Improved HG on the MPII validation set.</p><p>Both networks are built with our proposed binarized block.</p><p>Network architecture # parameters PCKh HG <ref type="figure">(Fig. 2)</ref> 6.2M 76% Improved HG <ref type="figure" target="#fig_1">(Fig. 11</ref>) 5.8M 76.6%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Stacked Binarized HG networks</head><p>Network stacking was recently shown to achieve state-ofthe-art results on human pose estimation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> when real-valued models are used. In this subsection, we explore whether the same holds for the binary case. Following <ref type="bibr" target="#b1">[2]</ref>, we stack and interconnect the networks as follows: The first network takes as input the RGB image and outputs a set of N heatmaps. The next network in the stack takes as input the sum of: (1) the input to the previous network, (2) the projection of the previously predicted heatmaps, and (3) the output of the last but one block from + <ref type="figure" target="#fig_1">Fig. 12</ref>. A two-stack binarized HG. All blocks are binarized, except for the very first and last layers showed in red colour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary,</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Binary, Original <ref type="figure" target="#fig_1">Fig. 13</ref>. Features extracted from the first layer (first column), middle layer (middle column) and right before the very last layer (right column) for real-valued and binary (ours and original) networks. As we move on to the last layers, activations become more noisy for the binary case, which we believe that it hurts the performance of the stacked networks.</p><p>the previous level. The resulting network for a stack of two is shown in <ref type="figure" target="#fig_1">Fig. 12</ref>.</p><p>As the results of <ref type="table" target="#tab_0">Table 11</ref> show, network stacking for the binary case behaves to some extent similarly to the realvalued case, however the gains from one stage to another are smaller, and performance seems to saturate faster. We believe that the main reason for this is that for the case of binary networks, activations are noisier especially for the last layers of the network. This is illustrated in <ref type="figure" target="#fig_1">Fig. 13</ref> where we compare the feature maps obtained from a real and the two types of binary networks compared in this paper (original, based on bottleneck and proposed). Clearly the feature maps for the binary case are more noisy and blurry as we move on to the last layers of the network. As network stacking relies on features from the earlier networks of the cascade and as these are noisy, we conclude that this has a negative impact on the overall network's performance.</p><p>Training. To speedup the training process, we trained the stacked version in a sequential manner. First, we trained the first network until convergence, then we added the second one on top of it, freezing its weights and training the second one. The process is repeated until all networks are added. Finally, the entire stack is trained jointly for 50 epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">ADDITIONAL EXPERIMENTS</head><p>In this section, we further show that the proposed block generalizes well producing consistent results across various datasets and tasks. To this end, we report results on the task of face parsing, also known as semantic facial part segmentation, which is the problem of assigning a categorical label to every pixel in a facial image. We constructed a dataset for facial part segmentation by joining together the 68 ground truth landmarks (originally provided for face alignment) to fully enclose each facial component. In total, we created seven classes: skin, lower lip, upper lip, inner mouth, eyes, nose and background. <ref type="figure" target="#fig_1">Fig. 14</ref> shows an example of a ground truth mask. We trained the network on the 300W dataset (approximately 3,000 images) and tested it on the 300W competition test set, both Indoor&amp;Outdoor subsets (600 images), using the same procedure described in Section 7. Architecture. We reused the same architecture for landmark localization, changing only the last layer in order to accommodate the different number of output channels (from 68 to 7). We report results for three different networks of interest: (a) a real-valued network using the original bottleneck block (called "Real, Bottleneck"), (b) a binary network using the original bottleneck block (called "Binary, Bottleneck"), and (c) a binary network using the proposed block (called "Binary, Ours"). To allow for a fair comparison, all networks have a similar number of parameters and depth. For training the networks, we used the Log-Softmax loss <ref type="bibr" target="#b30">[31]</ref>.</p><p>Results. <ref type="table" target="#tab_0">Table 12</ref> shows the obtained results. Similarly to our human pose estimation and face alignment experiments, we observe that the binarized network based on the proposed block significantly outperforms a similar-sized network constructed using the original bottleneck block, almost matching the performance of the real-valued network. Most of the performance improvement is due to the higher representation/learning capacity of our block, which is particularly evident for difficult cases like unusual poses, occlusions or challenging lighting conditions. For visual comparison, see <ref type="figure" target="#fig_1">Fig. 16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 12</head><p>Results on 300W (Indoor&amp;Outdoor). The pixel acc., mean acc. and mean IU are computed as in <ref type="bibr" target="#b30">[31]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>We proposed a novel block architecture, particularly tailored for binarized CNNs and localization visual tasks. During the process, we exhaustively evaluated various design choices, identified performance bottlenecks and proposed solutions. We showed that our hierarchical, parallel and multi-scale block enhances representational power, allowing for stronger relations to be learned without excessively increasing the number of network parameters. The proposed architecture is efficient and can run on limited resources. We verified the effectiveness of the proposed block on a wide range of fine-grained recognition tasks including human pose estimation, face alignment, and facial part segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground truth segmentation mask </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary bottleneck</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real bottleneck</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>(a) The original bottleneck layer of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>Then, we propose a series of architectural innovations in Subsections 4.2, 4.3, 4.4 and 4.5 (shown in Figs. 7b, 7c and 7d) each of which is evaluated and compared against the binarized residual block of Subsection 4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Cumulative error curves on MPII validation set for real-valued (red) and binary (blue) bottleneck blocks within the HG network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Examples of learned 3 × 3 binary filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Examples of features before and after a 1 × 1 convolutional layer. Often the features are copied over with little modifications, usually consisting in the details' removal. The contrast was altered for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(d) A variant of the MS block introduced in (c) after removing all convolutional layers with 1 × 1 filters (MS Without 1 × 1 filters). See Subsection 4.3. The proposed Hierarchical, Parallel &amp; MS (denoted in the paper as (Ours, final) block incorporates all ideas from (b), (c) and (d) with an improved gradient flow. See Subsection 4.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Cumulative error curves (a) on AFLW-PIFA, evaluated on all 34 points (CALE is the method of [33]), (b) on AFLW2000-3D on all points computed on a random subset of 696 images equally represented in [0 • , 30 • ], [30 • , 60 • ], [60 • , 90 • ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>of layers) PCKh, MPII validation set (%) Performance vs block depth (b) Depth vs PCKh-based performance on the MPII validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>The effect of varying the depth of the proposed binary block on performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>ResNetXt-like extension of (Ours, final) binary block. C represents the cardinality of the block. See also Subsection 7.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>validation set (%)Performance dependence on the module cardinality</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>7</head><label></label><figDesc>Number of parameters (milions)(b) Cardinality vs PCKh-based performance on the MPII validation set. Notice how the efficiency (the ratio between the number of parameters and PCKh) decreases as we increase the block cardinality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 .</head><label>10</label><figDesc>The effect of varying the cardinality of the proposed binary block on performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 14 .</head><label>14</label><figDesc>Example of a ground truth mask (right) produced by joining the 68 ground truth landmarks (left). Each colour denotes one of the seven classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 16 .</head><label>16</label><figDesc>Qualitative results on 300W (Indoor&amp;Outdoor). Observe that the proposed binarized network significantly outperforms the original binary one, almost matching the performance of the real-valued network. Georgios (Yorgos) Tzimiropoulos received the M.Sc. and Ph.D. degrees in Signal Processing and Computer Vision from Imperial College London, U.K. He is Assistant Professor with the School of Computer Science at the University of Nottingham, U.K. Prior to this, he was a Senior Researcher in the iBUG group, Department of Computing, Imperial College London. He is currently Associate Editor of the Image and Vision Computing Journal. He has worked on the problems of object detection and tracking, alignment and pose estimation, and recognition with humans and faces being the focal point of his research. For his work, he has used a variety of tools from Mathematical Optimization and Machine Learning. His current focus is on Deep Learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>PCKh error on MPII dataset for real-valued and binary bottleneck blocks within the HG network.</figDesc><table><row><cell>Crit.</cell><cell cols="2">Bottleneck (real) Bottleneck (binary)</cell></row><row><cell>Head</cell><cell>94.9</cell><cell>90.5</cell></row><row><cell>Shld</cell><cell>85.8</cell><cell>79.6</cell></row><row><cell>Elbow</cell><cell>76.9</cell><cell>63.0</cell></row><row><cell>Wrist</cell><cell>71.3</cell><cell>57.2</cell></row><row><cell>Hip</cell><cell>78.1</cell><cell>71.1</cell></row><row><cell>Knee</cell><cell>70.1</cell><cell>58.2</cell></row><row><cell>Ankle</cell><cell>63.2</cell><cell>53.4</cell></row><row><cell>PCKh</cell><cell>76.5</cell><cell>67.2</cell></row><row><cell># par.</cell><cell>3.5M</cell><cell>3.5M</cell></row></table><note>Binary Real Fig. 4. Examples of failure cases for the binarized HG (first row) and predictions of its real-valued counterpart (second row). The binary HG misses certain range of poses while having similar accuracy for the correct parts.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 PCKh</head><label>2</label><figDesc></figDesc><table /><note>-based comparison of different blocks on MPII validation set. # params refers to the number of parameters of the whole network.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>PCKh-based performance on MPII validation set for real-valued blocks: Our block is compared with a wider version of the original bottleneck so that both blocks have similar # parameters.</figDesc><table><row><cell>Layer type</cell><cell># parameters</cell><cell>PCKh</cell></row><row><cell>Bottleneck (wider)</cell><cell>7.0M</cell><cell>83.1%</cell></row><row><cell>(Ours, Final)</cell><cell>6.2M</cell><cell>85.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>PCKh-based comparison on MPII validation set. For "Ours, bin." we report the results of its best variation, which includes the ReLU layer introduced in Section 5.</figDesc><table><row><cell>Crit.</cell><cell>[2]</cell><cell cols="3">Ours, bin. Ours[1x], real Ours[8x], real</cell></row><row><cell>Head</cell><cell>97.3</cell><cell>94.7</cell><cell>96.8</cell><cell>97.4</cell></row><row><cell>Shld</cell><cell>96.0</cell><cell>89.6</cell><cell>93.8</cell><cell>96.0</cell></row><row><cell>Elbow</cell><cell>90.2</cell><cell>78.8</cell><cell>86.4</cell><cell>90.7</cell></row><row><cell>Wrist</cell><cell>85.2</cell><cell>71.5</cell><cell>80.3</cell><cell>86.2</cell></row><row><cell>Hip</cell><cell>89.1</cell><cell>79.1</cell><cell>87.0</cell><cell>89.6</cell></row><row><cell>Knee</cell><cell>85.1</cell><cell>70.5</cell><cell>80.4</cell><cell>86.1</cell></row><row><cell>Ankle</cell><cell>82.0</cell><cell>64.0</cell><cell>75.7</cell><cell>83.2</cell></row><row><cell>PCKh</cell><cell>89.3</cell><cell>78.1</cell><cell>85.5</cell><cell>89.8</cell></row><row><cell># par.</cell><cell>25M</cell><cell>6M</cell><cell>6M</cell><cell>25M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 NME</head><label>6</label><figDesc>• , 30 • ], [30 • , 60 • ] and [60 • , 90 • ]. All images are annotated with 34 points from a 3D perspective.</figDesc><table><row><cell cols="5">-based (%) comparison on AFLW test set. The evaluation is done</cell></row><row><cell cols="4">on the test set used in [34].</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">[0,30] [30,60] [60,90] mean</cell></row><row><cell>HyperFace [47]</cell><cell>3.93</cell><cell>4.14</cell><cell>4.71</cell><cell>4.26</cell></row><row><cell>AIO [34]</cell><cell>2.84</cell><cell>2.94</cell><cell>3.09</cell><cell>2.96</cell></row><row><cell>Ours</cell><cell>2.77</cell><cell>2.86</cell><cell>2.90</cell><cell>2.85</cell></row><row><cell cols="5">AFLW-PIFA [45] is a gray-scale subset of AFLW [44],</cell></row><row><cell cols="5">consisting of 5,200 images (3,901 for training and 1,299 for</cell></row><row><cell cols="5">testing) selected so that there is a balanced number of im-</cell></row><row><cell cols="2">ages for yaw angle in [0</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7 NME</head><label>7</label><figDesc></figDesc><table><row><cell cols="5">-based (%) comparison on AFLW-PIFA evaluated on visible</cell></row><row><cell cols="5">landmarks only. The results for PIFA, RCPR and PAWF are taken from</cell></row><row><cell></cell><cell></cell><cell>[32].</cell><cell></cell><cell></cell></row><row><cell cols="5">PIFA [45] RCPR [48] PAWF [32] CALE [33] Ours</cell></row><row><cell>8.04</cell><cell>6.26</cell><cell>4.72</cell><cell>2.96</cell><cell>3.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8 NME</head><label>8</label><figDesc></figDesc><table><row><cell cols="2">-based (%) based comparison on AFLW-PIFA evaluated on all 34</cell></row><row><cell cols="2">points, both visible and occluded.</cell></row><row><cell cols="2">CALE [33] Ours</cell></row><row><cell>4.97</cell><cell>4.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9 NME</head><label>9</label><figDesc></figDesc><table><row><cell cols="5">-based (%) based comparison on AFLW2000-3D evaluated on all</cell></row><row><cell cols="5">68 points, both visible and occluded. The results for RCPR, ESR and</cell></row><row><cell cols="3">SDM are taken from [46].</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">[0,30] [30,60] [60,90] Mean</cell></row><row><cell>RCPR(300W) [48]</cell><cell>4.16</cell><cell>9.88</cell><cell>22.58</cell><cell>12.21</cell></row><row><cell>RCPR(300W-LP) [48]</cell><cell>4.26</cell><cell>5.96</cell><cell>13.18</cell><cell>7.80</cell></row><row><cell>ESR(300W) [49]</cell><cell>4.38</cell><cell>10.47</cell><cell>20.31</cell><cell>11.72</cell></row><row><cell>ESR(300W-LP) [49]</cell><cell>4.60</cell><cell>6.70</cell><cell>12.67</cell><cell>7.99</cell></row><row><cell>SDM(300W) [50]</cell><cell>3.56</cell><cell>7.08</cell><cell>17.48</cell><cell>9.37</cell></row><row><cell>SDM(300W-LP) [50]</cell><cell>3.67</cell><cell>4.94</cell><cell>9.76</cell><cell>6.12</cell></row><row><cell>3DDFA [46]</cell><cell>3.78</cell><cell>4.54</cell><cell>7.93</cell><cell>5.42</cell></row><row><cell>3DDFA+SDM [46]</cell><cell>3.43</cell><cell>4.24</cell><cell>7.17</cell><cell>4.94</cell></row><row><cell>Ours</cell><cell>2.47</cell><cell>3.01</cell><cell>4.31</cell><cell>3.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 11</head><label>11</label><figDesc>Accuracy of stacked networks on MPII validation set. All networks are built with our proposed binarized block.</figDesc><table><row><cell cols="3"># stacks # parameters PCKh</cell></row><row><cell>1</cell><cell>6.2M</cell><cell>76%</cell></row><row><cell>2</cell><cell>11.0M</cell><cell>79.9%</cell></row><row><cell>3</cell><cell>17.8M</cell><cell>81.3%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Two-stage convolutional part heatmap regression for the 1st 3d face alignment in the wild (3dfaw) challenge,&quot; in ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finite precision error analysis of neural network hardware implementations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Holi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training deep neural networks with low precision multiplications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Talathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Annapureddy</surname></persName>
		</author>
		<title level="m">Fixed point quantization of deep convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">2d articulated human pose estimation and retrieval in (almost) unconstrained still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="214" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Upper body detection and tracking in extended signing sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="180" to="197" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3487" to="3494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3674" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
	<note>12th IEEE</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-pose face alignment via cnn-based dense 3d model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional aggregation of local evidence for large pose face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Face &amp; Gesture</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Godp: Globally optimized dual pathway deep network architecture for facial landmark localization in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fitting examples produced by our binary network on AFLW2000-3D dataset. Notice that our method copes well with extreme poses, facial expressions and lighting conditions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Qualitative results produced by our method on (a) AFLW2000-3D and (b) MPII datasets</title>
		<idno>Fig. 15</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modha</surname></persName>
		</author>
		<title level="m">Deep neural networks are robust to weight binarization and other non-linear distortions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fine-grained pose prediction, normalization, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07063</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COURSERA: Neural networks for machine learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Torch7: A matlablike environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W, no. EPFL-CONF-192376</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV-W</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pose-invariant 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01249</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adrian Bulat is currently a PhD student with the Computer Vision Laboratory at the University of Nottingham, under the supervision of Dr. Georgios Tzimiropoulos. He received his B.Eng. in Computer Engineering (2015) from the Technical University</title>
	</analytic>
	<monogr>
		<title level="j">Gheorghe Asachi</title>
		<imprint/>
	</monogr>
	<note>Romania</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
