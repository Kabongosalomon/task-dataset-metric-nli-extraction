<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset of 53,000+ Legal Holdings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Guha</surname></persName>
							<email>nguha@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
							<email>banderson@law.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henderson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Ho</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset of 53,000+ Legal Holdings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique. We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help. To address this, we first present CaseHOLD (Case Holdings On Legal Decisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we assess performance gains on CaseHOLD and existing legal NLP datasets. While a Transformer architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves performance, domain pretraining (using corpus of ≈3.5M decisions across all courts in the U.S. that is larger than BERT's) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD (gain of 7.2% on F1, representing a 12% improvement on BERT) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage resource-intensive pretraining and show that Transformerbased architectures, too, learn embeddings suggestive of distinct legal language. * These authors contributed equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>How can rapid advances in Transformer-based architectures be leveraged to address problems in law? One of the most significant advances in natural language processing (NLP) has been the advent of "pretrained" (or self-supervised) language models, starting with Google's BERT model <ref type="bibr" target="#b11">[12]</ref>. Such models are pretrained on a large corpus of general texts -Google Books and Wikipedia articlesresulting in significant gains on a wide range of fine-tuning tasks with much smaller datasets and have inspired a wide range of applications and extensions <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>One of the emerging puzzles for law has been that while general pretraining (on the Google Books and Wikipedia corpus) boosts performance on a range of legal tasks, there do not appear to be any meaningful gains from domain-specific pretraining (domain pretraining) using a corpus of law. Numerous studies have attempted to apply comparable Transformer architectures to pretrain language models on law, but have found marginal or insignificant gains on a range of legal tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. These results would seem to challenge a fundamental tenet of the legal profession: that legal language is distinct in vocabulary, semantics, and reasoning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44]</ref>. Indeed, a common refrain for the first year of U.S. legal education is that students should learn the "language of law": "Thinking like a lawyer turns out to depend in important ways on speaking (and reading, and writing) like a lawyer. " <ref type="bibr" target="#b28">[29]</ref>.</p><p>We hypothesize that the puzzling failure to find substantial gains from domain pretraining in law stem from the fact that existing arXiv:2104.08671v1 [cs.CL] 18 Apr 2021 finetuning tasks may be too easy and/or fail to correspond to the domain of the pretraining corpus task. We show that existing legal NLP tasks (whether a sentence overrules a prior case and classification of contractual terms of service) are simple enough for naive baselines (BiLSTM) or BERT (without domain-specific pretraining) to achieve high performance. Observed gains from domain pretraining are hence relatively small. Because U.S. law lacks any benchmark task that is comparable to the large, rich, and challenging datasets that have fueled the general field of NLP (e.g., SQuAD <ref type="bibr" target="#b35">[36]</ref>, GLUE <ref type="bibr" target="#b45">[46]</ref>, CoQA <ref type="bibr" target="#b36">[37]</ref>), we present a new dataset that simulates a fundamental task for lawyers: identifying the legal holding of a case. This CaseHOLD dataset (Case Holdings on Legal Decisions) provides 53,000+ multiple choice questions with prompts from a judicial decision and multiple potential holdings, one of which is correct, that could be cited. We construct this dataset using the rules of case citation <ref type="bibr" target="#b8">[9]</ref>, which allow us to match a proposition to a source through a comprehensive corpus of U.S. case law from 1965 to the present. Intuitively, we extract all legal citations and use the "holding statement," often provided in parenthetical propositions accompanying U.S. legal citations, to match context to holding <ref type="bibr" target="#b1">[2]</ref>. For instance, the context in a legal case might be:</p><p>In order to preserve a question for appellate review, the defendant must object stating the specific grounds for the ruling the party desired the court to make if the specific grounds were not apparent from the context. A general objection, when overruled, is ordinarily not adequate unless the evidence, considered as a whole, makes it clear that there is no purpose to be served from admitting the evidence.</p><p>This statement could be supported by the following legal citation, followed by the holding statement in parentheses:</p><p>State v. Perkins, 154 N.C. App. 148, 151-52 (2002) (holding that two general objections were insufficient to properly preserve the issue).</p><p>CaseHOLD extracts the context, legal citation, and holding statement and matches semantically similar, but inappropriate, holding propositions. This turns the identification of holding statements into a multiple choice task. We show that this task is difficult for conventional NLP approaches (BiLSTM F1 = 0.4 and BERT F1 = 0.6), even though law students and lawyers are able to solve the task at high accuracy. We then show that there are substantial and statistically significant performance gains from domain pretraining with a custom vocabulary (which we call Legal-BERT), using all available case law from 1965 to the present (a 7.2% gain in F1, representing a 12% boost from BERT). We then experimentally assess conditions for gains from domain pretraining with CaseHOLD and find that the size of the finetuning task is the principal other determinant of gains to domain-specific pretraining.</p><p>The code, the CaseHOLD dataset, and the Legal-BERT models presented here can be found at: https://github.com/reglab/casehold.</p><p>Our paper informs how researchers should decide when to engage in data and resource-intensive pretraining. Such decisions pose an important tradeoff, as cost estimates for fully pretraining BERT can be upward of $1M <ref type="bibr" target="#b40">[41]</ref>, but advances in legal NLP may also alleviate huge disparities in access to justice in the U.S. legal system <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47]</ref>. Our findings suggest that there is indeed something unique to legal language when faced with sufficiently challenging forms of legal reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The Transformer-based language model, BERT <ref type="bibr" target="#b11">[12]</ref>, which leverages a two step pretraining and fine-tuning framework, has achieved state-of-the-art performance on a diverse array of downstream NLP tasks. BERT, however, was trained on a general corpus of Google Books and Wikipedia, so much of the scientific literature has since focused on the question of whether the Transformer-based approach could be improved by domain-specific pretraining.</p><p>Outside of the law, for instance, Lee et al. <ref type="bibr" target="#b24">[25]</ref> show that BioBERT, a BERT model pretrained on biomedicine domain-specific corpora (PubMed abstracts and full text articles), can significantly outperform BERT on domain-specific biomedical NLP tasks. For instance, it achieves gains of 6-9% in strict accuracy compared to BERT <ref type="bibr" target="#b24">[25]</ref> for biomedical question answering tasks (BioASQ Task 5b and Task 5c) <ref type="bibr" target="#b44">[45]</ref>. Similarly, Beltagy et al. show improvements from domain pretraining with SciBERT, using a multi-domain corpus of scientific publications <ref type="bibr" target="#b2">[3]</ref>. On the ACL-ARC multiclass classification task <ref type="bibr" target="#b21">[22]</ref>, which contains example citations labeled with one of six classes, where each class is a citation function (background, motivation, uses, extension, comparison or contrast, future), SciBERT achieves gains of 7.07% in macro F1 <ref type="bibr" target="#b2">[3]</ref>. It is worth noting that this task is constructed from citation text, which is comparable to the CaseHOLD task we introduce in Section 3.</p><p>Yet work adapting this framework for the legal domain has not yielded comparable returns. Elwany at el. <ref type="bibr" target="#b13">[14]</ref> use a proprietary corpus of legal agreements to pretrain BERT and report "marginal" gains of 0.4 -0.7% on F1. They note that in some settings, such gains could still be practically important. Zhong et al. <ref type="bibr" target="#b48">[49]</ref> uses BERT pretrained on Chinese legal documents and finds no gains relative to non-pretrained NLP baseline models (e.g., LSTM). Similarly, <ref type="bibr" target="#b49">[50]</ref> finds that the same pretrained model performs poorly on a legal question and answer dataset.</p><p>Hendrycks et al. <ref type="bibr" target="#b18">[19]</ref> found that in zero-shot and few-shot settings, state-of-the-art models for question answering, GPT-3 and UnifiedQA, have lopsided performance across subjects, performing with near-random accuracy on subjects related to human values, such as law and morality, while performing up to 70% accuracy on other subjects. This result motivated their attempt to create a better model for the multistate bar exam by further pretraining RoBERTa <ref type="bibr" target="#b26">[27]</ref>, a variant of BERT, on 1.6M cases from the Harvard Law Library case law corpus. They found that RoBERTa finetuned on the bar exam task achieved 32.8% test accuracy without domain pretraining and 36.1% test accuracy with further domain pretraining. They conclude that while "additional pretraining on relevant high quality text can help, it may not be enough to substantially increase . . . performance. " Hendrycks et al. <ref type="bibr" target="#b17">[18]</ref> highlight that future research should especially aim to increase language model performance on tasks in subject areas such as law and moral reasoning since aligning future systems with human values and understanding of human approval/disapproval necessitates high performance on such subject specific tasks.</p><p>Chalkidis et al. <ref type="bibr" target="#b6">[7]</ref> explored the effects of law pretraining using various strategies and evaluate on a broader range of legal NLP tasks.</p><p>These strategies include (a) using BERT out of the box, which is trained on general domain corpora, (b) further pretraining BERT on legal corpora (referred to as LEGAL-BERT-FP), which is the method also used by Hendrycks et al. <ref type="bibr" target="#b18">[19]</ref>, and (c) pretraining BERT from scratch on legal corpora (referred to as LEGAL-BERT-SC). Each of these models is then finetuned on the downstream task. They report that a LEGAL-BERT variant, in comparison to tuned BERT, achieves a 0.8% improvement in F1 on a binary classification task derived from the ECHR-CASES dataset <ref type="bibr" target="#b4">[5]</ref>, a 2.5% improvement in F1 on the multi-label classification task derived from ECHR-CASES, and between a 1.1-1.8% improvement in F1 on multi-label classification tasks derived from subsets of the CONTRACTS-NER dataset <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. These gains are small when considering the substantial data and computational requirements of domain pretraining. Indeed, Hendrycks et al. <ref type="bibr" target="#b18">[19]</ref> concluded that the documented marginal difference does not warrant domain pretraining.</p><p>This existing work raises important questions for law and artificial intelligence. First, these results might be seen to challenge the widespread belief in the legal profession that legal language is distinct <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44]</ref>. Second, one of the core challenges in the field is that unlike general NLP, which has thrived on large benchmark datasets (e.g., SQuAD <ref type="bibr" target="#b35">[36]</ref>, GLUE <ref type="bibr" target="#b45">[46]</ref>, CoQA <ref type="bibr" target="#b36">[37]</ref>), there are few large and publicly available legal benchmark tasks for U.S. law. This is explained in part due to the expense of labeling decisions and challenges around compiling large sets of legal documents <ref type="bibr" target="#b31">[32]</ref>, leading approaches above to rely on non-English datasets <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> or proprietary datasets <ref type="bibr" target="#b13">[14]</ref>. Indeed, there may be a kind of selection bias in available legal NLP datasets, as they tend to reflect tasks that have been solved by methods often pre-dating the rise of self-supervised learning. Third, assessment standards vary substantially, providing little guidance to researchers on whether domain pretraining is worth the cost. Studies vary, for instance, in whether BERT is retrained with custom vocabulary, which is particularly important in fields where terms of art can defy embeddings of general language models. Moreover, some comparisons are between (a) BERT pretrained at 1M iterations and (b) domain-specific pretraining on top of BERT (e.g., 2M iterations) <ref type="bibr" target="#b24">[25]</ref>. Impressive gains might hence be confounded because the domain pretrained model simply has had more time to train. Fourth, legal language presents unique challenges in substantial part because of extensive and complicated system of legal citation. Work has shown that conventional tokenization that fails to account for the structure of legal citations can improperly present the legal text <ref type="bibr" target="#b19">[20]</ref>. For instance, sentence boundary detection (critical for BERT's next sentence prediction pretraining task) may fail with legal citations containing complicated punctuation <ref type="bibr" target="#b39">[40]</ref>. Just as using an in-domain tokenizer helps in multilingual settings <ref type="bibr" target="#b38">[39]</ref>, using a custom tokenizer should improve performance consistently for the "language of law. " Last, few have examined differences across the kinds of tasks where pretraining may be helpful.</p><p>We address these gaps for legal NLP by (a) contributing a new, large dataset with the task of identification of holding statements that comes directly from U.S. legal decisions, (b) assessing the conditions under which domain pretraining can help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE CASEHOLD DATASET</head><p>We present the CaseHOLD dataset as a new benchmark dataset for U.S. law. Holdings are, of course, central to the common law system. They represent the the governing legal rule when the law is applied to a particular set of facts. It is what is precedential and what litigants can rely on in subsequent cases. So central is the identification of holdings that it forms a canonical task for first-year law students to identify, state, and reformulate the holding. Thus, as for a law student, the goal of this task is two-fold: (1) understand case names and their holdings; (2) understand how to re-frame the relevant holding of a case to back up the proceeding argument.</p><p>CaseHOLD is a multiple choice question answering task derived from legal citations in judicial rulings. The citing context from the judicial decision serves as the prompt for the question. The answer choices are holding statements derived from citations following text in a legal decision. There are five answer choices for each citing text. The correct answer is the holding statement that corresponds to the citing text. The four incorrect answers are other holding statements.</p><p>We construct this dataset from the Harvard Law Library case law corpus (In our analyses below, the dataset is constructed from the holdout dataset, so that no decision was used for pretraining Legal-BERT.). We extract the holding statement from citations (parenthetical text that begins with "holding") as the correct answer and take the text before it as the citing text prompt. We insert a &lt;HOLD-ING&gt; token in the position of the citing text prompt where the holding statement was extracted. To select four incorrect answers for a citing text, we compute the TD-IDF similarity between the correct answer and the pool of other holding statements extracted from the corpus and select the most similar holding statements, to make the task more difficult. We set an upper threshold for similarity to rule out indistinguishable holding statements (here 0.75), which would make the task impossible. One of the virtues of this task setup is that we can easily tune the difficulty of the task by varying the context window, the number of potential answers, and the similarity thresholds. In future work, we aim to explore how modifying the thresholds and task difficulty affects results. In a human evaluation, the benchmark by a law student was an accuracy of 0.94. <ref type="bibr" target="#b0">1</ref> A full example of CaseHOLD consists of a citing text prompt, the correct holding statement answer, four incorrect holding statement answers, and a label 0-4 for the index of the correct answer. The ordering of indices of the correct and incorrect answers are random for each example and that unlike a multi-class classification task, the answer indices can be thought of as multiple choice letters (A, B, C, D, E), which do not represent classes with underlying meaning, but instead just enumerate the answer choices. <ref type="table" target="#tab_0">Table 1</ref> depicts a citation example consisting of a citing text prompt, its corresponding correct holding statement answer, and an incorrect holding statement answer choice. The full example has a DS score, a measure of domain specificity explained in Section 4, of 0.076. For simplicity, we use a fixed context window that may start mid-sentence. would result from pretrial publicity or the kind of prejudice that would require a change of venue. Moreover, the court finds that Johnson waived the issue by failing to renew or reurge her motion for a change of venue at the conclusion of jury selection on the ground that the voir dire of potential jurors demonstrated that the pool was so tainted with prejudice that she could not obtain a fair trial in this district. As the court observed in its pretrial ruling, at the second tier of the analysis of a motion for a change of venue, if the court concludes that no presumption of prejudice is warranted pretrial, the court must look at the voir dire testimony of potential trial jurors to determin 7 L.Ed.2d 909 <ref type="formula">(2004)</ref> holding that the defendant waived the issue of change of venue where the trial court denied the motion for a change of venue without prejudice stating that it was willing to reconsider the motion at any time during the jury selection process but the defendant never renewed the motion for a change of venue Holding Statement (incorrect answer)</p><p>holding that a change of venue has no affect on the applicable state law and that change of venue is but a change of courtrooms DS 0.076</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OTHER DATASETS</head><p>To provide a comparison on difficulty and domain specificity, we also rely on two other legal benchmark tasks. The three datasets are summarized in <ref type="table" target="#tab_2">Table 2</ref>. First, in terms of size, publicly available legal tasks are small compared to mainstream NLP datasets (e.g., SQuAD has 100,000+ questions). The cost of obtaining high-fidelity labeled legal datasets is precisely why pretraining is appealing for law <ref type="bibr" target="#b14">[15]</ref>. The Overruling dataset, for instance, required compensating attorneys to label each individual sentence. Once a company has collected that information, it may not want to distribute it freely for the research community. In the U.S. system, much of this meta-data is hence retained behind proprietary walls (e.g., Lexis and Westlaw), and the lack of large-scale U.S. legal NLP datasets has likely impeded scientific progress.</p><p>Second, <ref type="table" target="#tab_2">Table 2</ref> also provides a measure of domain specificity of each task. We calculate the average difference in pretrain loss between Legal-BERT (described more below) and BERT across tasks.</p><p>Intuitively, when the difference is large, the general corpus does not predict legal language very well. These values serve as a heuristic for task domain specificity. A positive value conveys that on average, Legal-BERT is able to reason more accurately about the task compared to base BERT after the pretraining phase, but before finetuning, which implies the task has higher legal domain-specificity.</p><p>The rank order from least to most domain-specific is: Terms of Service, Overruling, and CaseHOLD. This relative ordering makes substantive sense. CaseHOLD has high domain specificity since a holding articulates a court's precise, legal statement of the holding of a decision. The language of contractual terms-of-service may not be represented well in the case law corpus. At least on this measure, CaseHOLD appears to present a task where domain pretraining should help.</p><p>We now provide more detail of the two other benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overruling</head><p>The Overruling task is a binary classification task, where positive examples are overruling sentences and negative examples are nonoverruling sentences from the law. An overruling sentence is a statement that nullifies a previous case decision as a precedent, by a constitutionally valid statute or a decision by the same or higher ranking court which establishes a different rule on the point of law involved. The Overruling task dataset was provided by Casetext, a company focused on legal research software. Casetext selected positive overruling samples through manual annotation by attorneys and negative samples through random sampling sentences from the Casetext law corpus, consisting of the whole law. This procedure has a low false positive rate for negative samples because the prevalence of overruling sentences in the whole law is low. Less than 1% of cases overrule another case and within those cases, usually only a single sentence contains overruling language. Casetext validates this procedure by estimating the rate of false positives on a subset of sentences randomly sampled from the law and extrapolating this rate for the whole set of random samples to determine the proportion of sampled sentences to be reviewed by human reviewers for quality assurance.</p><p>Overruling has moderate to high domain specificity because the positive and negative overruling examples are sampled from the Casetext law corpus, so the language in the examples is quite specific to the law. The task's intermediate DS score in <ref type="table" target="#tab_2">Table 2</ref> supports this explanation. However, it is the easiest of the three legal benchmark tasks, since many overruling sentences are distinguishable from non-overruling sentences due to the specific and explicit language judges typically use when overruling. In his work on overruling language and speech act theory, Dunn cites several examples of judges employing an explicit performative form when overruling, using keywords such as "overrule", "disapprove", and "explicitly reject" in many cases <ref type="bibr" target="#b12">[13]</ref>. Language models, non-neural machine models, and even heuristics generally detect such keyword patterns effectively, so the structure of this task makes it less difficult compared to other tasks. Previous work has shown that SVM classifiers achieve high performance on similar tasks; Sulea et al. <ref type="bibr" target="#b30">[31]</ref> achieves a 96% F1 on predicting case rulings of cases judged by the French Supreme Court and Aletras et al. <ref type="bibr" target="#b0">[1]</ref> achieves 79% accuracy on predicting judicial decisions of the European Court of Human Rights.</p><p>The Overruling task is important for lawyers because the process of verifying the authorities of cases are still valid and cases have not been overruled is critical to ensuring the validity of legal arguments. This need has led to the broad adoption of proprietary systems, such as Shepard's (on Lexis Advance) and KeyCite (on Westlaw), which have become important legal research tools for most lawyers <ref type="bibr" target="#b10">[11]</ref>. High language model performance on the Overruling tasks could enable further automation of the shepardizing process.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, we show a positive example of an overruling sentence and a negative example of a non-overruling sentence from the Overruling task dataset. Positive examples have label 1 and negative examples have label 0. In the selected examples, the positive example has greater DS score because it contains phrases with distinct or unique meaning in law, such as "instant case", "first district", and "fourth district", whereas the negative example does not involve many words with meanings specific to law.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Terms of Service</head><p>The Terms of Service task is a binary classification task, where positive examples are potentially unfair contractual terms (clauses) from the terms of service in contract documents. Article 3 of the Directive 93/13 on Unfair Terms in Consumer Contracts <ref type="bibr" target="#b16">[17]</ref> defines an unfair contractual term as follows. A contractual term is unfair if: (1) it has not been individually negotiated; and (2) contrary to the requirement of good faith, it causes a significant imbalance in the parties rights and obligations, to the detriment of the consumer.</p><p>The Terms of Service dataset comes from Lippi et al. <ref type="bibr" target="#b25">[26]</ref>, which studies machine learning and natural language approaches for automating the detection of potentially unfair clauses in online terms of service and implements a system called CLAUDETTE based on the results of the study. The dataset was constructed from a corpus of 50 online consumer contracts. Clauses were manually annotated as clearly fair, potentially unfair, and clearly unfair. Positive examples were taken to be potentially unfair or clearly unfair clauses and negative examples were taken to be clearly fair clauses to dichotomize the task. Lippi et al. <ref type="bibr" target="#b25">[26]</ref> also studies a multi-class setting in which each clause is additionally labeled according to one of eight categories of clause unfairness (e.g. limitation of liability). We focus on the more general setting where clauses are only labeled according to whether they encompass any type of unfairness.</p><p>Terms of Service has low domain specificity relative to the Overruling and CaseHOLD tasks because examples are drawn from the terms of service text in consumer contracts. Such private law examples are less likely to be represented in the Casetext and Harvard Law Library case law corpus. Its lower DS score in <ref type="table" target="#tab_2">Table 2</ref> supports this qualitative assessment. The Terms of Service task is moderately difficult. Lippi et al. <ref type="bibr" target="#b25">[26]</ref> evaluate several models on the Terms of Service task, including several SVM classifiers, a CNN, and an LSTM. Excluding ensemble methods, they find that the classifier with highest F1 performance in the general setting is a single SVM exploiting bag-of-words features, which achieves a 76.9% F1. The Terms of Service task is useful for consumers, since automation of the detection of potentially unfair contractual terms could help consumers better understand the terms they agree to when signing a contract and make legal advice about unfair contracts more accessible and widely available for consumers seeking it. It could also help consumer protection organizations and agencies work more efficiently <ref type="bibr" target="#b25">[26]</ref>.</p><p>In <ref type="table" target="#tab_4">Table 4</ref>  <ref type="table" target="#tab_4">Table 4</ref> contain mostly common language, which is also true of the other examples in the Terms of Service dataset, and therefore have lower DS scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">METHODS</head><p>Our basic approach to understand the conditions for when domain pretraining may help is to use a series of pretrained BERT models, but to carefully vary one key modeling decision at a time. This is computationally expensive requiring approximately 16 TPU (64 GPU) core-days per 1M steps. First, we assess performance with base BERT. Second, we train BERT with twice the number of iterations to be able to compare the value of additional training. Third, we ingest the entire Harvard Law case corpus from 1965 to the present and pretrain Legal-BERT on the corpus. The size of this dataset (37GB) is substantial, representing 3,446,187 legal decisions across all federal and state courts, and is larger than the size of the BookCorpus/Wikipedia corpus originally used to train BERT (15GB). Fourth, we train a custom vocabulary variant of Legal-BERT. We provide a comparison to a BiLSTM baseline. We now provide details of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline</head><p>Our baseline architecture is a one-layer BiLSTM, with 300D word2vec vectors <ref type="bibr" target="#b29">[30]</ref>. For single-sentence tasks, Overruling and Terms of Service, we encode the sentence and pass the resulting vector to a softmax classifier. For CaseHOLD, each citation prompt has five answer choices associated with it. We concatenate the prompt with each one of the five answers, separated by the &lt;SEP&gt; token, to get five prompt-answer pairs. We independently encode each promptanswer pair and pass the resulting vector through a linear layer, then apply softmax over the concatenated outputs for the five pairs. We choose this architecture because it is comparable to the design suggested for fine-tuning BERT on multiple choice tasks in Radford et al. <ref type="bibr" target="#b34">[35]</ref>, where prompt-answer pairs are fed independently through BERT and a linear layer. In this architecture, we replace BERT with the BiLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">BERT</head><p>We use the base BERT model (uncased, 110M parameters) <ref type="bibr" target="#b11">[12]</ref> as our baseline BERT model. Because researchers in other disciplines have commonly performed domain pretraining starting with BERT's parameter values, we also train a model initialized with base BERT and pretrained for an additional 1,000,000 steps, using the same English Wikipedia corpus that BERT base was pretrained on. This facilitates a direct comparison to rule out gains solely from increased pretraining. We refer to this model, trained for 2,000,000 total steps, as BERT (double), and compare it to our two Legal-BERT variants, each pretrained for 2,000,000 total steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Legal-BERT</head><p>We pretrain two variants of BERT with the Harvard Law case corpus (https://case.law/) from 1965 to the present. <ref type="bibr" target="#b1">2</ref> We randomly sample 10% of decisions from this corpus as a holdout set, which we use to create the CaseHOLD dataset. The remaining 90% is used for pretraining. We preprocess the case law corpus with the sentence segmentation procedure and use the pretraining procedure described in Devlin et al. <ref type="bibr" target="#b11">[12]</ref>. One variant is initialized with the BERT base model and pretrained for an additional 1,000,000 steps using the case law corpus and the same vocabulary as BERT (uncased). The other variant, which we refer to as Custom Legal-BERT, is pretrained from scratch for 2,000,000 steps using the case law corpus and has a custom legal domain-specific vocabulary. The vocabulary set is constructed using SentencePiece <ref type="bibr" target="#b23">[24]</ref> on a subsample (appx. 13M) of sentences from our pretraining corpus, with the number of tokens fixed to 32,000. We pretrain both variants with sequence length 128 for 90% and sequence length 512 for 10% over the 2,000,000 steps total.</p><p>Both Legal-BERT and Custom Legal-BERT are pretrained using the masked language model (MLM) pretraining objective, with whole word masking. Whole word masking and other knowledge masking strategies, like phrase-level and entity-level masking, have been shown to yield substantial improvements on various downstream NLP tasks for English and Chinese text, by making the MLM objective more challenging and enabling the model to learn more about prior knowledge through syntactic and semantic information extracted from these linguistically-informed language units <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43]</ref>. More recently, Kang et al. <ref type="bibr" target="#b22">[23]</ref> posit that whole-word masking may be most suitable for domain adaptation on emrQA <ref type="bibr" target="#b32">[33]</ref>, a corpus for question answering on electronic medical records, because most words in emrQA are tokenized to sub-word Word-Piece tokens <ref type="bibr" target="#b47">[48]</ref> in base BERT due to the high frequency of unique, domain-specific medical terminologies that appears in emrQA, but are not in the base BERT vocabulary. Because the case law corpus shares this property of containing many domain-specific terms relevant to the law, which are likely tokenized into sub-words in base BERT, we chose to use whole word masking for pretraining the Legal-BERT variants on the legal domain-specific case law corpus.</p><p>The second pretraining task is next sentence prediction. Here, we use regular expressions to ensure that legal citations are included as part of a segmented sentence according to the Bluebook system of legal citation <ref type="bibr" target="#b8">[9]</ref>. Otherwise, the model could be poorly trained on improper sentence segmentation <ref type="bibr" target="#b39">[40]</ref>. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS 6.1 Base Setup</head><p>After pretraining the models as described in Methods, we fine-tune on the legal benchmark target tasks and evaluate the performance of each model. 6.1.1 Hyperparameter Tuning. We split each task dataset into a train and test set with an 80/20 split for hyperparameter tuning. For the baseline model, we performed a random search with batch size set to 16 and 32 over learning rates in the bounded domain 1e-5 to 1e-2, training for a maximum of 20 epochs. To set the model hyperparameters for fine-tuning our BERT and Legal-BERT models, we refer to the suggested hyperparameter ranges for batch size, learning rate and number of epochs in Devlin et al. <ref type="bibr" target="#b11">[12]</ref> as a reference point and perform two rounds of grid search for each task. We performed the coarse round of grid search with batch size set to 16 for Overruling and Terms of Service and batch size set to 128 for Citation, over learning rates: 1e-6, 1e-5, 1e-4, training for a maximum of 4 epochs. From the coarse round, we discovered that the optimal learning rates for the legal benchmark tasks were smaller than the lower end of the range suggested in Devlin et al. <ref type="bibr" target="#b11">[12]</ref>, so we performed a finer round of grid search over a range that included smaller learning rates. For Overruling and Terms of Service, we performed the finer round of grid search over batch sizes <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b31">32)</ref> and learning rates (5e-6, 1e-5, 2e-5, 3e-5, 5e-5), training for a maximum of 4 epochs. For CaseHOLD, we performed the finer round of grid search with batch size set to 128 over learning rates (1e-6, 3e-6, 5e-6, 7e-6, 9e-6), training for a maximum of 4 epochs. We report the hyperparameters used for evaluation in <ref type="table">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Fine-tuning and Evaluation.</head><p>For the BERT-based models, we use the input transformations described in Radford et al. <ref type="bibr" target="#b34">[35]</ref> for fine-tuning BERT on classification and multiple choice tasks, which convert the inputs for the legal benchmark tasks into token sequences that can be processed by the pretrained model, followed by a linear layer and a softmax. For the CaseHOLD task, we avoid making extensive changes to the architecture used for the two classification tasks by converting inputs consisting of a prompt and five answers into five prompt-answer pairs (where the prompt and answer are separated by a delimiter token) that are each passed independently through our pretrained models followed by a linear layer, then take a softmax over the five concatenated outputs. For Overruling and Terms of Service, we use a single NVIDIA V100 (16GB) GPU to finetune on each task. For CaseHOLD, we used eight NVIDIA V100 (32GB) GPUs to finetune on the task.</p><p>We use 10-fold cross-validation to evaluate our models on each task. We use F1 score as our performance metric for the Overruling and Terms of Service tasks and macro F1 score as our performance metric for CaseHOLD, reporting mean F1 scores over 10 folds. We report our model performance results in <ref type="table" target="#tab_5">Table 5</ref> and report statistical significance from (paired) -tests with 10 folds of the test data to account for uncertainty.</p><p>From the results of the base setup, for the easiest Overruling task, the difference in F1 is between BERT (double) and Legal-BERT is 0.5% and BERT (double) and Custom Legal-BERT is 1.6%. Both of these differences are marginal. For the task with intermediate difficulty, Terms of Service, we find that BERT (double) with further pretraining BERT on the general domain corpus increases performance over base BERT by 5.1%, but the Legal-BERT variants with domain-specific pretraining do not outperform BERT (double) substantially. This is likely because Terms of Service has low domain-specificity, so pretraining on legal domain-specific text does not help the model learn information that is highly relevant to the task. We note that BERT (double), with 77.3% F1, and Custom Legal-BERT, with 78.7% F1, outperform the highest performing model from Lippi et al. <ref type="bibr" target="#b25">[26]</ref> for the general setting of Terms of Service, by 0.4% and 1.8% respectively. For the most difficult and domain-specific task, CaseHOLD, we find that Legal-BERT and Custom Legal-BERT both substantially outperform BERT (double) with gains of 5.7% and 7.2% respectively. Custom Legal-BERT achieves the highest F1 performance for CaseHOLD, with a macro F1 of 69.5%.</p><p>We run paired -tests to validate the statistical significance of model performance differences for a 95% confidence interval. The mean differences between F1 for paired folds of BERT (double) and base BERT are statistically significant for the Terms of Service task, with -value &lt; 0.001. Additionally, the mean differences between F1 for paired folds of Legal-BERT and BERT (double) with -value &lt; 0.001 and the mean differences between F1 for paired folds of Custom Legal-BERT and BERT (double) with -value &lt; 0.001 are statistically significant for the CaseHOLD task. The substantial performance gains from the Legal-BERT model variants were achieved likely because the CaseHOLD task is adequately difficult and highly domain-specific in terms of language.</p><p>These results outline an increasing relationship between the legal domain specificity of a task, as measured by the DS score (compatible with our qualitative assessments of the tasks), and the degree to which prior legal knowledge captured by the model through unsupervised pretraining improves performance. Additionally, the Overruling results suggest that there exists an interplay between the legal domain specificity of a task and the difficulty of the task, as measured by baseline performance on non-attention based models. Gains from attention based models and domain pretraining may be limited for lower difficulty tasks, even those with intermediate DS scores, such as Overruling, likely because the task is easy enough provided local context that increased model domain awareness is only marginally beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Task Variants</head><p>To provide a further assessment on the conditions for pretraining, we evaluate the performance and sensitivity of our models on three task variants of CaseHOLD, the task for which we observe the most substantial gains from domain pretraining. We vary the task on three dimensions: the volume of training data available for finetuning (train volume), the difficulty of the prompt as controlled by the length of the prompt (prompt difficulty), and the level of domain specificity of the prompt (domain match). We hypothesize that these dimensions -data volume, prompt difficulty, and domain specificity -capture the considerations practitioners must account for in considering whether pretraining is beneficial for their use case. For the task variants, we split the CaseHOLD task dataset into three train and test set folds using an 80/20 split over three random seeds and evaluate on each fold. We report results as the mean F1 over the three folds' test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Train Volume.</head><p>For the train volume variant, keeping the test set constant, we vary the train set size to be of size 1, 10, 100, 500, 1,000, 5,000, 10,000, and the full train set. We find that the Legal-BERT gains compared to BERT (double) are strongest with low train volume and wash out with high train volume. As we expect, Legal-BERT gains are larger when the finetuning dataset is smaller. In settings with limited training data, the models must rely more on prior knowledge and Legal-BERT's prior knowledge is more relevant to the highly domain-specific task due to pretraining on legal domain-specific text, so we see stronger gains from Legal-BERT compared to BERT (double). For a training set size of 1, the mean gain in Legal-BERT is 17.6% ± 3.73, the maximal gain across train set sizes.</p><p>This particular variant is well-motivated because it has often been challenging to adapt NLP for law precisely because there is limited labeled training data available. Legal texts typically require specialized legal knowledge to annotate, so it can often be prohibitively expensive to construct large structured datasets for the legal domain <ref type="bibr" target="#b15">[16]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Prompt Difficulty.</head><p>For the difficulty variant, we vary the citing text prompt difficulty, by shortening the length of the prompt to the first words. The average length of a prompt in the CaseHOLD task dataset is 136 words, so we take the first = 5, 10, 20, 40, 60, 80, 100 words of the prompt and the full prompt. We find that the prompt difficulty variant does not result in a clear pattern of increasing gains from Legal-BERT over BERT (double) above 20 words, though we would expect to see the gains grow as the prompt is altered more. However, a 2% drop in gain is seen in the 5 word prompt (the average F1 gap above 20 words is 0.062, while at 5 is 0.0391).</p><p>One possible reason we do not observe a clear pattern may be that the prompt length upper bounds the degree to which we can manipulate the prompt and vary this axis; the expected relationship may be more clearly observed for a dataset with longer prompts. Additionally, BERT models are known to disregard word order <ref type="bibr" target="#b41">[42]</ref>. It is possible that beyond 5 words, there is a high likelihood that a key word or phrase is encountered that Legal-BERT has seen in the pretraining data and can attend to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Domain Match.</head><p>For the domain match variant, we weight the predictions for the test set when calculating F1 by sorting the examples in ascending order by their DS score and weighting each example by the its rank order. Intuitively, this means the weighted F1 score rewards correct predictions on examples with higher domain specificity more. This method allows us to keep train volume constant and also avoid changing the domain specificity of the examples in the train set, which would occur if we took the test set to be examples corresponding to a certain range of DS scores, and still observe the effects of the domain specificity of the examples in the test set on model performance. We expect that the gains in Legal-BERT compared to BERT are stronger for the weighted F1 than the unweighted F1. We find that the mean gain in Legal-BERT over three folds is greater for the weighted F1 compared to the unweighted F1, but only by a difference of 0.8% ± 0.154, as shown in <ref type="table" target="#tab_6">Table 6</ref>. One possible reason this occurs is that the range of DS scores across examples in the CaseHOLD task is relatively small, so some similarly domain-specific examples may have fairly different rankbased weights. In <ref type="figure" target="#fig_3">Figure 3</ref>, we show histograms of the DS scores of examples for Terms of Service, CaseHOLD, and 1,000 examples sampled (without replacement) from each task. Notice that the Terms of Service examples are skewed towards negative DS scores and the CaseHOLD examples are skewed towards positive DS scores so the range of DS scores within a task is limited, while the examples sampled from both tasks span a larger range, explaining the small gains from the domain match variant, but more substantial gains for CaseHOLD from Legal-BERT compared to Terms of Service. In other words, because the CaseHOLD task is already quite domain specific, variation within the corpus may be too range-restricted to provide a meaningful test of domain match.</p><p>Further work could instead examine domain match by pretraining on specific areas of law (e.g., civil law) and fine-tuning on other areas (e.g., criminal law), but the Harvard case law corpus does not have meaningful case / issue type features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Limitations</head><p>While none of the CaseHOLD cases exist in the pretraining dataset, some of Legal-BERT gains on the CaseHOLD task may be attributable to having seen key words tied to similar holding formulations in the pretraining data. As mentioned, this is part of the goal of the task: understanding the holdings of important cases in a minimally labeled way and determining how the preceding context may affect the holding. This would explain the varying results in the prompt length variant of the CaseHOLD task: gains could be mainly coming from attending to only a key word (e.g., case name) in the context. This may also explain how Legal-BERT is able to achieve zero-shot gains in the train volume variant of the task. BERT, may have also seen some of the cases and holdings in English Wikipedia, 4 potentially explaining its zero-shot performance improvements over random in the train volume variant. Future work on the CaseHOLD dataset may wish to disentangle memorization of case names from the framing of the citing text, but we provide a strong baseline here. One possible mechanism for this is via a future variant of the CaseHOLD task where a case holding is paraphrased to indicate bias toward a different viewpoint from the contextual framing. This would reflect the first-year law student exercise of re-framing a holding to persuasively match their argument and isolate the two goals of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Our results resolve an emerging puzzle in legal NLP: if legal language is so unique, why have we seen only marginal gains to domain pretraining in law? Our evidence suggests that these results can be explained by the fact that existing legal NLP benchmark tasks are either too easy or not domain matched to the pretraining corpus. Our paper shows the largest gains documented for any legal task from pretraining, comparable to the largest gains reported by SciBERT and BioBERT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>. Our paper also shows the highest performance documented for the general setting of <ref type="bibr" target="#b3">4</ref> See, e.g., https://en.wikipedia.org/wiki/Supreme_Court_of_Missouri which contains a list of cases and their holdings. the Terms of Service task <ref type="bibr" target="#b25">[26]</ref>, suggesting substantial gains from domain pretraining and tokenization.</p><p>Using a range of legal language tasks that vary in difficulty and domain-specificity, we find BERT already achieves high performance for easy tasks, so that further domain pretraining adds little value. For the intermediate difficulty task that is not highly domainspecific, domain pretraining can help, but gain is most substantial for for highly difficult and domain-specific tasks.</p><p>These results suggest important future research directions. First, we hope that the new CaseHOLD dataset will spark interest in solving the challenging environment of legal decisions. Not only are many available benchmark datasets small or unavailable, but they may also be biased toward solvable tasks. After all, a company would not invest in the Overruling task (baseline F1 with BiLSTM of 0.91), without assurance that there are significant gains to paying attorneys to labeling the data. Our results show that domain pretraining may enable a much wider range of legal tasks to be solved.</p><p>Second, while the creation of large legal NLP datasets is impeded by the sheer cost of attorney labeling, CaseHOLD also illustrates an advantage of leveraging domain knowledge for the construction of legal NLP datasets. Conventional segmentation would fail to take advantage of the complex system of legal citation, but investing in such preprocessing enables better representation and extraction of legal texts.</p><p>Third, our research provides guidance for researchers on when pretraining may be appropriate. Such guidance is sorely needed, given the significant costs of language models, with one estimate suggesting that full pretraining of BERT with a 15GB corpus can exceed $1M. Deciding whether to pretrain itself can hence have significant ethical, social, and environmental implications <ref type="bibr" target="#b3">[4]</ref>. Our research suggests that many easy tasks in law may not require domain pretraining, but that gains are most likely when ground truth labels are scarce and the task is sufficiently in-domain. But because estimates of domain-specificity across tasks match our qualitative understanding, this heuristic can also be deployed to determine whether pretraining is worth it. The Custom Legal-BERT results on CaseHOLD suggest that for other difficult and high DS legal tasks, experimentation with custom, task relevant approaches, such as leveraging corpora from task-specific subdomains, building domainspecific model vocabularies, or applying tokenization/segmentation tailored to the characteristics of in-domain text, may yield substantial gains. Bender et al. <ref type="bibr" target="#b3">[4]</ref> discussed the significant environmental costs associated with training large language models and in particular, transferring an existing model to a new task or developing new models, which can multiply training costs by thousands of times, since these workflows require retraining to experiment with different model architectures and hyperparameters. DS provides a quick metric for future practitioners to evaluate when resource intensive experimentation on custom or new models may be warranted on other legal tasks. DS scores may also be readily extended to estimate the domain-specificity of tasks in other domains with existing pretrained models like SciBERT and BioBERT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>In sum, we have shown that a new benchmark task, the Case-HOLD dataset, and a comprehensively pretrained Legal-BERT model illustrate the conditions for domain pretraining and suggests that language models, too, can embed what may be unique to legal language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, we show a positive example of a potentially unfair clause and a negative example of a fair clause from the Terms of Service dataset. Positive examples have label 1 and negative examples have label 0. The selected examples in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Mean macro F1 scores over 3 folds, with ±1.96 × standard error, for train volume variant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Mean macro F1 scores over 3 folds, with ±1.96 × standard error, for prompt difficulty variant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Histograms of DS scores of examples for Terms of Service task, CaseHOLD task, and 1,000 examples sampled from each task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>CaseHOLD example</figDesc><table><row><cell>Citing Text (prompt)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Dataset overview</figDesc><table><row><cell>Dataset</cell><cell>Source</cell><cell>Task Type</cell><cell>Size</cell><cell>DS</cell></row><row><cell cols="2">Overruling Casetext</cell><cell cols="3">Binary classification 2,400 -0.028</cell></row><row><cell>Terms of</cell><cell>Lippi et</cell><cell cols="3">Binary classification 9,414 -0.085</cell></row><row><cell>Service</cell><cell>al. [26]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CaseHOLD Authors</cell><cell cols="3">Multiple choice QA 53,137 0.084</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Overruling examples</figDesc><table><row><cell>Passage</cell><cell>Label</cell><cell>DS</cell></row><row><cell>for the reasons that follow, we approve the first district</cell><cell>1</cell><cell>0.051</cell></row><row><cell>in the instant case and disapprove the decisions of the</cell><cell></cell><cell></cell></row><row><cell>fourth district.</cell><cell></cell><cell></cell></row></table><note>a subsequent search of the vehicle revealed the pres- ence of an additional syringe that had been hidden inside a purse located on the passenger side of the vehicle.0 -0.072</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Terms of Service examples</figDesc><table><row><cell>Passage</cell><cell>Label</cell><cell>DS</cell></row><row><cell>occasionally we may, in our discretion, make changes</cell><cell>1</cell><cell>0.023</cell></row><row><cell>to the agreements.</cell><cell></cell><cell></cell></row><row><cell>this section contains service-specific terms that are in</cell><cell>0</cell><cell>-0.068</cell></row><row><cell>addition to the general terms.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Test performance, with ±1.96 × standard error, aggregated across 10 folds. Mean F1 scores are reported for Overruling and Terms of Service. Mean macro F1 scores are reported for CaseHOLD. The best scores are in bold.</figDesc><table><row><cell>Model</cell><cell>Baseline</cell><cell>BERT</cell><cell>BERT (double)</cell><cell>Legal-BERT</cell><cell>Custom Legal-BERT</cell></row><row><cell>Number of Pretraining Steps</cell><cell>-</cell><cell>1M</cell><cell>2M</cell><cell>2M</cell><cell>2M</cell></row><row><cell>Vocabulary Size (domain)</cell><cell>-</cell><cell cols="3">30,522 (general) 30,522 (general) 30,522 (general)</cell><cell>32,000 (legal)</cell></row><row><cell>Overruling</cell><cell>0.910 ± 0.012</cell><cell>0.958 ± 0.005</cell><cell>0.958 ± 0.005</cell><cell>0.963 ± 0.007</cell><cell>0.974 ± 0.005</cell></row><row><cell>Terms of Service</cell><cell>0.712 ± 0.020</cell><cell>0.722 ± 0.015</cell><cell>0.773 ± 0.019</cell><cell>0.750 ± 0.018</cell><cell>0.787 ± 0.013</cell></row><row><cell>CaseHOLD</cell><cell>0.399 ± 0.005</cell><cell>0.613 ± 0.005</cell><cell>0.623 ± 0.003</cell><cell>0.680 ± 0.003</cell><cell>0.695 ± 0.003</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Mean gain in Legal-BERT over 3 folds, for domain match variant.</figDesc><table><row><cell cols="4">Mean macro F1 BERT (double) Legal-BERT Mean Gain</cell></row><row><cell>Unweighted</cell><cell>0.620</cell><cell>0.679</cell><cell>0.059</cell></row><row><cell>Weighted</cell><cell>0.717</cell><cell>0.784</cell><cell>0.067</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset of 53,000+ Legal Holdings Lucia Zheng * zlucia@stanford.edu Stanford University Stanford, California, USA Neel Guha * nguha@stanford.edu Stanford University Stanford, California, USA Brandon R. Anderson banderson@law.stanford.edu Stanford University Stanford, California, USA Peter Henderson phend@stanford.edu Stanford University</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This human benchmark was done on a pilot iteration of the benchmark dataset and may not correspond to the exact TF-IDF threshold presented here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use this observation period because there is a significant change in the number of reporters around this period and it corresponds to the modern post-Civil Rights Act era.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Where the vagaries of legal citations create detectable errors in sentence segmentation (e.g., sentences with fewer than 3 words), we omit the sentence from the corpus.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Devshi Mehrotra and Amit Seru and for research assistance, Casetext for providing the Overruling dataset, Stanford's Institute for Human-Centered Artificial Intelligence for cloud computing support, and Pablo Arredondo, Urvashi Khandelwal, Chris Manning, and Javed Qadrud-Din for helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A HYPERPARAMETERS </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting judicial decisions of the European Court of Human Rights: A natural language processing perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tsarapatsanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preoţiuc-Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Lampos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Harvesting and Utilizing Explanatory Parentheticals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arredondo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SCL Rev</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">659</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1371" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmargaret</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural Legal Judgment Prediction in English</title>
		<idno type="DOI">10.18653/v1/P19-1424</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1424" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4317" to="4323" />
		</imprint>
	</monogr>
	<note>Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting Contract Elements</title>
		<idno type="DOI">10.1145/3086512.3086515</idno>
		<ptr target="https://doi.org/10.1145/3086512.3086515" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Edition of the International Conference on Articial Intelligence and Law</title>
		<meeting>the 16th Edition of the International Conference on Articial Intelligence and Law<address><addrLine>London, United Kingdom; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
	<note>ICAIL &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LEGAL-BERT: The Muppets straight out of Law School</title>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.261</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.findings-emnlp.261" />
	</analytic>
	<monogr>
		<title level="m">Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2898" to="2904" />
		</imprint>
	</monogr>
	<note>Findings of the Association for Computational Linguistics: EMNLP 2020</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prodromos Malakasiotis, and Ion Androutsopoulos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilias</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manos</forename><surname>Fergadiotis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1x6fa95UH" />
	</analytic>
	<monogr>
		<title level="j">Neural Contract Element Extraction Revisited. Workshop on Document Intelligence at NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Columbia Law Review, The Harvard Law Review</title>
	</analytic>
	<monogr>
		<title level="m">Columbia Law Review Ass&apos;n, Harvard Law Review Ass&apos;n, and Yale Law Journal</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>The University of Pennsylvania Law Review, and The Yale Law Journal</orgName>
		</respStmt>
	</monogr>
	<note>The Bluebook: A Uniform System of Citation. 21st ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08101</idno>
		<title level="m">Pre-Training with Whole Word Masking for Chinese BERT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Citators: Past, Present, and Future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dabney</surname></persName>
		</author>
		<idno type="DOI">https:/arxiv.org/abs/https:/doi.org/10.1080/02703190802365671</idno>
		<ptr target="https://doi.org/10.1080/02703190802365671" />
	</analytic>
	<monogr>
		<title level="j">Legal Reference Services Quarterly</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="165" to="190" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How judges overrule: Speech act theory and the doctrine of stare decisis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pintip Hompluem Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yale LJ</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page">493</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">BERT Goes to Law School: Quantifying the Competitive Advantage of Access to Large Legal Corpora in Contract Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Elwany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Oberoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00473</idno>
		<ptr target="http://arxiv.org/abs/1911.00473" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithmic accountability in the administrative state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Freeman Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yale J. on Reg</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">800</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>David Freeman Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano-Florentino</forename><surname>Sharkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cuéllar</surname></persName>
		</author>
		<title level="m">Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies. Administrative Conference of the United States</title>
		<meeting><address><addrLine>Washington DC, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Council Directive 93/13/EEC of 5 April 1993 on unfair terms in consumer contracts</title>
		<imprint>
			<date type="published" when="1993" />
			<publisher>European Union</publisher>
		</imprint>
	</monogr>
	<note>European Union</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Critch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02275</idno>
		<title level="m">Dawn Song, and Jacob Steinhardt. 2021. Aligning AI With Shared Human Values</title>
		<imprint/>
	</monogr>
	<note>cs.CY</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03300</idno>
		<title level="m">Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding</title>
		<imprint/>
	</monogr>
	<note>cs.CY</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">LexNLP: Natural language processing and information extraction for legal and regulatory texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Bommarito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">Martin</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Detterman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03688</idno>
		<ptr target="http://arxiv.org/abs/1806.03688" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving Pre-training by Representing and Predicting Spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00300" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Measuring the Evolution of a Scientific Field through Citation Frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raine</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00028</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00028" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minki</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.493</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.493" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6102" to="6120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<title level="m">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btz682" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemysław</forename><surname>Pałka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Contissa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Lagioia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Wolfgang</forename><surname>Micklitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Sartor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Torroni</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10506-019-09243-2</idno>
		<ptr target="https://doi.org/10.1007/s10506-019-09243-2" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="139" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The language of the law</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mellinkoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Wipf and Stock Publishers</publisher>
			<pubPlace>Eugene, Oregon</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The language of law school: learning to&quot; think like a lawyer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Mertz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring the Use of Text Classification in the Legal Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Octavia-Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Zampieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liviu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd Workshop on Automated Semantic Analysis of Information in Legal Texts (ASAIL)</title>
		<meeting>2nd Workshop on Automated Semantic Analysis of Information in Legal Texts (ASAIL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How to build a more open justice system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Pah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">L</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Sanga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">D</forename><surname>Clopton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dicola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><forename type="middle">Davis</forename><surname>Mersey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><forename type="middle">S</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><forename type="middle">J</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís A. Nunes</forename><surname>Amaral</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aba6914</idno>
		<ptr target="https://science.sciencemag.org/content/369/6500/134.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="page" from="134" to="136" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">emrQA: A Large Corpus for Question Answering on Electronic Medical Records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anusri</forename><surname>Pampari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1258</idno>
		<ptr target="https://doi.org/10.18653/v1/D18-1258" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2357" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Queudot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Charton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Jean</forename><surname>Meurs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">2020. Improving Access to Justice with Legal Chatbots. Stats</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="356" to="375" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
		<ptr target="https://doi.org/10.18653/v1/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coqa: A conversational question answering challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A primer in bertology: What we know about how bert works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15613</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sentence boundary detection in adjudicatory decisions in the united states. Traitement automatique des langues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaromir</forename><surname>Savelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">D</forename><surname>Grabmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barak</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Shoham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08900</idno>
		<title level="m">The Cost of Training NLP Models: A Concise Overview</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koustuv</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Parthasarathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00010</idno>
		<title level="m">Unnatural Language Inference</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">ERNIE: Enhanced Representation through Knowledge Integration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Legal Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Tiersma</surname></persName>
		</author>
		<ptr target="https://books.google.com/books?id=Sq8XXTo3A48C" />
		<imprint>
			<date type="published" when="1999" />
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago, Illinois</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ion Androutsopoulos, and Georgios Paliouras. 2015. An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergios</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Polychronopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Almirantis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Baskiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Artiéres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Heino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliana</forename><surname>Barrio-Alvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schroeder</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-015-0564-6</idno>
		<ptr target="https://doi.org/10.1186/s12859-015-0564-6" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
		<ptr target="https://doi.org/10.18653/v1/W18-5446" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">AI Goes to Court: The Growing Landscape of AI for Access to Justice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://medium.com/legal-design-and-innovation/ai-goes-to-court-the-growing-landscape-of-ai-for-access-to-justice-3f58aca4306f" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.466</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.466" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5218" to="5230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">JEC-QA: A Legal-Domain Question Answering Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6519</idno>
		<ptr target="https://doi.org/10.1609/aaai.v34i05.6519" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9701" to="9708" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
