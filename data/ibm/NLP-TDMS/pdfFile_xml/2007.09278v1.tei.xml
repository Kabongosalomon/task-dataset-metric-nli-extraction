<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XingGAN for Person Image Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
							<email>hao.tang@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Huawei Research</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">XingGAN for Person Image Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative Adversarial Networks (GANs)</term>
					<term>Person Image Generation</term>
					<term>Appearance Cues</term>
					<term>Shape Cues</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel Generative Adversarial Network (Xing-GAN or CrossingGAN) for person image generation tasks, i.e., translating the pose of a given person to a desired one. The proposed Xing generator consists of two generation branches that model the person's appearance and shape information, respectively. Moreover, we propose two novel blocks to effectively transfer and update the person's shape and appearance embeddings in a crossing way to mutually improve each other, which has not been considered by any other existing GAN-based image generation work. Extensive experiments on two challenging datasets, i.e., Market-1501 and DeepFashion, demonstrate that the proposed XingGAN advances the state-of-the-art performance both in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at https://github.com/Ha0Tang/XingGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of person image generation aims to generate photo-realistic person images conditioned on an input person image and several desired poses. This task has a wide range of applications such as person image/video generation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref> and person re-identification <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b27">28]</ref>. Exiting methods such as <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b34">35]</ref> have achieved promising performance on this challenging task. For example, Zhu et al. <ref type="bibr" target="#b44">[45]</ref> recently proposed a conditional GAN model that comprises a sequence of pose-attentional transfer blocks. Wherein, each block transfers certain regions it attends to and progressively generates the desired person image.</p><p>Although <ref type="bibr" target="#b44">[45]</ref> performed an interesting exploration, we still observe unsatisfactory aspects and visual artifacts in the generated person images due to several reasons. First, <ref type="bibr" target="#b44">[45]</ref> stacks several convolution layers to generate the attention maps of the shape features, then the generated attention maps are used to attentively highlight the appearance features. Since convolutional operations are building blocks that process one local neighborhood at a time, this means that they cannot capture the joint influence between the appearance and the shape features. Second, the attention maps in <ref type="bibr" target="#b44">[45]</ref> are only produced by using one single modality, i.e., the pose, leading to insufficiently accurate correlations arXiv:2007.09278v1 [cs.CV] 17 Jul 2020 <ref type="figure">Fig. 1</ref>: Overview of the proposed Xing generator. Both the Shape-guided Appearance-based generation (SA) and the Appearance-guided Shape-based generation (AS) branches consist of a sequence of SA and AS blocks in a crossing way. All these components are trained in an end-to-end fashion so that the SA branch and AS branch can benefit from each other to generate more shapeconsistent and appearance-consistent person images.</p><p>for both modalities (i.e., the pose and the image modality), and thus misguiding the image generation.</p><p>Based on these observations, we propose a novel Generative Adversarial Network (XingGAN or CrossingGAN), which consists of a Xing generator, a shape-guided discriminator, and an appearance-guided discriminator. The overall framework is shown in <ref type="figure">Fig. 1</ref>. The Xing generator consists of three parts, i.e., a Shape-guided Appearance-based generation (SA) branch, an Appearanceguided Shape-based generation (AS) branch, and a co-attention fusion module. Specifically, the proposed SA branch contains a sequence of SA blocks, which aim to progressively update the appearance representation under the guidance of the shape representation, while the proposed AS branch contains a sequence of AS blocks, which aim to progressively update the shape representation under the guidance of the appearance representation. We also present a novel crossing operation in both SA and AS blocks to capture the joint influence between the image modality and the pose modality by creating attention maps jointly produced by both modalities. Moreover, we introduce a co-attention fusion model to better fuse the final appearance and shape features to generate the desired person images. We present an appearance-guided discriminator and a shape-guided discriminator to jointly judge how likely is that the generated image contains the same person in the input image and how well the generated image aligns with the targeted pose, respectively. The proposed XingGAN is trained in an end-toend fashion so that the generation branches can enjoy the mutually improved benefits from each other.</p><p>We conduct extensive experiments on two challenging datasets, i.e., Market-1501 <ref type="bibr" target="#b43">[44]</ref> and DeepFashion <ref type="bibr" target="#b19">[20]</ref>. Qualitative and quantitative results demonstrate that XingGAN achieves better results than state-of-the-art methods, regarding both visual fidelity and alignment with targeted person poses.</p><p>To summarize, the contributions of our paper are three-fold:</p><p>-We propose a novel XingGAN (or CrossingGAN) for person image generation. It explores cascaded guidance with two different generation branches, and aims at progressively producing a more detailed synthesis from both person shape and appearance embeddings. -We propose SA and AS blocks, which effectively transfer and update person shape and appearance features in a crossing way to mutually improve each other, and are able to significantly boost the quality of the final outputs.</p><p>-Extensive experiments clearly demonstrate the effectiveness of XingGAN, and show new state-of-the-art results on two challenging datasets, i.e., Market-1501 <ref type="bibr" target="#b43">[44]</ref> and DeepFashion <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b7">[8]</ref> consist of a generator and a discriminator where the goal of the generator is to produce photo-realistic images so that the discriminator cannot tell the generated images apart from real images. GANs have shown the capability of generating photo-realistic images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>. However, it is still hard for vanilla GANs to generate images in a controlled setting. To fix this limitation, Conditional GANs (CGANs) <ref type="bibr" target="#b22">[23]</ref> have been proposed. Image-to-Image Translation aims to learning the translation mapping between target and input images. CGANs have achieved decent results in pixelwise aligned image-to-image translation tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1]</ref>. For example, Isola et al.</p><p>propose Pix2pix <ref type="bibr" target="#b11">[12]</ref>, which adopts CGANs to generate the target domain images based on the input domain images, such as photo-to-map, sketch-to-image, and night-to-day. However, pixel-wise alignment is not suitable for person image generation tasks due to the shape deformation between the input person image and target person image. Person Image Generation. To remedy this, several works started to use poses to guide person image generation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45]</ref>. For example, Ma et al. first present PG2 <ref type="bibr" target="#b20">[21]</ref>, which is a two-stage model to generate the target person images based on an input image and the target poses. Moreover, Siarohin et al. propose PoseGAN <ref type="bibr" target="#b30">[31]</ref>, which requires an extensive affine transformation computation to deal with the input-output misalignment caused by pose differences. Zhu et al.</p><p>propose Pose-Transfer <ref type="bibr" target="#b44">[45]</ref>, which contains a sequence of pose-attentional transfer blocks to generate the target person image progressively. Besides the aforementioned supervised methods, several works focus on solving this task in an unsupervised setting <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>. For instance, Pumarola et al. propose an unsupervised framework <ref type="bibr" target="#b26">[27]</ref> to generate person images, which induces some geometric errors as revealed in their paper. Note that the aforementioned methods adopt human keypoints or skeleton as pose guidance, which are usually extracted by using OpenPose <ref type="bibr" target="#b3">[4]</ref>. In addition, several works adopt DensePose <ref type="bibr" target="#b23">[24]</ref>, 3D pose <ref type="bibr" target="#b17">[18]</ref>, and segmented pose <ref type="bibr" target="#b5">[6]</ref> to generate person images because they contain more information about body depth and part segmentation, producing better results with more texture details. However, the keypoint-based pose representation is much cheaper and more flexible than the DensePose, 3D pose, segmented pose representations, and can be more easily applied to practical applications. Therefore, we favor keypoint-based pose representation in this paper. Image-Guidance Conditioning Schemes. Recently, there were proposed many schemes to incorporate the extra guidance (e.g., human poses <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45]</ref>, segmentation maps <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, facial landmarks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42]</ref>, etc) into an image-to-image translation model, which can be divided into four categories, i.e., input concatenation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>, feature concatenation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, one-way guidance-toimage interaction <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26]</ref>, two-way guidance-and-image interaction <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>The most straightforward way of conditioning the guidance is to concatenate the input image and the guidance along the channel dimension. For example, C2GAN <ref type="bibr" target="#b34">[35]</ref> takes the input person image and the targeted poses as input to output the corresponding targeted person images. Instead of concatenating the guidance and the image at the input, several works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7]</ref> concatenate their feature representations at a certain layer. For instance, PG2 <ref type="bibr" target="#b20">[21]</ref> concatenates the embedded pose feature with the embedded image feature at the bottleneck fully connected layer. Another more general scheme is to use the guidance to guide the generation of the image. For example, Siarohin et al. <ref type="bibr" target="#b30">[31]</ref> first learn an affine transformation between the input and the target pose, then they use it to 'move' the feature maps between the input image and the targeted image. Unlike existing one-way guidance-to-image interaction schemes that allow information flow only from the guidance to the input image, a recent scheme, i.e., two-way guidance-and-image interaction, also considers the information flow from the input image back to the guidance <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b0">1]</ref>. For example, Zhu et al. <ref type="bibr" target="#b44">[45]</ref> propose an attention-based GAN model to simultaneously update the person's appearance and shape features under the guidance of each other, and show that the proposed two-way guidance-and-image interaction strategy leads to better performance on person image generation tasks.</p><p>Contrary to the existing two-way guidance-and-image interaction schemes <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b0">1]</ref> that allow both the image and guidance to guide and update each other in a local way, we show that the proposed cross-conditioning strategy can further improve the performance of person image generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Xing Generative Adversarial Networks</head><p>We start by presenting the details of the proposed XingGAN ( <ref type="figure">Fig. 1</ref>) consisting of three parts, i.e., a Shape-guided Appearance-based generation (SA) branch modeling the person shape representation, an Appearance-guided Shape-based generation (AS) branch modeling the person appearance representation, and a Co-Attention Fusion (CAF) module for fusing these two branches. In the following, we first present the design of the two proposed generation branches, and then introduce the co-attention fusion module. Lastly, we present the proposed two discriminators, the overall optimization objective and implementation details.</p><p>The inputs of the proposed Xing generator are the source image I s , the source pose P s , and the target pose P t . The goal is to translate the pose of the person in the source image I s from the source pose P s to the target pose P t , thus synthesizing a photo-realistic person image I t . In this way, the source image I s provides the appearance information and the poses (P s , P t ) provide the shape information to the Xing generator for synthesizing the desired person image. Shape-Guided Appearance-Based Generation. The proposed Shape-guided Appearance-based generation (SA) branch consists of an image encoder and a series of the proposed SA blocks. The source image I s is first fed into the image encoder to produce the appearance code F I 0 , as shown in <ref type="figure">Fig. 1</ref>. The encoder consists of two convolutional layers in our experiments. The SA branch contains several cascaded SA blocks which progressively update the initial appearance code F I 0 to the final appearance code F I T under the guidance of the AS branch. As we can see in <ref type="figure">Fig. 1</ref>, all SA blocks have an identical network structure. Consider the t-th block in <ref type="figure" target="#fig_0">Fig. 2</ref>, whose inputs are the appearance code F I t−1 ∈R c×h×w and the shape code F P t−1 ∈R c×h×w . The output is the refined appearance code F I t ∈R c×h×w . Specifically, given the appearance code F I t−1 , we first feed it into a convolution layer to generate a new appearance code C, where C∈R c×h×w . Then we reshape C to R c×(hw) , where n=hw is the number of pixels. At the same time, the SA block receives the shape code F P t−1 from the AS branch, which is also fed into a convolution layer to produce a new shape code B∈R c×h×w and then reshape to R c×(hw) . After that, we perform a matrix multiplication between the transpose of C and B, and apply a Softmax layer to produce a correlation matrix P ∈R (hw)×(hw) ,</p><formula xml:id="formula_0">p ji = exp(B i C j ) n i=1 exp(B i C j ) ,<label>(1)</label></formula><p>where p ji measures the impact of the i-th position of B on the j-th position of the appearance code C. In this crossing way, the SA branch can capture more joint influence between the appearance code F I t−1 and shape code F P t−1 , producing a richer appearance code F I t . Note that Eq. (1) has a close relationship with the non-local operator proposed by Wang et al. <ref type="bibr" target="#b37">[38]</ref>. The major difference is that the non-local operator in <ref type="bibr" target="#b37">[38]</ref> computes the pairwise similarity within the same feature map to incorporate global information, whereas the proposed crossing way computes the pairwise similarity between different feature maps, i.e., the person appearance and shape feature maps.</p><p>After that, we feed F I t−1 into a convolution layer to produce a new appearance code A∈R c×h×w and reshape it to R c×(hw) . We then perform a matrix multiplication between A and the transpose of P and reshape the result to R c×h×w . Finally, we multiply the result by a scale parameter α and conduct an elementwise sum operation with the original appearance code F I t−1 to obtain the refined appearance code F I t ∈ R c×h×w ,</p><formula xml:id="formula_1">F I t = α n i=1 (p ji A i ) + F I t−1 ,<label>(2)</label></formula><p>where α is 0 in the beginning and but is gradually updated. By doing so, each position of the refined appearance code F I t is a weighted sum of all positions of the shape code F P t−1 and the previous appearance code F I t−1 . Thus, it has a global contextual view between F P t−1 and F I t−1 , and it selectively aggregates useful contexts according to the correlation matrix P . Appearance-Guided Shape-Based Generation. In our preliminary experiments, we observe that only the SA generation branch is not sufficient to learn such a complex deformable translation process. Intuitively, since the shape features can guide the appearance features, we believe the appearance features can also be used to guide the shape features in turn. Therefore, we also propose an Appearance-guided Shape-based generation (AS) branch. The proposed AS branch mainly consists of a pose encoder and a sequence of AS blocks, as shown in <ref type="figure">Fig. 1</ref>. The source pose P s and target pose P t are first concatenated along the channel dimension and then fed into the pose encoder to produce the initial shape representation F P 0 . The pose encoder has the same network structure as the image encoder. Note that to capture the dependency between the two poses, we only adopt one pose encoder.</p><p>The AS branch contains several cascaded AS blocks, which progressively update the initial shape code F P 0 to the final shape code F P T under the guidance of the SA branch. All AS blocks have the same network structure, as illustrated in <ref type="figure">Fig. 1</ref>. Consider the t-th block in <ref type="figure">Fig. 3</ref>, whose inputs are the shape code F P t−1 ∈R c×h×w and the appearance code F I t−1 ∈R c×h×w . The output is the refined shape code F P t ∈R c×h×w . Specifically, given the shape code F P t−1 , we first feed it into a convolution layer to generate a new shape code H, where H∈R c×h×w . We then reshape H to R c×(hw) . At the same time, the AS block receives the appearance code F I t−1 from the SA branch, which is also fed into a convolution layer to produce a new appearance code E and then reshape it to R c×(hw) . After that, we perform a <ref type="figure">Fig. 3</ref>: Structure of the proposed AS block, which takes the previous shape code F P t−1 and the previous appearance code F I t−1 as inputs and obtains the shape code F P t in a crossing way. The symbols ⊕, ⊗ and s ○ and c ○ denote elementwise addition, element-wise multiplication, Softmax activation, and channel-wise concatenation, respectively. matrix multiplication between the transpose of H and E, and apply a Softmax layer to produce another correlation matrix Q∈R (hw)×(hw) ,</p><formula xml:id="formula_2">q ji = exp(E i H j ) n i=1 exp(E i H j ) ,<label>(3)</label></formula><p>where q ji measures the impact of i-th position of E on the j-th position of the shape code H. n=hw is the number of pixels. Meanwhile, we feed F P t−1 into a convolution layer to produce a new shape code D∈R c×h×w and reshape it to R c×(hw) . We then perform a matrix multiplication between D and the transpose of Q and reshape the result to R c×h×w . Finally, we multiply the result by a scale parameter β and conduct an element-wise sum operation with the original shape code F P t−1 . The result is then concatenated with the appearance code F I t and fed into a convolution layer to obtain the updated shape code F P t ∈R c×h×w ,</p><formula xml:id="formula_3">F P t = Concat(β n i=1 (q ji D i ) + F P t−1 , F I t ),<label>(4)</label></formula><p>where Concat(·) denotes the channel-wise concatenation operation and β is a parameter. By doing so, each position in the refined shape code F P t is a weighted sum of all positions in the appearance code F I t−1 and previous shape code F P t−1 . Co-Attention Fusion. The proposed Co-Attention Fusion (CAF) module consists of two parts, i.e., generating intermediate results and co-attention maps. These co-attention maps are used to spatially select from both the intermediate generations and the input image, and are combined to synthesize a final output. This idea of the proposed CAF module comes from the multi-channel attention selection module in SelectionGAN <ref type="bibr" target="#b35">[36]</ref>. However, there are three differences: (i)</p><p>We use two generation branches to generate intermediate results, i.e., SA branch and AS branch. (ii) Attention maps are generated by the combination of both shape and appearance features, so the model learns more correlations between the two features. (iii) We also produce the input attention map, which aims to select useful content from the input image for generating the final image.</p><p>We consider two directions to generate intermediate results. </p><formula xml:id="formula_4">I I i = Tanh(F I T W I i + b I i ), for i = 1, · · · , N I P i = Tanh(F P T W P i + b P i ), for i = 1, · · · , N<label>(5)</label></formula><p>where two convolution operations are performed with N convolutional filters To generate the co-attention map which reflects the correlation between the appearance F I T and shape F P T codes, we first stack both F I T and F P T along the channel axes, and then feed them into a group of filters</p><formula xml:id="formula_5">{W I i , b I i } N i=1 and {W P i , b P i } N i=1 .</formula><formula xml:id="formula_6">{W A i , b A i } 2N +1 i=1</formula><p>to generate the corresponding 2N +1 co-attention maps,</p><formula xml:id="formula_7">I A i = Softmax(Concat(F I T , F P T )W A i + b A i ), for i = 1, · · · , 2N +1<label>(6)</label></formula><p>where Softmax is a channel-wise Softmax function used for the normalization, and Concat(·) denotes the channel-wise concatenation operation. Finally, the learned co-attention maps are used to perform a channel-wise selection from each intermediate generation and the input image as follows,</p><formula xml:id="formula_8">I t = (I A 1 ⊗ I I 1 ) ⊕ · · · (I A 2N ⊗ I P 2N ) ⊕ (I A 2N +1 ⊗ I s ),<label>(7)</label></formula><p>where I t represents the final synthesized person image selected from the multiple diverse results and the input image. ⊗ and ⊕ denote the element-wise multiplication and addition, respectively. Optimization Objective. We use three different losses as our full optimization objective, i.e., adversarial loss L gan , pixel loss L l1 , and perceptual loss L p ,</p><formula xml:id="formula_9">min G max D I ,D P L = λ gan L gan + λ l1 L l1 + λ p L p ,<label>(8)</label></formula><p>where λ gan , λ l1 and λ p are the weights, measuring corresponding contributions of each loss to the total loss L. The total adversarial loss is derived from the appearance-guided discriminator D I and the shape-guided discriminator D P , which aims to judge how likely is that I t contains the same person in I s and how well I t aligns with the target pose P t , respectively. The L1 pixel loss is used to compute the difference between the generated image I t and the real target image I t , i.e., L l1 =||I t − I t || 1 . The perceptual loss L p is used to reduce pose distortions and make the generated images look more natural and smooth, i.e., L p =||φ(I t ) − φ(I t )|| 1 , where φ denotes the outputs of several layers in the pre-trained VGG19 network <ref type="bibr" target="#b31">[32]</ref>. Implementation Details. We follow the training procedures of GANs and alternatively train the proposed Xing generator G and two discriminators (D I , D P ). During training, G takes I s , P s and P t as input and outputs a translated person image I t with target pose P t . Specifically, I s is fed to the SA branch, and P s , P t are fed to the AS branch. For the adversarial training, (I s , I t ) and (I s , I t ) are fed to the appearance-guided discriminator D P for ensuring appearance consistency. (P t , I t ) and (P t , I t ) are fed to the shape-guided discriminator D P for ensuring shape consistency. Adam optimizer <ref type="bibr" target="#b14">[15]</ref> is used to train the proposed XingGAN for around 90K iterations with β 1 =0.5 and β 2 =0.999. We set T =9 in the proposed Xing generator and N =10 in the proposed co-attention fusion module on both datasets. λ gan , λ l1 and λ p in Eq. (8) are set to 5, 50 and 50, respectively. For the decoders, the kernel size of convolutions for generating the intermediate images and co-attention maps are 3×3 and 1×1, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We follow <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45]</ref> and conduct experiments on two challenging datasets, i.e., Market-1501 <ref type="bibr" target="#b43">[44]</ref> and DeepFashion <ref type="bibr" target="#b19">[20]</ref>. Images on Market-1501 and DeepFashion are rescaled to 128×64 and 256×256, respectively. To generate human skeletons as training data, we employ OpenPose <ref type="bibr" target="#b3">[4]</ref> to extract human joints. In this way, both P s and P t consist of an 18-channel heat map encoding the positions of 18 joints of a human body. We also filter out images where no human is detected. Thus, we collect 101,966 training pairs and 8,570 testing pairs on DeepFashion. For Market-1501, we have 263,632 training and 12,000 testing pairs. Note that to better evaluate the proposed XingGAN, the person identities of the training set do not overlap with those of the testing set. Evaluation Metrics. We follow <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45]</ref> and adopt Structure Similarity (SSIM) <ref type="bibr" target="#b38">[39]</ref>, Inception Score (IS) <ref type="bibr" target="#b28">[29]</ref>, and their masked versions, i.e., Mask-SSIM and Mask-IS, as the evaluation metrics. Moreover, we adopt the PCKh score proposed in <ref type="bibr" target="#b44">[45]</ref> to explicitly assess the shape consistency. Quantitative Comparisons. We compare the proposed XingGAN with several leading methods, i.e., PG2 <ref type="bibr" target="#b20">[21]</ref>, DPIG <ref type="bibr" target="#b21">[22]</ref>, VUnet <ref type="bibr" target="#b6">[7]</ref>, PoseGAN <ref type="bibr" target="#b30">[31]</ref>, Pose-Warp <ref type="bibr" target="#b1">[2]</ref>, CMA <ref type="bibr" target="#b4">[5]</ref>, C2GAN <ref type="bibr" target="#b34">[35]</ref>, BTF <ref type="bibr" target="#b0">[1]</ref> and Pose-Transfer <ref type="bibr" target="#b44">[45]</ref>. Quantitative results measured by SSIM, IS, Mask-SSIM, Mask-IS, and PCKh metrics are shown in <ref type="table" target="#tab_2">Table 1</ref>. Note that previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref> did not release the train/test split, thus we use their well-trained models and re-evaluate their performance on our testing set as in Pose-Transfer <ref type="bibr" target="#b44">[45]</ref>. Although our testing set inevitably includes some of their training samples, XingGAN still achieves the best results in terms of SSIM, IS, Mask-SSIM, and Mask-IS metrics on both datasets. For the PCKh metric, <ref type="bibr" target="#b44">[45]</ref> obtains slightly better results than XingGAN. However, we observe that the images generated by XingGAN are more realistic and have less visual artifacts than those generated by <ref type="bibr" target="#b44">[45]</ref> (see <ref type="figure" target="#fig_1">Fig. 4 and 5)</ref>. Qualitative Comparisons. Results compared with PG2 <ref type="bibr" target="#b20">[21]</ref>, VUnet <ref type="bibr" target="#b6">[7]</ref> and PoseGAN <ref type="bibr" target="#b30">[31]</ref> are shown on the left of <ref type="figure" target="#fig_1">Fig. 4 and 5</ref>. We can see that the proposed XingGAN achieves much better results than PG2, VUnet, and PoseGAN on both datasets, especially at appearance details and the integrity of generated persons. Moreover, to evaluate the effectiveness of XingGAN, we compare it with a stronger baseline, i.e., Pose-Transfer <ref type="bibr" target="#b44">[45]</ref>. Results are shown on the right of <ref type="figure">Fig. 4</ref> and 5. We can see that XingGAN also generates much better person images having fewer visual artifacts than Pose-Transfer. For instance, Pose-Transfer <ref type="bibr" target="#b44">[45]</ref> always generates a lot of visual artifacts in the background as shown in <ref type="figure" target="#fig_1">Fig. 5</ref>. Human Evaluation. We follow the evaluation protocol of <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45]</ref> and recruited 30 volunteers to conduct a user study. Participants were shown a sequence of images and asked to give an instant judgment about each image within a second. Specifically, we randomly select 55 real and 55 fake images (generated by our model) and shuffle them. The first 10 of them are used for practice and the remaining 100 images are used for evaluation. Results compared with PG2 <ref type="bibr" target="#b20">[21]</ref>, PoseGAN <ref type="bibr" target="#b30">[31]</ref>, Pose-Transfer <ref type="bibr" target="#b44">[45]</ref> and C2GAN <ref type="bibr" target="#b34">[35]</ref> are shown in <ref type="table" target="#tab_3">Table 2</ref>. We observe that the proposed XingGAN achieves the best results on all measurements compared with the leading methods, further validating that the generated images by our model are more sharp and photo-realistic. Variants of XingGAN. We conduct extensive ablation studies on Market-1501 <ref type="bibr" target="#b43">[44]</ref> to evaluate different components of our XingGAN. XingGAN has four baselines as shown in <ref type="table" target="#tab_4">Table 3</ref>: (i) 'SA' means only using the proposed Shapeguided Appearance-based generation branch. (ii) 'AS' means only adopting the <ref type="figure">Fig. 4</ref>: Qualitative comparison with PG2 <ref type="bibr" target="#b20">[21]</ref>, VUnet <ref type="bibr" target="#b6">[7]</ref>, PoseGAN <ref type="bibr" target="#b30">[31]</ref> and Pose-Transfer <ref type="bibr" target="#b44">[45]</ref> on Market-1501. proposed Appearance-guided Shape-based generation branch. (iii) 'SA+AS' combines both branches to produce the final person images. (iv) 'SA+AS+CAF' is our full model and employs the proposed Co-Attention Fusion module. Effect of Dual-Branch Generation. The results of the ablation study are shown in <ref type="table" target="#tab_4">Table 3</ref>. We see that the proposed SA branch achieves only 0.239 and 0.768 in SSIM and Mask-SSIM, respectively. When we only use the proposed AS branch, the values of SSIM and Mask-SSIM are improved to 0.286 and 0.798, respectively. Thus we conclude that the AS branch is more effective than the SA branch for generating photo-realistic person images. The AS branch takes the person poses as input and aims to learn person appearance representations, while the SA branch takes the person image as input and targets to learn person shape  representations. Learning the appearance representations is much easier than learning the shape representations since there are shape deformations between the input person image and the desired person image, leading the AS branch to achieve better results than the SA branch. Next, when adopting the combination of the proposed SA and AS branches, the performance in terms of SSIM and Mask-SSIM further boosts. However, the results in terms of IS and Mask-IS do not decline too much. Moreover, <ref type="figure" target="#fig_2">Fig. 6 (left)</ref> shows some qualitative examples of the ablation study. We observe that the visualization results of 'SA', 'AS', and 'SA+AS' are consistent with the quantitative results. Therefore, both quantitative and qualitative results confirm the effectiveness of the proposed dual-branch generation strategy. Effect of Co-Attention Fusion. 'SA+AS+CAF' outperforms the 'SA+AS' baseline with around 0.065, 0.003, and 0.009 gain on Mask-IS, SSIM, and Mask-SSIM, respectively. This means that the proposed co-attention fusion model indeed learns more correlations between the appearance and shape representations for generating the targeted person images, confirming our design motiva-  tion. Moreover, the proposed CAF module obviously improves the quality of the visualization results, as shown in the column 'Full' of <ref type="figure" target="#fig_2">Fig. 6</ref>. Lastly, we show the learned co-attention maps and the generated intermediate results. These co-attention maps are complementary, which could be qualitatively verified by visualizing the results in <ref type="figure" target="#fig_3">Fig. 7</ref>. It is clear that they have learned different activated content between the generated intermediate results and the input image for generating the final person images. Effect of The Xing Generator. The proposed Xing generator has two important network designs. One is the carefully designed Xing block, consisting of two sub-blocks, i.e., SA block and AS block. The Xing blocks jointly model both shape and appearance representations in a crossing way and enjoying the mutually improved benefits from each other. The other one is the cascaded network design, which deals with the complex and deformable translation problem progressively. Thus, we further conduct two experiments, one is to show the advantage of the progressive generation strategy by varying the number of the proposed Xing blocks, and the other is to explore the advantage of the Xing block by replacing it with the residual block <ref type="bibr" target="#b12">[13]</ref> and PATB <ref type="bibr" target="#b44">[45]</ref> resulting in two generators named Resnet generator and PATN generator in <ref type="table" target="#tab_5">Table 4</ref>, respectively.</p><p>Quantitative and qualitative results are shown in <ref type="table" target="#tab_5">Table 4</ref> and <ref type="figure" target="#fig_2">Fig. 6 (right)</ref>. We observe that the proposed Xing generator with 9 blocks works the best. However, increasing the number of blocks further reduces generation performance. This could be attributed to the proposed Xing block. Only a few blocks are needed to capture the useful appearance and shape representations and the connection between them. Thus, we adopt 9 Xing blocks as default in our experiments for both datasets. Moreover, we see that the proposed Xing generator with only 5 Xing blocks outperforms both ResNet and PATN generators with 13 blocks on most metrics, which further certifies that our Xing generator has a good appearance and shape modeling capabilities with a very few blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We propose a novel XingGAN for the challenging person image generation task. It uses cascaded guidance with two different generation branches, and learns a deformable translation mapping from both person shape and appearance features. Moreover, we propose two novel blocks to effectively update person shape and appearance features in a crossing way. Extensive experiments based on human judgments and automatic evaluation metrics show that XingGAN achieves new state-of-the-art results on two challenging datasets. Lastly, we believe that the proposed blocks and the XingGAN framework can be easily extended to address other GAN-based generation and even multi-modality fusion tasks. Acknowledgment: This work has been partially supported by the Italy-China collaboration project TALENT.</p><p>This document provides additional experimental results on the person image generation task. First, we compare the proposed XingGAN with the most state-of-the-art method Pose-Transfer <ref type="bibr" target="#b44">[45]</ref> (Sec. 6). Additionally, we show more ablation results of the proposed XingGAN (Sec. 7). Lastly, we also provide the visualization results of the generated co-attention maps (Sec. 8).</p><p>6 State-of-the-Art Comparisons In <ref type="figure" target="#fig_4">Fig. 8 and 9</ref>, we provide more generation results of the proposed XingGAN and Pose-Transfer <ref type="bibr" target="#b44">[45]</ref> on both the Market-1501 <ref type="bibr" target="#b43">[44]</ref> and DeepFashion <ref type="bibr" target="#b19">[20]</ref> datasets. Note that we generated the results of Pose-Transfer <ref type="bibr" target="#b44">[45]</ref> using the well-trained models provided by the authors 1 for fair comparisons. We observe that the proposed XingGAN consistently achieves photo-realistic results with fewer visual artifacts than Pose-Transfer on both challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">More Ablation Results</head><p>In <ref type="figure" target="#fig_6">Fig. 10</ref>, we provide more qualitative ablation comparisons of the proposed XingGAN on Market-1501. These results further demonstrate the advantage of each component of the proposed XingGAN. Moreover, we observe that our full model consistently generates more coherent and natural person images.</p><p>In <ref type="figure" target="#fig_7">Fig. 11</ref>, we show the results of varying the number of the proposed Xing blocks on Market-1501. We observe that adopting about 9 Xing blocks makes the generated person images more natural and realistic, revealing the benefits of our progressive generation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Visualization of Co-Attention Maps</head><p>We also provide more visualization results of the generated co-attention maps and intermediate results in <ref type="figure" target="#fig_0">Fig. 12</ref>. We show 10 randomly chosen intermediate results, their corresponding 10 co-attention maps, and the input attention map. It is clear that these co-attention maps have learned different activated content between the generated intermediate results and the input image for generating the final person images, revealing the effectiveness of the proposed co-attention fusion module.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Structure of the proposed SA block which takes the previous appearance code F I t−1 and the previous shape code F P t−1 as input and obtains the appearance code F I t in a crossed non-local way. The symbols ⊕, ⊗ and s ○ and c ○ denote element-wise addition, element-wise multiplication, Softmax activation, and channel-wise concatenation, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative comparison with PG2<ref type="bibr" target="#b20">[21]</ref>, VUnet<ref type="bibr" target="#b6">[7]</ref>, PoseGAN<ref type="bibr" target="#b30">[31]</ref> and Pose-Transfer<ref type="bibr" target="#b44">[45]</ref> on DeepFashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Ablation study of the proposed XingGAN on Market-1501. (left) Results of different variants of the proposed XingGAN. (right) Results of varying the number of the proposed Xing blocks. 'B' stands for the proposed Xing Blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Visualization of intermediate results and co-attention maps generated by the proposed XingGAN on Market-1501. We randomly show four intermediate results, the corresponding four co-attention maps and the input attention map. Attention maps are normalized for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative comparison with Pose-Transfer [45] on Market-1501.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Qualitative comparison with Pose-Transfer [45] on DeepFashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Ablation study results of different variants of the proposed XingGAN on Market-1501.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Ablation study results of varying the number of the proposed Xing blocks on Market-1501. 'B' stands for the proposed Xing Blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 :</head><label>12</label><figDesc>Visualization of intermediate results and co-attention maps generated by the proposed XingGAN on Market-1501.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>One is generating multiple intermediate image synthesis results from the final appearance code F I T , and the other is generating multiple intermediate image synthesis results from the final shape code F P T . Specifically, the appearance code F I T is fed into a decoder to generate N intermediate results I I ={I</figDesc><table><row><cell>I i } N i=1 , and followed by a Tanh activation function. Meanwhile, the final shape code F P T is fed into another de-coder to generate another N intermediate results I P ={I P i } N i=1 , and also followed</cell></row><row><cell>by a Tanh activation function. Both can be formulated as,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Thus, the 2N intermediate results and the input image I s can be regarded as the candidate image pool.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results on Market-1501 and DeepFashion. For all metrics, higher is better. ( * ) denotes the results tested on our testing set.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell>Market-1501</cell><cell></cell><cell></cell><cell></cell><cell cols="2">DeepFashion</cell></row><row><cell></cell><cell cols="8">SSIM IS Mask-SSIM Mask-IS PCKh SSIM IS PCKh</cell></row><row><cell>PG2 [21]</cell><cell cols="2">0.253 3.460</cell><cell>0.792</cell><cell>3.435</cell><cell>-</cell><cell cols="2">0.762 3.090</cell><cell>-</cell></row><row><cell>DPIG [22]</cell><cell cols="2">0.099 3.483</cell><cell>0.614</cell><cell>3.491</cell><cell>-</cell><cell cols="2">0.614 3.228</cell><cell>-</cell></row><row><cell>PoseGAN [31]</cell><cell cols="2">0.290 3.185</cell><cell>0.805</cell><cell>3.502</cell><cell>-</cell><cell cols="2">0.756 3.439</cell><cell>-</cell></row><row><cell>C2GAN [35]</cell><cell cols="2">0.282 3.349</cell><cell>0.811</cell><cell>3.510</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BTF [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.767 3.220</cell><cell>-</cell></row><row><cell>PG2  *  [21]</cell><cell cols="2">0.261 3.495</cell><cell>0.782</cell><cell cols="5">3.367 0.73 0.773 3.163 0.89</cell></row><row><cell>PoseGAN  *  [31]</cell><cell cols="2">0.291 3.230</cell><cell>0.807</cell><cell cols="5">3.502 0.94 0.760 3.362 0.94</cell></row><row><cell>VUnet  *  [7]</cell><cell cols="2">0.266 2.965</cell><cell>0.793</cell><cell cols="5">3.549 0.92 0.763 3.440 0.93</cell></row><row><cell>PoseWarp  *  [2]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.764 3.368 0.93</cell></row><row><cell>CMA  *  [5]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.768 3.213 0.92</cell></row><row><cell cols="3">Pose-Transfer  *  [45] 0.311 3.323</cell><cell>0.811</cell><cell cols="5">3.773 0.94 0.773 3.209 0.96</cell></row><row><cell cols="3">XingGAN (Ours) 0.313 3.506</cell><cell>0.816</cell><cell cols="5">3.872 0.93 0.778 3.476 0.95</cell></row><row><cell>Real Data</cell><cell cols="2">1.000 3.890</cell><cell>1.000</cell><cell cols="5">3.706 1.00 1.000 4.053 1.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>User study of person image generation (%). R2G means the percentage of real images rated as generated w.r.t. all real images. G2R means the percentage of generated images rated as real w.r.t. all generated images. The results of other methods are reported from their papers. Transfer<ref type="bibr" target="#b44">[45]</ref> 32.23 63.47 19.14 31.78 XingGAN (Ours) 35.28 65.16 21.61 33.75</figDesc><table><row><cell>Method</cell><cell cols="3">Market-1501 DeepFashion</cell></row><row><cell></cell><cell cols="3">R2G G2R R2G G2R</cell></row><row><cell>PG2 [21]</cell><cell>11.2 5.5</cell><cell cols="2">9.2 14.9</cell></row><row><cell>PoseGAN [31]</cell><cell cols="3">22.67 50.24 12.42 24.61</cell></row><row><cell>C2GAN [35]</cell><cell>23.20 46.70</cell><cell>-</cell><cell>-</cell></row><row><cell>Pose-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison of different variants of the proposed XingGAN on Market-1501. For all metrics, higher is better. 'SA', 'AS' and 'CAF' stand for the proposed SA branch, AS branch and co-attention fusion module, respectively.</figDesc><table><row><cell>Variants of XingGAN</cell><cell cols="2">IS Mask-IS SSIM Mask-SSIM</cell></row><row><cell>SA</cell><cell>3.849 3.645 0.239</cell><cell>0.768</cell></row><row><cell>AS</cell><cell>3.796 3.810 0.286</cell><cell>0.798</cell></row><row><cell>SA + AS</cell><cell>3.558 3.807 0.310</cell><cell>0.807</cell></row><row><cell cols="2">SA + AS + CAF (Full) 3.506 3.872 0.313</cell><cell>0.816</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison and ablation study of the proposed Xing generator on Market-1501. For all metrics, higher is better.</figDesc><table><row><cell>Method</cell><cell cols="2">IS Mask-IS SSIM Mask-SSIM</cell></row><row><cell>Xing Generator (1 blocks)</cell><cell>3.378 3.713 0.310</cell><cell>0.812</cell></row><row><cell>Xing Generator (3 blocks)</cell><cell>3.241 3.866 0.316</cell><cell>0.813</cell></row><row><cell>Xing Generator (5 blocks)</cell><cell>3.292 3.860 0.313</cell><cell>0.812</cell></row><row><cell>Xing Generator (7 blocks)</cell><cell>3.293 3.871 0.310</cell><cell>0.810</cell></row><row><cell>Xing Generator (9 blocks)</cell><cell>3.506 3.872 0.313</cell><cell>0.816</cell></row><row><cell cols="2">Xing Generator (11 blocks) 3.428 3.712 0.286</cell><cell>0.793</cell></row><row><cell cols="2">Xing Generator (13 blocks) 3.708 3.679 0.257</cell><cell>0.774</cell></row><row><cell cols="2">Resnet Generator (5 blocks) 3.236 3.807 0.297</cell><cell>0.802</cell></row><row><cell cols="2">Resnet Generator (9 blocks) 3.077 3.862 0.301</cell><cell>0.802</cell></row><row><cell cols="2">Resnet Generator (13 blocks) 3.134 3.731 0.300</cell><cell>0.797</cell></row><row><cell cols="2">PATN Generator (5 blocks) 3.273 3.870 0.309</cell><cell>0.809</cell></row><row><cell cols="2">PATN Generator (9 blocks) 3.323 3.773 0.311</cell><cell>0.811</cell></row><row><cell cols="2">PATN Generator (13 blocks) 3.274 3.797 0.314</cell><cell>0.808</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tengteng95/Pose-Transfer</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Guided image-to-image translation with bi-directional feature transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Albahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2019) 3, 4</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthesizing images of humans in unseen poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two-stream video classification with crossmodality attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Soft-gated warping-gan for pose-guided person image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A variational u-net for conditional appearance and shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Coordinate-based texture inpainting for pose-guided human image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sevastopolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakhitov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pose guided human image synthesis by view disentanglement and enhanced weighting loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ilyes Lakhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint image filtering with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dense intrinsic appearance flow for human pose transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS (2017) 1, 3, 4, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disentangled person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dense pose transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alp Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised person image synthesis in arbitrary poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pose-normalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deformable gans for posebased human image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised person image generation with semantic parsing transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gesturegan for hand gesture-togesture translation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM MM</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Cycle in cycle generative adversarial networks for keypoint-guided image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM MM</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-channel attention selection gan with cascaded semantic guidance for cross-view image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Local class-specific and global image-level generative adversarial networks for semantic-guided scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Texturegan: Controlling deep image synthesis with texture patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pose guided human video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Few-shot adversarial learning of realistic neural talking head models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Real-time user-guided image colorization with learned deep priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2015) 2, 3, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Progressive pose attention transfer for person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2019) 1, 3, 4, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
