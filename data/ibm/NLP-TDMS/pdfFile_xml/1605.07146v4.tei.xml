<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wide Residual Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
							<email>sergey.zagoruyko@enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Université Paris-Est</orgName>
								<address>
									<settlement>École des Ponts ParisTech Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
							<email>nikos.komodakis@enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Université Paris-Est</orgName>
								<address>
									<settlement>École des Ponts ParisTech Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wide Residual Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>SERGEY ZAGORUYKO AND NIKOS KOMODAKIS: WIDE RESIDUAL NETWORKS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layerdeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks have seen a gradual increase of the number of layers in the last few years, starting from AlexNet <ref type="bibr" target="#b15">[16]</ref>, VGG <ref type="bibr" target="#b25">[26]</ref>, Inception <ref type="bibr" target="#b29">[30]</ref> to Residual <ref type="bibr" target="#b10">[11]</ref> networks, corresponding to improvements in many image recognition tasks. The superiority of deep networks has been spotted in several works in the recent years <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>. However, training deep neural networks has several difficulties, including exploding/vanishing gradients and degradation. Various techniques were suggested to enable training of deeper neural networks, such as well-designed initialization strategies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>, better optimizers <ref type="bibr" target="#b28">[29]</ref>, skip connections <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>, knowledge transfer <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref> and layer-wise training <ref type="bibr" target="#b24">[25]</ref>.</p><p>The latest residual networks <ref type="bibr" target="#b10">[11]</ref> had a large success winning ImageNet and COCO 2015 competition and achieving state-of-the-art in several benchmarks, including object classification on ImageNet and CIFAR, object detection and segmentation on PASCAL VOC and MS COCO. Compared to Inception architectures they show better generalization, meaning the features can be utilized in transfer learning with better efficiency. Also, follow-up work showed that residual links speed up convergence of deep networks <ref type="bibr" target="#b30">[31]</ref>. Recent follow-up work explored the order of activations in residual networks, presenting identity mappings in residual blocks <ref type="bibr" target="#b12">[13]</ref> and improving training of very deep networks. Successful training of very deep networks was also shown to be possible through the use of highway networks c 2016. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.  <ref type="figure">Figure 1</ref>: Various residual blocks used in the paper. Batch normalization and ReLU precede each convolution (omitted for clarity) <ref type="bibr" target="#b27">[28]</ref>, which is an architecture that had been proposed prior to residual networks. The essential difference between residual and highway networks is that in the latter residual links are gated and weights of these gates are learned.</p><p>Therefore, up to this point, the study of residual networks has focused mainly on the order of activations inside a ResNet block and the depth of residual networks. In this work we attempt to conduct an experimental study that goes beyond the above points. By doing so, our goal is to explore a much richer set of network architectures of ResNet blocks and thoroughly examine how several other different aspects besides the order of activations affect performance. As we explain below, such an exploration of architectures has led to new interesting findings with great practical importance concerning residual networks.</p><p>Width vs depth in residual networks. The problem of shallow vs deep networks has been in discussion for a long time in machine learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref> with pointers to the circuit complexity theory literature showing that shallow circuits can require exponentially more components than deeper circuits. The authors of residual networks tried to make them as thin as possible in favor of increasing their depth and having less parameters, and even introduced a «bottleneck» block which makes ResNet blocks even thinner.</p><p>We note, however, that the residual block with identity mapping that allows to train very deep networks is at the same time a weakness of residual networks. As gradient flows through the network there is nothing to force it to go through residual block weights and it can avoid learning anything during training, so it is possible that there is either only a few blocks that learn useful representations, or many blocks share very little information with small contribution to the final goal. This problem was formulated as diminishing feature reuse in <ref type="bibr" target="#b27">[28]</ref>. The authors of <ref type="bibr" target="#b13">[14]</ref> tried to address this problem with the idea of randomly disabling residual blocks during training. This method can be viewed as a special case of dropout <ref type="bibr" target="#b26">[27]</ref>, where each residual block has an identity scalar weight on which dropout is applied. The effectiveness of this approach proves the hypothesis above.</p><p>Motivated by the above observation, our work builds on top of <ref type="bibr" target="#b12">[13]</ref> and tries to answer the question of how wide deep residual networks should be and address the problem of training. In this context, we show that the widening of ResNet blocks (if done properly) provides a much more effective way of improving performance of residual networks compared to increasing their depth. In particular, we present wider deep residual networks that significantly improve over <ref type="bibr" target="#b12">[13]</ref>, having 50 times less layers and being more than 2 times faster. We call the resulting network architectures wide residual networks. For instance, our wide 16-layer deep network has the same accuracy as a 1000-layer thin deep network and a comparable number of parameters, although being several times faster to train. This type of experiments thus seem to indicate that the main power of deep residual networks is in residual blocks, and that the effect of depth is supplementary. We note that one can train even better wide residual networks that have twice as many parameters (and more), which suggests that to further improve performance by increasing depth of thin networks one needs to add thousands of layers in this case.</p><p>Use of dropout in ResNet blocks. Dropout was first introduced in <ref type="bibr" target="#b26">[27]</ref> and then was adopted by many successful architectures as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref> etc. It was mostly applied on top layers that had a large number of parameters to prevent feature coadaptation and overfitting. It was then mainly substituted by batch normalization <ref type="bibr" target="#b14">[15]</ref> which was introduced as a technique to reduce internal covariate shift in neural network activations by normalizing them to have specific distribution. It also works as a regularizer and the authors experimentally showed that a network with batch normalization achieves better accuracy than a network with dropout. In our case, as widening of residual blocks results in an increase of the number of parameters, we studied the effect of dropout to regularize training and prevent overfitting. Previously, dropout in residual networks was studied in <ref type="bibr" target="#b12">[13]</ref> with dropout being inserted in the identity part of the block, and the authors showed negative effects of that. Instead, we argue here that dropout should be inserted between convolutional layers. Experimental results on wide residual networks show that this leads to consistent gains, yielding even new state-of-theart results (e.g., 16-layer-deep wide residual network with dropout achieves 1.64% error on SVHN).</p><p>In summary, the contributions of this work are as follows:</p><p>• We present a detailed experimental study of residual network architectures that thoroughly examines several important aspects of ResNet block structure.</p><p>• We propose a novel widened architecture for ResNet blocks that allows for residual networks with significantly improved performance.</p><p>• We propose a new way of utilizing dropout within deep residual networks so as to properly regularize them and prevent overfitting during training.</p><p>• Last, we show that our proposed ResNet architectures achieve state-of-the-art results on several datasets dramatically improving accuracy and speed of residual networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Wide residual networks</head><p>Residual block with identity mapping can be represented by the following formula:</p><formula xml:id="formula_0">x l+1 = x l + F(x l , W l )<label>(1)</label></formula><p>where x l+1 and x l are input and output of the l-th unit in the network, F is a residual function and W l are parameters of the block. Residual network consists of sequentially stacked residual blocks.</p><p>In <ref type="bibr" target="#b12">[13]</ref> residual networks consisted of two type of blocks:</p><p>• basic -with two consecutive 3 × 3 convolutions with batch normalization and ReLU preceding convolution: conv3 × 3-conv3 × 3 <ref type="figure">Fig.1</ref>(a)</p><p>• bottleneck -with one 3 × 3 convolution surrounded by dimensionality reducing and expanding 1 × 1 convolution layers: conv1 × 1-conv3 × 3-conv1 × 1 <ref type="figure">Fig.1</ref>  <ref type="table">Table 1</ref>: Structure of wide residual networks. Network width is determined by factor k.</p><p>Original architecture <ref type="bibr" target="#b12">[13]</ref> is equivalent to k = 1. Groups of convolutions are shown in brackets where N is a number of blocks in group, downsampling performed by the first layers in groups conv3 and conv4. Final classification layer is omitted for clearance. In the particular example shown, the network uses a ResNet block of type B(3, 3).</p><p>Compared to the original architecture <ref type="bibr" target="#b10">[11]</ref> in <ref type="bibr" target="#b12">[13]</ref> the order of batch normalization, activation and convolution in residual block was changed from conv-BN-ReLU to BN-ReLUconv. As the latter was shown to train faster and achieve better results we don't consider the original version. Furthermore, so-called «bottleneck» blocks were initially used to make blocks less computationally expensive to increase the number of layers. As we want to study the effect of widening and «bottleneck» is used to make networks thinner we don't consider it too, focusing instead on «basic» residual architecture.</p><p>There are essentially three simple ways to increase representational power of residual blocks:</p><p>• to add more convolutional layers per block • to widen the convolutional layers by adding more feature planes</p><p>• to increase filter sizes in convolutional layers As small filters were shown to be very effective in several works including <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref> we do not consider using filters larger than 3×3. Let us also introduce two factors, deepening factor l and widening factor k, where l is the number of convolutions in a block and k multiplies the number of features in convolutional layers, thus the baseline «basic» block corresponds to l = 2, k = 1. The general structure of our residual networks is illustrated in table 1: it consists of an initial convolutional layer conv1 that is followed by 3 groups (each of size N) of residual blocks conv2, conv3 and conv4, followed by average pooling and final classification layer. The size of conv1 is fixed in all of our experiments, while the introduced widening factor k scales the width of the residual blocks in the three groups conv2-4 (e.g., the original «basic» architecture is equivalent to k = 1). We want to study the effect of representational power of residual block and, to that end, we perform and test several modifications to the «basic» architecture, which are detailed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Type of convolutions in residual block</head><p>Let B(M) denote residual block structure, where M is a list with the kernel sizes of the convolutional layers in a block. For example, B(3, 1) denotes a residual block with 3 × 3 and 1 × 1 convolutional layers (we always assume square spatial kernels). Note that, as we do not consider «bottleneck» blocks as explained earlier, the number of feature planes is always kept the same across the block. We would like to answer the question of how important each of the 3 × 3 convolutional layers of the «basic» residual architecture is and if they can be substituted by a less computationally expensive 1 × 1 layer or even a combination of 1 × 1 and 3 × 3 convolutional layers, e.g., B(1, 3) or B(1, 3). This can increase or decrease the representational power of the block. We thus experiment with the following combinations (note that the last combination, i.e., B(3, 1, 1) is similar to effective Network-in-Network <ref type="bibr" target="#b19">[20]</ref> architecture):</p><formula xml:id="formula_1">1. B(3, 3) -original «basic» block 2. B(3, 1, 3) -with one extra 1 × 1 layer 3. B(1, 3, 1) -with the same dimensionality of all convolutions, «straightened» bottleneck 4. B(1, 3) -the network has alternating 1 × 1 -3 × 3 convolutions everywhere 5. B(3, 1) -similar idea to the previous block 6. B(3, 1, 1) -Network-in-Network style block</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Number of convolutional layers per residual block</head><p>We also experiment with the block deepening factor l to see how it affects performance. The comparison has to be done among networks with the same number of parameters, so in this case we need to build networks with different l and d (where d denotes the total number of blocks) while ensuring that network complexity is kept roughly constant. This means, for instance, that d should decrease whenever l increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Width of residual blocks</head><p>In addition to the above modifications, we experiment with the widening factor k of a block. While the number of parameters increases linearly with l (the deepening factor) and d (the number of ResNet blocks), number of parameters and computational complexity are quadratic in k. However, it is more computationally effective to widen the layers than have thousands of small kernels as GPU is much more efficient in parallel computations on large tensors, so we are interested in an optimal d to k ratio.</p><p>One argument for wider residual networks would be that almost all architectures before residual networks, including the most successful Inception <ref type="bibr" target="#b29">[30]</ref> and VGG <ref type="bibr" target="#b25">[26]</ref>, were much wider compared to <ref type="bibr" target="#b12">[13]</ref>. For example, residual networks WRN-22-8 and WRN-16-10 (see next paragraph for explanation of this notation) are very similar in width, depth and number of parameters to VGG architectures.</p><p>We further refer to original residual networks with k = 1 as «thin» and to networks with k &gt; 1 as «wide». In the rest of the paper we use the following notation: WRN-n-k denotes a residual network that has a total number of convolutional layers n and a widening factor k (for example, network with 40 layers and k = 2 times wider than original would be denoted as WRN-40-2). Also, when applicable we append block type, e.g. WRN-40-2-B(3, 3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dropout in residual blocks</head><p>As widening increases the number of parameters we would like to study ways of regularization. Residual networks already have batch normalization that provides a regularization effect, however it requires heavy data augmentation, which we would like to avoid, and it's not always possible. We add a dropout layer into each residual block between convolutions as shown in <ref type="figure">fig. 1(d)</ref> and after ReLU to perturb batch normalization in the next residual block and prevent it from overfitting. In very deep residual networks that should help deal with diminishing feature reuse problem enforcing learning in different residual blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head><p>For experiments we chose well-known CIFAR-10, CIFAR-100, SVHN and ImageNet image classification datasets. CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b16">[17]</ref> consist of 32 × 32 color images drawn from 10 and 100 classes split into 50,000 train and 10,000 test images. For data augmentation we do horizontal flips and take random crops from image padded by 4 pixels on each side, filling missing pixels with reflections of original image. We don't use heavy data augmentation as proposed in <ref type="bibr" target="#b8">[9]</ref>. SVHN is a dataset of Google's Street View House Numbers images and contains about 600,000 digit images, coming from a significantly harder real world problem. For experiments on SVHN we don't do any image preprocessing, except dividing images by 255 to provide them in [0,1] range as input. All of our experiments except ImageNet are based on <ref type="bibr" target="#b12">[13]</ref> architecture with pre-activation residual blocks and we use it as baseline. For ImageNet, we find that using pre-activation in networks with less than 100 layers does not make any significant difference and so we decide to use the original ResNet architecture in this case. Unless mentioned otherwise, for CIFAR we follow the image preprocessing of <ref type="bibr" target="#b7">[8]</ref> with ZCA whitening. However, for some CIFAR experiments we instead use simple mean/std normalization such that we can directly compare with <ref type="bibr" target="#b12">[13]</ref> and other ResNet related works that make use of this type of preprocessing.</p><p>In the following we describe our findings w.r.t. the different ResNet block architectures and also analyze the performance of our proposed wide residual networks. We note that for all experiments related to «type of convolutions in a block» and «number of convolutions per block» we use k = 2 and reduced depth compared to <ref type="bibr" target="#b12">[13]</ref> in order to speed up training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type of convolutions in a block</head><p>We start by reporting results using trained networks with different block types B (reported results are on CIFAR-10). We used WRN-40-2 for blocks B <ref type="figure" target="#fig_5">(1, 3, 1)</ref>  <ref type="figure" target="#fig_5">B(3, 1, 3)</ref> is faster than others by a small margin.</p><p>Based on the above, blocks with comparable number of parameters turned out to give more or less the same results. Due to this fact, we hereafter restrict our attention to only WRNs with 3 × 3 convolutions so as to be also consistent with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of convolutions per block</head><p>We next proceed with the experiments related to varying the deepening factor l (which represents the number of convolutional layers per block). We show indicative results in table 3, where in this case we took WRN-40-2 with 3 × 3 convolutions and trained several networks with different deepening factor l ∈ [1, 2, 3, 4], same number of parameters (2.2×10 6 ) and same number of convolutional layers.</p><p>As can be noticed, B(3, 3) turned out to be the best, whereas B(3, 3, 3) and B(3, 3, 3, 3) had the worst performance. We speculate that this is probably due to the increased difficulty in optimization as a result of the decreased number of residual connections in the last two cases. Furthermore, B(3) turned out to be quite worse. The conclusion is that B(3, 3) is optimal in terms of number of convolutions per block. For this reason, in the remaining experiments we only consider wide residual networks with a block of type B(3, 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Width of residual blocks</head><p>As we try to increase widening parameter k we have to decrease total number of layers. To find an optimal ratio we experimented with k from 2 to 12 and depth from 16 to 40. The results are presented in table 4. As can be seen, all networks with 40, 22 and 16 layers see consistent gains when width is increased by 1 to 12 times. On the other hand, when keeping the same fixed widening factor k = 8 or k = 10 and varying depth from 16 to 28 there is a consistent improvement, however when we further increase depth to 40 accuracy decreases (e.g., WRN-40-8 loses in accuracy to .</p><p>We show additional results in <ref type="table" target="#tab_3">table 5</ref> where we compare thin and wide residual networks. As can be observed, wide WRN-40-4 compares favorably to thin ResNet-1001 as it achieves better accuracy on both CIFAR-10 and CIFAR-100. Yet, it is interesting that these networks have comparable number of parameters, 8.9×10 6 and 10.2×10 6 , suggesting that depth does not add regularization effects compared to width at this level. As we show further in benchmarks, WRN-40-4 is 8 times faster to train, so evidently depth to width ratio in the original thin residual networks is far from optimal.</p><p>Also, wide WRN-28-10 outperforms thin ResNet-1001 by 0.92% (with the same minibatch size during training) on CIFAR-10 and 3.46% on CIFAR-100, having 36 times less layers (see <ref type="table" target="#tab_3">table 5</ref>). We note that the result of 4.64% with ResNet-1001 was obtained with batch size 64, whereas we use a batch size 128 in all of our experiments (i.e., all other results  <ref type="table">Table 4</ref>: Test error (%) of various wide networks on CIFAR-10 and CIFAR-100 (ZCA preprocessing).</p><p>reported in table 5 are with batch size 128). Training curves for these networks are presented in <ref type="figure">Figure 2</ref>.</p><p>Despite previous arguments that depth gives regularization effects and width causes network to overfit, we successfully train networks with several times more parameters than ResNet-1001. For instance, wide WRN-28-10 (  <ref type="table" target="#tab_3">Table 5</ref>: Test error of different methods on CIFAR-10 and CIFAR-100 with moderate data augmentation (flip/translation) and mean/std normalzation. We don't use dropout for these results. In the second column k is a widening factor. Results for <ref type="bibr" target="#b12">[13]</ref> are shown with minibatch size 128 (as ours), and 64 in parenthesis. Our results were obtained by computing median over 5 runs.</p><p>In general, we observed that CIFAR mean/std preprocessing allows training wider and deeper networks with better accuracy, and achieved 18.3% on CIFAR-100 using WRN-40-10 with 56 × 10 6 parameters <ref type="table" target="#tab_8">(table 9)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100</head><p>ResNet-164(error 24.33%) WRN-28-10(error 19.25%) <ref type="figure">Figure 2</ref>: Training curves for thin and wide residual networks on CIFAR-10 and CIFAR-100. Solid lines denote test error (y-axis on the right), dashed lines denote training loss (y-axis on the left).</p><p>and establishing a new state-of-the-art result on this dataset. To summarize:</p><p>• widening consistently improves performance across residual networks of different depth;</p><p>• increasing both depth and width helps until the number of parameters becomes too high and stronger regularization is needed;</p><p>• there doesn't seem to be a regularization effect from very high depth in residual networks as wide networks with the same number of parameters as thin ones can learn same or better representations. Furthermore, wide networks can successfully learn with a 2 or more times larger number of parameters than thin ones, which would require doubling the depth of thin networks, making them infeasibly expensive to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dropout in residual blocks</head><p>We trained networks with dropout inserted into residual block between convolutions on all datasets. We used cross-validation to determine dropout probability values, 0.3 on CIFAR and 0.4 on SVHN. Also, we didn't have to increase number of training epochs compared to baseline networks without dropout. Dropout decreases test error on CIFAR-10 and CIFAR-100 by 0.11% and 0.4% correnspondingly (over median of 5 runs and mean/std preprocessing) with WRN-28-10, and gives improvements with other ResNets as well <ref type="table">(table 6)</ref>. To our knowledge, that was the first result to approach 20% error on CIFAR-100, even outperforming methods with heavy data augmentation. There is only a slight drop in accuracy with WRN-16-4 on CIFAR-10 which we speculate is due to the relatively small number of parameters.</p><p>We notice a disturbing effect in residual network training after the first learning rate drop when both loss and validation error suddenly start to go up and oscillate on high values until the next learning rate drop. We found out that it is caused by weight decay, however making it lower leads to a significant drop in accuracy. Interestingly, dropout partially removes this effect in most cases, see figures 2, 3.</p><p>The effect of dropout becomes more evident on SVHN. This is probably due to the fact that we don't do any data augmentation and batch normalization overfits, so dropout adds   a regularization effect. Evidence for this can be found on training curves in <ref type="figure" target="#fig_5">figure 3</ref> where the loss without dropout drops to very low values. The results are presented in table 6. We observe significant improvements from using dropout on both thin and wide networks. Thin 50-layer deep network even outperforms thin 152-layer deep network with stochastic depth <ref type="bibr" target="#b13">[14]</ref>. We additionally trained WRN-16-8 with dropout on SVHN (table 9), which achieves 1.54% on SVHN -the best published result to our knowledge. Without dropout it achieves 1.81%.</p><p>Overall, despite the arguments of combining with batch normalization, dropout shows itself as an effective techique of regularization of thin and wide networks. It can be used to further improve results from widening, while also being complementary to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet and COCO experiments</head><p>For ImageNet we first experiment with non-bottleneck ResNet-18 and ResNet-34, trying to gradually increase their width from 1.0 to 3.0. The results are shown in table 7. Increasing width gradually increases accuracy of both networks, and networks with a comparable number of parameters achieve similar results, despite having different depth. Althouth these networks have a large number of parameters, they are outperfomed by bottleneck networks, which is probably either due to that bottleneck architecture is simply better suited for Ima-geNet classification task, or due to that this more complex task needs a deeper network. To test this, we took the ResNet-50, and tried to make it wider by increasing inner 3 × 3 layer width. With widening factor of 2.0 the resulting WRN-50-2-bottleneck outperforms ResNet-152 having 3 times less layers, and being significantly faster. WRN-50-2-bottleneck is only slightly worse and almost 2× faster than the best-performing pre-activation ResNet-200, althouth having slightly more parameters <ref type="table" target="#tab_7">(table 8)</ref>. In general, we find that, unlike CIFAR, ImageNet networks need more width at the same depth to achieve the same accuracy. It is however clear that it is unnecessary to have residual networks with more than 50 layers due to computational reasons.</p><p>We didn't try to train bigger bottleneck networks as 8-GPU machines are needed for that.   We also used WRN-34-2 to participate in COCO 2016 object detection challenge, using a combination of MultiPathNet <ref type="bibr" target="#b31">[32]</ref> and LocNet <ref type="bibr" target="#b6">[7]</ref>. Despite having only 34 layers, this model achieves state-of-the-art single model performance, outperforming even ResNet-152 and Inception-v4-based models.</p><p>Finally, in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational efficiency</head><p>Thin and deep residual networks with small kernels are against the nature of GPU computations because of their sequential structure. Increasing width helps effectively balance computations in much more optimal way, so that wide networks are many times more efficient than thin ones as our benchmarks show. We use cudnn v5 and Titan X to measure forward+backward update times with minibatch size 32 for several networks, the results are in the figure 4. We show that our best CIFAR wide WRN-28-10 is 1.6 times faster than thin ResNet-1001. Furthermore, wide WRN-40-4, which has approximately the same accuracy as ResNet-1001, is 8 times faster. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>In all our experiments we use SGD with Nesterov momentum and cross-entropy loss. The initial learning rate is set to 0.1, weight decay to 0.0005, dampening to 0, momentum to 0.9 and minibatch size to 128. On CIFAR learning rate dropped by 0.2 at 60, 120 and 160 epochs and we train for total 200 epochs. On SVHN initial learning rate is set to 0.01 and we drop it at 80 and 120 epochs by 0.1, training for total 160 epochs. Our implementation is based on Torch <ref type="bibr" target="#b5">[6]</ref>. We use <ref type="bibr" target="#b20">[21]</ref> to reduce memory footprints of all our networks. For ImageNet experiments we used fb.resnet.torch implementation <ref type="bibr" target="#b9">[10]</ref>. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We presented a study on the width of residual networks as well as on the use of dropout in residual architectures. Based on this study, we proposed a wide residual network architecture that provides state-of-the-art results on several commonly used benchmark datasets (including CIFAR-10, CIFAR-100, SVHN and COCO), as well as significant improvements on ImageNet. We demonstrate that wide networks with only 16 layers can significantly outperform 1000-layer deep networks on CIFAR, as well as that 50-layer outperform 152-layer on ImageNet, thus showing that the main power of residual networks is in residual blocks, and not in extreme depth as claimed earlier. Also, wide residual networks are several times faster to train. We think that these intriguing findings will help further advances in research in deep neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figures 1(a) and 1(c) show schematic examples of «basic» and «basic-wide» blocks respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, B(3, 1), B(1, 3) and B(3, 1, 1) as these blocks have only one 3 × 3 convolution. To keep the number of parameters comparable we trained other networks with less layers: WRN-28-2-B(3, 3) and WRN-22-2-B(3, 1, 3). We provide the results including test accuracy in median over 5 runs and time per training epoch in the table 2. Block B(3, 3) turned out to be the best by a little margin, and B(3, 1) with B(3, 1, 3) are very close to B(3, 3) in accuracy having less parameters and less layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, giving a total improvement of 4.4% over ResNet-1001</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Training curves for SVHN. On the left: thin and wide networks, on the right: effect of dropout. Solid lines denote test error (y-axis on the right), dashed lines denote training loss (y-axis on the left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Time of forward+backward update per minibatch of size 32 for wide and thin networks(x-axis denotes network depth and widening factor). Numbers beside bars indicate test error on CIFAR-10, on top -time (ms). Test time is a proportional fraction of these benchmarks. Note, for instance, that wide WRN-40-4 is 8 times faster than thin ResNet-1001 while having approximately the same accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>:</head><label></label><figDesc>Test error (%, median over 5 runs) on CIFAR-10 of residual networks with k = 2 and different block types. Time column measures one training epoch.</figDesc><table><row><cell>block type depth # params time,s CIFAR-10 B(1, 3, 1) 40 1.4M 85.8 6.06 B(3, 1) 40 1.2M 67.5 5.78 B(1, 3) 40 1.3M 72.2 6.42 B(3, 1, 1) 40 1.3M 82.2 5.86 B(3, 3) 28 1.5M 67.5 5.73 B(3, 1, 3) 22 1.1M 59.9 5.78 Table 2l CIFAR-10 1 6.69 2 5.43 3 5.65 4 5.93 Table 3: Test error (%, me-dian over 5 runs) on CIFAR-10 of WRN-40-2 (2.2M)</cell></row><row><cell>with various l.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>table 5 )</head><label>5</label><figDesc>and wide WRN-40-10 (table 9) have respectively 3.6 and 5 times more parameters than ResNet-1001 and both outperform it by a significant margin.</figDesc><table><row><cell></cell><cell cols="4">depth-k # params CIFAR-10 CIFAR-100</cell></row><row><cell>NIN [20]</cell><cell></cell><cell></cell><cell>8.81</cell><cell>35.67</cell></row><row><cell>DSN [19]</cell><cell></cell><cell></cell><cell>8.22</cell><cell>34.57</cell></row><row><cell>FitNet [24]</cell><cell></cell><cell></cell><cell>8.39</cell><cell>35.04</cell></row><row><cell>Highway [28]</cell><cell></cell><cell></cell><cell>7.72</cell><cell>32.39</cell></row><row><cell>ELU [5]</cell><cell></cell><cell></cell><cell>6.55</cell><cell>24.28</cell></row><row><cell>original-ResNet[11]</cell><cell>110 1202</cell><cell>1.7M 10.2M</cell><cell>6.43 7.93</cell><cell>25.16 27.82</cell></row><row><cell>stoc-depth[14]</cell><cell>110 1202</cell><cell>1.7M 10.2M</cell><cell>5.23 4.91</cell><cell>24.58 -</cell></row><row><cell></cell><cell>110</cell><cell>1.7M</cell><cell>6.37</cell><cell>-</cell></row><row><cell>pre-act-ResNet[13]</cell><cell>164</cell><cell>1.7M</cell><cell>5.46</cell><cell>24.33</cell></row><row><cell></cell><cell>1001</cell><cell>10.2M</cell><cell>4.92(4.64)</cell><cell>22.71</cell></row><row><cell></cell><cell>40-4</cell><cell>8.9M</cell><cell>4.53</cell><cell>21.18</cell></row><row><cell>WRN (ours)</cell><cell>16-8</cell><cell>11.0M</cell><cell>4.27</cell><cell>20.43</cell></row><row><cell></cell><cell>28-10</cell><cell>36.5M</cell><cell>4.00</cell><cell>19.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>ILSVRC-2012 validation error (single crop) of non-bottleneck ResNets for various widening factors. Networks with a comparable number of parameters achieve similar accuracy, despite having 2 times less layers.</figDesc><table><row><cell>Model</cell><cell cols="4">top-1 err, % top-5 err, % #params time/batch 16</cell></row><row><cell>ResNet-50</cell><cell>24.01</cell><cell>7.02</cell><cell>25.6M</cell><cell>49</cell></row><row><cell>ResNet-101</cell><cell>22.44</cell><cell>6.21</cell><cell>44.5M</cell><cell>82</cell></row><row><cell>ResNet-152</cell><cell>22.16</cell><cell>6.16</cell><cell>60.2M</cell><cell>115</cell></row><row><cell>WRN-50-2-bottleneck</cell><cell>21.9</cell><cell>6.03</cell><cell>68.9M</cell><cell>93</cell></row><row><cell>pre-ResNet-200</cell><cell>21.66</cell><cell>5.79</cell><cell>64.7M</cell><cell>154</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>ILSVRC-2012 validation error (single crop) of bottleneck ResNets. Faster WRN-50-2-bottleneck outperforms ResNet-152 having 3 times less layers, and stands close to pre-ResNet-200.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>table 9</head><label>9</label><figDesc>we summarize our best WRN results over various commonly used datasets.</figDesc><table><row><cell>Dataset</cell><cell>model</cell><cell>dropout</cell><cell>test perf.</cell></row><row><cell>CIFAR-10</cell><cell>WRN-40-10</cell><cell></cell><cell>3.8%</cell></row><row><cell>CIFAR-100</cell><cell>WRN-40-10</cell><cell></cell><cell>18.3%</cell></row><row><cell>SVHN</cell><cell>WRN-16-8</cell><cell></cell><cell>1.54%</cell></row><row><cell cols="2">ImageNet (single crop) WRN-50-2-bottleneck</cell><cell></cell><cell>21.9% top-1, 5.79% top-5</cell></row><row><cell>COCO test-std</cell><cell>WRN-34-2</cell><cell></cell><cell>35.2 mAP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Best WRN performance over various datasets, single run results. COCO model is based on WRN-34-2 (wider basicblock), uses VGG-16-based AttractioNet proposals, and has a LocNet-style localization part. To our knowledge, these are the best published results for CIFAR-10, CIFAR-100, SVHN, and COCO (using non-ensemble models).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgements</head><p>We thank startup company VisionLabs and Eugenio Culurciello for giving us access to their clusters, without them ImageNet experiments wouldn't be possible. We also thank Adam Lerer and Sam Gross for helpful discussions. Work supported by EC project FP7-ICT-611145 ROBOSPECT.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS 2010</title>
		<meeting>AISTATS 2010</meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling learning algorithms towards AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Large Scale Kernel Machines</title>
		<editor>Léon Bottou, Olivier Chapelle, D. DeCoste, and J. Weston</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the complexity of shallow and deep neural network classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22th European Symposium on Artificial Neural Networks</title>
		<meeting><address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus). CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>abs/1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Locnet: Improving localization accuracy for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML&apos;13)</title>
		<editor>Sanjoy Dasgupta and David McAllester</editor>
		<meeting>the 30th International Conference on Machine Learning (ICML&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<title level="m">Fractional max-pooling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training and investigating residual nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wilber</surname></persName>
		</author>
		<ptr target="https://github.com/facebook/fb.resnet.torch" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1603.05027</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1603.09382</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<editor>David Blei and Francis Bach</editor>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/~kriz/cifar.html" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning (ICML&apos;07)</title>
		<editor>Zoubin Ghahramani</editor>
		<meeting>the 24th International Conference on Machine Learning (ICML&apos;07)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deeply-Supervised Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Optnet -reducing memory usage in torch neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<ptr target="https://github.com/fmassa/optimize-net" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the number of linear regions of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><forename type="middle">F</forename><surname>Montúfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2924" to="2932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning made easier by linear transformations in perceptrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS-12)</title>
		<editor>Neil D. Lawrence and Mark A. Girolami</editor>
		<meeting>the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS-12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="924" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">FitNets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>Arxiv report 1412.6550</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning complex, extended sequences using the principle of history compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="242" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<editor>Sanjoy Dasgupta and David Mcallester</editor>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Inception-v4, inceptionresnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>abs/1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A multipath network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
