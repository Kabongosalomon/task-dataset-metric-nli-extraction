<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Constrained Dominant Sets for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leulseged</forename><surname>Tesfaye</surname></persName>
							<email>leulseged.alemu@unive.it</email>
							<affiliation key="aff0">
								<orgName type="department">Ca&apos; Foscari</orgName>
								<orgName type="institution">University of Venice</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
							<email>pelillo@unive.it</email>
							<affiliation key="aff0">
								<orgName type="department">Ca&apos; Foscari</orgName>
								<orgName type="institution">University of Venice</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">European Centre for Living Technology</orgName>
								<orgName type="institution">ECLT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@crcv.ucf.edu</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CRCV</orgName>
								<orgName type="institution" key="instit2">University of Central Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Constrained Dominant Sets for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose an end-to-end constrained clustering scheme to tackle the person re-identification (reid) problem. Deep neural networks (DNN) have recently proven to be effective on person re-identification task. In particular, rather than leveraging solely a probe-gallery similarity, diffusing the similarities among the gallery images in an end-to-end manner has proven to be effective in yielding a robust probe-gallery affinity. However, existing methods do not apply probe image as a constraint, and are prone to noise propagation during the similarity diffusion process. To overcome this, we propose an intriguing scheme which treats person-image retrieval problem as a constrained clustering optimization problem, called deep constrained dominant sets (DCDS). Given a probe and gallery images, we re-formulate person re-id problem as finding a constrained cluster, where the probe image is taken as a constraint (seed) and each cluster corresponds to a set of images corresponding to the same person. By optimizing the constrained clustering in an end-to-end manner, we naturally leverage the contextual knowledge of a set of images corresponding to the given person-images. We further enhance the performance by integrating an auxiliary net alongside DCDS, which employs a multi-scale Resnet. To validate the effectiveness of our method we present experiments on several benchmark datasets and show that the proposed method can outperform state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification aims at retrieving the most similar images to the probe image, from a large scale gallery set captured by camera networks. Among the challenges which hinder person re-id tasks, include background clutter, Pose, view and illumination variation can be mentioned.</p><p>Person re-id can be taken as a person retrieval problem based on the ranked similarity score, which is obtained from the pairwise affinities between the probe and the dataset images. However, relying solely on the pairwise affinities of probe-gallery images, ignoring the underlying contextual information between the gallery images often leads to an undesirable similarity ranking. To tackle this, several works have been reported, which employ similarity diffusion to estimate a second order similarity that considers the intrinsic manifold structure of the given affinity matrix <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Similarity diffusion is a process of exploiting the contextual information between all the gallery images to provide a context sensitive similarity. Nevertheless, all these methods do not leverage the advantage of deep neural networks. Instead, they employ the similarity diffusion process as a post-processing step on the top of the DNN model. Aiming to improve the discriminative power of a DNN model, there have been recent works which incorporate a similarity diffusion process in an end-to-end manner <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Following <ref type="bibr" target="#b4">[5]</ref>, which applies a random walk in an end-to-end fashion for solving semantic segmentation problem, authors in <ref type="bibr" target="#b24">[25]</ref> proposed a group-shuffling random walk network for fully utilizing the affinity information between gallery images in both the training and testing phase. Also, the authors of <ref type="bibr" target="#b25">[26]</ref> proposed similarity-guided graph neural network (SGGNN) to exploit the relationship between several prob-gallery image similarities. However, most of the existing graph-based end-to-end learning methods apply the similarity diffusion without considering any constraint or attention mechanism to the specific query image. Due to that the second order similarity these methods yield is highly prone to noise. To tackle this problem, one possible mechanism could be to guide the similarity propagation by providing seed (or constraint) and let the optimization process estimate the optimal similarity between the seed and nearest neighbors, while treating the seed as our attention point. To formalize this idea, in this paper, we model person re-id problem as finding an internally coherent and externally incoherent constrained cluster in an end-to-end fashion. To this end, we adopt a graph and game theoretic method called constrained dominant sets in an end-to-end manner. To the best of our knowledge, we are the first ones to integrate the well known unsupervised clustering method called dominant sets in a DNN model. To summarize, the contributions of the proposed work are:</p><p>• For the very first time, the dominant sets clustering method is integrated in a DNN and optimized in endto-end fashion.</p><p>• A one-to-one correspondence between person reidentification and constrained clustering problem is established.</p><p>• State-of-the-art results are significantly improved.</p><p>The paper is structured as follow. In section 2, we review the related works. In section 3, we discuss the proposed method with a brief introduction to dominant sets and constrained dominant sets. Finally, in section 4, we provide an extensive experimental analysis on three different benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Person re-id is one of the challenging computer vision tasks due to the variation of illumination condition, backgrounds, pose and viewpoints. Most recent methods train DNN models with different learning objectives including verification, classification, and similarity learning <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b0">[1]</ref>. For instance, verification network (V-Net) <ref type="bibr" target="#b18">[19]</ref>, <ref type="figure" target="#fig_0">Figure 1</ref>(b), applies a binary classification of image-pair representation which is trained under the supervision of binary softmax loss. Learning accurate similarity and robust feature embedding has a vital role in the course of person re-identification process. Methods which integrate siamese network with contrastive loss are a typical showcase of deep similarity learning for person re-id <ref type="bibr" target="#b7">[8]</ref>. The optimization goal of these models is to estimate the minimum distance between the same person images, while maximizing the distance between images of different persons. However, these methods focus on the pairwise distance ignoring the contextual or relative distances. Different schemes have tried to overcome these shortcomings. In <ref type="figure" target="#fig_0">Figure 1</ref>(c), triplet loss is exploited to enforce the correct order of relative distances among image triplets <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b41">[42]</ref> . In <ref type="figure" target="#fig_0">Figure 1</ref>(d), Quadruplet loss <ref type="bibr" target="#b7">[8]</ref> leverages the advantage of both contrastive and triplet loss, thus it is able to maximize the intraclass similarity while minimizing the inter-class similarity. Emphasizing the fact that these methods entirely neglect the global structure of the embedding space, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> proposed graph based end-to-end diffusion methods shown in <ref type="figure" target="#fig_0">Figure 1</ref>(e).</p><p>Graph based end-to-end learning. Graph-based methods have played a vital role in the rapid growth of computer vision applications in the past. However, lately, the advent of deep convolutional neural networks and their tremendous achievements in the field has attracted great attention of researchers. Accordingly, researchers have made a significant effort to integrate, classical methods, in particular, graph theoretical methods, in end-to-end learning. Shen et al. <ref type="bibr" target="#b25">[26]</ref> developed two constructions of deep convolutional networks on a graph, the first one is based upon hierarchical clustering of the domain, and the other one is based on the spectrum of graph Laplacian. Yan et al. <ref type="bibr" target="#b36">[37]</ref> proposed a model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which provides a capability to automatically learn both the spatial and temporal pattern of data. Bertasius et al. <ref type="bibr" target="#b4">[5]</ref> designed a convolutional random walk (RWN), where by jointly optimizing the objective of pixelwise affinity and semantic segmentation they are able to address the problem of blobby boundary and spatially fragmented predictions. Likewise, <ref type="bibr" target="#b24">[25]</ref> integrates random walk method in end-to-end learning to tackle person re-identification problem. In <ref type="bibr" target="#b24">[25]</ref>, through the proposed deep random walk and the complementary feature grouping and group shuffling scheme, the authors demonstrate that one can estimate a robust probe-gallery affinity. Unlike recent Graph neural network (GNN) methods <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b6">[7]</ref>, Shen et al. <ref type="bibr" target="#b25">[26]</ref> learn the edge weights by exploiting the training label supervision, thus they are able to learn more accurate feature fusion weights for updating nodes feature.</p><p>Recent applications of dominant sets. Dominant sets (DS) clustering <ref type="bibr" target="#b23">[24]</ref> and its constraint variant constrained dominant sets (CDS) <ref type="bibr" target="#b39">[40]</ref> have been employed in several recent computer vision applications ranging from person tracking <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, geo-localization <ref type="bibr" target="#b40">[41]</ref>, image retrieval <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b1">[2]</ref>, 3D object recognition <ref type="bibr" target="#b31">[32]</ref>, to Image segmentation and co-segmentation <ref type="bibr" target="#b38">[39]</ref>. Zemene et al. <ref type="bibr" target="#b39">[40]</ref> presented CDS with its applications to interactive Image segmentation. Following, <ref type="bibr" target="#b38">[39]</ref> uses CDS to tackle both image segmentation and co-segmentation in interactive and unsupervised setup. Wang et al. <ref type="bibr" target="#b31">[32]</ref> recently used dominant sets clustering in a recursive manner to select representative images from a collection of images and applied a pooling operation on the refined images, which survive at the recursive selection process. Nevertheless, none of the above works have attempted to leverage the dominant sets algorithm in an end-to-end manner.</p><p>In this work, unlike most of the existing graph-based DNN model, we propose a constrained clustering based scheme in an end-to-end fashion, thereby, leveraging the contextual information hidden in the relationship among person images. In addition, the proposed scheme significantly magnifies the inter-class variation of different person-images while reducing the intra-class variation of the same person-images. The big picture of our proposed method is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>(f), as can be seen, the objective here is to find a coherent constrained cluster which incorporates the given probe image P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this work, we cast probe-gallery matching as optimizing a constrained clustering problem, where the probe image is treated as a constraint, while the positive images to the probe are taken as members of the constrained-cluster. Thereby, we integrate such clustering mechanism into a deep CNN to learn a robust features through the leveraged contextual information. This is achieved by traversing through the global structure of the given graph to induce a compact set of images based on the given initial similarity(edge-weight).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dominant Sets and Constrained Dominant Sets</head><p>Dominant sets is a graph theoretic notion of a cluster, which generalizes the concept of a maximal clique to edgeweighted graphs. First, the data to be clustered are represented as an undirected edge-weighted graph with no selfloops, G = (V, E, w), where V = {1, ..., M } is the vertex set, E ⊆ V × V is the edge set, and w : E → R * + is the (positive) weight function. Vertices in G correspond to data points, edges represent neighborhood relationships, and edge-weights reflect similarity between pairs of linked vertices. As customary, we represent the graph G with the corresponding weighted adjacency (or similarity) matrix, which is the M × M nonnegative, symmetric matrix A = (a ij ), defined as a ij = w(i, j), if (i, j) ∈ E, and a ij = 0 otherwise. Note that the diagonal elements of the adjacency matrix A are always set to zero indicating that there is no self-loops in graph G. As proved in <ref type="bibr" target="#b23">[24]</ref>, one can extract a coherent cluster from a given graph by solving</p><formula xml:id="formula_0">a quadratic program f (x) as, maximize f (x) = x Ax, subject to x ∈ ∆ (1)</formula><p>where, ∆ is the standard simplex of R n . Zemene et. al <ref type="bibr" target="#b39">[40]</ref> proposed an extension of dominant sets which allows one to constrained the clustering process to contain intended constraint nodes P . Constrained dominant set (CDS) is an extensions of dominant set which contains a parameterized regularization term that controls the global shape of the energy landscape. When the regularization parameter is zero the local solutions are known to be in one-to-one correspondence with the dominant sets. A compact constrained cluster could be easily obtained from a given graph by defining a paramertized quadratic program as,</p><formula xml:id="formula_1">maximize f α P (X) = x (A − αÎ P )x, subject to x ∈ ∆<label>(2)</label></formula><p>where,Î P refers to M × M diagonal matrix whose diagonal elements are set to zero in correspondence to the probe P and to 1 otherwise</p><formula xml:id="formula_2">. Let α &gt; λ max (A V \P ), where λ max (A V \P ) is the largest eigenvalue of the principal sub- matrix of A indexed by the element of V \P. If x is a lo- cal maximizer of f α P (x) in ∆, then δ(x) ∩ P = ∅, where, δ(x) = i ∈ V : x i &gt; 0.</formula><p>We refer the reader to <ref type="bibr" target="#b39">[40]</ref> for the proof. Equations 1 and 2 can be simply solved with a straightforward continuous optimization technique from evolutionary game theory called replicator dynamics, as follows:</p><formula xml:id="formula_3">x i (t + 1) = x i (t) (Ax(t)) i x(t) Ax(t) .<label>(3)</label></formula><p>for i = 1, ..., M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modeling person re-id as a Dominant Set</head><p>Recent methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b4">[5]</ref> have proposed different models, which leverage local and group similarity of images in an end-to-end manner. Authors in <ref type="bibr" target="#b6">[7]</ref> define a group similarity which emphasizes the advantages of estimating a similarity of two images, by employing the dependencies among the whole set of images in a given group. In this work, we establish a natural connection between finding a robust probe-gallery similarity and constrained dominant sets. Let us first elaborate the intuitive concept of finding a coherent subset from a given set based on the global similarity of given images. For simplicity, we represent person-images as vertices of graph G, and their similarity as edge-weight w ij . Given vertices V, and S ⊆ V be a non-empty subset of  ,% <ref type="figure">Figure 2</ref>. Workflow of the proposed DCDS. Given n number of gallery images, G, and probe image P , we first extract their Resent101 features right before the global average pooling (GAP) layer, which are then fed to CDS-Net (upper stream) and V-Net (lower stream) branches. In the CDS-branch, after applying GAP, we compute the similarity between M 2 pair of probe-gallery image features, fp and f T Gi using their dot products, where T denotes a transpose. Thereby, we obtain M × M affinity matrix. Then, we run CDS taking the probe image as a constraint to find the solution x * ∈ IR M ×1 (similarity), and the dissimilarity, x * d , is computed as an additive inverse of the similarity x * . Likewise, in the lower stream we apply elementwise subtraction on M pair of probe-gallery features. This is followed by GAP, batch normalization (BN), and fully connected layer (FC) to obtain probe-gallery similarity score, R ∈ IR M ×1 , and probe-gallery dissimilarity score, D ∈ IR M ×1 . Afterward, we elementwise multiply x * and R, and x * d and D, to find the final similarity, Fs, and disimilarity, F d , scores, respectively. Finally, to find the prediction loss of our model, we apply a cross entropy loss, the ground truth (Gt) is given as Gt ∈ IR M ×1 . (1) Given Graph  <ref type="formula">4)</ref>, similarly, shows the relative weight of g1, wp,g 1 ,g 2 (g1) &gt; 0. (5) shows the relative weight of g2, wp,g 1 ,g 2 (g2) &gt; 0. And, (6) is a coherent subset (cluster) extracted from the graph given in (1). vertices and i ∈ S, average weighted degree of each i with regard to S is given as</p><formula xml:id="formula_4">Gallery G • • Resnet101 R • • • • ! % , % ! " , " • • • • • • • • • • -. • • • • $ " $" $% -$" / -$% /</formula><formula xml:id="formula_5">% &amp; % &amp; % ' % ' % ! P % ' % &amp; P % &amp; % ' P P % ' % &amp; % &amp; % &amp; P % ' P P % !</formula><formula xml:id="formula_6">φ S (i, j) = a ij − 1 |S| k∈S a ik ,</formula><p>where φ S (i, j) measures the (relative) similarity between node j and i, with respect to the average similarity between node i and its neighbors in S. Note that φ S (i, j) can be either positive or negative. Next, to each vertex i ∈ S we assign a weight defined (recursively) as follows:</p><formula xml:id="formula_7">w S (i) = 1, if |S| = 1, j∈S\{i} φ S\{i} (j, i)w S\{i} (j), otherwise (4) where w ij (i) = w ij (j) = a ij for all i, j ∈ V (i = j).</formula><p>Intuitively, w S (i) gives us a measure of the overall similarity between vertex i and the vertices of S \ {i}, with respect to the overall similarity among the vertices in S \ {i}.</p><p>Hence, a positive w S (i) indicates that adding i into its neighbors in S will raise the internal coherence of the set, whereas in the presence of a negative value we expect the overall coherence to decline. In CDS, besides the additional feature, which allows us to incorporate a constraint element in the resulting cluster, all the characters of DS are inherited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">A Set of a person images as a constrained cluster</head><p>We cast person re-identification as finding a constrained cluster, where, elements of the cluster correspond to a set of same person images and the constraint refers to the probe image used to extract the corresponding cluster. As customary, let us consider a given mini-batch with M number of person-images, and each mini batch with k person identities (ID), thus, each person-ID has Ω = M/k images in the given mini-batch. Note that, here, instead of a random sampling we design a custom sampler which samples k number of person IDs in each mini-batch. Let</p><formula xml:id="formula_8">B = {I 1 p1 , ...I Ω p1 , I 1 p2 , ...I Ω p2 , ...I 1 p k , .</formula><p>..I Ω pk } refers to the set of images in a single mini-batch. Each time when we consider image I 1 p1 as a probe image P , images which belong to the same person id, {I 2 p1 , I 3 p1 ...I k p1 }, should be assigned a large membership score to be in that cluster. In contrast, the remaining images in the mini-batch should be assigned significantly smaller membership-score to be part of that cluster. Note that our ultimate goal here is to find a constrained cluster which comprises all the images of the corresponding person given in that specific mini-batch. Thus, each participant in a given mini-batch is assigned a membershipscore to be part of a cluster. Furthermore, the characteristics vector, which contains the membership scores of all participants is always a stochastic vector, meaning that M i=1 z i = 1, where z i denotes the membership score of each image in the cluster.</p><p>As can be seen from the toy example in <ref type="figure" target="#fig_4">Figure 3</ref>, the initial pairwise similarities between the query and gallery images hold valuable information, which define the relation of nodes in the given graph. However, it is not straightforward to redefine the initial pairwise similarities in a way which exploit the inter-images relationship. Dominant Sets (DS) overcome this problem with defining a weight of each image p, g 1 , g 2 , g 3 with regard to subset S\i as depicted in Figure3, (2−5), respectively. As can be observed from <ref type="figure" target="#fig_4">Figure 3</ref>, adding node g 3 to cluster S degrades the coherency of cluster S = {p, g 1 , g 2 , g 3 }, whereas the relative similarity of the remaining images with respect to set S = {p, g 1 , g 2 } has a positive impact on the coherency of the cluster. It is evident that the illustration in <ref type="figure" target="#fig_4">Figure 3</ref> verifies that the proposed DCDS (Deep Constrained Dominant Set) could easily measure the contribution of each node in the graph and utilize it in an end-to-end learning process. Thereby, unlike a siamese, triplet and quadruplet based contrastive methods, DCDS consider the whole set of images in the mini-batch to measure the similarity of image pairs and enhance the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">CDS Based End-to-end Learning</head><p>In this section, we discuss the integration of CDS in endto-end learning. We adopt a siamese based Resent101, with a novel verification loss to find probe-gallery similarity, R, and dissimilarity, D scores. As can be seen from <ref type="figure">Figure 2</ref>, we have two main branches: CDS network branch (CDS-Net) and verification network branch (V-Net). In the CDS-Net, the elements of pairwise affinity matrix are computed first as a dot product of the global pooling feature of a pair of images. Afterward, the replicator dynamics <ref type="bibr" target="#b35">[36]</ref> is applied, which is a discrete time solver of the parametrized quadratic program, Equ. 5, whose solution corresponds to the CDS. Thus, assuming that there are M images in the given mini-batch, the replicator dynamics, Equ. 3, is recursively applied M times taking each image in the minibatch as a constraint. Given graph G = (V, E, w) and its corresponding adjacency matrix A ∈ R M ×M , and probe P ⊆ V. First, a proper modification of the affinity matrix A is applied by setting parameter −α to the diagonal corresponding to the subset V \P and zero to the diagonal corresponding to the constraint image P . Next, the modified adjacency matrix, B, is feed to the Replicator dynamics, by initiating the dynamics with a characteristic vector of uniform distribution x t0 , such that initially all the images in the mini-batch are assigned equal membership probability to be part of the cluster. Then, to find a constrained cluster a parametrized quadratic program is defined as:</p><formula xml:id="formula_9">maximize f α P (x) i = x Bx where, B = A − αÎ p . subject to x ∈ ∆ (5) The solution, x * i , of f α P (x)</formula><p>i is a characteristics vector which indicates the probability of each gallery image to be included in a cluster, containing the probe image P i . Thus, once we obtain the CDS,</p><formula xml:id="formula_10">x * i = [z i g1 , z i g2 ...z i g M ], for each probe image, we store each solution x * i , in Y ∈ IR M ×M , as Y =    x * i . . . x * M    =    z 1 g1 z 1 g2 · · · z 1 g M . . . . . . . . . z M g1 z M g2 · · · z M g M    .</formula><p>Likewise, for each probe, P i , we store the probe-gallery similarity, R, and dissimilarity, D, obtained from the V-Net (shown in <ref type="figure">Figure 2</ref> </p><formula xml:id="formula_11">F s = β(Y ) ⊗ (1 − β)(S ), F d = β(Y d ) ⊗ (1 − β)(D ), where, Y d = δ − Y<label>(6)</label></formula><p>δ is empirically set to 0.3. We then vectorize F s and F d into IR (M 2 ×2) , where, the first column stores the dissimilarity score, while the second column stores the similarity score. Afterward, we simply apply cross entropy loss to find the prediction loss. The intriguing feature of our model is that it does not need any custom optimization technique, it can be end-to-end optimized through a standard back-propagation algorithm. Note that, <ref type="figure">Figure 2</ref> illustrates the case of a single probe-gallery, whereas Equ. 6 shows the solution of M probe images in a given mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Auxiliary Net</head><p>In this work, we integrate an auxiliary net to further improve the performance of our model. The auxiliary net is trained based on the multi-scale prediction of Resnet50 <ref type="bibr" target="#b14">[15]</ref>. It is a simple yet effective architecture, whereby we  <ref type="figure">Figure 4</ref>. Illustrates the auxiliary net, which consists of two branches which are jointly trained. We first use features at different layers, S1, S2, S3, and then feed these to Global Maxpooling (GMP), Conv, BN, Relu and FC layers for further encoding. We then compute triplet losses employing the features from the lower three streams after Relu, shown by yellow, blue, and red circles. Next, after the final FC layer, we compute the cross-entropy loss for each of the six different outputs, Oi, from the upper and lower stream shown by distinct colored-boxes. Note that even if the upper and lower stream apply the same operations, on S1, S2 and S3, they do not share the weights; thus the encoding is different. We finally compute the final loss as the sum of the average of the triplet and cross entropy losses.  <ref type="figure">Figure 5</ref>. During testing, given a probe and gallery images, we extract DCDS and auxiliary features and concatenate them to find a single vector. Afterward, we build M x M affinity matrix and run CDS with constraint expansion mechanism to find the final probe-gallery similarity rank.</p><p>can easily compute both triplet and cross entropy loss of different layers of Resnet50 <ref type="bibr" target="#b14">[15]</ref>, hence further enhancing the learning capability. Consequently, we compute the average of both losses to find the final loss. As can be observed from <ref type="figure">Figure 4</ref>, we employ three features at different layers from Resnet50 conv5 x Layer, and then we fed these three features to the subsequent layers, MP, Conv, BN, and FC layers. Next, we compute triplet and cross entropy loss for each feature which comes from the Relu and FC layers, respectively. During testing phase we concatenate the features that come from the DCDS and Auxiliary Net to find 4096 dimensional feature. We then apply CDS to find the final ranking score, (See <ref type="figure">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Constraint Expansion During Testing</head><p>We propose a new scheme (illustrated in <ref type="figure">Figure 6</ref>) to expand the number of constraints in order to guide the similarity propagation during the testing phase. Given an affinity matrix, which is constructed using the features obtained from the concatinated feature (shown in <ref type="figure">Figure 5</ref>), we first collect k-NN's of the probe image. Then, we run CDS on the graph of the NNs. Next, from the resulting constrained cluster, we select the one with the highest member-ship score, which is used as a constraint in the subsequent step. We then use multiple-constraints and run CDS over the entire graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To validate the performance of our method we have conducted several experiments on three publicly available benchmark datasets, namely CUHK03 <ref type="bibr" target="#b18">[19]</ref>, Market1501 <ref type="bibr" target="#b42">[43]</ref>, and DukeMTMC-reID <ref type="bibr" target="#b44">[45]</ref>.  <ref type="bibr" target="#b13">[14]</ref> is utilized. DukeMTMC-reID is generated from a tracking dataset called DukeMTMC. DukeMTMC is captured by 8 high-resolution cameras, and person-bounding box is manually cropped; it is organized as 16,522 images of 702 person for training and 18, 363 images of 702 person for testing. Evaluation Metrics: Following the recent person re-id methods, we use mean average precision (mAP) as suggested in <ref type="bibr" target="#b42">[43]</ref>, and Cumulated Matching Characteristics (CMC) curves to evaluate the performance of our model. Furthermore, all the experiments are conducted using the standard single query setting <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and evaluation metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We implement DCDS based on Resnet101 <ref type="bibr" target="#b14">[15]</ref> architecture, which is pretrained on imagenet dataset. We adopt the training strategy of Kalayeh et al. <ref type="bibr" target="#b15">[16]</ref>, and aggregate eight different person re-id benchmark dataset to train our model. In total, the merged dataset contains 89,091 images, which comprises 4937 person-ID (detail of the eight datasets is given in the supplementary material). We first train our model using the merged dataset (denoted as multi-dataset (MD)) for 150 epochs and fine-tune it with CUHK03, Mar-ket1501, and DukeMTMC-reID dataset. To train our model using the merged dataset, we set image resolution to 450 × 150. Subsequently, for fine-tuning the model we set image resolution to 384 × 128. Mini-batch size is set to 64, each mini-batch has 16 person-ID and each person-ID has  <ref type="figure">Figure 6</ref>. Given a constraint (probe-image) P j , we first collect k-NNs to the probe-image, based on the pairwise similarities. Subsequently, we run CDS on the graph of the k-NN. Then, based on the cluster membership score obtained, we choose image I i , with the highest membership score and re-run CDS, considering P j and I i as constraints, over the graph of the all set of images, I M , in the minibatch. Afterward, we consider the solution as our final rank.  <ref type="table">Table 1</ref>. A comparison of the proposed method with state-of-theart methods on Market1501 dataset. Upper block, without reranking methods. Lower block, with re-ranking method, w/RR, <ref type="bibr" target="#b45">[46]</ref>. and random erasing <ref type="bibr" target="#b46">[47]</ref>. For optimization we use Adam, we initially set the learning rate to 0.0001, and drop it by 0.1 in every 40 epochs. The fusing parameter in Equ. 6, β, is set to 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Market1501 Datasets</head><p>As can be seen from <ref type="table">Table 1</ref>, on Market dataset, our proposed method improves state-of-the-art method <ref type="bibr" target="#b15">[16]</ref> by 2.5%, 1.21%, and 0.6% in mAP, rank-1 and rank-5  <ref type="figure">Figure 7</ref>. Illustrates different experimental analysis performed on Market1501 dataset. a) shows the impact of fusing parameter β in Equ. 6. b) shows the performance of our model with varying the number of images per person in a given batch. c) and d) illustrate the similarity between the probe and gallery images obtained from the baseline and DCDS method, respectively. It can be observed that the baseline method has given larger similarity values for false positive samples (red asterisks above the blue dashed-line) and smaller similarity values for false negative samples (green circles below the blue dashed-line). On the other hand, the proposed DCDS has efficiently assigned the appropriate similarity scores to the true positive and negative samples.</p><p>scores, respectively. Moreover, comparing to state-of-theart graph-based DNN method, SGGNN <ref type="bibr" target="#b25">[26]</ref>, the improvement margins are 3%, 2.5%, and 2% in mAP, rank-1, and rank-5 score, respectively. Thus, our framework has significantly demonstrated its benefits over state-of-the-art graphbased DNN models. To further improve the result we have adapted a re-ranking scheme <ref type="bibr" target="#b45">[46]</ref>, and we compare our method with state-of-the art methods which use a re-ranking method as a post-processing. As it can be seen from <ref type="table">Table  1</ref>, our method has gain mAP of 2.2% over HSP <ref type="bibr" target="#b15">[16]</ref>, and 10.5 % over SGGNN <ref type="bibr" target="#b25">[26]</ref>, 10.8 % over DGSRW. <ref type="table" target="#tab_5">Table 5</ref> shows the performance of our method on CUHK03 dataset. Since most of the Graph-based DNN models report their result on the standard protocol <ref type="bibr" target="#b19">[20]</ref>, we have experimented on the standard evaluation protocol, to make fair comparison. As can be observed from <ref type="table" target="#tab_5">Table 5</ref>, our method gain a marginal improvement in the mAP. Using a reranking method <ref type="bibr" target="#b45">[46]</ref>, we have reported a competitive result in all evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on CUHK03 Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on DukeMTMC-reID Dataset</head><p>Likewise, in DukeMTMC-reID dataset, the improvements of our proposed method is noticeable. Our method has surpassed state-of-the-art method <ref type="bibr" target="#b15">[16]</ref>    <ref type="table">Table 4</ref>. A comparison of the proposed method with state-of-theart methods on DukeMTMC-reID dataset.Upper block, without reranking methods. Lower block, with re-ranking method,w/RR, <ref type="bibr" target="#b45">[46]</ref>. <ref type="bibr" target="#b24">[25]</ref>, SGGNN <ref type="bibr" target="#b25">[26]</ref> and GCSL <ref type="bibr" target="#b6">[7]</ref> by 9.1%, 7.3%, and 6% in mAP, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>To investigate the impact of each component in our architecture, we have performed an ablation study. Thus, we have reported the contributions of each module in <ref type="table" target="#tab_2">Table 2</ref>. To make a fair comparison with the baseline and graphbased DNN models, the ablations study is conducted in a single-dataset (SD) setup. Improvements over the Baseline. As our main contribution is the DCDS, we examine its impact over the baseline method. The baseline method refers to the lower branch of our architecture that incorporates the verification network, which has also been utilized in <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. On Market1501 dataset, DCDS provides improvements of 9.2%, 6.8% and 3.6% in mAP, rank-1, and rank-5 scores, respectively, over the baseline method; whereas in DukeMTMC-reID dataset the proposed DCDS improves the baseline method by 8.0%, 5.5% and 1.7% in mAP, rank-1, and rank-5 scores, respectively. Comparison with graph-based deep models. We compare our method with recent graph-based-deep models, which adapt similar baseline method as ours, such as <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. As a result, on DukeMTMC-reID dataset our method surpass <ref type="bibr" target="#b24">[25]</ref> by 9.1%/6.8%, and <ref type="bibr" target="#b25">[26]</ref> by 17.9 % / 7.4 % in mAP / rank-1 scores. In light of this, We can conclude that incorporating a constrained-clustering mechanism in end-to-end learning has a significant benefit on finding a robust similarity ranking. In addition, experimental findings demonstrate the superiority of DCDS over existing graph-based DNN models. Parameter analysis. Experimental results by varying several parameters are shown in <ref type="figure">Figure 7</ref>. <ref type="figure">Figure 7(a)</ref> shows the effect of fusing parameter, β, Equ. (6) on the mAP. Thereby, we can observe that the mAP tends to increase with a larger β value. This shows that the result gets better when we deviate much from the CDS branch. <ref type="figure">Figure 7(b)</ref> shows the impact of the number of images per person-ID (Ω) in a given batch. We have experimented setting Ω to 4, 8, and 16, as can be seen, we obtain a marginal improvement when we set Ω to 16. However, considering the direct relationship between the running time and Ω, the improvement is negligible. c) and d) show probe-gallery similarity obtained from baseline and DCDS method, using three different probe-images, with a batch size of 64, and setting Ω to 4, 8 and 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we presented a novel insight to enhance the learning capability of a DNN through the exploitation of a constrained clustering mechanism. To validate our method, we have conducted extensive experiments on several benchmark datasets. Thereby, the proposed method not only improves state-of-the-art person re-id methods but also demonstrates the benefit of incorporating a constrainedclustering mechanism in the end-to-end learning process. Furthermore, the presented work could naturally be extended to other applications which leverage a similaritybased learning. As a future work, we would like to investigate dominant sets clustering as a loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Same person Images Different person Images</head><p>Output (, -. /0123) Output (56 .3 /0123) <ref type="figure">Figure 8</ref>. On the right hand side, the target matrix is shown. There are total 16 persons in the mini-batch and 4 images per ID (Ω = 4), batch size = 64. In the target matrix, the white-blocks represent the similarity between the same person-images in the mini-batch, whereas the black-blocks of the matrix define the dissimilarities between different person images. In the similarity matrix shown left ( after one epoch) and middle (after 70 th epochs) each row of the output matrix denotes the fused similarity obtained from the CDS-Net and V-Net, per Equ. <ref type="bibr" target="#b5">(6)</ref> in the main manuscript. Thus, we optimize our model until we obtain an output with a similar distribution of the target matrix. As can be seen, our model has effectively learned and gives a similarity matrix (shown in the middle) which is closer to the target matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>In the supplementary material, we provide additional experiments on cross-dataset person-re-identification (re-id) using the proposed deep constrained dominant sets (DCDS) on Market1501 dataset. In section one, we summarize the datasets we used in our experiments. In section two, we present the experiments we have performed on crossdataset person re-id. And, in section three, we provide hyper parameter analysis on DukeMTMC-reID and CUHK03 datasets. <ref type="figure">Figure 8</ref> illustrates an example of our method training-output (left) and learning objective, target matrix, (right). <ref type="figure">Figure 9</ref> demonstrates the similarity fusing process, between the V-Net and CDS-Net, alongside sample qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>In multiple dataset (MD) setup, we first train our model on eight datasets: CUHK03 <ref type="bibr" target="#b19">[20]</ref>, CUHK01 <ref type="bibr" target="#b17">[18]</ref>, Mar-ket1501 <ref type="bibr" target="#b42">[43]</ref>, DukeMTMC-reID <ref type="bibr" target="#b44">[45]</ref>, Viper <ref type="bibr" target="#b9">[10]</ref>, MSMT17 <ref type="bibr" target="#b34">[35]</ref>, GRID <ref type="bibr" target="#b22">[23]</ref>, and ILIDS <ref type="bibr" target="#b43">[44]</ref>. Next, we fine-tune and evaluate on each of CUHK03 <ref type="bibr" target="#b19">[20]</ref>, Market1501 <ref type="bibr" target="#b42">[43]</ref>, and DukeMTMC-reID <ref type="bibr" target="#b44">[45]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Cross-datasets Evaluation</head><p>Due to the lack of abundant labeled data, cross-dataset person re-id has attracted great interest. Recently, Fan et al. <ref type="bibr" target="#b12">[13]</ref> have developed a progressive clustering-based method to attack cross-dataset person re-id problem. To further val-  idate our proposed DCDS, we apply our method on crossdataset person re-id problem and compare it with progressive unsupervised learning (PUL) <ref type="bibr" target="#b12">[13]</ref>. To this end, we train our model on DukeMTMC-reID and CUHK03 datasets and test it on Market1501 dataset. We then compare it with PUL <ref type="bibr" target="#b12">[13]</ref>, which has also been trained on CUHK03 and DukeMTMC-reID datasets. As can be observed from Table 5, even though our proposed method is not intended for cross-dataset re-id, it has gained a substantial improvements over PUL <ref type="bibr" target="#b12">[13]</ref>, that was mainly designed to attack person re-id problem in a cross-dataset setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameter Analysis</head><p>Similar to the parameter analysis reported in the main manuscript, we report hyper parameter analysis on DukeMTMC-reID and CUHK03 dataset. The performance of our method with respect to the fusing parameters on DukeMTMC-reID and CUHK03 are shown in <ref type="figure" target="#fig_0">Figure 10</ref> (a) and <ref type="figure" target="#fig_0">Figure 10 (b)</ref>, respectively. Thereby, as can be observed, the results show similar phenomena as in Mar-ket1501, where the mAP increases with a larger β value. <ref type="figure" target="#fig_0">Figure 11</ref> shows the similarity distribution given by the baseline and the proposed DCDS using three different probe-images, with a batch size of 64, and setting Ω to 4, 8 and 16.  <ref type="figure">Figure 9</ref>. Exemplar results obtained as a result of the similarity fusion between the V-Net and CDS-Net. The Upper-row shows the probe and gallery similarity (R) obtained from the V-Net, where the green circles show persons similar to the probe (shown by purple-circle), while the red circles denote persons different from the probe image. Middle-row shows the workflow in CDS-Net. First, graph G is formed using the similarity obtained from the dot products. We then construct the modified affinity matrix B, followed by application of replicator dynamics on B to obtain the probe gallery similarity (X * ). Finally, We elementwise multiply X * and R to find the final probe-gallery similarity (Fs), shown in the third row. The intensity of the edges in, G, R, x * , and Fs define the similarity value, where the bold ones denote larger similarity values, whereas the pale-edges depict smaller similarity values. (! " ) (! # ) ($ # ) ($ " ) <ref type="bibr">Figure 11</ref>. Shows experimental analysis performed on CUHK03 (1 a,b ), and DukeMTMC-reID (2 a,b ) datasets. 1a, 2a and 1 b , 2 b illustrate the similarity between the probe-gallery images obtained from the baseline and DCDS method, respectively. It can be observed that the baseline method has assigned larger similarity values for false positive samples (red asterisks above the blue dashed-line) and smaller similarity values for false negative samples (green circles below the blue dashed-line). On the other hand, the proposed DCDS has efficiently assigned the appropriate similarity scores to the true positive and negative samples. Note that, for better visibility, we have randomly assigned a large (close to 1) self-similarity value to the probe (blue-circle).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Shows a variety of existing classification and similaritybased deep person re-id models. (a) Depicts a classification-based deep person re-id model, where P i refers to the i th person. (b) Illustrates a verification network whereby the similarity S and dissimilarity D for a pair of images is found. (c) A Triplet loss based DNN, where A, P, N indicate anchor, positive, and negative samples, respectively. (d) A quadruplet based DNN (e) Conventional diffusion-based DNN, which leverages the similarities among all the images in the gallery to learn a better similarity. (f) The proposed deep constrained dominant sets (DCDS), where, P indicates the constraint (probe-image); and, images in the constrained cluster, the enclosed area, indicates the positive samples to the probe image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>9 , 10 ( 5 )</head><label>9105</label><figDesc>,%&amp;,%' # = +*+ " #,%&amp;,%' % &amp; = +-. 9 " #,%&amp;,%' % ' = +-.(6) a coherent cluster ' " #,%&amp;,%',%! % ! = −-.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Let S = {P, g1, g2, g3} comprises probe, P, and gallery images gi. As can be observed from the above toy example, the proposed method asses the contribution of each participant node i ∈ S with respect to the subset S\i.(1) shows graph G, showing the pairwise similarities of query-gallery images. (2-5) show the relative weight (Equ. 4) of each node with respect to the overall similarity between i and S\i. (2) wp,g 1 ,g 2 ,g 3 (g3) &lt; 0, shows that the Node {g3} with Node {P, g1, g2} has a negative impact on the coherency of the cluster. (3) shows that clustering {P } with {g1} and {g2} has a positive contribution to the compactness of set {P, g1, g2}. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>) in S and D as, S = [R 1 , R 2 , ...R M ] and D = [D 1 , D 2 , ...D M ]. Next, we fuse the similarity obtained from the CDS branch with the similarity from the V-Net as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Performance of our model with respect to fusing parameter β, on (a) CUHK03, and (b) DukeMTMC-reID, datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies on the proposed method. SD and MD respectively refer to the method trained on single and multiple-aggregated datasets. Baseline is the proposed method without CDS branch.</figDesc><table><row><cell>by 1.7%/1.6% in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>A comparison of the proposed method with state-of-theart methods on CUHK03 dataset.</figDesc><table><row><cell>Methods</cell><cell cols="3">mAP rank-1 rank-5</cell></row><row><cell>SGGNN [26] ECCV18</cell><cell>68.2</cell><cell>81.1</cell><cell>88.4</cell></row><row><cell>DKPM [27] CVPR18</cell><cell>63.2</cell><cell>80.3</cell><cell>89.5</cell></row><row><cell>DGSRW [25] CVPR18</cell><cell>66.4</cell><cell>80.7</cell><cell>88.5</cell></row><row><cell>GCSL [7] CVPR18</cell><cell>69.5</cell><cell>84.9</cell><cell>-</cell></row><row><cell>CPC [33] CVPR18</cell><cell cols="2">59.49 76.44</cell><cell>-</cell></row><row><cell>MLFN [6] CVPR18</cell><cell>62.8</cell><cell>81.0</cell><cell>-</cell></row><row><cell>RAPR [34] CVPR18</cell><cell>80.0</cell><cell>84.4</cell><cell>-</cell></row><row><cell>PA [28] ECCV18</cell><cell>64.2</cell><cell>82.1</cell><cell>90.2</cell></row><row><cell>HSP [16] CVPR18</cell><cell>73.3</cell><cell>85.9</cell><cell>92.9</cell></row><row><cell>Ours</cell><cell>75.5</cell><cell>87.5</cell><cell>-</cell></row><row><cell>P A w/RR [28] ECCV18</cell><cell>83.9</cell><cell>88.3</cell><cell>93.1</cell></row><row><cell cols="2">HSP w/RR [16] CVPR18 84.99</cell><cell>88.9</cell><cell>94.27</cell></row><row><cell>Ours w/RR</cell><cell>86.1</cell><cell>88.5</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Train on Duke, CUHK03 → Test on Market1501</figDesc><table><row><cell>Methods</cell><cell>mAP</cell><cell>rank-1</cell></row><row><cell cols="2">PUL [13] 20.5</cell><cell>45.5</cell></row><row><cell>Ours</cell><cell>24.5</cell><cell>51.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>A comparison of the proposed method with PUL [13] on Market1501 dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">images. We also experiment only using a single dataset for training and testing, denoted as single-dataset (SD). For data augmentation, we apply random horizontal flipping</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research is partly supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. D17PC00345. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3908" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-feature fusion for image retrieval using constrained dominant sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Alemu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<idno>abs/1808.05075</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3356" to="3365" />
			<date type="published" when="2017-07-21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ensemble diffusion for retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="774" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional random walk networks for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2109" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep CRF for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8649" to="8658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: A deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1320" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based CNN with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International workshop on performance evaluation of track-ing and surveillance</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diffusion processes for retrieval revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1320" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>83:1-83:18</idno>
	</analytic>
	<monogr>
		<title level="j">TOMCCAP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2012 -11th Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Daejeon, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Revised Selected Papers, Part I</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Person re-identification by manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09-15" />
			<biblScope unit="page" from="3567" to="3571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-camera activity correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dominant sets and pairwise clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="172" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep group-shuffling random walk for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2265" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person reidentification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="508" to="526" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XV</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end deep kronecker-product matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="6886" to="6895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Partaligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="418" to="437" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiobject tracking using dominant sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Tesfaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zemene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Computer Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="289" to="297" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-target tracking in multiple non-overlapping cameras using constrained dominant sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Tesfaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zemene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1706.06196</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dominant set clustering and pooling for multi-view 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person reidentification with cascaded pairwise convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1470" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Resource aware person re-identification across multiple resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="8042" to="8051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person transfer GAN to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Evolutionary Game Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Weibull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Constrained dominant sets for retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zemene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Alemu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Cancún, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-04" />
			<biblScope unit="page" from="2568" to="2573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dominant sets for &quot;constrained&quot; image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zemene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Alemu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<idno>abs/1707.05309</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interactive image segmentation using constrained dominant sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zemene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="278" to="294" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large-scale image geo-localization using dominant sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zemene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Tesfaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="148" to="161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="3239" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">British Machine Vision Association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Associating groups of people</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by GAN improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3652" to="3661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
