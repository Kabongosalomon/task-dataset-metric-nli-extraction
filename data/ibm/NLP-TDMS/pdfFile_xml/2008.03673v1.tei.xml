<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Space Augmentation for Long-Tailed Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
							<email>pchu@temple.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Temple University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
							<email>xbian@google.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Liu</surname></persName>
							<email>sliu@ge.com</email>
							<affiliation key="aff2">
								<orgName type="department">GE Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<email>hling@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Temple University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Space Augmentation for Long-Tailed Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world data often follow a long-tailed distribution as the frequency of each class is typically different. For example, a dataset can have a large number of under-represented classes and a few classes with more than sufficient data. However, a model to represent the dataset is usually expected to have reasonably homogeneous performances across classes. Introducing class-balanced loss and advanced methods on data re-sampling and augmentation are among the best practices to alleviate the data imbalance problem. However, the other part of the problem about the under-represented classes will have to rely on additional knowledge to recover the missing information.</p><p>In this work, we present a novel approach to address the long-tailed problem by augmenting the under-represented classes in the feature space with the features learned from the classes with ample samples. In particular, we decompose the features of each class into a class-generic component and a class-specific component using class activation maps. Novel samples of under-represented classes are then generated on the fly during training stages by fusing the class-specific features from the under-represented classes with the class-generic features from confusing classes. Our results on different datasets such as iNaturalist, ImageNet-LT, Places-LT and a long-tailed version of CIFAR have shown the state of the art performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have shown considerable success in a wide variety of visual recognition tasks. Its effectiveness and generalizability have been well proved by many state-of-the-art work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30]</ref> and a wide variety of real-world applications in different industries <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12]</ref>. However, there is often one underlying condition that each category of interest needs to be well represented.</p><p>To quantify the "representativeness" of data can be a challenging problem itself. In practice, it is usually scrutinized using different heuristics, and Work was done at GE Research. arXiv:2008.03673v1 [cs.CV] 9 Aug 2020 <ref type="figure" target="#fig_5">Fig. 1</ref>: Left: With limited but well-spread data, the optimal decision boundary search can be recovered by sample re-weighting/loss balancing. Right: Without sufficient sample coverage, the "optimal direction" to move the decision boundary becomes unclear. In this paper, augmented samples are generated to recover the underlying distribution. one common criterion could be the balance of a dataset. Indeed, many public datasets are intentionally organized to have the same number of samples from each class <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b23">24]</ref>. For problems such as segmentation and detection which are hard to ensure exact balanced data, it is always preferable to ensure good data coverage in a way that the rare classes still have sufficient data and are hence well represented <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>However, real-world visual understanding problems are often fine-grained and long-tailed. To achieve human-level visual understanding, it almost implies the ability to distinguish the subtle differences between fine-grained categories and to robustly handle the presence of rare categories <ref type="bibr" target="#b0">[1]</ref>. In fact, these two properties of real-world data usually accompany each other as a large number of finegrained categories often leads to a highly imbalanced dataset, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. For example, in iNaturalist dataset 2017 <ref type="bibr" target="#b39">[40]</ref> for species classification, there are a total of 5089 classes with the largest classes more than 1000 samples and the smallest classes fewer than 10. In iNaturalist competition 2019, even with an effort to filter out species that have insufficient observations and to further cap the maximum class size to be 500, there is still serious imbalance in the dataset as the smallest classes around 10 samples. Similar data distribution can be observed in other applications, such as a UAV-based object detection dataset <ref type="bibr" target="#b48">[49]</ref> and COCO <ref type="bibr" target="#b27">[28]</ref>.</p><p>Like many supervised learning algorithms, the performance of deep neural networks also suffers when the training data is highly imbalanced <ref type="bibr" target="#b8">[9]</ref>. The problem can get worse when the categories with fewer data are severely undersampled to the extent that the variation within each category is not fully captured by the given data <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>The common presence of long-tailed data in real-world problems has led to several effective practices to achieve an overall performance improvement of a given machine learning model. For example, data manipulation such as augmen-tation, under-sampling and over-sampling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10]</ref>, and balanced loss function design (e.g., focal-loss <ref type="bibr" target="#b26">[27]</ref> and class-balanced loss <ref type="bibr" target="#b8">[9]</ref>), are the two mainstream approaches. These practices often improve the performance reasonably yet the improvement deteriorates when certain categories are severely underrepresented, as shown in <ref type="figure" target="#fig_5">Fig. 1</ref>. Specifically, these methods are often designed to move the class decision boundary to reduce the bias introduced by imbalanced classes. However, when a class is severely under-represented such that it is hard to draw its complete data distribution, finding the right direction to adjust the decision boundary becomes challenging. We therefore focus on exploring the information learned from the head classes (the ones with ample samples) to help the tail classes (the under-represented ones) in a long-tailed dataset.</p><p>In this work, we present a novel method to address the long-tailed data classification problem by augmenting the tail classes in the feature space using the information from the head classes. In particular, we insert an attention unit with the help of the class activation map (CAM) <ref type="bibr" target="#b45">[46]</ref> to filter out class-specific features and class-generic features from each class. For the samples of each tail class, we augment the high-level features (from high-level deep network layers) by mixing its class-specific features with the class-generic features from the head classes. This method is based on two underlying assumptions: 1) information from the head classes, represented as class-generic features, can help to recover the distribution of tail classes; and 2) the class-generic and class-specific features can be extracted and re-mixed to generate novel samples in the high-level feature space due to a more "linear" representation at that level. We have designed an end-to-end training pipeline to efficiently perform such feature space augmentation, and evaluated our method on artificially created long-tailed CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b23">[24]</ref>, ImageNet-LT, Places-LT <ref type="bibr" target="#b28">[29]</ref> and naturally longtailed datasets such as iNaturalist 2017 &amp; 2018 <ref type="bibr" target="#b39">[40]</ref>. Our approach has shown the state of the art performance on these long-tailed datasets compared to other mainstream deep learning models on data imbalance problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we first discuss the two directly related approaches, learning with balanced loss and data augmentation, and then discuss the difference and relation of our approach to few-shot learning and transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning with Balanced Loss</head><p>One of the most common and often most effective practices is learning with balanced loss. The key to such approaches is to counter the effect of a skewed data distribution by adjusting the weights of the samples from the small classes in the loss function. It is typically accomplished by: 1) over-sampling or undersampling <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref> to achieve an even data distribution across various classes, and/or 2) assigning proper weights to the loss terms corresponding to the tail classes <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Specifically, these approaches treat the issue of long-tailed data as an optimization problem such that an "optimal" classification boundary can be recovered by carefully adjusting the weight/frequency of each data point in the training set. They typically have the advantage of a relatively clean implementation by either adjusting the loss function <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> or manipulating the input batch <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b49">50]</ref>, and hence were widely adopted in practice <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b49">50]</ref>. However, when the samples of the tail classes are far from sufficient to recover the true distribution, the performance of such methods deteriorates <ref type="bibr" target="#b8">[9]</ref>.</p><p>Two works along this direction, class-balanced loss <ref type="bibr" target="#b8">[9]</ref> and focal loss <ref type="bibr" target="#b26">[27]</ref> draw our attention in particular for their generic applications on deep learning models. Specifically, focal loss weights the loss term of each sample based on the probability generated from the last soft-max layer <ref type="bibr" target="#b26">[27]</ref>. It implicitly gives higher weights to samples from the tail classes to counter the bias introduced by the sample size. In <ref type="bibr" target="#b8">[9]</ref>, the concept of the effective number is introduced to calculate the weight of each class in the loss term.</p><p>Note that our approach can be used jointly with approaches such as focal loss <ref type="bibr" target="#b26">[27]</ref> and potentially gain the benefits from both. For example, we can use the feature space augmentation approach to facilitate the performance of the tail classes, and at the same time, balanced loss methods such as focal loss can give higher weights to the hard examples regardless of the class label during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Synthesis and Augmentation</head><p>Generating and synthesizing new samples to compensate the small sample size of a tail class is a natural way to improve the performance of deep learning models on long-tailed data. These samples can be either generated from similar samples <ref type="bibr" target="#b4">[5]</ref> or synthesized based on the given information of a dataset <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b14">15]</ref>. The general application of data augmentation in different deep learning models also boosts the interest of developing more sophisticated data augmentation methods. For example, in <ref type="bibr" target="#b6">[7]</ref>, an input image is partitioned into local regions which are shuffled during training. In <ref type="bibr" target="#b7">[8]</ref>, local regions of an image are replaced by unlabeled data to generate synthetic images to help training. In <ref type="bibr" target="#b40">[41]</ref>, a parametric generative model takes noise and existed samples to hallucinate new samples to support training.</p><p>As directly manipulating the raw input images may as well introduce unexpected noise, feature vectors are instead generated by training a function to learn the relation between a pair of samples from one class and applies it to the samples in another <ref type="bibr" target="#b16">[17]</ref>. Furthermore, recent progress in generative adversarial networks (GAN) have inspired advanced methods using generative models to address the data insufficiency problem <ref type="bibr" target="#b42">[43]</ref>.</p><p>In contrast to the existing approaches on augmenting feature vectors, we focus on modeling the feature space itself rather than training a heavily parametric model that applies to all different classes. The decomposition of feature space is then used to formulate novel training samples in the feature space on the fly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transfer Learning</head><p>Past works in the domain of transfer learning and few-shot learning <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b46">47]</ref> have been conducted to solve the long-tailed problem. Our work shares a similar assumption with these works that the information from the head classes can be used to help the tail classes. However, we explicitly distinguish the generic features and specific features from each class instead of making strong assumptions on the general transferability of knowledge from the head classes to the tail classes. Specifically, in <ref type="bibr" target="#b41">[42]</ref>, a meta-network is trained to predict the many-shot parameters from few-shot model parameters using data from the head classes with the assumption that the model parameters from different classes share a similar dynamic behavior even if the size of the training set varies. In <ref type="bibr" target="#b1">[2]</ref>, the representation is shared in general with different embedding approaches across different classes. In <ref type="bibr" target="#b43">[44]</ref>, the variance of the head classes is learned and transferred to the tail classes with the underlying assumption that each class has its own mean but a shared variance. In <ref type="bibr" target="#b31">[32]</ref>, visually similar classes are clustered together in order to reduce the level of data imbalance. Knowledge can then be transferred from each cluster to its sub-classes during the fine tuning stage of deep networks for object detection specifically.</p><p>In comparison, we intentionally separate the features of each class into classspecific features and class-generic features. Only class-generic features from head classes are seen as transferable knowledge and are hence used for feature space augmentation on the tail classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Problem of Long Tail</head><p>In this section, we first analyze the underlying issues of long-tailed data that affect model performance (Sec. 3.1), and then explore deeper into the feature space of DNNs and illustrate a novel way to alleviate the problem (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two Reasons of Model Performance Drop</head><p>Long-tailed data hurt the performance of learning-based classification models mainly due to the following two issues: (1) data imbalance which is relatively easy to solve, and (2) missing coverage of the data distribution caused by limited data, which is harder to deal with.</p><p>The data imbalance issue has been discussed in several recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref> with good solutions proposed to minimize its impact. This problem is essentially about the bias introduced by the different number of samples in the dataset. With carefully designed sampling schemes and/or loss weights, we can compensate this negative impact and move the classification decision boundary in the right direction. For example, a common practice of training on an imbalanced dataset is to over-sample the small classes or under-sample the large classes <ref type="bibr" target="#b9">[10]</ref>. This is built upon the assumption that the underlying decision boundary is indeed well-defined with the given data, and hence with careful adjustment we can find its optimal location.  However, when there is simply no sufficient data for the tail classes to recover their underlying distribution, the problem of finding an optimal decision boundary becomes ill-defined. In this scenario, it becomes extremely difficult to guess the location of the decision boundary without recovering the distribution first. We hypothesize that the knowledge obtained from the head classes can help with solving the issue.</p><p>We further elaborate the issue in <ref type="figure" target="#fig_1">Fig. 3</ref> by plotting the feature distribution of 4 classes in CIFAR-10. The features are from the last fully-connected (FC) layer of ResNet-18 and then embedded in 2-D space. When the ship class is underrepresented, as shown in the left graph, simply moving the decision boundary will not provide the optimal decision boundary (as shown in the right graph) as if there were sufficient samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Class Activation Map and Feature Decomposition</head><p>With limited data in the tail classes and ample data in the head classes, it seems natural to use the knowledge learned from the head classes to help recovering the missing information in the tail classes. However, we have to be careful to differentiate the class-generic information that can be used to recover the distribution of the tail classes from the class-specific information that may mislead the recovery of the distribution of the tail classes.</p><p>Inspired by the recent works on attention and visual explanation <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b35">36]</ref>, we find that deep neural network features can be decomposed into two such components in a similar fashion. In particular, let us define class activation map M c of class c as in <ref type="bibr" target="#b45">[46]</ref>,</p><formula xml:id="formula_0">M c (x, y) = k w c k f k (x, y),<label>(1)</label></formula><p>where f k (x, y) is the feature vector in location (x, y) of channel k, and w c k the weights of the last layer of classifier corresponding to class c. The larger value of M c (x, y), the more important of feature vector at (x, y) is to class c, and vice versa.</p><p>We further normalize the value of M c (x, y) to the range of 0 and 1. Therefore, given a pair of thresholds 0 &lt; τ s , τ g &lt; 1, we can decompose the class activation  </p><formula xml:id="formula_1">M s c = sgn(M c − τ s ) M c ,<label>(2)</label></formula><formula xml:id="formula_2">M g c = sgn(τ g − M c ) M c ,<label>(3)</label></formula><p>where is the Hadamard product between two tensors, sgn(x) = 1 for x ≥ 0 and sgn(x) = 0 for x &lt; 0. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the scatter plot of class-generic features and class-specific features of different classes from CIFAR-10 (More results can be seen in the supplemental material). We can see that after decomposition, even when embedded in a 2-D space, the class-specific features are clearly more separated than class-generic features. In general, we have observed a much stronger correlation between class-generic features than class-specific features across different classes and different datasets. These results further substantiate our approach on using class-generic features to augment the tail classes during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>We propose a two-phase training scheme to leverage the class-generic information to recover the distribution of tail classes, as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. In Phase-I, samples from all classes are used to learn the feature representation and a base classifier. In Phase-II, online feature space augmentation is applied to generate novel samples for tail classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Initial Feature Learning</head><p>In the Phase-I training, we use all images in the dataset to learn the feature subnetwork and the base classifier. In order to calculate the class activation maps in the following steps, we choose a network architecture that contains a single FC layer as the final classifier, which takes input from a global average pooling layer as illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>. A number of the modern deep convolutional neural network architectures fit into this category, e.g., ResNet <ref type="bibr" target="#b18">[19]</ref>, DenseNet <ref type="bibr" target="#b20">[21]</ref>, MobileNet <ref type="bibr" target="#b33">[34]</ref>, and EfficientNet <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Space Augmentation</head><p>With the pre-trained feature sub-network and the classifier, augmented samples can be generated in the feature space on the fly by mixing the class-specific features from the given tail class and the class-generic features.</p><p>One question we need to address is that, given a tail class, how to choose the classes from which the class-generic features will be extracted. A naive solution would be to randomly select the classes from the training dataset. However, the class-generic features from different classes may vary with each other, and such features of a randomly selected class cannot always guarantee a good recovery of the classification decision boundary. From the perspective of the optimal classification decision boundary, we observe that the "nearby" classes in the feature space, i.e. the most "confusing" classes with respect to the given tail class, have the biggest impact on recovering the previously ill-defined decision boundary, as seen in <ref type="figure" target="#fig_1">Fig. 3</ref>. Specifically, we calculate the classification scores for all other classes for each training sample in a given tail class, and then find its top N f confusing classes by ranking the average classification scores of other classes over all samples within the tail class.</p><p>As described in Sec. 3.2, we use class activation maps to separate the classgeneric and class-specific information from a given image. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, the feature sub-network trained in Phase-I is used to extract feature maps for each input image. The weights of the linear classifier trained in Phase-I are adapted to form the 1 × 1 convolutional filter for each class. For each input image, the filter associated with the ground truth class is applied on its feature maps to generate the class activation map which is further normalized to the range of [0, 1] for consistency. Two independent thresholds τ g and τ s are used to extract the corresponding binarized masks for class-generic features and class-specific features following Eq. 2.</p><p>The class-generic information in the confusing classes is then leveraged to generate the augmented samples of each tail class in order to recover its intrinsic data distribution. Directly blending information at the pixel level often introduces artificial edges and hence imposes bias to the augmented samples. We, therefore, conduct the fusion in the feature space to suppress the noise and potential bias. In particular, for each real sample in the tail class, we sample N a images from its N f confusing classes. The class-specific features from the sample are then combined with the class-generic features from the N a samples in a linear way. A random combination ratio is generated to guide the fusion by randomly drawing class-generic and class-specific feature vectors to form an augmented sample for the tail class. By randomly modulating the combination ratio between the class-generic and class-specific features, the sample variance {u} ← Find confusing classes for class c 10:</p><p>for u in {u} do 11:</p><p>Fu ← Draw one sample from class u 12:</p><p>M g u ← Mu &lt; τg 13:</p><formula xml:id="formula_3">F g u ← M g u Fu</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>Generate combination ratio γ ∈ (0, 1) Total L spatial locations in end for 20:</p><p>{F k } ← Draw Nt(1 + Na) samples from head classes 21:</p><p>Append {F k } to bout 22: end for is built into this augmentation procedure. In the end, a total of N a augmented samples are generated for each real sample from the tail class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fine Tuning with Online Augmented Samples</head><p>The augmented samples are generated online to fine tune the network trained in Phase-I to improve the performance of the tail classes. In each batch, we sample N t images from the tail classes, and generate N a augmented samples online for each of the real samples, which creates a batch including N t (1 + N a ) samples from the tail classes. The same number of images are also randomly drawn from the head classes to balance the distribution. Thus, a batch of size 2N t (1 + N a ) is generated online for each fine tuning iteration. We summarize this process in Alg. 1.</p><p>Fine tuning is performed on the layers after the features being extracted. Since the augmentation is conducted in the feature space, augmented samples can be generated at any stage of the network. However, the deeper features, compared to its shallow counterparts, are more linearly separable, which greatly help the fusion of features from the tail classes and their confusing classes. Moreover, richer spatial information in the lower-level feature maps may introduce artifacts to bias the model training. We analyze the detailed effect of augmenting samples at different depths in Sec. 5.4. We choose the features right before the last average pooling layer to help with the classification performance and at the same time to realize a simple design. Since the average pooling layer accumulates features in all spatial locations, the spatial distribution of class-generic and class-specific features become irrelevant in the augmented samples. Therefore, when combining, only the ratio between the two types of features needs to be given. Finally, we use the augmented batches to fine tune the FC classifier layer as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments on the artificially created long-tailed CIFAR dataset <ref type="bibr" target="#b8">[9]</ref> with various simulated imbalance factors, ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>, Places-LT <ref type="bibr" target="#b28">[29]</ref> and the real world long-tailed iNaturalist 2017 and 2018 <ref type="bibr" target="#b39">[40]</ref> datasets to validate the proposed method. Deep residual network (ResNet) with various depth are employed in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Long-tailed CIFAR</head><p>To demonstrate the effectiveness of the proposed method, long-tailed versions of CIFAR dataset are generated following the protocol mentioned in <ref type="bibr" target="#b8">[9]</ref>   rate for Phase-I is 0.1 and decreases by 1/10 every 150 epochs. The feature subnetwork and base classifier are trained for 300 epochs. In Phase-II, the learning rate is fixed at 0.001. The classifier is fine tuned for 6,400 iterations with a batch size of 128. For each real sample in tail classes, we choose N a = N f = 3. A sample learning curve for long-tailed CIFAR-10 with an imbalance factor of 200 using ResNet-18 is shown in <ref type="figure">Fig. 6</ref>. The performance of the proposed method is compared with a baseline setting of the same learning rate but without the feature space augmentation. After the Phase-II feature space augmentation, the accuracy of the proposed method on the validation set increases about 7% during the fine tuning stage, while no noticeable change in accuracy for the baseline setting is observed.</p><p>To further illustrate the improvement, the confusion matrix before Phase-II and its changes are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. After Phase-I training, tail classes show poor accuracy on the validation set due to insufficient training samples in those classes. Most mis-classified samples fall into the first several head classes, where most training samples belong to, as indicated in the left bottom corner of the confusion matrix. After Phase-II fine tuning, significant improvement is observed for the diagonal elements of the tail classes. The off-diagonal elements decrease accordingly. Although the accuracy of the head classes decrease slightly, due to dramatic improvement in the tail classes, the overall accuracy still increases.</p><p>The complete classification performance on different imbalance factors of the two dataset are shown in Tab. 1 and 2. The method using the same ResNet with cross-entropy loss and traditional data augmentation on input images is referred as Baseline in Tab. 1 and 2. In our experiments, we compare our method with  the state of the arts on addressing the long-tailed problem, including Classbalanced (CB) loss <ref type="bibr" target="#b8">[9]</ref> based method and Focal Loss (FL) from <ref type="bibr" target="#b26">[27]</ref> with various choices of hyper-parameters and augmentation based method <ref type="bibr" target="#b25">[26]</ref>. Our method outperforms all other methods in both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ImageNet-LT and Places-LT Dataset</head><p>We also evaluate the proposed method on two constructed large-scale longtailed datasets ImageNet-LT and Places-LT <ref type="bibr" target="#b28">[29]</ref>. ImageNet-LT is a subset of the ILSVRC2012 dataset. Its training set is drawn from the original training set following the Pareto distribution with α = 6, which results 115.8K images from 1000 categories with a maximum of 1280 images per category and a minimum of 5 images per category (IM = 256). The original validation set with balanced 50K images is used as test set in our experiments. Places-LT dataset is constructed similarly with ImageNet-LT from Places-2 dataset. Finally, 184.5K images from 365 categories are collected, where the largest class contains 4980 images while the smallest ones with 5 images (IM = 996). The test set contains balanced 36.5K images.</p><p>For fair comparison, we use the same scratch ResNet-10 for ImageNet-LT and pre-trained ResNet-152 for Places-LT as in <ref type="bibr" target="#b28">[29]</ref>. The numerical results and comparison with other peer methods are reported in Tab. 3. We also evaluate the combination of other balanced loss methods with the proposed method in these experiments. The different losses are applied in the Phase-I training. We use "Ours+FL" to refer the experiments using Focal Loss and "Ours" for ordinary cross-entropy loss. Both of our methods achieve comparable performance with the state-of-the-art method.</p><p>Note that, "Ours+FL" shows better performance than "Ours" in the ImageNet-LT dataset while "Ours" is better in Places-LT. Feature maps generated in the shallow network as ResNet-10 is not as sparse as in ResNet-152. Therefore, as explained in Sec. 4.3, the feature space augmentation delivers more performance boost to ResNet-152. On the other hand, our class balanced training batch generation achieves a similar effect as other balanced loss methods in the Phase-II fine tuning. But applying those losses in the Phase-I may still improve the performance when poor Phase-I performance affects CAM quality. <ref type="table">Table 4</ref>: Top-1 classification accuracy on iNaturalist ( * : results from literature).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">iNaturalist</head><p>iNaturalist is a real-world fine-grained species classification dataset. Its 2017 version contains 579,184 training images of 5,089 categories, and its 2018 version <ref type="bibr" target="#b39">[40]</ref> has 437,513 training samples in 8,142 classes. The imbalance factor for iNaturalist 2017 is 435 and 500 for iNaturalist 2018. For both versions, there are three validation samples for each class. We adapt ResNet-50, ResNet-101 and ResNet-152 in our experiments, all with 224 × 224 input image size. The similar training strategy with CIFAR datasets is adapted for iNaturalist. In Phase-I, the starting learning rate is 0.1 and reduced every 30 epochs for total of 100 epochs. In Phase-II, fine tuning is performed with a fixed learning rate of 0.001 for 200 iterations. The top-1 classification accuracy for the validation set of the two datasets are reported in Tab. 4. We compare the proposed method with class-balanced cross-entropy loss on ResNet-152 and class-balanced focal loss on ResNet-101/50. Our method has shown the best performance in all the settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Analysis</head><p>One major hyper-parameter in the proposed method is how to separate the head classes from the tail classes. Specifically, the classes are first sorted in the descent order by the number of training samples in each class as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The first h classes are chosen as head classes. In order to unify the choice between different imbalance factors of datasets, we introduce h r ∈ (0, 1) which is the ratio between the number of samples in the head classes and the total number of samples. Different h r choices against the Phase-II classification accuracy are evaluated in <ref type="figure" target="#fig_8">Fig. 8</ref>. The curves among different datasets show peaks around h r = 0.95. On the left of peaks, fewer samples or classes are used as the head class, and thus class-generic features cannot be drawn sufficiently for feature augmentation. On the right side, fewer classes are selected as the tail classes, and therefore some classes with insufficient training samples will not be fine tuned with augmented samples. For consistency, we choose the minimum of h that satisfies h r ≥ 0.9 in all the CIFAR experiments.</p><p>We also investigate the classification performance when applying the feature space augmentation at different depths of the network. The ResNet architecture we adapted usually consists of four convolutional blocks. We plug our feature  Accuracy (%) CIFAR10 IM200 CIFAR10 IM100 CIFAR100 IM200 CIFAR100 IM100 <ref type="figure">Fig. 9</ref>: Classification accuracy by applying feature space augmentation at different depth of ResNet-18.</p><p>space augmentation after each of the last three convolutional blocks of ResNet-18 on CIFAR dataset. When augmenting features after Block2 and Block3, classspecific features in F g u are replaced with the class-specific features in F g c with random ratio to generate augmented samples, where spatial information of F g u is preserved. The corresponding classification accuracy after Phase-II fine tuning is shown in <ref type="figure">Fig. 9</ref>. From <ref type="figure">Fig. 9</ref>, one can observe that feature augmentation after Block4 gains the best performance among different datasets and imbalance factors. The feature maps closer to the input side contain more spatial information, which also introduces additional artifacts into the augmented samples. Features generated by Block4 are directly passed into the global pooling layer where the noise in the spatial dimension introduced by augmentation can be eliminated. Moreover, the linearity of the high-level feature space helps the final linear operation of the fusion. We, therefore, apply the feature space augmentation after Block4. We also compare the performance of only sampling balanced finetuning batch without augmentation applied, which is refered as "No Aug" in <ref type="figure">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel learning scheme to address the problem of training the deep convolutional neural network based classifier with long-tailed datasets. In detail, by combining the class-generic features in head classes with class-specific features in tail classes, augmented samples are online generated to enhance the performance of the tail classes. Results on long-tailed version CIFAR-10/100, ImageNet-LT, Places-LT and real-world long-tailed iNaturalist 2017/2018 datasets have shown the effectiveness of proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Supplementary</head><p>In the supplementary material, we first present the learning curve of different network architectures on CIFAR-10 and CIFAR-100. We have observed a significant improvement compared to the baseline model training using conventional data augmentation (used in ImageNet ResNet model training) across different network architectures and dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Feature Space Visualization</head><p>As discussed in Section 3.2, We present the scatter plot of feature decomposition of different network architectures on CIFAR-10 and CIFAR-100. It shows a generic trend that the class-specific features are significantly more separated compared to class-generic features from the same class. This result further substantiate the assumption that after separating the class-specific features, the remaining class-generic features from the head classes can be helpful to recover the loss information of the tail classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-Generic</head><p>Class-Specific </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The sorted sample size of each class from different dataset follows similar long-tailed distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The difference between the two "optimal" decision boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Left: class-specific features, Right: class-generic features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Overview of the proposed two-phase learning scheme. map M c into two parts, M s c and M g c , to separate the feature vectors into classspecific features and class-generic features as follows,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1</head><label>1</label><figDesc>Online Feature Augmentation 1: Input: All training images features F and their CAM M. 2: Output: Training batch with augmented feature samples bout. 3: Initialize output batch bout. 4: for i = 1, . . . , Nt do 5: Fc ← Draw one sample from tail classes 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fi</head><label></label><figDesc>Draw with repeat and excluding all zeros feature 15: {f s c } ← Draw γL feature vectors from F s c 16: {f g u } ← Draw (1 − γ)L feature vectors from F g u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Learning Curve for long-tailed CIFAR-10 with an imbalance factor of 200 using ResNet-18. The overall accuracy of the validation dataset is illustrated. Confusion matrix for CIFAR-10 (upper) and CIFAR-100 (bottom) at IM 200 using ResNet-18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Phase-II performance dependence on the ratio of total training samples used as the head class sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>34 Fig. 10 :</head><label>3410</label><figDesc>CIFAR-10 with IM factor of 200 using ResNet-Learning Curve</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>CIFAR-10 Feature space visualization for different network architectures. Feature space visualization for different subset of CIFAR-100 using ResNet-18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>as IM = max({N i })/ min({N i }). Five datasets of different imbalance factors, {10, 20, 50, 87.24 82.32 75.16 70.22 91.03 87.32 82.74 78.58 71.42 CB [9] β = 0.9 90.79 86.61 81.9 75.16 69.16 91.03 87.18 82.48 75.99 70.0 CB [9] β = 0.999 90.54 86.83 81.81 76.4 69.83 90.74 87.24 81.66 74.85 70.08 CB [9] β = 0.9999 89.61 86.05 80.4 75.04 69.21 90.69 86.9 81.06 75.74 68.79 FL [27] γ = 0.5 90.66 86.61 81.55 74.99 69.06 90.76 87.18 81.91 76.5 69.87 FL [27] γ = 1.0 90.59 86.83 81.79 74.07 68.23 90.7 87.24 81.34 76.44 70.02 FL [27] γ = 2.0 90.5 86.05 81.25 75.13 68.27 90.08 86.9 82.44 75.Classification accuracy on long-tailed CIFAR-10. 57.02 48.15 43.51 38.58 64.14 58.03 48.44 42.94 38.84 CB [9] β = 0.999 61.76 55.3 44.28 32.19 26.61 63.05 54.13 40.89 32.65 26.2 CB [9] β = 0.9999 60.71 53.93 42.02 31.32 25.91 62.28 53.64 40.03 29.82 26.63 FL [27] γ = 0.5 62.64 57.02 47.9 42.82 38.73 64.36 58.45 48.31 42.72 36.18 FL [27] γ = 1.0 62.85 57.22 47.76 42.81 40.47 64.83 58.78 48.24 42.64 37.29 FL [27] γ = 2.0 63.37 57.15 47.0 42.18 40.31 64.48 58.55 47.47 43.33 38.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet-18</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet-34</cell><cell></cell></row><row><cell>IM</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell><cell>200</cell></row><row><cell>Baseline</cell><cell cols="10">90.73 58 69.87</cell></row><row><cell>SLA [26]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.58</cell><cell>-</cell><cell>-</cell><cell>80.24</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="10">91.75 88.54 84.51 80.57 77.06 91.2 89.26 84.49 82.06 75.52</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet-18</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet-34</cell><cell></cell></row><row><cell>IM</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell><cell>200</cell></row><row><cell>Baseline</cell><cell cols="10">62.59 57.09 48.55 43.65 38.87 63.87 57.55 48.07 43.55 37.5</cell></row><row><cell>CB [9] β = 0.9</cell><cell cols="10">63.1 11</cell></row><row><cell>SLA [26]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.89</cell><cell>-</cell><cell>-</cell><cell>45.53</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="10">65.08 58.69 51.9 46.57 42.84 65.29 59.75 52.17 48.51 41.46</cell></row></table><note>100, 200}, are created for both CIFAR-10 and CIFAR-100, where an imbalance factor is defined as where N i is the number of training samples of the i-th class. ResNet with depth 18 and 34 are adapted for this experiment. We use the original validation set of the CIFAR-10 and CIFAR-100 to evaluate the performance. The baseline network and the proposed method are implemented in PyTorch and run on a Xeon CPU of 2.1 GHz and Tesla V100 GPU. The initial learning</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Classification accuracy on long-tailed CIFAR-100.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Top-1 classification accuracy on ImageNet-LT and Places-LT.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sharing representations for long tail computer vision problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiscale fully convolutional network with application to industrial inspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>WACV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Image block augmentation for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on learning from imbalanced datasets II</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A guide to deep learning in healthcare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00941</idno>
		<title level="m">Deep active learning over the long tail</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adasyn: Adaptive synthetic sampling approach for imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rethinking data augmentation: Self-supervision and self-distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05872</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep imbalanced attribute classification using visual attention aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICML</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A comparative study of cost-sensitive boosting algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Feature generating networks for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Feature transfer learning for deep face recognition with long-tail data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09014</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Training cost-sensitive neural networks with methods addressing the class imbalance problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Visdrone-vdt2018: The vision meets drone video detection and tracking challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
