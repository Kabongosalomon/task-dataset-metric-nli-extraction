<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Attention Networks for Visual Reference Resolution in Visual Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gi-Cheon</forename><surname>Kang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeseo</forename><surname>Lim</surname></persName>
							<email>jaeseolim@snu.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
							<email>btzhang@snu.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surromind</forename><surname>Robotics</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Attention Networks for Visual Reference Resolution in Visual Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual dialog (VisDial) is a task which requires a dialog agent to answer a series of questions grounded in an image. Unlike in visual question answering (VQA), the series of questions should be able to capture a temporal context from a dialog history and utilizes visually-grounded information. Visual reference resolution is a problem that addresses these challenges, requiring the agent to resolve ambiguous references in a given question and to find the references in a given image. In this paper, we propose Dual Attention Networks (DAN) for visual reference resolution in VisDial. DAN consists of two kinds of attention modules, REFER and FIND. Specifically, REFER module learns latent relationships between a given question and a dialog history by employing a multi-head attention mechanism. FIND module takes image features and reference-aware representations (i.e., the output of REFER module) as input, and performs visual grounding via bottom-up attention mechanism. We qualitatively and quantitatively evaluate our model on VisDial v1.0 and v0.9 datasets, showing that DAN outperforms the previous state-of-the-art model by a significant margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Thanks to the recent progresses in natural language processing and computer vision, there has been an extensive amount of effort towards developing a cognitive agent that jointly understand natural language and vision information. Over the last few years, vision-language tasks such as image captioning <ref type="bibr" target="#b24">(Xu et al., 2015)</ref> and visual question answering (VQA) <ref type="bibr" target="#b1">(Antol et al., 2015;</ref><ref type="bibr" target="#b0">Anderson et al., 2018)</ref> have provided a testbed for developing a cognitive agent. However, the agent performing these tasks still has a long way to go to be used in real-world applications (e.g., aiding visually impaired users, interacting with humanoid robots) in that it does not consider the continuous interaction over time. Specifically, the interaction in image captioning is that the agent simply talks to human about visual content, without any input from human. While the VQA agent takes a question as input, it is required to answer a single question about a given image.</p><p>Visual dialog (VisDial) <ref type="bibr" target="#b3">(Das et al., 2017</ref>) task has been introduced as a generalized version of VQA. A dialog agent needs to answer a series of questions such as "How many people are in the image?", "Are they indoors or outside?", utilizing not only visually-grounded information but also contextual information from a dialog history. To address these two challenges, researchers have recently tackled a problem called visual reference resolution in VisDial. The problem of visual reference resolution is to resolve ambiguous expressions on their own (e.g., it, they, any other) and ground them to a given image.</p><p>In this paper, we address the visual reference resolution in a visual dialog task. We first hypothesize that humans address the visual reference resolution through a two-step process: (1) linguistically resolve the ambiguous questions by recalling the dialog history from one's memory and (2) find a local region of a given image for the resolved questions. For example, as shown in <ref type="figure">Figure 1</ref>, the question "Does it look like a nice one?" is ambiguous on its own because we do not know what "it" refers to. So we believe that humans try to recall the dialog history and implicitly find out "it" refers to the "skateboard". After the resolution step, we believe that they will finally try to find the skateboard in the image and answer the question. For these processes, we propose Dual Attention Networks (DAN) which consists of two kinds of attention modules, RE-FER and FIND. REFER module learns to retrieve the relevant previous dialogs for clarifying am- <ref type="figure">Figure 1</ref>: An overview of Dual Attention Networks (DAN). We propose two kinds of attention modules, REFER and FIND. REFER learns latent relationships between a given question and a dialog history to retrieve the relevant previous dialogs. FIND performs visual grounding, taking image features and reference-aware representations (i.e., the output of REFER). ⊗, ⊕, and denote matrix multiplication, concatenation and element-wise multiplication, respectively. The multi-layer perceptron is omitted in this figure for simplicity. biguous questions. Inspired by the self-attention mechanism <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref>, REFER module computes multi-head attention over all previous dialogs in a sentence-level fashion, followed by feed-forward networks to get the referenceaware representations. FIND module takes image features and the reference-aware representations, and performs visual grounding via bottom-up attention mechanism. From this pipeline, we expect our proposed model to be capable of question disambiguation by using REFER module and ground the resolved reference properly to the given image.</p><p>The main contributions of this paper are as follows. First, we propose Dual Attention Networks (DAN) for visual reference resolution in visual dialog based on REFER and FIND modules. Second, we validate our proposed model on the largescale datasets: VisDial v1.0 and v0.9. Our model achieves a new state-of-the-art results compared to other methods. We also conduct ablation studies by four criteria to demonstrate the effectiveness of our proposed components. Third, we make a comparison between DAN and our baseline model to demonstrate the performance improvements on semantically incomplete questions needed to be clarified. Finally, we perform qualitative analysis of our model, showing that DAN reasonably attends to the dialog history and salient image regions.</p><p>Our code is available at https://github. com/gicheonkang/DAN-VisDial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual Dialog. Visual dialog (VisDial) task was recently proposed by <ref type="bibr" target="#b3">(Das et al., 2017)</ref>, providing a testbed for research on the interplay between computer vision and dialog systems. Accordingly, a dialog agent performing this task is not only required to find visual groundings of linguistic expressions but also capture semantic nuances from human conversation. Attentionbased approaches were primarily proposed to address these challenges, including memory networks <ref type="bibr" target="#b3">(Das et al., 2017)</ref>, history-conditioned image attentive encoder <ref type="bibr" target="#b12">(Lu et al., 2017)</ref>, sequential co-attention <ref type="bibr" target="#b23">(Wu et al., 2018)</ref>, and synergistic coattention networks <ref type="bibr" target="#b4">(Guo et al., 2019)</ref>.</p><p>Visual Reference Resolution. Recently, researchers have tackled a problem called visual reference resolution <ref type="bibr" target="#b18">(Seo et al., 2017;</ref><ref type="bibr" target="#b9">Kottur et al., 2018;</ref><ref type="bibr" target="#b14">Niu et al., 2018)</ref> in VisDial. To resolve visual references, <ref type="bibr" target="#b18">(Seo et al., 2017)</ref> proposed an attention memory which stores a sequence of previous visual attention maps in memory slots. They retrieved the previous visual attention maps by applying a soft attention over all the memory slots and combined it with a current visual attention. Furthermore, <ref type="bibr" target="#b9">(Kottur et al., 2018)</ref> attempted to resolve visual references at a word-level, relying on an off-the-shelf parser. Similar to the attention memory <ref type="bibr" target="#b18">(Seo et al., 2017)</ref>, they proposed a reference pool which stores visual attention maps of recognized entities and retrieved the weighted sum of the visual attention maps by applying a soft attention. <ref type="bibr" target="#b14">(Niu et al., 2018)</ref> proposed a recursive visual attention model that recursively reviews the previous dialogs and refines the current visual attention. The recursion is continued until the question itself is determined to be unambiguous. A binary decision whether the questions is ambiguous or not is made by Gumbel-Softmax approximation <ref type="bibr" target="#b6">(Jang et al., 2016;</ref><ref type="bibr" target="#b13">Maddison et al., 2016)</ref>. To resolve the visual references, above approaches attempted to retrieve the visual attention of the previous dialogs, and applied it on the current visual attention. These approaches have limitations in that they store all previous visual attentions, while researches in human memory system show that the visual sensory-memory, due to its rapid decay property, hardly stores all previous visual attentions <ref type="bibr" target="#b21">(Sperling, 1960;</ref><ref type="bibr" target="#b19">Sergent et al., 2011)</ref>. Based on this biologically inspired motivation, our proposed model calculates the current visual attention by using linguistic cues (i.e., dialog history).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Algorithm</head><p>In this section, we formally describe the visual dialog task and our proposed algorithm, Dual Attention Networks (DAN). The visual dialog task <ref type="bibr" target="#b3">(Das et al., 2017)</ref> is defined as follows. A dialog agent is given an input such as an image I, a follow-up question at round t as Q t , and a dialog history (including the image caption) until round t − 1,</p><formula xml:id="formula_0">H = ( C H 0 , (Q 1 , A gt 1 ) H 1 , · · · , (Q t−1 , A gt t−1 ) H t−1 ).</formula><p>A gt t denotes the ground truth answer (i.e., human response) at round t. By using these inputs, the agent is asked to rank a list of 100 candidate answers, A t = A 1 t , · · · , A 100 t . Given the problem setup, DAN for visual dialog task can be framed as an encoder-decoder architecture: (1) an encoder that jointly embeds the input (I, Q t , H) and (2) a decoder that converts the embedded representation into the ranked listÂ t . From this point of view, DAN consists of three components which are REFER, FIND, and the answer decoder. As shown in <ref type="figure">Figure 1</ref>, REFER module learns to attend relevant previous dialogs to re-solve the ambiguous references in a given question Q t . FIND module learns to attend to the spatial image features that the output of REFER module describes. Answer decoder ranks the list of candidate answers A t given the output of FIND module.</p><p>We first introduce the language features, as well as the image features in Sec. 3.1. Then we describe the detailed architectures of the REFER and FIND modules in Sec. 3.2 and 3.3, respectively. Finally, we present the answer decoder in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Representation</head><p>Language Features. We first embed each word in the follow-up question Q t to {w t,1 , · · · , w t,T } by using pre-trained GloVe <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref> embeddings, where T denotes the number of tokens in Q t . We then use a two-layer LSTM, generating a sequence of hidden states {u t,1 , · · · , u t,T }. Note that we use the last hidden state of the LSTM u t,T as a question feature, denoted as q t ∈ R L .</p><formula xml:id="formula_1">u t,i = LSTM(w t,i , u t,i−1 ) (1) q t = u t,T<label>(2)</label></formula><p>Also, each element in the dialog history {H i } t−1 i=0 and the candidate answers</p><formula xml:id="formula_2">A i t 100 i=1 are embed- ded as the follow-up question, yielding {h i } t−1 i=0 ∈ R t×L and o i t 100 i=1 ∈ R 100×L . Q t ,</formula><p>H, and A t are embedded with same word embedding vector and three different LSTMs.</p><p>Image Features. Inspired by bottom-up attention <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref>, we use the Faster R-CNN <ref type="bibr" target="#b16">(Ren et al., 2015)</ref> pre-trained with Visual Genome <ref type="bibr" target="#b10">(Krishna et al., 2017)</ref> to extract the object-level image features. We denote the output features as v ∈ R K×V , where K and V are the total number of object detection features per image and dimension of the each feature, respectively. We adaptively extract the number of object features K ranging from 10 to 100 for reflecting the complexity of each image. K is fixed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">REFER Module</head><p>In this section, we formally describe the singlelayer REFER module. Given the question and dialog history features, REFER module aims to attend to the most relevant elements of dialog history with respect to the given question. Specifically, we first compute scaled dot product attention <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> in multi-head settings which are called multi-head attention. Let q t and</p><formula xml:id="formula_3">M t = {h i } t−1</formula><p>i=0 be the question and dialog history feature vectors, respectively. q t and M t are projected to d ref dimensions by different and learnable projection matrices. We then conduct dot product of these two projected matrices, divide by d ref , and apply the softmax function to obtain the attention weights on the all elements in the dialog history. It is formulated as below,</p><formula xml:id="formula_4">head n = Attention(q t W q n , M t W m n ) (3) Attention(a, b) = softmax( ab d ref )b (4) where W q n ∈ R L×d ref and W m n ∈ R L×d ref .</formula><p>Note that dot product attention is computed h times with different projection matrices, yielding {head n } h n=1 . Accordingly, we can get the multi-head representations x t , concatenating all {head n } h n=1 , followed by linear projection. Also, we can computex t by applying a residual connection , followed by layer normalization <ref type="bibr">(Ba et al., 2016)</ref>.</p><formula xml:id="formula_5">x t = (head 1 ⊕ · · · ⊕ head h ) W o (5) x t = LayerNorm(x t + q t )<label>(6)</label></formula><p>where ⊕ denotes the concatenation operation, and W o ∈ R hd ref ×L is the projection matrix. Next, we applyx t to two-layer feed-forward networks with a ReLU in between, where W f 1 ∈ R L×2L and W f 2 ∈ R 2L×L . The residual connection and layer normalization is also applied in this step.</p><formula xml:id="formula_6">c t = ReLU(x t W f 1 + b f 1 )W f 2 + b f 2 (7) c t = LayerNorm(c t +x t ) (8) e ref t =ĉ t ⊕ q t<label>(9)</label></formula><p>Finally, REFER module returns the referenceaware representations by concatenating the contextual representationĉ t and the original question representation q t , denoted as e ref t ∈ R 2L . In this work, we use d ref = 256. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the pipeline of the REFER module.</p><p>Furthermore, we stack the REFER modules in multiple layers to get a high-level abstraction of the reference-aware representations. Details are to be discussed in Sec. 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">FIND Module</head><p>Instead of relying on the visual attention maps of the previous dialogs as in <ref type="bibr" target="#b18">(Seo et al., 2017;</ref><ref type="bibr" target="#b9">Kottur et al., 2018;</ref><ref type="bibr" target="#b14">Niu et al., 2018)</ref>, we expect the FIND module to attend to the most relevant regions of the image with respect to the reference-aware representations (i.e., the output of REFER module). In order to implement the visual grounding for the reference-aware representations, we take inspiration from bottom-up attention mechanism <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref>. Let v ∈ R K×V and e ref t ∈ R 2L be the image feature vectors and reference-aware representations, respectively. We first project these two vectors to d f ind dimensions and compute soft attention over all the object detection features as follows:</p><formula xml:id="formula_7">r t = f v (v) f ref (e ref t )<label>(10)</label></formula><formula xml:id="formula_8">α t = softmax(r t W r + b r )<label>(11)</label></formula><p>where f v (·) and f ref (·) denote the two-layer multi-layer perceptrons which convert to d f ind dimensions, and W r ∈ R d f ind ×1 is the projection matrix for the softmax activation. denotes hadamard product (i.e., element-wise multiplication). From these equations, we can get the visual attention weights α t ∈ R K×1 . Next, we apply the visual attention weights to v and compute the vision-language joint representations as follows:</p><formula xml:id="formula_9">v t = K j=1 α t,j v j (12) z t = f v (v t ) f ref (e ref t )<label>(13)</label></formula><formula xml:id="formula_10">e f ind t = z t W z + b z<label>(14)</label></formula><p>where f v (·) and f ref (·) also denote the two-layer multi-layer perceptrons which convert to d f ind dimensions, and W z ∈ R d f ind ×L is the projection matrix.</p><p>Note that e f ind t ∈ R L is the output representations of the encoder as well as FIND module which is decoded to score the list of candidate answers. In this work, we use d f ind = 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answer Decoder</head><p>Answer decoder computes each score of candidate answers via a dot product with the embedded representation e f ind t , followed by a softmax activation to get a categorical distribution over the candidates. Let O t = o i t 100 i=1 ∈ R 100×L be the feature vectors of 100 candidate answers. The distribution p t is formulated as follows:</p><formula xml:id="formula_11">p t = softmax(e f ind t O t )<label>(15)</label></formula><p>In training phase, DAN is optimized by minimizing the cross-entropy loss between the one-hot encoded label vector (i.e., y t ) and probability distribution (i.e., p t ).</p><formula xml:id="formula_12">L(θ) = − k y t,k log p t,k<label>(16)</label></formula><p>Where p t,k denotes the probability of the k-th candidate answer at round t. In test phase, the list of candidate answers is ranked by the distribution p t , and evaluated by the given metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe the details of our experiments on the VisDial v1.0 and v0.9 datasets. We first introduce the VisDial datasets, evaluation metrics, and implementation details in Sec. 4.1, Sec. 4.2, and Sec. 4.3, respectively. Then we report the quantitative results by comparing our proposed model with the state-of-the-art approaches and baseline model in Sec. 4.4. Then, we conduct the ablation studies by four criteria to report the relative contributions of each components in Sec. 4.5. Finally, we provide the qualitative results in Sec. 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our proposed model on the VisDial v0.9 and v1.0 dataset. VisDial v0.9 dataset <ref type="bibr" target="#b3">(Das et al., 2017)</ref> has been collected from two annotators chatting log about MS-COCO <ref type="bibr" target="#b11">(Lin et al., 2014)</ref> images. Each dialog is made up of an image, a caption from MS-COCO dataset and 10 QA pairs. As a result, VisDial v0.9 dataset contains 83k dialogs and 40k dialogs as train and validation splits, respectively. Recently, VisDial v1.0 dataset <ref type="bibr" target="#b3">(Das et al., 2017)</ref> has been released with an additional 10k COCO-like images from Flickr. Dialogs for the additional images have been collected similar to v0.9. Overall, VisDial v1.0 dataset contains 123k (all dialogs from v0.9), 2k, and 8k dialogs as train, validation, and test splits, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We evaluate individual responses at each question in a retrieval setting according to <ref type="bibr" target="#b3">(Das et al., 2017)</ref>. Specifically, the dialog agent is given a list of 100 candidate answers of each question and asked to rank the list. There are three kinds of evaluation metrics for retrieval performance: (1) mean rank of human response, (2) recall@k (i.e., existence of the human response in top-k ranked response), and (3) mean reciprocal rank (MRR). Mean rank, recall@k, and MRR are highly correlated with the rank of human response. In addition, <ref type="bibr" target="#b3">(Das et al., 2017)</ref> proposed to use the robust evaluation metric, normalized discounted cumulative gain (NDCG). NDCG takes into account all relevant answers from the ranked list, where the relevance scores are densely annotated for VisDial v1.0 test split. NDCG penalizes the lower rank of the candidate answers with high relevance scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>The dimension of image features V and hidden states in all LSTM L is 2048 and 512, respectively. All the language intputs are embedded into a 300dimensional vector initialized by GloVe <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref>. The number of attention heads h is fixed to 4 except for the ablation study that changes it. We apply Adam optimizer (Kingma and Ba, 2014) with learning rate 1 ×10 −3 , decreased by 1 ×10 −4 per epoch until epoch 7, decayed by 0.5 per epoch from 8 to 12 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quantitative Results</head><p>Compared Methods. We compare our proposed model with the state-of-the-art approaches on VisDial v1.0 and v0.9 datasets, which can be categorized into three groups: (1) Fusion-based approaches (LF and HRE <ref type="bibr" target="#b3">(Das et al., 2017)</ref>), (2) Attention-based approaches (MN <ref type="bibr" target="#b3">(Das et al., 2017)</ref>, HCIAE <ref type="bibr" target="#b12">(Lu et al., 2017)</ref>, CoAtt <ref type="bibr" target="#b23">(Wu et al., 2018)</ref> and Synergistic <ref type="bibr" target="#b4">(Guo et al., 2019)</ref>), and <ref type="formula">(3)</ref> Approaches that deal with visual reference resolution in VisDial (AMEM (Seo et al., 2017), CorefNMN <ref type="bibr" target="#b9">(Kottur et al., 2018)</ref> and RvA <ref type="bibr" target="#b14">(Niu et al., 2018)</ref>). Our proposed model belongs to the third category.</p><p>Results on VisDial v1.0 and v0.9 datasets. As shown in <ref type="table">Table 1</ref>, DAN significantly outperforms all other approaches on NDCG, MRR, and R@1, including the previous state-of-the-art method, Synergistic <ref type="bibr" target="#b4">(Guo et al., 2019</ref>  higher than all other methods on both single ground-truth answer (R@1) and all relevant answers on average (NDCG).</p><p>Results on ensemble model. We report the performance of ensemble model in comparison with the top-three entries in the leaderboard 1 of Vis-Dial Challenge 2018. We ensemble six DAN models, using the number of attention heads (i.e., h) ranging from one to six. We average the probability distribution (i.e., p t ) of the six models to rank the candidate answers. In <ref type="table" target="#tab_2">Table 2</ref>, our model significantly outperforms all three challenge entries, including the challenge winner model, Synergistic <ref type="bibr" target="#b4">(Guo et al., 2019)</ref>. They ensembled ten models with different weight initialization and also used bottom-up attention features <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref> as image features.</p><p>Results on semantically complete &amp; incomplete questions. We first define the questions that contain one or more pronouns (i.e., it, its, they, their, them, these, those, this, that, he, his, him, she,  <ref type="table">Table 3</ref>: VisDial v1.0 validation performance on the semantically complete (SC) and incomplete (SI) questions. We observe that SI questions obtain more benefits from the dialog history than SC questions.</p><p>her) as the semantically incomplete (SI) questions. Also, we can declare the questions that do not have pronouns as semantically complete (SC) questions. Then, we have checked the contribution of the reference-aware representations for the SC and SI questions, respectively. Specifically, we make a comparison between DAN, which utilizes reference-aware representations (i.e., e ref t ), and No REFER, which exploits question representations (i.e., q t ) only. From the <ref type="table">Table 3</ref>, we draw three observations: (1) DAN shows significantly better results than the No REFER model for SC questions. It validates that the context from dialog history enriches the question information, even when the question is semantically complete.</p><p>(2) SI questions obtain more benefits from the dialog history than SC questions. It indicates that DAN is more robust to the SI questions than SC questions. (3) A dialog agent faces greater difficulty in answering SI questions compared to SC questions. No REFER is equivalent to the FIND + RPN model in the ablation study section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>In this section, we perform ablation study on Vis-Dial v1.0 validation split with the following four model variants: (1) Model only using the single attention module, (2) Model that uses different image features (pre-trained VGG-16 is used), (3) Model that does not use the residual connection in REFER module, and (4) Model that stacks the RE-FER modules up to four layers with each different number of attention heads.</p><p>Single Module. The first four rows in <ref type="table" target="#tab_5">Table 4</ref> show the performance of a single module. FIND denotes the use of FIND module only, and RE-FER denotes the use of single-layer REFER module only. Specifically, REFER uses the output of REFER module as the encoder outputs. On   the other hand, FIND does not take the referenceaware representations (i.e., e ref t ) but the question feature (i.e., q t ). The single models show relatively poor performance compared with the dual module model. We believe that the results validate two hypotheses: (1) VisDial task requires contextual information from dialog history as well as the visually-grounded information.</p><p>(2) REFER and FIND modules have complementary modeling abilities.</p><p>Image Features in FIND Module. To report the impact of image features, we replace the bottomup attention features <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref> with ImageNet pre-trained VGG-16 <ref type="bibr" target="#b20">(Simonyan and Zisserman, 2014)</ref> features. In detail, we use the output of the VGG-16 pool5 layer as image features. In <ref type="table" target="#tab_5">Table 4</ref>, RPN denotes the use of the region proposal networks <ref type="bibr" target="#b16">(Ren et al., 2015)</ref> which are equivalent to the use of bottom-up attention features. Similar to VQA task, we observe that DAN with bottom-up attention features achieves better performance than with VGG-16 features. In other words, the use of object-level features boosts the MRR performance of DAN.</p><p>Residual Connection in REFER Module. We also conduct an ablation study to investigate the effectiveness of the residual connection in REFER module. As shown in <ref type="table" target="#tab_5">Table 4</ref>, the use of the residual connection (i.e., Res) boosts the MRR score of DAN. In other words, DAN utilizes the excellence of deep residual learning as in <ref type="bibr" target="#b17">Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b25">Yang et al., 2016;</ref><ref type="bibr" target="#b7">Kim et al., 2016;</ref><ref type="bibr" target="#b22">Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stack of REFER Modules &amp; Attention Heads.</head><p>We stack the REFER modules up to four layers with each different number of attention heads, h ∈ {1, 2, 4, 8, 16, 32, 64}. In other words, we conduct the ablation experiments with twenty-eight models to set the hyperparameters of our model. <ref type="figure" target="#fig_1">Figure 3</ref> shows the results of the ablation experiments. For n ≥ 2, REFER (n) indicates that DAN uses a stack of n identical REFER modules. Specifically, for each pair of successive modules, the output of the previous REFER module is fed into the next REFER module as a query (i.e., q t ). Due to the small number of elements in each dialog history, the overall performance pattern shows a tendency to decrease as the number of attention heads in-creases. It turns out that the two-layer REFER module with four attention heads (i.e., REFER (2) and h = 4) performs the best among all models in ablation study, recording 64.17% on MRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Results</head><p>In this section, we visualize the inference mechanism of our proposed model. <ref type="figure" target="#fig_2">Figure 4</ref> shows the qualitative results of DAN. Given a question that is needed to be clarified, DAN correctly answers the question by selectively attending to each element of the dialog history and salient image regions. In case of the visual attention, we mark the object detection features with top five attention weights of each image. On the other hand, the attention weights from REFER module are represented as shading; darker shading indicates the larger attention weight for each element of the dialog history. These attention weights are calculated by averaging over all the attention heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce Dual Attention Networks (DAN) for visual reference resolution in visual dialog task. DAN explicitly divides the visual reference resolution problem into a two-step process. Rather than relying on the previous visual attention maps as in prior works, DAN first linguistically resolves ambiguous references in a given question by using REFER module. Then, it grounds the resolved references in the image by using FIND module. We empirically validate our proposed model on VisDial v1.0 and v0.9 datasets. DAN achieves the new state-of-the-art performance, while being simpler and more grounded.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the single-layer REFER module. REFER module focuses on the latent relationship between the follow-up question and a dialog history to resolve ambiguous references in the question. We employ two submodule: multi-head attention and feed-forward networks. Multi-head attention computes the h number of soft attentions over all elements of dialog history by using scaled dot product attention. Then, it returns the h number of heads which are weighted by the attentions. Followed by the two-layer feed-forward networks, REFER module finally returns the reference-aware representations e ref t . ⊕ and Dotted line denote the concatenation operation and linear projection operation by the learnable matrices, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Ablation study on a different number of attention heads and REFER stacks. REFER (n) indicates that DAN uses a stack of n identical REFER modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on the VisDial v1.0 dataset. We visualize the attention over dialog history from REFER module and the visual attention from FIND module. The object detection features with top five attention weights are marked with colored box. A red colored box indicates the most salient visual feature. Also, the attention from REFER module is represented as shading, darker shading indicates the larger attention weight for each element of the dialog history. Our proposed model not only responds to the correct answer, but also selectively pays attention to the previous dialogs and salient image regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>R@k), and mean rank. The higher the better for NDCG, MRR, and R@k, while the lower the better for mean rank. DAN outperforms all other models across NDCG, MRR, and R@1 on both datasets. NDCG is not supported in v0.9 dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">VisDial v1.0 (test-std)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">VisDial v0.9 (val)</cell></row><row><cell></cell><cell cols="9">NDCG MRR R@1 R@5 R@10 Mean MRR R@1 R@5 R@10 Mean</cell></row><row><cell>LF (Das et al., 2017)</cell><cell cols="5">45.31 55.42 40.95 72.45 82.83</cell><cell cols="4">5.95 58.07 43.82 74.68 84.07</cell><cell>5.78</cell></row><row><cell>HRE (Das et al., 2017)</cell><cell cols="5">45.46 54.16 39.93 70.45 81.50</cell><cell cols="4">6.41 58.46 44.67 74.50</cell><cell>4.22</cell><cell>5.72</cell></row><row><cell>MN (Das et al., 2017)</cell><cell cols="5">47.50 55.49 40.98 72.30 83.30</cell><cell cols="4">5.92 59.65 45.55 76.22 85.37</cell><cell>5.46</cell></row><row><cell>HCIAE (Lu et al., 2017)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">62.22 48.48 78.75 87.59</cell><cell>4.81</cell></row><row><cell>AMEM (Seo et al., 2017)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">62.27 48.53 78.66 87.43</cell><cell>4.86</cell></row><row><cell>CoAtt (Wu et al., 2018)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">63.98 50.29 80.71 88.81</cell><cell>4.47</cell></row><row><cell cols="6">CorefNMN (Kottur et al., 2018) 54.70 61.50 47.55 78.10 88.80</cell><cell cols="4">4.40 64.10 50.92 80.18 88.81</cell><cell>4.45</cell></row><row><cell>RvA (Niu et al., 2018)</cell><cell cols="5">55.59 63.03 49.03 80.40 89.83</cell><cell cols="4">4.18 66.34 52.71 82.97 90.73</cell><cell>3.93</cell></row><row><cell>Synergistic (Guo et al., 2019)</cell><cell cols="5">57.32 62.20 47.90 80.43 89.95</cell><cell>4.17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DAN (ours)</cell><cell cols="5">57.59 63.20 49.63 79.75 89.35</cell><cell cols="4">4.30 66.38 53.33 82.42 90.38</cell><cell>4.04</cell></row><row><cell cols="10">Table 1: Retrieval performance on VisDial v1.0 and v0.9 datasets, measured by normalized discounted cumulative</cell></row><row><cell cols="5">gain (NDCG), mean reciprocal rank (MRR), recall @k (</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test-std performance of ensemble model on VisDial v1.0 dataset. We cite top-three entries from VisDial Challenge 2018 Leaderboard.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on VisDial v1.0 validation split. Res and RPN denote the residual connection and the region proposal networks, respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://evalai.cloudcv.org/web/ challenges/challenge-page/103/ leaderboard</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements The authors would like to thank Woosuk Choi, Seunghee Yang, Junseok Park, and Delilah Hollweg for helpful comments and editing. This work was partly supported by the Korea government (2015-0-00310-SW.StarLab, 2017-0-01772-VTT, 2018-0-00622-RMI, 2019-0-01367-BabyMind, 10060086-RISF, P0006720-GENKO), and the ICT at Seoul National University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Visual dialog</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Imagequestion-answer synergistic network for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09774</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual coreference resolution in visual dialog using neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="153" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="314" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02664</idno>
		<title level="m">Recursive visual attention in visual dialog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual reference resolution using attention memory for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3719" to="3729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Top-down modulation of human early visual cortex after stimulus offset supports successful postcued report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Sergent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Barbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geraint</forename><surname>Driver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1921" to="1934" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The information available in brief visual presentations. Psychological monographs: General and applied</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Sperling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Are you talking to me? reasoned visual dialog generation through adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6106" to="6115" />
		</imprint>
	</monogr>
	<note>Ian Reid, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
