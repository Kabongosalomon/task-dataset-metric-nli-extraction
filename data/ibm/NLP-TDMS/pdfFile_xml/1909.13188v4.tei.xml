<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding and Stabilizing GANs&apos; Training Dynamics using Control Theory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Understanding and Stabilizing GANs&apos; Training Dynamics using Control Theory</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial networks (GANs) are effective in generating realistic images but the training is often unstable. There are existing efforts that model the training dynamics of GANs in the parameter space but the analysis cannot directly motivate practically effective stabilizing methods. To this end, we present a conceptually novel perspective from control theory to directly model the dynamics of GANs in the function space and provide simple yet effective methods to stabilize GANs' training. We first analyze the training dynamic of a prototypical Dirac GAN and adopt the widely-used closed-loop control (CLC) to improve its stability. We then extend CLC to stabilize the training dynamic of normal GANs, where CLC is implemented as a squared L2 regularizer on the output of the discriminator. Empirical results show that our method can effectively stabilize the training and obtain state-of-the-art performance on data generation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b12">(Goodfellow et al., 2014)</ref> have shown promise in generating realistic natural images <ref type="bibr" target="#b2">(Brock et al., 2018)</ref> and facilitating unsupervised and semi-supervised learning <ref type="bibr" target="#b20">Li et al., 2017;</ref><ref type="bibr" target="#b5">Donahue &amp; Simonyan, 2019)</ref>. In GANs, an implicit generator G is defined by mapping a noise distribution to the data space. Since no density function is defined for the implicit generator, the maximum likelihood estimate is infeasible for GANs. Instead, a discriminator D is introduced to estimate the density ratio between the data distribution p and the generating distribution p G by telling the real sam-1 Dept. of Comp. Sci. &amp; Tech., Institute for AI, BN-Rist Center, Tsinghua-Bosch ML Center, THBI Lab, Tsinghua University, Beijing, China. Correspondence to: Jun Zhu &lt;dc-szj@mail.tsinghua.edu.cn&gt;.</p><p>Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). ples from fake ones. G aims to recover the data distribution by maximizing this ratio. This framework is formulated as a minimax optimization problem, which can be solved by optimizing G and D alternately. In practice, however, GANs suffers from the instability of training <ref type="bibr" target="#b11">(Goodfellow, 2016)</ref>, where divergency and oscillations are often observed <ref type="bibr" target="#b21">(Liang et al., 2018;</ref><ref type="bibr" target="#b3">Chavdarova &amp; Fleuret, 2018)</ref>.</p><p>Early methods <ref type="bibr" target="#b23">(Mao et al., 2017;</ref><ref type="bibr" target="#b13">Gulrajani et al., 2017;</ref><ref type="bibr" target="#b1">Arjovsky et al., 2017;</ref><ref type="bibr" target="#b6">Du et al., 2018)</ref> introduce different types of divergences to improve the training process of GANs. Their theoretical analyses assume that D achieves its optimum when training G. However, the practical training process (e.g., alternative stochastic gradient descent) often violates the above assumption and therefore is not guaranteed to converge to the desired equilibrium. Several empirical regularizations <ref type="bibr" target="#b26">(Miyato et al., 2018;</ref><ref type="bibr" target="#b13">Gulrajani et al., 2017;</ref><ref type="bibr">Zhang et al., 2019)</ref> are used to improve the training process whereas no stability can be guaranteed.</p><p>Recently, <ref type="bibr" target="#b24">Mescheder et al. (2017)</ref> and <ref type="bibr" target="#b27">Nagarajan &amp; Kolter (2017)</ref> directly model the training dynamics of GANs, i.e. how the parameters develop over time. Formally, the dynamic is defined as the gradient flow of the parameters. The stability of the dynamic is fully determined by the eigenvalues of the Jacobian matrix of the gradient flow. Indeed, the stability analysis in a linear prototypical GAN (i.e. Dirac GAN <ref type="bibr" target="#b25">(Mescheder et al., 2018)</ref>) is elegant. However, this analysis does not directly motivate effective algorithms to stabilize GANs' training. To our knowledge, such methods do not report competitive image generation results to the state-of-the-art GANs <ref type="bibr" target="#b26">(Miyato et al., 2018)</ref>.</p><p>In this paper, we understand and stabilize GANs' training dynamics from the perspective of control theory. Based on the recipe for control theory, we can not only analyze the dynamics of Dirac GAN formally, but also develop practically effective stabilizing methods for nonlinear dynamics <ref type="bibr" target="#b17">(Khalil, 2002)</ref>. Specifically, we start from revisiting the Dirac GAN example with the WGAN's objective function in Sec. 3. By utilizing the Laplace transform <ref type="bibr" target="#b31">(Widder, 2015)</ref> (LT), the training dynamics of both D and G can be modeled in the frequency domain instead of the time domain in previous methods <ref type="bibr" target="#b24">(Mescheder et al., 2017;</ref>. These types of dynamics are well studied in control theory and <ref type="bibr">arXiv:1909.13188v4 [cs.</ref>LG] 8 Jul 2020 the stability can be easily inferred. The analysis can be simply generalized to other objective functions with local linearization. Given the instability of GANs, the recipe for control theory provides a set of tools to stabilize their dynamics. We first adopt the closed-loop control (CLC) to successfully stabilize the dynamic of Dirac GAN with theoretical guarantee. Besides, extensive empirical results in control theory show that the CLC is also helpful in nonlinear settings <ref type="bibr" target="#b17">(Khalil, 2002)</ref>. It inspires us to extend our proposal to normal GANs by modeling D and G's dynamics in the function space where these dynamics and Dirac GAN's dynamics share similar forms and characters. The CLC is implemented as a regularization term to D's objective function which penalizes the squared L2 norm of the output of D as we described in Sec. 4.1. We therefore refer our method as CLC-GAN. CLC-GAN is verified on an 1-dimension toy example as well as the natural images including CIFAR10 <ref type="bibr" target="#b18">(Krizhevsky et al., 2009)</ref> and CelebA <ref type="bibr" target="#b22">(Liu et al., 2015)</ref>. The results demonstrate that our method can successfully stabilize the dynamics of GANs and achieve state-of-the-art performance.</p><p>Our contributions are summarized as:</p><p>• We formally analyze the training dynamics of GANs from a novel perspective of control theory, which is generally applicable to different objective functions. • We propose to use the CLC as an effective method to stabilize the training of GANs, while other advanced control methods can be explored in future. • The simulated results on Dirac GAN agree with the theoretical analysis and CLC-GAN achieves the stateof-the-art performance on natural image generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminary</head><p>In this section, we present the recipe for control theory, especially under the Laplace transform, which is powerful to model dynamic systems and design stabilizing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Modeling Dynamic Systems</head><p>In control theory, a signal is represented as a function over time t, i.e., in the time domain <ref type="bibr" target="#b16">(Kailath, 1980)</ref>. A dynamic 1 represents how one signal (i.e., output, denoted by y(t)) develops with respect to another signal (i.e., input, denoted by u(t)) over time. A natural representation of a dynamic is a differential equation (DE) 2 :</p><formula xml:id="formula_0">dy(t) dt = f (y(t), u(t)),<label>(1)</label></formula><p>together with an initial condition y(0) = y 0 . Note that f (·, ·), y(t) and u(t) can be vector valued functions. We 1 For simplicity, we use dynamic for dynamic system. <ref type="bibr">2</ref> We consider ordinary differential equations in this paper. assume y 0 = 0 unless specified. A dynamic is linear if f (·, ·) is a linear function.</p><p>Besides the time domain, a signal can also be represented as a function of frequency s, i.e., in the frequency domain. A DE of a linear dynamic in the time domain can be converted to a simple algebraic equation in the frequency domain, which can largely simplify the solving process and stability analysis of a dynamic. Laplace transform <ref type="bibr" target="#b31">(Widder, 2015)</ref> (LT) is a widely-adopted operator to convert signals from the time domain to the frequency domain. Formally, LT is given by:</p><formula xml:id="formula_1">F(h)(s) = ∞ 0 h(t)e −st dt = H(s),<label>(2)</label></formula><p>where h is a signal in the time domain, and s = σ + ωi ∈ C with real numbers σ and ω. The real and imaginary parts of H(s) ∈ C denote the gain and phase of the frequency s in h. In this paper, we use bold lowercase letters (e.g., y, u) to denote signals in the time domain and bold capital letters (e.g. Y , U ) to denote signals in the frequency domain.</p><p>Leveraging LT, the derivation over time t can be represented as multiplying a factor s in the frequency domain:</p><formula xml:id="formula_2">F( dh(t) dt ) = sF(h).<label>(3)</label></formula><p>Therefore, by applying LT to both sides of a DE in Eqn.</p><p>(1), a linear dynamic can be solved by the formal rules of algebra and represented in the form of Y (s) = T (s)U (s), where T (s) is a simple rational fraction called transfer function <ref type="bibr" target="#b16">(Kailath, 1980)</ref>. The transfer function can facilitate the stability analysis, as detailed in Sec. 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Stability Analysis</head><p>In general, we require a dynamic to be stable. Although different definitions exist, we consider the widely adopted asymptotic stability 3 <ref type="bibr" target="#b16">(Kailath, 1980)</ref> in this paper.</p><p>Definition 1. For a constant input u(t) = u c , a point y e is called an equilibrium point of a dynamic represented in Eqn.</p><p>(1), if f (y e , u c ) = 0. A dynamic is called asymptotically stable if for every &gt; 0, there exists σ &gt; 0 such that if ||y(0) − y e || &lt; σ, then for every t &gt; 0, ||y(t) − y e || &lt; and lim t→∞ ||y(t) − y e || = 0. Here || · || is a norm defined in the vector space of y.</p><p>In the frequency domain, the stability can be directly inferred from the transfer function. Formally, we define poles as the roots of the denominator in a transfer function. The stability of a linear dynamic is fully determined by its poles as summarized in the following proposition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Control Methods</head><p>For an unstable dynamic, control theory provides a set of methods to improve its stability. Among them, the closedloop control <ref type="bibr" target="#b16">(Kailath, 1980)</ref> (CLC) is one of the most popular ones and robust to nonlinearity in dynamics practically.</p><p>The central idea is to modify the transfer function by feeding the output back to the input such that all poles have negative real parts. Specifically, we introduce an additional dynamics called controllers with transfer functions T b (s) to adjust the output signal and input signal respectively. The controller takes Y (s) as input and output the feedback signal</p><formula xml:id="formula_3">Y b = T b (s)Y (s)</formula><p>. We then substitute the difference between U and Y b (i.e., M = U − Y b ) for input in the original dynamics, resulting the output signal as Y (s) = T (s)M (s). The relationship between the input U (s) and the output Y (s) is:</p><formula xml:id="formula_4">Y (s) = T (s)(U (s) − T b (s)Y (s)).<label>(4)</label></formula><p>Further, the whole controlled dynamic is given as:</p><formula xml:id="formula_5">Y (s) = T (s) 1 + T b (s)T (s) U (s).<label>(5)</label></formula><p>With a properly designed T b , the poles of the dynamic in Eqn. (5) can have negative real parts and the dynamic is stabilized. In the following, we first model and stabilize the training dynamic of Dirac GAN: a simplified GAN with linear dynamics in Sec. 3 and then we generalize it to the realistic setting in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analyzing Dirac GAN by Control Theory</head><p>In this section, we focus on the Dirac GAN <ref type="bibr" target="#b25">(Mescheder et al., 2018)</ref>, which is a widely adopted example to analyze the stability of GANs. Previous work <ref type="bibr" target="#b24">(Mescheder et al., 2017;</ref><ref type="bibr" target="#b9">Gidel et al., 2018)</ref> uses the Jacobian matrix to analyze the stability of dynamics whereas does not directly provide an approach to stabilize it. Instead, we revisit this example from the perspective of control theory and develop a principled method that not only analyzes but also improves the stability of various GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Modeling Dynamics</head><p>We first model the dynamics of the Dirac GANs in the language of control theory, which can facilitate the stability analysis and improvement in Sec. 3.2. In Dirac GAN, G is defined as p G (x) = δ(x − θ) where δ(·) is the Dirac delta function, and D is defined as D(x) = φx. θ and φ are the parameters of G and D respectively. The data distribution is p(x) = δ(x − c) with a constant c. Generally, the objective functions of D and G can be written as:</p><formula xml:id="formula_6">max φ V 1 (φ; θ) = h 1 (D(c)) + h 2 (D(θ)), max θ V 2 (θ; φ) = h 3 (D(θ)).<label>(6)</label></formula><p>Here h i (·) : R → R is a scalar function for i ∈ {1, 2, 3}.</p><p>Assuming that the equilibrium point of D is a zero function as in most GANs <ref type="bibr" target="#b12">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b1">Arjovsky et al., 2017)</ref>, it is required that h 1 (·) and h 3 (·) are increasing functions and h 2 (·) is a decreasing function around zero. For instance, when h 1 (x) = h 3 (x) = log(σ(x)) and h 2 (x) = log(1 − σ(x)) with σ(·) denoting the sigmoid function, we obtain the vanilla GAN <ref type="bibr" target="#b12">(Goodfellow et al., 2014)</ref>.</p><p>Since θ and φ are updated using gradient descent, we can denote the training trajectories as signals θ and φ. The dynamics are defined by the following gradient flow:</p><formula xml:id="formula_7">dφ(t) dt = ∂V 1 (φ; θ) ∂φ | φ=φ(t),θ=θ(t) , dθ(t) dt = ∂V 2 (θ; φ) ∂θ | φ=φ(t),θ=θ(t) .<label>(7)</label></formula><p>Specifically, for the dynamics of D, we have:</p><formula xml:id="formula_8">∂V 1 (φ; θ) ∂φ = dh 1 (D(c)) dφ + dh 2 (D(θ)) dφ .<label>(8)</label></formula><p>Similarly, for the dynamics of G, we have:</p><formula xml:id="formula_9">∂V 2 (θ; φ) ∂θ = dh 3 (D(θ)) dD(θ) ∂D(θ) ∂θ .<label>(9)</label></formula><p>Substituting D(x) = φx to Eqn. (8) and Eqn. (9), the dynamics of Dirac GAN can be summarized as:</p><formula xml:id="formula_10">dφ(t) dt = h 1 (φ(t)c)c + h 2 (φ(t)θ(t))θ(t), dθ(t) dt = h 3 (φ(t)θ(t))φ(t),<label>(10)</label></formula><p>where h i (·) denotes the derivative of h i (·) for i ∈ {1, 2, 3}.</p><p>From the perspective of control theory (see details in Sec. 2.1), Eqn. (10) represents a dynamic in the time domain, which is natural to understand but difficult to analyze.</p><p>Converting it to the frequency domain by the Laplace transform (LT) can simplify the analysis. It requires a case by case derivation for different GANs due to the specific forms of the objective functions (i.e., different choices of h i (·)). We will first use WGAN as an example to present the analyzing process and then generalize it to other objectives via the local linearization technique in Sec. 3.3. In WGAN 4 , we have h 1 (x) = h 3 (x) = x and h 2 (x) = −x. Let the output y(t) = (θ(t), φ(t)) and the input u(t) = c, ∀t &gt; 0. Then, the dynamic in Eqn. (8) and Eqn. (9) is instantiated as:</p><formula xml:id="formula_11">dy(t) dt = 0 1 −1 0 θ(t) φ(t) + 0 u(t) = f (y(t), u(t)).<label>(11)</label></formula><p>Applying LT F(·) in Eqn.</p><p>(2) to both sides of Eqn. <ref type="formula" target="#formula_0">(11)</ref>, the dynamic can be represented in the frequency domain as:</p><formula xml:id="formula_12">sΦ(s) = U (s) − Θ(s), sΘ(s) = Φ(s).<label>(12)</label></formula><p>where Θ, Φ, U represent θ, φ, u in the frequency domain, e.g., U (s) = F(u)(s). Then we can solve the dynamics of Φ and Θ according to the formal rules of algebra as:</p><formula xml:id="formula_13">Φ(s) = s s 2 +1 U (s), Θ(s) = 1 s Φ(s) = 1 s 2 +1 U (s).<label>(13)</label></formula><p>In the frequency domain, the output signal can be represented as a multiplication between the transfer function (see Sec. 2.1) and the input signal. Specifically, in Eqn. <ref type="formula" target="#formula_0">(13)</ref>, the transfer function of φ is T D (s) = s s 2 +1 and the transfer function of θ is T G (s) = 1 s 2 +1 . According to Proposition 1, the stability of a dynamic is fully characterized by the poles of the transfer function (i.e., the roots of the denominator). The poles of both θ and φ are ±i according to Eqn. (13). Therefore, both θ and φ are oscillatory instead of converging to the equilibrium point (θ e , φ e ) = (c, 0). The simulated dynamic of Dirac GAN is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analyzing and Improving Stability</head><p>Control theory provides extensive methods <ref type="bibr" target="#b17">(Khalil, 2002)</ref> to improve the stability of dynamics without changing the desired equilibrium. In this paper, the widely used closedloop control (CLC) is introduced in Sec. 2.3 for its simplicity. We emphasize that advanced control methods can potentially result in more stable GANs and we leave it as future work.</p><p>Before applying the CLC, we emphasize that there are two requirements to be satisfied simultaneously: 1) applying the CLC needs to stabilize the dynamics of D and G; 2) it should not change the equilibrium point of G, i.e., p G = p.</p><p>For the first requirement, the dynamic of θ in Dirac GAN is</p><formula xml:id="formula_14">dθ(t) dt = h 3 (φ(t)θ(t))φ(t)</formula><p>, which indicates that stabilizing φ to zero can also stabilize the dynamic of θ. Therefore, we only need to introduce the CLC to D. The central idea of the CLC is to adjust the transfer function by introducing an auxiliary controller. Here we adopt a simple and widely used controller T b (s) = λ. Intuitively, it is an amplifier with negative feedback from output to input according to Eqn. (4) and λ 5 is the coefficient for the amplitude of the feedback. Substituting T b with λ in Eqn. (5), the transfer function T cD of the controlled φ is given by:</p><formula xml:id="formula_15">T cD (s) = s s 2 +1 1 + λs s 2 +1 = s s 2 + λs + 1 .<label>(14)</label></formula><p>With a positive λ, all of poles in the controlled dynamic have negative real parts, and hence it is a stable dynamic. We also demonstrate the simulated results of the controlled dynamic with different values of λ in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>For the second requirement, the CLC will not change the equilibrium point of Dirac GAN. In the time domain, the CLC is equivalent to adjust the dynamics of φ as:</p><formula xml:id="formula_16">dφ dt = c − θ(t) − λφ(t).<label>(15)</label></formula><p>Since the equilibrium point of D is a zero function, i.e., φ e = 0, then we still have dy(t) dt = 0 at y = (θ e , φ e ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Extending to Other Objectives</head><p>The proposed method is not limited to WGAN but can be generalized to other GANs <ref type="bibr" target="#b12">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b23">Mao et al., 2017)</ref>, which may have nonlinear objective functions.</p><p>We leverage a standard technique called local linearization <ref type="bibr" target="#b17">(Khalil, 2002)</ref> to approximate the original dynamics as a linear one around the equilibrium point. For example, the objective function of D in the vanilla GAN is:</p><formula xml:id="formula_17">max φ V s (φ, θ) = log(σ(φc)) + log(1 − σ(φθ)). (16)</formula><p>The dynamic of φ is nonlinear because of the sigmoid func-5 λ is a hyperparameter and we analyze its sensitivity in Sec. 6.  <ref type="figure" target="#fig_0">Fig. 1</ref> and Appendix A), and those of normal GANs are stable empirically (see <ref type="figure" target="#fig_1">Fig. 2</ref>).</p><formula xml:id="formula_18">T D (s) Stability Dirac GAN/normal GAN T cD (s) Stability with CLC Dirac GAN/normal GAN WGAN s/(s 2 + 1) / 1/(s 2 + λs + 1) / Hinge-GAN s/(s 2 + 1) / 1/(s 2 + λs + 1) / SGAN 2s/(4s 2 + 2s + 1) / 1/(4s 2 + (2λ + 2)s + 1) / LSGAN s/(s 2 + 4s + 1) / 1/(s 2 + (λ + 4)s + 1) / tion, which is given by: dφ(t) dt = ∂V s (φ, θ) ∂φ | φ=φ(t),θ=θ(t) = σ (φ(t)c) σ(φ(t)c) c − σ (φ(t)θ(t)) 1 − σ(φ(t)θ(t)) θ(t),<label>(17)</label></formula><p>where σ (·) is the derivative of σ(·). Local linearization approximates the original dynamic by the first order Taylor expansion at the equilibrium point (c, 0):</p><formula xml:id="formula_19">∂V s (φ, θ) ∂φ ≈ ∂V s (φ, θ) ∂φ | φ=0,θ=c + ∂ 2 V s (φ, θ) ∂φ 2 | φ=0,θ=c φ + ∂ 2 V s (φ, θ) ∂θ∂φ | φ=0,θ=c (θ − c) = − 1 2 φ − 1 2 (θ − c).<label>(18)</label></formula><p>Note that the stability is determined by the local character of the equilibrium point, around which the residual in Eqn. <ref type="formula" target="#formula_0">(18)</ref> is negligible. Therefore, we have a linear approximation and the the analysis in Sec. 3.2 applies. We summarize the stability characters for all GANs in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Extensions to Normal GANs</head><p>In Sec. 3, we show that the dynamic of Dirac GAN can be formally analyzed and stabilized based on the recipe for control theory. Besides, the CLC can successfully stabilize nonlinear dynamics in control theory <ref type="bibr" target="#b17">(Khalil, 2002)</ref>. This two facts inspire us to stabilize the training dynamic of a normal GAN (i.e., parameterized by neural networks) by incorporating the CLC. Unlike previous methods <ref type="bibr" target="#b25">(Mescheder et al., 2018)</ref> which mainly focus on the dynamics of parameters of D and G, we instead model the dynamics of G and D in the function space, i.e., D = D(x, t) and G = G(z, t). It can simplify the analysis and build the connections between the Dirac GAN and the normal GANs.</p><p>Following the notation in Sec. 3, the objective function of a general GAN is: </p><formula xml:id="formula_20">max D V 1 (D; G) = E p(x) [h 1 (D(x))] + E p G (x) [h 2 (D(x))], max G V 2 (G; D) = E pz(z) [h 3 (D(G(z)))].<label>(19</label></formula><formula xml:id="formula_21">Sample a batch of {x r } ∼ p, {x f } ∼ p G of N samples. 5: Update B r with {x r }. Update B f with {x f }. 6: Sample a batch of x r ∼ B r , x f ∼ B f of N samples respectively. 7:</formula><p>Estimate the objective of D:</p><formula xml:id="formula_22">U(D) = 1 N [ x∈{xr} D(x) − x∈{x f } D(x)] − λ N [ x∈{x r } D 2 (x) + x∈{x f } D 2 (x)].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Update D to maximize U(D) with learning rate η.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Estimate the objective of G:</p><formula xml:id="formula_23">U(G) = 1 N x∈{x f } D(x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Update G to maximize U(G) with learning rate η. 11: until Convergence According to the calculus of variations <ref type="bibr" target="#b8">(Gelfand et al., 2000)</ref>, the gradient of V 1 (D) with respect to the function D is:</p><formula xml:id="formula_24">∂V 1 (D; G) ∂D = p dh 1 (D) dD + p G dh 2 (D) dD ,<label>(20)</label></formula><formula xml:id="formula_25">where dhi(D) dD (x) = dhi(u) du | u=D(x) = dhi(D(x)) dD(x) for i ∈ {1, 2}. The gradient of V 2 (G) with respect to G is: ∂V 2 (G) ∂G = p z dh 3 (D(G)) dG ,<label>(21)</label></formula><formula xml:id="formula_26">where dh3(D(G)) dG (z) = dh3(D(G(z))) dD(G(z)) ∂D(G(z))</formula><p>∂G(z) . Therefore, the dynamics of D and G in normal GANs can be denoted generally as:</p><formula xml:id="formula_27">dD(x, t) dt = p(x) dh 1 (D(x)) dD(x, t) + p G (x) dh 2 (D(x)) dD(x) , ∀x, dG(z, t) dt = p z (z) dh 3 (D(G(z))) dD(G(z)) ∂D(G(z)) ∂G(z) , ∀z.<label>(22)</label></formula><p>Note that the above dynamics is quiet similar to the dynamic of Dirac GAN by substituting G and D for θ and φ in Eqn. (8) and Eqn. (9) respectively. Specifically, in both dynamics, the discriminators take the weighted summation of p and p G . For the generator, both of them depend on the ∂D(G(z)) ∂G(z) . The above similarity between Dirac GANs and normal GANs inspires us to directly apply the CLC in nonlinear settings. Our empirical results in various settings (see Sec. 6) demonstrate the effectiveness of the proposed method, which agrees with the above analysis and <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementing CLC in GANs</head><p>According to Sec. 3.2, we apply the CLC with a controller T b (s) = λ to normal GANs. The resulting dynamic of D is</p><formula xml:id="formula_28">dD(x, t) dt = ∂V 1 (D; G) ∂D − λD(x), ∀x.<label>(23)</label></formula><p>Note that D will be optimized by gradient descent in the implementation and we need to design a proper objective function whose gradient flow is equivalent to Eqn. (23). Therefore, we introduce an auxiliary regularization term to the original GANs and get:</p><formula xml:id="formula_29">V 1 (D; G) = V 1 (D; G) − λ 2 x∈X D 2 (x)dx,<label>(24)</label></formula><p>where X denotes the space of x, e.g., X = [−1, 1] c×w×h for image generation of size w × h × c. Below, we denote R(D) = x∈X D 2 (x)dx, which is the squared 2-norm of the function D over the space of x. Intuitively, minimizing R(D) encourages D to converge to a zero function.</p><p>The regularization term R(D) is proportional to the expectation of D 2 with respect to a uniform distribution p u (x) defined on X , i.e., R(D) ∝ E pu(x) [D 2 (x)]. However, directly estimating R(D) is not sample efficient since most of samples in X is meaningless and do not provide useful training signals to stabilize D. Instead, we maintain two buffers B r and B f of fix size N b to store the old real samples and fake samples, respectively. We define a uniform distribution</p><formula xml:id="formula_30">p t u (x) on B t = B t r ∪ B t f to approximate R(D) as: R t (D) = x∈X p t u (x)D 2 (x)dx.<label>(25)</label></formula><p>where R t (D) denotes the regularization term at time t. R t (D) is estimated using Monte Carlo and these buffers are updated with replacement. As analyzed below, using R t (D) to approximate R(D) will not change the equilibrium and stability. The training procedure is presented in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Theoretical Analysis</head><p>Below, we first prove that the regularization term in Eqn. (25) will not change the desirable equilibrium point of GANs, i.e., p G = p, as summarized in Lemma 1. Here we follow the identical assumption as in <ref type="bibr" target="#b12">Goodfellow et al. (2014)</ref>. Further, under mild assumptions as in <ref type="bibr" target="#b25">Mescheder et al. (2018)</ref>, CLC-GAN locally converges to the equilibrium, as summarized in Theorem 1. Theorem 1. (Proof in Appendix C) Under the Assumptions 1, 2 and 3 in Appendix C with sufficient small learning rate and large λ, the parameters of CLC-GAN locally converge to the equilibrium with alternative gradient descent.</p><p>We provide the experimental results in Sec. 6 to empirical validate our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Some recent work directly models the training process of GANs. <ref type="bibr" target="#b24">Mescheder et al. (2017)</ref> and <ref type="bibr" target="#b27">Nagarajan &amp; Kolter (2017)</ref> model the dynamics of GANs in the parameter space and stabilize the training dynamics using gradient-based regularization. However, the above methods do not model the whole training dynamics explicitly and cannot generalize to natural images. Then <ref type="bibr" target="#b25">Mescheder et al. (2018)</ref> propose a prototypical example Dirac GAN to understand GANs' training dynamics and stabilize GANs using simplified gradient penalties. <ref type="bibr" target="#b9">Gidel et al. (2018)</ref> analyze the effect of momentum based on the Dirac GAN and propose the negative momentum. Though the above methods provide an elegant understanding of the training dynamics, this understanding does not provide a practically effective algorithm to stabilize nonlinear GANs' training and they fail to report competitive results to the state-of-the-art (SOTA) methods <ref type="bibr" target="#b26">(Miyato et al., 2018)</ref>. Instead, we revisits the Dirac GAN from the perspective of control theory, which provides a set of tools and extensive experience to stabilize it. Based on the recipe, we advance the previous SOTA results on image generation. <ref type="bibr" target="#b7">Feizi et al. (2017)</ref> is another related work that analyzes the stability of GANs using the Lyapunov function, which is a general approach in control theory. However, it only focuses on the stability analysis whereas cannot provide stabilizing methods. In our paper, we are interested in building SOTA GANs in practise and therefore we leverage the classical control theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We now empirically verify our method on the widelyadopted CIFAR10 <ref type="bibr" target="#b18">(Krizhevsky et al., 2009</ref>) and CelebA <ref type="bibr" target="#b22">(Liu et al., 2015)</ref> datasets. CIFAR10 consists of 50,000 natural images of size 32 × 32 and CelebA consists of 202,599 face images of size 64 × 64. The quantitative results are from the corresponding papers or reproduced on the official code for fair comparison. Specifically, we use the exactly same architectures for both D and G with our baseline methods, where the ResNet <ref type="bibr" target="#b14">(He et al., 2016)</ref> with the ReLU activation <ref type="bibr" target="#b10">(Glorot et al., 2011</ref>) is adopted 6 . The batch size is 64, and the buffer size N b is set to be 100 times of the batch size for all settings. We manually select the coefficient λ among {1, 2, 5, 10, 15, 20} in Reg-GAN's setting and among {0.05, 0.1, 0.2, 0.5} in SN-GAN's setting. We use the Inception Score (IS) <ref type="bibr" target="#b30">(Salimans et al., 2016)</ref> to evaluate the image quality on CIFAR10 and FID score <ref type="bibr" target="#b13">(Gulrajani et al., 2017)</ref> on both CIFAR10 and CelebA. More details about the experimental setting and further results on a synthetic dataset can be found in Appendix E.</p><p>We compare with two typical families of GANs. The first one is referred as unregularized GANs, including WGAN , SGAN <ref type="bibr" target="#b12">(Goodfellow et al., 2014)</ref>, LSGAN <ref type="bibr" target="#b23">(Mao et al., 2017)</ref> and Hinge-GAN <ref type="bibr" target="#b26">(Miyato et al., 2018)</ref>.The second one is referred as reguarlized GANs, including Reg-GAN <ref type="bibr" target="#b25">(Mescheder et al., 2018)</ref> and SN-GAN <ref type="bibr" target="#b26">(Miyato et al., 2018)</ref>. We emphasize that the reg-6 Our code is provided HERE.  ularzied GANs are the previous SOTA methods and our implementations are based on the officially released code. For clarity, we refer to our method as CLC-GAN(·) with the hyperparameter λ denoted in the parentheses.</p><p>In the following, we will demonstrate that (1) the CLC can stabilize GANs using less computational cost than competitive regularizations and is applicable to various objective functions ;</p><p>(2) CLC-GAN provides a consistent improvement on the quantitative results in different settings compared to related work <ref type="bibr" target="#b25">(Mescheder et al., 2018)</ref> and surpasses previous state-of-the-art (SOTA) GANs <ref type="bibr" target="#b26">(Miyato et al., 2018;</ref><ref type="bibr">Zhang et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">CLC-GAN is stable</head><p>In the linear case, the simulated results in <ref type="figure" target="#fig_0">Fig. 1</ref> demonstrate that CLC-GAN can stabilize the Dirac GAN, which agrees with our theoretical analysis in Sec. 3.2.</p><p>In normal GANs, we compare CLC-GAN with a wide range of GANs <ref type="bibr" target="#b12">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b23">Mao et al., 2017;</ref><ref type="bibr" target="#b26">Miyato et al., 2018)</ref> and their regularized version in <ref type="bibr" target="#b25">(Mescheder et al., 2018)</ref> in terms of training stability qualitatively. The learning curves are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The top panel shows the IS on CIFAR10 and the bottom one shows FID on CelebA.</p><p>In both panels, the training dynamics of unregularized GANs are not stable. On CIFAR10, the unregularized GANs all diverge from the data distribution and on CelebA they even diverge at the very beginning. Indeed, their FID results on CelebA are over 300 which is too large to be shown in the figure. Among unregularized GANs, LSGAN and SGAN are more stable than WGAN on CIFAR10 which is consistent to our analysis in <ref type="table" target="#tab_1">Table 1</ref>. However, none of them provide converged results, nor can they generalize to larger images in CelebA. We hypothesize that the nonlinearity in neural networks is the main reason for the divergence behaviour. Instead, CLC-GAN can succetssfully avoid the oscillatory behaviour and regularize GANs towards the data distribution. The robustness of CLC-GAN in the nonlinear dynamics agrees with the theoretical analysis in <ref type="table" target="#tab_1">Table 1</ref> and the experience in control theory, which are the main motivations of our paper. In conclusion, the comparison between the unregularized GANs and their controlled versions show the effectiveness of the proposed method.</p><p>Indeed, Reg-GAN can also stabilize the training dynamics. In comparison, the CLC-GANs are computationally efficient and achieve better results after convergence. First, unlike the gradient penalty which implies a non-trivial running time <ref type="bibr" target="#b19">(Kurach et al., 2018)</ref>, CLC-GANs directly regularize the activation of D and require less computational cost. For instance, our method can conduct approximate 8 iterations per second of training on CelebA whereas Reg-GAN can only conduct 4 iterations per second on Geforce 1080Ti. Second, CLC-GANs provide higher IS on CIFAR10 and lower FID on CelebA as qualitatively shown in the learning curves. The quantitative results are summarized in the following subsection. <ref type="figure" target="#fig_2">Fig. 3 &amp; Fig. 4</ref> show the generated samples. Those from CLC-GAN are semantically meaningful in all setting and are at least competitive to the ones from very strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Quantitative Results</head><p>We now present the quantitative results on CIFAR10 in the settings that include different objective functions, neural network architectures and the values of λ. The IS and FID are shown in <ref type="table" target="#tab_4">Table 3</ref> and <ref type="table" target="#tab_3">Table 2</ref> respectively. The comparisons among different settings are given within the tables.</p><p>First, our method provides a consistent improvements on both IS and FID on CIFAR10. For FID, CLC-GANs decrease it from 28 to 23 compared to Reg-GAN. For IS, CLC-GANs surpass previous SOTA GANs. Specifically, CLC-GANs achieve IS over 8.45 with various objectives without using spectral normalization, which is a significant improvement compare to related works, including SN-GAN <ref type="bibr" target="#b26">(Miyato et al., 2018)</ref> and CR-GAN <ref type="bibr">(Zhang et al., 2019)</ref>.</p><p>Second, CLC-GAN is also applicable to SN-GAN's architecture and improve its performance, whereas most gradientbased regularizations fail to introduce significant improvement <ref type="bibr" target="#b19">(Kurach et al., 2018)</ref>. Unlike SN-GAN whose performance largely depends on the objective functions, CLC-SN-GAN provides stable training dynamics consistently.</p><p>Finally, CLC-GAN is not very sensitive to the hyperparameter λ given the normalization used in D. When batch normalization is adopted, CLC-GANs with λ = 2, 5, 10 all achieve SOTA IS and a large improvement on FID. When spectral normalization <ref type="bibr" target="#b26">(Miyato et al., 2018)</ref> is used, a relatively smaller λ is required. Besides the reported results with λ = 0.1, CLC-SN-GANs with λ ∈ {0.05, 0.2} achieves IS over 8.4 consistently using Hinge loss. The underlying mechanism of the difference between the two types of normalizations is unclear. We hypothesize that it is because D is a Lipschitz-1 function with spectral normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Discussions</head><p>In this paper, we propose a novel perspective to understand the dynamics of GANs and a stabilizing method called CLC-GAN. We model the dynamics of the Dirac GAN with linear objectives theoretically in the frequency domain and extend the analysis to nonlinear objectives using local linearization. By leveraging the recipe for control theory, we propose a stabilizing method called CLC to improve Dirac GAN's stability and generalize CLC to normal GANs. The simulated results on Dirac GAN and empirical results on normal GANs demonstrate that our method can stabilize a wide range of GANs and provide better convergence results.</p><p>Although CLC-GAN provides promising results, further analyses can be done to achieve better results. On one hand, our analysis mainly focuses on the continuous cases, where the practical implementation optimizes both G and D in discrete time steps. In this case, the Z-transform is a better tool than LT used in this paper. On the other hand, we approximate the dynamics in the function space using the update in the parameter space, which can be improved by recent analyses of GANs in the function space <ref type="bibr" target="#b15">(Johnson &amp; Zhang, 2018)</ref>. Finally, modern control theory and nonlinear control methods <ref type="bibr" target="#b17">(Khalil, 2002)</ref> can potentially help GANs to achieve better performance. These are promising directions for the future work.</p><p>A. Dynamics for different GANs.</p><p>In this section, we apply the local linearization technique to Dirac GANs with various objective functions, including vanilla GAN, non-saturation GAN <ref type="bibr" target="#b12">(Goodfellow et al., 2014)</ref>, LS-GAN <ref type="bibr" target="#b23">(Mao et al., 2017)</ref> and Hinge-GAN <ref type="bibr" target="#b26">(Miyato et al., 2018)</ref>. Following the notations in the main body, the training dynamics of general Dirac GANs are given by:</p><formula xml:id="formula_31">dφ(t) dt = ∂V 1 (φ; θ) ∂φ | φ=φ(t),θ=θ(t) (26) = h 1 (φ(t)c)c − h 2 (φ(t)θ(t))θ(t), (27) dθ(t) dt = ∂V 2 (θ; φ) ∂θ | φ=φ(t),θ=θ(t) (28) = h 3 (φ(t)θ(t))φ(t).<label>(29)</label></formula><p>By applying the local linearization technique to both φ and θ around the equilibrium point (φ c , θ c ) = (0, c), the dynamic can be approximated as:</p><formula xml:id="formula_32">dφ(t) dt dθ(t) dt ≈ ∂ 2 V1(φ;θ) ∂φ 2 ∂ 2 V1(φ;θ) ∂θ∂φ ∂ 2 V2(φ;θ) ∂φ∂θ ∂ 2 V2(φ;θ) ∂θ 2 φ(t) − φ e θ(t) − θ e (30) = T φ(t) − φ e θ(t) − θ e = T φ(t) θ(t) − c ,<label>(31)</label></formula><p>and T can be denoted as:</p><formula xml:id="formula_33">T = h 1 (φc)c 2 + h 2 (φθ)θ 2 h 2 (φθ) + h 2 (φθ)θφ h 3 (φθ) + h 3 (φθ)φθ h 3 (φθ)φ 2 .</formula><p>Here h i (x) is the second order derivative of h i (x) for i ∈ {1, 2, 3}. Below we assume c = 1 and start the case by case analysis for various types of GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Vanilla GAN</head><p>In vanilla GAN, we have:</p><formula xml:id="formula_34">h 1 (x) = log(σ(x)), (32) h 2 (x) = log(1 − σ(x)), (33) h 3 (x) = − log(1 − σ(x)),<label>(34)</label></formula><p>where σ(·) denotes the sigmoid function. Then we have:</p><formula xml:id="formula_35">h 1 (x) = (1 − σ(x)), h 1 (x) = −σ(x)(1 − σ(x)), (35) h 2 (x) = −σ(x), h 2 (x) = −σ(x)(1 − σ(x)), (36) h 3 (x) = σ(x), h 3 (x) = σ(x)(1 − σ(x)).<label>(37)</label></formula><p>and for T :</p><formula xml:id="formula_36">T = − 1 2 − 1 2 1 2 0 .<label>(38)</label></formula><p>It indicates that</p><formula xml:id="formula_37">Φ(s) = − 1 2s + 1 (Θ(s) − C(s))<label>(39)</label></formula><formula xml:id="formula_38">Θ(s) = 1 2s Φ(s).<label>(40)</label></formula><p>Then we can solve the dynamics of vanilla GAN as:</p><formula xml:id="formula_39">Φ(s) = 2s 4s 2 +2s+1 C(s), Θ(s) = 1 4s 2 +2s+1 C(s).<label>(41)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Non-saturation GAN</head><p>Non-saturation GAN (NS-GAN) shares the same equilibrium point and the objective function for the discriminator. It modifies h 3 as h 3 (x) = log(σ(x)) and we have:</p><formula xml:id="formula_40">h 3 (x) = (1 − σ(x)), h 3 (x) = −σ(x)(1 − σ(x)). (42)</formula><p>By substituting the above equation to T , the dynamic of NS-GAN is equivalent to vanilla GAN and therefore shares the same transfer function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Hinge GAN</head><p>For Hinge GAN, we have:</p><formula xml:id="formula_41">h 1 (x) = min{−1 + x, 0}, (43) h 2 (x) = min{−1 − x, 0}, (44) h 3 (x) = x.<label>(45)</label></formula><p>Then we have:</p><formula xml:id="formula_42">h 1 (x) = 1, h 1 (x) = 0, (46) h 2 (x) = −1, h 2 (x) = 0,<label>(47)</label></formula><formula xml:id="formula_43">h 3 (x) = 1.h 3 (x) = 0.<label>(48)</label></formula><p>Therefore the Hinge GAN actually shares the same dynamics as WGAN around the equilibrium point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Least Square GAN</head><p>The objective function of least square GAN (LS-GAN) is:</p><formula xml:id="formula_44">V 1 (φ; θ) = −(φc − 1) 2 − (φθ) 2 ,<label>(49)</label></formula><formula xml:id="formula_45">V 2 (θ; φ) = −(φθ) 2 .<label>(50)</label></formula><p>In this case, there's no equilibrium point. We modify the discriminator as D(x) = φx + 0.5 which is equilvalent to convert the objective functions as follows:</p><formula xml:id="formula_46">h 1 (x) = −(x − 0.5) 2 ,<label>(51)</label></formula><formula xml:id="formula_47">h 2 (x) = −(x + 0.5) 2 ,<label>(52)</label></formula><formula xml:id="formula_48">h 3 (x) = −(x − 0.5) 2 ,<label>(53)</label></formula><p>and therefore we have:</p><formula xml:id="formula_49">h 1 (x) = −2(x − 0.5), h 1 (x) = −2, (54) h 2 (x) = −2(x + 0.5), h 2 (x) = −2, (55) h 3 (x) = −2(x − 0.5), h 3 (x) = −2.<label>(56)</label></formula><p>Then the T can be denoted as:</p><formula xml:id="formula_50">T = −4 −1 1 0 .<label>(57)</label></formula><p>We have:</p><formula xml:id="formula_51">Φ(s) = − 1 s + 4 (Θ(s) − C(s)),<label>(58)</label></formula><formula xml:id="formula_52">Θ(s) = 1 s Φ(s).<label>(59)</label></formula><p>Then we can solve the dynamics of LSGAN as:</p><formula xml:id="formula_53">Φ(s) = s s 2 +4s+1 C(s), Θ(s) = 1 s 2 +4s+1 C(s).<label>(60)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamics with Lipschitz Continuity</head><p>In this section, we prove that around the equilibrium, the dynamics of regularized D with Lipschitz constraint is equivalent to the unregularized D as in Eqn. <ref type="bibr">(20)</ref>. With the dynamics defined by the corresponding gradient flow, we only need to prove that updating D according to Eqn. (20) will not violate the Lipschitz constraints, at least locally around the equilibrium. Here we make the following assumptions:</p><p>1. Both p(x) and p G (t, x) are C 1 -smooth: dp(x) dx and dp G (t,x) dx exists and is continuous ∀ t.</p><p>2. q(x) → 0 and dq(x) dx → 0 when x → 0 for q ∈ {p, p G }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">There exists an</head><formula xml:id="formula_54">M such that | dq(x) dx | 2 &lt; M for q ∈ {p, p G }.</formula><p>The above assumptions are satisfied for most probability density functions.</p><p>The distance in the function space is defined as d(p 1 , p 2 ) = sup x∈R n |p 1 (x) − p 2 (x)| which always exists because of the 2-nd conditions above. We define Ω L = {p(x)|p(x) ∈ C 1 , | dp(x) dx | 2 &lt; L ∀x.} and B( ) = {p(x)|p(x) ∈ C 1 , sup x |p(x)| &lt; }. Then we have the follow proposition:</p><formula xml:id="formula_55">Proposition 1. There exists η &gt; 0, such that ∀D(x) ∈ Ω 0.5 , we have D(x) + η(p(x) − p G (x)) ∈ Ω 1 . Proof. By denoting D (x) = D(x) + η(p(x) − p G (x)), We have: d(D(x) + η(p(x) − p G (x))) dx (61) = dD(x) dx + η( p(x) dx − p G (x) dx ).</formula><p>Therefore, we have</p><formula xml:id="formula_56">| d(D(x) + η(p(x) − p G (x))) dx | 2 (62) ≤| dD(x) dx | 2 + η(| p(x) dx | 2 + | p G (x) dx )| 2 (63) ≤0.5 + η(M + M ).<label>(64)</label></formula><p>By letting η = 1 4M , we have | d(D ) dx | 2 ≤ 0.75. Therefore we have D (x) ∈ Ω 1 .</p><p>The above proposition indicates that when D(x) is sufficient close to the equilibrium and the learning rate is sufficient small, then the dynamics of D still follows Eqn. (11) for Dirac <ref type="bibr">GAN and Eqn. (22)</ref> for normal GANs. The simulated results of Dirac GAN in <ref type="figure" target="#fig_0">Fig. 1</ref> and the bad performance of SN-GAN with WGAN's objective in Sec. 6.2 agree with this argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Theoretical Analysis of CLC-GAN C.1. Proof of Lemma 1</head><p>Under the non-parametric setting following <ref type="bibr" target="#b12">Goodfellow et al. (2014)</ref>, the equilibrium of GAN's minimax problem is achieved when p G = p and D(x) = 0 for all x. Besides, for the regularization term introduced by CLC-GAN:</p><formula xml:id="formula_57">R t (D) = x∈X p t u (x)D 2 (x)dx,<label>(65)</label></formula><p>it also achieves optimum when D(x) = 0 for all x. Therefore, regularizing D's training dynamic with Eqn. (65) will not change the equilibrium of GANs. This argument for other variants of GANs remains the same under the condition that the equilibrium of unregularized GANs' minimax problem is achieved when D(x) = 0 for all x around the data distribution. This assumption is meet by most variants of GANs <ref type="bibr" target="#b23">Mao et al., 2017;</ref><ref type="bibr" target="#b26">Miyato et al., 2018;</ref><ref type="bibr">Zhang et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Proof of Theorem 1</head><p>In this subsection, we provide the proof of Theorem 1, whose proof procedure mainly follows <ref type="bibr" target="#b25">Mescheder et al. (2018)</ref>. We first denote that G θ is the generator parameterized by θ ∈ R |θ| and D φ is the discriminator parameterized by φ ∈ R |φ| . Before going to the proof of Theorem 1, we first provide the assumptions we made, which are similar to <ref type="bibr" target="#b25">(Mescheder et al., 2018)</ref>.</p><p>We first assume the data distribution can be captured by the generator G, which is identical to the Assumption I in <ref type="bibr" target="#b25">(Mescheder et al., 2018)</ref> as:</p><p>Assumption 1. There exists a discriminator D * parameterized by φ * and a generator G * parameterized by θ * , such that p G * (x) = p(x) for all x and D * (x) = 0 in some local neighbourhood of the support of the data distribution p(x).</p><p>Besides, we define</p><formula xml:id="formula_58">h(φ) = E p(x) [D 2 φ (x)],<label>(66)</label></formula><p>Then we define a manifold over the parameter space of φ and θ as follows:</p><formula xml:id="formula_59">M θ = {θ|p G θ (x) = p(x), ∀x},<label>(67)</label></formula><formula xml:id="formula_60">M φ = {φ|h(φ) = 0}.<label>(68)</label></formula><p>Here we use G θ to denote the generator parameterized by θ.</p><p>To state the second assumption, we need</p><formula xml:id="formula_61">g(θ) = E p G θ (x) [∂ φ D φ (x)| φ=φ * ].<label>(69)</label></formula><p>Then we provide the second assumption by follow <ref type="bibr" target="#b25">Mescheder et al. (2018)</ref> as follows:</p><p>Assumption 2. There are -balls around B (φ * ) and B (θ * ) around φ * and θ * , such that M φ ∩ B (φ * ) and M θ ∩ B (θ * ) define C 1 -manifolds. Moreover, the following conditions hold:</p><formula xml:id="formula_62">• if v ∈ R |φ| is not in the tangent space of M φ at φ * , then we have ∂ 2 v h(φ * ) = 0. • if w ∈ R |θ| is not in the tangent space of M θ at θ * ,</formula><p>then we have ∂ w g(θ * ) = 0.</p><p>The validness of GAN's training dynamics requires the following assumption:</p><p>Assumption 3. The functions h 1 , h 2 , h 3 requires the following conditions:</p><p>• h 1 (0) &gt; 0, h 2 (0) &lt; 0 and h 3 (0) &gt; 0.</p><p>• |h 1 (0)| = |h 2 (0)| = |h 3 (0)|.</p><p>Here | · | denotes the absolute value.</p><p>The formal statement of Theorem 1 is given as follows:</p><p>Theorem 1. Assume the assumptions 1, 2 and 3 hold for φ * and θ * . For small enough learning rate and λ &gt; −h 1 (0) − h 2 (0), λ &gt; 0, training GANs with objectives formulated in Eqn. (19) and the regularization term in Eqn. (25) ensures locally convergence with alternative gradient descent.</p><p>Proof. The gradient flow defined by GAN's objective function is given as:</p><formula xml:id="formula_63">v(φ, θ) = ∇ φ V 1 (D; G) ∇ θ V 2 (G; D) .<label>(70)</label></formula><p>Here we have:</p><formula xml:id="formula_64">∇ φ V 1 (D; G) = E p(x) [h 1 (D(x)) ∂D(x) ∂φ ] (71) +E p G (x) [h 2 (D(x)) ∂D(x) ∂φ ],<label>(72)</label></formula><p>and</p><formula xml:id="formula_65">∇ θ V 2 (G; D) = E pz(z) h 3 (D(G(z)))) ∂D(G(z)) ∂G(z) ∂G(z) ∂θ .<label>(73)</label></formula><p>Then the Jacobian matrix of the gradient flow is:</p><formula xml:id="formula_66">J U (φ, θ) = ∇ 2 φ V 1 (D; G) ∇ 2 θφ V 1 (D; G) ∇ 2 φθ V 2 (G; D) ∇ 2 θ V 2 (G; D) (74) = J DD (φ, θ) J GD (φ, θ) J DG (φ, θ) J GG (φ, θ) .<label>(75)</label></formula><p>Note that at the equilibrium point (φ * , θ * ), we have D * (x) = 0 around the support the data distribution. Therefore, we have ∂D(x) ∂x = 0 and ∂ 2 D(x) ∂x 2 = 0 for x ∼ p(x). It is easy to verify that J GG (φ * , θ * ), i.e., ∇ 2 θ V 2 (G * ; D * ), is a zero matrix. Similar, we have:</p><formula xml:id="formula_67">J GD (φ * , θ * ) = ∇ θ (E p G (x) [h 2 (D(x)) ∂D(x) ∂φ ]) = ∇ θ (E pz(z) [h 2 (D(G(z))) ∂D(G(z)) ∂φ ]) = E pz(z) [h 2 (0) ∂ 2 D(G(z)) ∂θ∂φ ] = E pz(z) [h 2 (0) ∂ 2 D(G(z)) ∂G(z)∂φ ∂G(z) ∂θ ]<label>(76)</label></formula><p>and</p><formula xml:id="formula_68">J DG (φ * , θ * ) = ∇ φ E pz(z) h 3 (D(G(z)))) ∂D(G(z)) ∂G(z) ∂G(z) ∂θ = E pz(z) h 3 (D(G(z)))) ∂ 2 D(G(z)) ∂φ∂G(z) ∂G(z) ∂θ = E pz(z) h 3 (0) ∂ 2 D(G(z)) ∂φ∂G(z) ∂G(z) ∂θ<label>(77)</label></formula><p>Since h 2 (0) = −h 3 (0), we have J DG = −J T GD . Note that with sufficient small learning rate, we have p t u (x) = p G (x) = p(x) around the equilibrium. Then we provide the gradient flow and it's Jacobian matrix of the regularization term</p><formula xml:id="formula_69">V R (φ, θ) = ∇ φ − R t (D) ∇ θ − R t (D) .<label>(78)</label></formula><p>Since R t (D) is simply a function of φ, ∇ θ R t (D) is a zero vector. The Jacobian matrix of the regularization term is given as:</p><formula xml:id="formula_70">J R (φ, θ) = ∇ 2 φ − R t (D) 0 0 0 .<label>(79)</label></formula><p>With the Jacobian matrix of the regularized dynamics formulated as:</p><formula xml:id="formula_71">J = J U + J R = J DD − ∇ 2 φ R t (D) J DG J GD 0 ,<label>(80)</label></formula><p>We can directly follow the proof of Theorem 4.1 in <ref type="bibr" target="#b25">Mescheder et al. (2018)</ref> in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Interpreting CLC-GAN in the parameter space</head><p>In this paper, we mainly analyze our proposed method in the function space, including dynamic analysis and controller designing. Instead, our proposed method can also be interpreted as certain regularization terms on the Jacobian matrix of the training dynamics. Below we provide a formal demonstration.</p><p>First, we denote the equilibrium of G and D as (θ * , φ * ), where p G (x; θ * ) = p(x) and D(x; φ * ) = 0 for all x. Note that φ * is also a global minimum point of the regularization term R(D) = D 2 (x)dx. Then we have ∂ 2 R(D) ∂φ 2 0.</p><p>We denote U (D, G) as the objective function of the minimax optimization problem in WGAN without CLC regularization. Then the Jacobian matrix of the training dynamic can be denoted as:</p><formula xml:id="formula_72">J = ∂ 2 U (D,G) ∂φ 2 ∂ 2 U (D,G) ∂φ∂θ ∂ 2 U (D,G) ∂θ∂φ ∂ 2 U (D,G) ∂θ 2 .<label>(81)</label></formula><p>Because of the linearity of the derivation operation, the training dynamics of the WGAN with CLC regularization is denoted as:</p><formula xml:id="formula_73">J = J − J L = J − ∂ 2 L(D) ∂φ 2 0 0 0 ,<label>(82)</label></formula><p>where we abuse the 0 to denote the zero matrix with certain size to match the size of J. Since ∂ 2 R(D) ∂φ 2 0, we have −J L 0. Therefore, the CLC regularization introduces a negative semi-definite matrix to the original Jacobian matrix, which is helpful to stabilize the training dynamics of GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Understanding Existing Work as Closed-loop Control</head><p>A side contribution of this paper is to understand existing methods <ref type="bibr" target="#b9">(Gidel et al., 2018)</ref> uniformly as certain CLC controllers. The momentum is an example where <ref type="bibr" target="#b9">Gidel et al. (2018)</ref> provide some theoretical analysis of momentum in training GANs. Here we re-analyze the momentum using Dirac GAN under the perspective of control theory.</p><p>The momentum method <ref type="bibr" target="#b28">(Qian, 1999)</ref> is powerful when training neural networks, whose theoretical formulation is given by:</p><formula xml:id="formula_74">φ t+1 = βφ t + (1 − β)∇φ t , φ t+1 = φ t + ηφ t+1 ,<label>(83)</label></formula><p>where ∇φ is the input of φ's dynamic, i.e., u D = c − θ.</p><p>The β is the coefficient for the exponential decay. However, momentum instead is not helpful when training GANs <ref type="bibr" target="#b29">(Radford et al., 2015;</ref><ref type="bibr" target="#b25">Mescheder et al., 2018;</ref><ref type="bibr" target="#b2">Brock et al., 2018;</ref><ref type="bibr" target="#b13">Gulrajani et al., 2017)</ref> where smaller β or even zero is recommended to achieve better performance.</p><p>In control theory, the momentum is equivalent to adding an exponential decay to the input of the dynamics :h</p><formula xml:id="formula_75">(t) = t 0 h(u) exp(−τ (t − u))du.<label>(84)</label></formula><p>The LT of an exponential decay dynamic is 1 s+τ , i.e., H(s) = 1 s+τ H(s). τ &gt; 0 denotes the decay coefficient which depends on β. Therefore, we can formulate the dynamics of Dirac GAN in the following:   With a positive τ , there is at least one pole of this dynamic whose real part is larger than 0, indicating the instability of the dynamics for GANs with momentum. The result is consistent with <ref type="bibr" target="#b9">(Gidel et al., 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Further Experimental Results on Synthetic Data</head><p>In this section, we evaluate our proposed method on a mixture of Gaussian on the two dimensions. The data distribution consists of 8 2D isotropic Gaussian distributions arranged in a ring, where the radius of the ring is 1, and the deviation of each component Gaussian distribution is 0.05. For the coefficient λ, we follow the setting in the spectral normalization as λ ∈ {0.01, 0.05, 0.1}. We adopt two-layer MLPs for both the generator and the discriminator which consist of 128 − 512 units. The batch size is is 512.</p><p>The generated results are illustrated in <ref type="figure">Fig. 5</ref> and we further provide the dynamics of the generator distribution in <ref type="figure">Fig. 6</ref>. As we can see, the unregularized WGAN and SGAN suffer from severe model collapse problem and cannot cover the whole data distribution. Besides, the oscillation can be observed during the training process of WGAN: the generator distribution oscillates among the modes of data distribution. Our method can successfully cover all modes compared to the WGAN and SGAN and the dynamics are converged instead of oscillation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The simulated dynamic of Dirac GAN for θ (left) and φ (right) with c = 1. The curve of WGAN shows the oscillation while Other curves of CLC-GAN show that the closed loop control helps convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The learning curve of the baselines and our proposed method. Top: The Inception Score of CIFAR10. Bottom: The FID score of CelebA. We plot the curves with respect to the time for better representation of the computational cost.Lemma 1. Under the non-parametric setting, CLC-GAN has the same equilibrium as the original GAN, i.e, p G = p and D(x) = 0 for all x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The generated results of CIFAR10 dataset. From top left to bottom right: WGAN-GP, Reg-WGAN, CLC-WGAN(5), CLC-SGAN(5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The generated results of CelebA dataset. From top left to bottom right: WGAN-GP, Reg-WGAN, CLC-WGAN(15), CLC-SGAN(15).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>u) − θ(u)) exp(−τ (t − u))du, dφ dt = m φ (t),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>applying LT, we have M φ (s) = 1 s+τ (C(s) − Θ(s)) and Φ can be represented as: Φ(s) = s s 3 + τ s 2 + 1 C(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The stability characters for the widely-used GANs. Please refer to Appendix A for detailed derivation, which adopts the local linearization technique introduced in Sec. 3.3. With CLC, the training dynamics of Dirac GANs are stable theoretically (see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Buffer size N b , feedback coefficient λ, batch size N , initialized G and D, learning rate η. 2: Initialize B r and B f for real samples and fake samples respectively.</figDesc><table><row><cell>Algorithm 1 Cloosed-loop Control GAN</cell></row><row><cell>1: Input:</cell></row><row><cell>)</cell></row></table><note>3: repeat4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The FID Score on CIFAR10. The results reported here are the best results over the training process and are averaged over 3 runs.</figDesc><table><row><cell>Method</cell><cell>WGAN</cell><cell>SGAN</cell></row><row><cell>No Regularization</cell><cell>105.21</cell><cell>28.51</cell></row><row><cell>Reg-GAN</cell><cell>30.43</cell><cell>28.39</cell></row><row><cell cols="3">Gradient Penalty CLC-GAN(2) CLC-GAN(5) CLC-GAN(10) 21.14 ± 1.84 22.20 ± 2.07 28.20 − 23.53 ± 1.22 21.63 ± 0.47 21.46 ± 1.57 21.52 ± 0.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The Inception score on CIFAR10. †<ref type="bibr" target="#b32">(Yang et al., 2017)</ref>, ‡<ref type="bibr" target="#b26">(Miyato et al., 2018)</ref>, §(Zhang et al., 2019). Results of CLC-GAN are averages over 3 runs. ) 8.42 ± .06 8.28 ± .05 8.49 ± .08 CLC-GAN(5) 8.49 ± .07 8.44 ± .08 8.54 ± .03 CLC-GAN(10) 8.38 ± .10 8.47 ± .09 8.46 ± .00</figDesc><table><row><cell>Method</cell><cell>WGAN</cell><cell>SGAN</cell><cell>Hinge</cell></row><row><cell>LR-GAN  †</cell><cell>-</cell><cell>7.17</cell><cell>-</cell></row><row><cell>SN-GAN  ‡</cell><cell>-</cell><cell>-</cell><cell>8.22</cell></row><row><cell>CR-GAN  §</cell><cell>-</cell><cell>8.40</cell><cell>-</cell></row><row><cell>Gradient Penalty</cell><cell>7.82</cell><cell>-</cell><cell>-</cell></row><row><cell>Reg-GAN</cell><cell>7.34</cell><cell>7.37</cell><cell>7.37</cell></row><row><cell>CLC-GAN(2SN-GAN</cell><cell>3.29</cell><cell>8.17</cell><cell>8.28</cell></row><row><cell>CLC-SN-GAN(0.1)</cell><cell cols="3">8.14 ± .02 8.30 ± .09 8.54 ± .03</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This definition is consistent with existing work in<ref type="bibr" target="#b24">Mescheder et al. (2017)</ref> and<ref type="bibr" target="#b25">Mescheder et al. (2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We ignore the Lipschitz continuity of D for simplicity but the equilibrium point and its local convergence do not change. See theoretical analysis and empirical evidence in Appendix B.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A pid controller approach for stochastic optimization of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8522" to="8531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sgan: An alternative training of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chavdarova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9407" to="9415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02544</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning implicit generative models by teaching explicit ones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03870</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Understanding gans: the lqg setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Farnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ginart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10793</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Calculus of variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Courier Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Negative momentum for improved game dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Hemmat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lepriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04740</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Composite functional gradient learning of generative adversarial models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06309</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="volume">156</biblScope>
			<pubPlace>Prentice-Hall Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Nonlinear systems. Upper Saddle River</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Khalil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04720</idno>
		<title level="m">A large-scale study on regularization and normalization in gans</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Triple generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4088" to="4098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Generative adversarial network training is a continual learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11083</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The numerics of gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1825" to="1835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04406</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient descent gan optimization is locally stable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5585" to="5595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Laplace transform (PMS-6). Princeton university press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Widder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lr-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01560</idno>
		<title level="m">Layered recursive generative adversarial networks for image generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12027,2019.WGANSGANReg-WGANCLC-WGAN(0.1)CLC-WGAN(0.01</idno>
		<title level="m">Consistency regularization for generative adversarial networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The generated samples for mixture of gaussian distribution. The red points demonstrate the location of data distribution and the blue points are generated samples. Each distribution is plotted using kernel density estimation with 50</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>000 samples</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wgan-Gp Figure</surname></persName>
		</author>
		<title level="m">The training dynamics of various GANs on synthetic data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
