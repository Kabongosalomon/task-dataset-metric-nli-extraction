<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatio-Temporal Covariance Descriptors for Action and Gesture Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Sanin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conrad</forename><surname>Sanderson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NICTA</orgName>
								<address>
									<postBox>PO Box 6020</postBox>
									<postCode>4067</postCode>
									<settlement>St Lucia</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of ITEE</orgName>
								<orgName type="institution">University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatio-Temporal Covariance Descriptors for Action and Gesture Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/WACV.2013.6475006</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new action and gesture recognition method based on spatio-temporal covariance descriptors and a weighted Riemannian locality preserving projection approach that takes into account the curved space formed by the descriptors. The weighted projection is then exploited during boosting to create a final multiclass classification algorithm that employs the most useful spatio-temporal regions. We also show how the descriptors can be computed quickly through the use of integral video representations. Experiments on the UCF sport, CK+ facial expression and Cambridge hand gesture datasets indicate superior performance of the proposed method compared to several recent state-of-the-art techniques. The proposed method is robust and does not require additional processing of the videos, such as foreground detection, interest-point detection or tracking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video-based classification plays a key role in human motion analysis fields such as action and gesture recognition. Both fields have shown promising applications in many areas, including security and surveillance, contentbased video analysis, human-computer interaction and animation. According to a recent survey on recognition of human activities <ref type="bibr" target="#b27">[28]</ref>, the focus has shifted to methods that do not rely on human body models, where the information is extracted directly from the images and hence being less dependent on reliable segmentation and tracking algorithms. Such image representation methods can be categorised into global and local based approaches <ref type="bibr" target="#b21">[22]</ref>.</p><p>Methods with global image representation encode visual information as a whole. Ali and Shah <ref type="bibr" target="#b0">[1]</ref> extract a series of kinematic features based on optical flow. A group of kinematic modes is found using principal component analysis. Guo et al. <ref type="bibr" target="#b5">[6]</ref> encode the same kinematic features using sparse representation of covariance matrices. Several methods first divide the region of interest into a fixed spatial or temporal grid, extract features inside each cell and then combine them into a global representation. For example, this can be achieved using local binary patterns (LBP) <ref type="bibr" target="#b10">[11]</ref>, or histograms of oriented gradients (HOG) <ref type="bibr" target="#b26">[27]</ref>. Global representations are sensitive to viewpoint, noise and occlusions which may lead to unreliable classification. Furthermore, global representations depend on reliable localisation of the region of interest <ref type="bibr" target="#b21">[22]</ref>.</p><p>Local representations are designed to deal with the abovementioned issues by describing the visual information as a collection of patches, usually at the cost of increased computation. Laptev and Lindeberg <ref type="bibr" target="#b13">[14]</ref> extract interest points using a 3D Harris corner detector and use the points for modelling the actions. One of the major drawbacks is the low number of interest points that are able to remain stable across an image sequence. A common solution is to work with windowed data, extracting salient regions which can be represented using Gabor filtering <ref type="bibr" target="#b3">[4]</ref>.</p><p>Wang et al. <ref type="bibr" target="#b30">[31]</ref> showed that dense sampling approaches tend to perform better compared to interest point based approaches. Dense sampling is typically done for a set of patches inside the region of interest. Features are extracted from each patch to form a descriptor. These descriptor representations differ from grid-based global representations in that they can have an arbitrary position and size, and that the patches are not combined to form a single representation but form a set of multiple representations. Examples are HOG and HOF (histogram of oriented flow) descriptors <ref type="bibr" target="#b14">[15]</ref>, SIFT descriptors <ref type="bibr" target="#b16">[17]</ref>, and their respective spatiotemporal versions, HOG3D <ref type="bibr" target="#b30">[31]</ref> and 3D SIFT <ref type="bibr" target="#b25">[26]</ref>. Because of the likely large number of descriptors and/or their high dimensionality, comparing sets of descriptors is often not straightforward. This has led to compressed representations such as formulating sets of descriptors as bags-ofwords <ref type="bibr" target="#b20">[21]</ref>.</p><p>In this paper we propose the use of spatio-temporal covariance descriptors for action and gesture recognition tasks. Flat region covariance descriptors were first proposed for the task of object detection and classification in images <ref type="bibr" target="#b28">[29]</ref>. Each covariance descriptor represents the features inside an image region as a normalised covariance matrix. They have led to improved results over related descriptors such as HOG, in terms of detection performance as well as robustness to translation and scale <ref type="bibr" target="#b28">[29]</ref>. Furthermore, covariance matrices provide a low dimensional representation which enables efficient comparison between sets of covariance descriptors.</p><p>The proposed spatio-temporal descriptors, which we name Cov3D, belong to the group of symmetric positive definite matrices which do not form a vector space. They can be formulated as a connected Riemannian manifold, and taking into account the non-linear nature of the space of the descriptors may lead to improved classification results. The most common approach for classification on manifolds is to first map the points into an appropriate Euclidean representation <ref type="bibr" target="#b15">[16]</ref> and then use traditional machine learning methods. A recent example of mapping is the Riemannian locality preserving projection (RLPP) technique <ref type="bibr" target="#b7">[8]</ref>.</p><p>The Cov3D descriptors are extracted from spatiotemporal windows inside sample videos, with the number of possible windows being very large. As such, we use a boosting approach to search the windows to find a subset which is the most useful for classification. We propose to extend RLPP by weighting (WRLPP), in order to take into account the weights of the training samples. This weighted projection leads to a better representation of the neighbourhoods around the most critical training samples during each boosting iteration. The proposed Cov3D descriptors, in conjunction with the classification approach based on WRLPP boosting, lead to a state-of-the-art method for action and gesture recognition.</p><p>We continue the paper as follows. In Section 2 we describe the spatio-temporal covariance descriptors, and use the concept of integral video to enable fast calculation inside any spatio-temporal window. In Section 3, we first overview the concept of Riemannian manifolds formulated in the context of positive definite symmetric matrices, and then detail the proposed boosting classification approach based on weighted Riemannian locality preserving projection. In Section 4, we compare the performance of the proposed method against several recent state-of-the-art methods on three benchmark datasets. Concluding remarks and possible future directions are given in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Cov3D Descriptors</head><p>In this section we first present the general form of the proposed spatio-temporal covariance descriptors (Cov3D), an algorithm for their fast calculation, and finally how they can be specialised for action and gesture recognition. For convenience, we follow the notation in <ref type="bibr" target="#b28">[29]</ref>.</p><p>Let V be the sequence of images {It} T t=1 and F be the</p><formula xml:id="formula_0">W × H × T × d dimensional feature video extracted from V : F (x, y, t) = Φ(V, x, y, t)<label>(1)</label></formula><p>where the function Φ can be any mapping such as intensity, colour, gradients, or optical flow. For a given spatio-</p><formula xml:id="formula_1">temporal window R ⊂ F , let {z i } S i=1 be the d-dimensional feature vectors inside R.</formula><p>The region R is represented with the d × d covariance matrix of the feature vectors:</p><formula xml:id="formula_2">Cov3D R = 1 S − 1 S i=1 (z i − µ)(z i − µ) T<label>(2)</label></formula><p>where µ is the mean of the points. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the construction of a covariance descriptor inside a spatio-temporal window. Examples of feature vectors specific for action and gesture recognition are given in Section 2.2.</p><p>Representing a spatio-temporal window with a covariance matrix has several advantages: (i) it is a lowdimensional representation which is independent on the size of the window, (ii) the impact of noisy samples is reduced through the averaging during covariance computation, (iii) it is a straightforward method of fusing correlated features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fast computation</head><p>Integral images are an intermediate image representation used for the fast calculation of region sums <ref type="bibr" target="#b29">[30]</ref>. The concept has been extended to image sequences <ref type="bibr" target="#b9">[10]</ref>, where the integral images are stacked to form an integral video, and can be used to compute spatio-temporal region sums in constant time. For a video V , its integral video IV is defined as:</p><formula xml:id="formula_3">IV (x , y , t ) = x≤x y≤y t≤t V (x, y, t)<label>(3)</label></formula><p>Tuzel et al. <ref type="bibr" target="#b28">[29]</ref> used the integral image representations for fast calculation of flat region covariances. Here we extend the idea for fast calculation of covariance matrices inside a spatio-temporal window using the integral video representation. The (i, j)-th element of the covariance matrix defined in (2) can be expressed as:</p><formula xml:id="formula_4">Cov3D R (i, j) = 1 S-1 S k=1 z k (i)z k (j) − 1 S S k=1 z k (i) S k=1 z k (j) (4)</formula><p>where z k (i) refers to the i-th element of the k-th vector. To find the covariance in a given spatio-temporal window R, we have to compute the sum of each feature dimension, z(i) d i=1 , as well as the sum of the multiplication of any two feature dimensions, z(i)z(j) i,j=1...d . With d representing the number of dimensions, the covariance of any spatio-temporal window can be computed in O(d 2 ) time, as follows.</p><p>We need to compute a total of d + d 2 integral videos. Let P be the W × H × T × d tensor of the integral videos:</p><formula xml:id="formula_5">P (x , y , t , i) = x≤x y≤y t≤t F (x, y, t)(i) (5) where F (x, y, t)(i) is the i-th element of vector F (x, y, t). Furthermore, let Q be the W × H × T × d × d</formula><p>tensor of the second-order integral videos:</p><formula xml:id="formula_6">Q(x , y , t , i, j) = x≤x y≤y t≤t F (x, y, t)(i) · F (x, y, t)(j) (6) for i, j = 1, . . . , d.</formula><p>The complexity of calculating the tensors is O(d 2 W HT ). The d-dimensional feature vector p x,y,t and the d × d dimensional matrix Q x,y,t can be obtained from the above tensors using: <ref type="figure">Fig. 2</ref>. The covariance of the spatio-temporal window bounded by (0, 0, 0) and (x, y, t) is:</p><formula xml:id="formula_7">p x,y,t = [ P (x, y, t, 1), . . . , P (x, y, t, d) ] T (7) Q x,y,t =    Q(x, y, t, 1, 1) · · · Q(x, y, t, 1, d) . . . . . . . . . Q(x, y, t, d, 1) · · · Q(x, y, t, d, d)    (8) Let R(x 1 , y 1 , t 1 ; x 2 , y 2 , t 2 ) be the spatio-temporal window of points {(x, y, t)|x 1 ≤ x ≤ x 2 , y 1 ≤ y ≤ y 2 , t 1 ≤ t ≤ t 2 }, as shown in</formula><formula xml:id="formula_8">Cov3D R(0,0,0; x,y,t) = 1 S − 1 Q x,y,t − 1 S p x,y,t p T x,y,t<label>(9)</label></formula><p>where S = x · y · t. Similarly, after a few rearrangements, the covariance of the region R(x 1 , y 1 , t 1 ; x 2 , y 2 , t 2 ) can be computed as:</p><formula xml:id="formula_9">Cov3D R(x 1 ,y 1 ,t 1 ; x 2 ,y 2 ,t 2 ) = 1 S − 1 Q x 2 ,y 2 + Q x 1 −1,y 1 −1 − Q x 2 ,y 1 −1 − Q x 1 −1,y 2 − 1 S p x 2 ,y 2 + p x 1 −1,y 1 −1 − p x 2 ,y 1 −1 − p x 1 −1,y 2 p x 2 ,y 2 + p x 1 −1,y 1 −1 − p x 2 ,y 1 −1 − p x 1 −1,y 2 T<label>(10)</label></formula><p>where</p><formula xml:id="formula_10">p x,y = p x,y,t 2 − p x,y,t 1 (11) Q x,y = Q x,y,t 2 − Q x,y,t 1<label>(12)</label></formula><p>and S = (</p><formula xml:id="formula_11">x 2 − x 1 + 1) · (y 2 − y 1 + 1) · (t 2 − t 1 + 1)</formula><p>.  <ref type="figure">Figure 2</ref>. Integral feature video. The spatio-temporal window R is bounded by (x 1 , y 1 , t 1 ) and (x 2 , y 2 , t 2 ). Each point in R is a d dimensional vector, where d is the number of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Features and regions</head><p>Commonly used features for action and gesture recognition include intensity gradients and optical flow. Previous studies have shown the benefit of combining both types of features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>. We define the feature mapping Φ, present in (1), as the following combination of gradient and opticalflow based features, extracted from pixel location (x, y, t):</p><formula xml:id="formula_12">Φ(V, x, y, t) = [ x y t g o ] T<label>(13)</label></formula><p>where g = |Ix| |Iy| |Ixx| |Iyy|</p><formula xml:id="formula_13">I 2 x + I 2 y arctan |Iy| |Ix| (14) o = u v ∂u ∂t ∂v ∂t ∂u ∂x + ∂v ∂y ∂v ∂x − ∂u ∂y<label>(15)</label></formula><p>The first four gradient based features in <ref type="formula" target="#formula_0">(14)</ref> represent the first and second order intensity gradients at pixel location (x, y). The last two gradient based features correspond to the gradient magnitude and gradient orientation. The optical-flow based features in <ref type="bibr" target="#b14">(15)</ref> represent, in order: the horizontal and vertical components of the flow vector, the first order derivatives of the flow components with respect to t, and the spatial divergence and vorticity of the flow field as defined in <ref type="bibr" target="#b0">[1]</ref>. Each descriptor is hence a 15 × 15 matrix, as Φ(V, x, y, t) has 15 dimensions.</p><p>For reliable recognition, several regions (and hence several descriptors) are typically used. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the spatiotemporal windows of two descriptors which can be used for recognition of face expressions. With the defined mapping, the input video V is mapped to F , a 15-dimensional feature video. Since the cardinality of the set of spatio-temporal windows {R ⊂ F } is very large, we only consider windows of a minimum size and increment their location and size by a minimum interval value. Further specifics on the windows used in the experiments are given in Section 4.</p><p>Following <ref type="bibr" target="#b28">[29]</ref>, each covariance descriptor Cov3D R , is normalised with respect to the covariance descriptor of the region containing the full feature video, Cov3D F , to improve the robustness against illumination variations:</p><formula xml:id="formula_14">Cov3D R = diag(Cov3D F ) -1 2 Cov3D R diag(Cov3D F ) -1 2<label>(16)</label></formula><p>where diag(Cov3DF ) is equal to Cov3DF at the diagonal entries and the rest is set to zero. Two examples of Cov3D windows that, together, can be useful for the recognition of face expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Classification of Actions and Gestures</head><p>The Cov3D descriptors are symmetric positive definite matrices of size d × d, which can be formulated as a connected Riemannian manifold (Sym + d ) <ref type="bibr" target="#b6">[7]</ref>. In this section we first briefly overview Riemannian manifolds, followed by describing the proposed weighted Riemannian locality preserving projection (WRLPP) that allows mapping from Riemannian manifolds to Euclidean spaces. We then describe a classification algorithm that uses WRLPP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Riemannian manifolds</head><p>A manifold can be considered as a continuous surface lying in a higher dimensional Euclidean space. Formally, a manifold is a topological space which is locally similar to an Euclidean space <ref type="bibr" target="#b28">[29]</ref>. Intuitively, the tangent space T X is the plane tangent to the surface of the manifold at point X.</p><p>A point Y on the manifold can be mapped to a vector in the tangent space T X using the logarithm map operator log X . For Sym + d the logarithm map is defined as:</p><formula xml:id="formula_15">log X (Y ) = X 1 2 log X − 1 2 Y X − 1 2 X 1 2<label>(17)</label></formula><p>where log (·) is the matrix logarithm operator. Given the eigenvalue decomposition of a symmetric matrix, Σ = U DU T , the matrix logarithm can be computed via:</p><formula xml:id="formula_16">log(Σ) = U log(D)U T<label>(18)</label></formula><p>where log(D) is a diagonal matrix, with each diagonal element equal to the logarithm of the corresponding element in D.</p><p>The minimum length curve connecting two points on the manifold is called the geodesic, and the distance between two points is given by the length of this curve. Geodesics are related to the tangents in the tangent space. For Sym + d , the distance between two points on the manifold can be found via:</p><formula xml:id="formula_17">d 2 (X, Y ) = trace log 2 X − 1 2 Y X − 1 2<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Weighted RLPP</head><p>The usual approach for classification on manifolds is to first map the points into an appropriate Euclidean representation <ref type="bibr" target="#b28">[29]</ref> and then use traditional machine learning methods. Points in the manifold can be mapped into a fixed tangent space (such as T I where I is the identity matrix) <ref type="bibr" target="#b5">[6]</ref>. Since distances in the manifold are only locally preserved in the tangent space, better results can be achieved by considering the tangent space at the Karcher mean, the point which minimises the distances among the samples, as shown in <ref type="bibr" target="#b28">[29]</ref>. Improved results have been obtained by considering multiple tangent spaces <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>. A more complex approach involves using training data to create a mapping that tries to preserve the relations between points, such as the RLPP approach <ref type="bibr" target="#b7">[8]</ref>.</p><p>RLPP is based on Laplacian eigenmaps <ref type="bibr" target="#b1">[2]</ref>. Given N training points X = {X 1 , X 2 , · · · , X N } from the underlying Riemannian manifold M, the local geometrical structure of M can be modelled by building an adjacency graph G. The simplest form of G is a binary graph obtained based on the nearest neighbour properties of Riemannian points: two nodes are connected by an edge if one node is among the k nearest neighbours of the other node. From the adjacency graph G we can find the degree and Laplacian matrices, respectively:</p><formula xml:id="formula_18">D(i, i) = k G(i, k)<label>(20)</label></formula><formula xml:id="formula_19">L = D − G<label>(21)</label></formula><p>where the degree matrix D is a diagonal matrix of size N × N , with diagonal entries indicating the the number of edges of each node in the adjacency graph. RLPP also uses a heat pseudo-kernel matrix K, with the (i, j)-th element constructed via:</p><formula xml:id="formula_20">K(i, j) = k(X i , X j ) = exp − d(X i , X j ) σ<label>(22)</label></formula><p>where d(·, ·) is the geodesic distance defined in <ref type="bibr" target="#b18">(19)</ref>. The final mapping can be found through the following generalised eigenvalue problem <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_21">KLK T A = λKDK T A<label>(23)</label></formula><p>where the eigenvectors with the r smallest eigenvalues form the projection matrix A. The number of possible Cov3D descriptors inside a sample video is very large. As such, we elected to use boosting to search for a subset of the best descriptors for classification. We could use the original RLPP mapping approach to map the matrices as vectors at each boosting iteration. However, as shown in <ref type="bibr" target="#b28">[29]</ref>, the sample weights can be used to generate a mapping which is more appropriate for the critical training samples. Therefore, we propose a modified projection, specifically designed to be used during boosting, which uses sample weights to generate the final mapping. We refer to this approach as weighted Riemannian locality preserving projection (WRLPP).</p><p>In the modified projection, the adjacency graph G is replaced with a weighted adjacency graph G, defined as:</p><formula xml:id="formula_22">G = W GW<label>(24)</label></formula><p>where W is a diagonal matrix with diagonal values that correspond to the vector of sample weights [w 1 , w 2 , . . . , w N ].</p><p>Using the weighted adjacency graph, edges involving critical samples (ie. samples with higher weights) become more important and their geometrical structure is better preserved. The modified projection approach is detailed in Algorithm 1.</p><p>Once the the projection matrix A has been obtained, a given point C (a Cov3D matrix) on the manifold can then be mapped to Euclidean space via:</p><formula xml:id="formula_23">WRLPP(C) = A T K C<label>(25)</label></formula><p>where K C = [k(X 1 , C), k(X 2 , C), · · ·, k(X N , C)] T , with k(·, ·) defined in <ref type="bibr" target="#b21">(22)</ref>, and {X i } N i=1 representing the training points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 : obtaining weighted RLPP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: Training samples (covariance matrices), labels and weights</head><formula xml:id="formula_24">{(X i , y i , w i )} N i=1 , X i ∈ M • Create Riemannian pseudo-kernel matrix: K(i, j) = exp − d(X i ,X j ) σ using (19) as d(·, ·)</formula><p>• Construct weighted adjacency graph:</p><formula xml:id="formula_25">G(i, j) =    w i · w j if y i = y j and X j is among the k nearest neighbours of X i in K. 0 otherwise • Obtain the weighted degree N × N diagonal matrix: D(i, i) = k G(i, k)</formula><p>• Calculate the weighted Laplacian matrix:</p><formula xml:id="formula_26">L = D − G •</formula><p>The eigenvectors with the r smallest eigenvalues of the Rayleigh</p><formula xml:id="formula_27">quotient K DK T K LK T form the projection matrix A. Output: Projection model λ = {A, {X i } N i=1 }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classification</head><p>As mentioned in the preceding section, we have chosen to use boosting to find a subset of the best descriptors for classification, as the number of possible Cov3D descriptors inside a sample video is large. For simplicity, we used a combination of one-vs-one LogitBoost classifiers <ref type="bibr" target="#b4">[5]</ref> to achieve multiclass classification.</p><p>We start with a brief description of binary LogitBoost classification, with class labels y i ∈ {0, 1}. The probability of sample x belonging to class 1 is represented by:</p><formula xml:id="formula_28">p(x) = exp{F (x)} exp{F (x)} + exp{−F (x)}<label>(26)</label></formula><p>where F (x) = 1 2 M m=1 gm(x), with g(x) representing a weak learner.</p><p>The LogitBoost algorithm learns a set of M weak learners by minimising the negative binomial log likelihood of the data. A weighted least squares regression gm(x) of training points x i ∈ R d is fitted to response values z i ∈ R, with weights w i , where</p><formula xml:id="formula_29">w i = p(x i )(1 − p(x i ))<label>(27)</label></formula><formula xml:id="formula_30">z i = y i − p(x i ) p(x i )(1 − p(x i ))<label>(28)</label></formula><p>As we are using Cov3D descriptors (covariance matrices) as input data, we adapt the weak learners gm(·) to use the projected descriptors. In other words, gm(x) is replaced with gm(WRLPP(X)), with X representing a covariance matrix.</p><p>For every unique pair of classes, we train a one-vs-one LogitBoost classifier as follows. Only the samples belonging to the pair of classes are used for training the binary classifier. One class is selected to be the positive class and the other as the negative class. For each boosting iteration, we search for the region whose Cov3D descriptor best separates positive from negative samples. The descriptor is calculated for all the training samples and mapped to vector </p><formula xml:id="formula_31">z j = y j −p(V j ) p(V j )(1−p(V j )) , w j = p(V j )(1 − p(V j ))</formula><p>* For each spatio-temporal window Rs · Construct the descriptors X j,s = Cov3D j,Rs · From {(X j,s , y j , w j )} N j=1 obtain the projection model W RLP Ps using Algorithm 1 · Map the data points x j,s =W RLP Ps(X j,s ) using (25) · Fit function gs(x) by weighted least-squares regression of z j to x j,s using weights</p><formula xml:id="formula_32">w j * Update F (V ) ← F (V ) + 1 2 fm(V ),</formula><p>where fm is the best classifier among {fs} which minimises the negative binomial log-likelihood one-vs-one classifiers space with WRLPP, using the sample weights calculated for the current boosting iteration. Once in vector space, we fit a linear regression and use it as the weak LogitBoost classifier.</p><formula xml:id="formula_33">− N j=1 [y j log(p(x j )) + (1 − y j ) log(1 − p(x j ))] * Update p(V ) ← e F (V ) e F (V ) +e −F (V</formula><p>To prevent overfitting, the number of weak classifiers on each one-vs-one classifier is controlled by a probability margin between the last accepted positive and the last rejected negative. Both margin samples are determined by the target detection rate (dr) and the target false positive rejection rate (rr). The final multiclass classifier is a set of one-vs-one classifiers. Each one-vs-one classifier C &lt;k,l&gt; , where k and l are the labels of its two classes, has a positive class y &lt;k,l&gt; and a threshold τ &lt;k,l&gt; . The positive class is the label of the class deemed to be positive and the threshold is found via boosting. Algorithm 2 summarises the training process.</p><p>Dataset UCF <ref type="bibr" target="#b23">[24]</ref> CK+ <ref type="bibr" target="#b17">[18]</ref> Cambridge <ref type="bibr" target="#b11">[12]</ref> Type A sample video V is classified as follows. Given a onevs-one classifier C &lt;k,l&gt; , the probability of a sample video V belonging to the positive class y &lt;k,l&gt; is evaluated using:</p><formula xml:id="formula_34">C &lt;k,l&gt; (V ) = M m=1</formula><p>gm WRLPPm( Cov3D Rm ) − τ &lt;k,l&gt; <ref type="bibr" target="#b28">(29)</ref> After evaluating V with all the one-vs-one classifiers in the set, the sample is labelled as the class a which maximises:</p><formula xml:id="formula_35">C(V ) = arg max a i =a C &lt;a,i&gt; (V ) signa(C &lt;a,i&gt; (V ))<label>(30)</label></formula><p>where signa(C &lt;a,i&gt; (V )) is sign(C &lt;a,i&gt; (V )) if a is the positive class y&lt;a,i&gt;, or 1 − sign(C&lt;a,i&gt;(V )) otherwise. In other words, V is labelled as the class with greater probability sum, selecting all the one-vs-one classifiers that evaluate to that class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We compared the performance of the proposed algorithm against baseline approaches as well as several state-of-theart methods. We used three benchmark datasets, with an overview of the datasets shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>In the following subsections, we first present an evaluation of several Riemannian to Euclidean space mapping approaches, justifying the use of the weighted RLPP. We then follow with experiments showing the performance on sport actions, facial expressions and hand gestures.</p><p>Unless otherwise stated, no pre-processing was performed in the input sequences and all the recognition results were obtained using 5-fold cross validation to divide the samples into training and testing sets.</p><p>In all cases we used the following parameters: 0.95 detection rate, 0.95 false positive rejection rate, 0.5 margin. Furthermore, since the search space of spatio-temporal windows is very large, we restricted the minimum size of the windows, as well as the minimum increment on location and size of the windows, to <ref type="bibr">1 8</ref> of the frame size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison of mapping approaches</head><p>In <ref type="figure" target="#fig_6">Fig. 4</ref>, we compare the following six Riemannian to Euclidean space mapping (Sym + d → R) approaches which can be used during boosting: (i) no mapping (ie., using a  vectorised representation of the upper-triangle of the covariance matrix), (ii) projection to a fixed tangent space <ref type="bibr" target="#b5">[6]</ref>, (iii) projection to the weighted Karcher mean of the samples <ref type="bibr" target="#b28">[29]</ref>, (iv) projection using k-tangent spaces <ref type="bibr" target="#b24">[25]</ref>, (v) mapping the points with the original RLPP method <ref type="bibr" target="#b7">[8]</ref>, and (vi) mapping the points with the proposed WRLPP approach.</p><p>Since the mapping approach affects individual binary classifiers, we show results per classifier with detection error trade-off curves. We chose the one-vs-one classifiers between conflicting class pairs (where samples of one class are misclassified as the other class) on the Cambridge hand gesture recognition dataset (which is described in Section 4.4). Each point on the curve represents the average of all the chosen classifiers. The curves were obtained by varying the classification threshold τ in Algorithm 2.</p><p>With the exception of the original RLPP method, incrementally better results are obtained by using the mapping approaches in the mentioned order, as they provide increasingly better vector representations of the manifold space. Although RLPP is designed to provide a better representation compared to tangent-based approaches, it appears not to be appropriate for boosting as it does not take into account the sample weights of critical training points. The proposed WRLPP method addresses this problem, resulting in the best overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">UCF sport dataset</head><p>The UCF sport action dataset <ref type="bibr" target="#b23">[24]</ref> consists of ten categories of human actions, containing videos with nonuniform backgrounds where both the camera and the subject might be moving. We use the regions of interest provided with the dataset.</p><p>We compared the Cov3D approach against the following methods: HOG3D <ref type="bibr" target="#b30">[31]</ref>, hierarchy of discriminative Method Performance HOG3D <ref type="bibr" target="#b30">[31]</ref> 85.60% HDN <ref type="bibr" target="#b12">[13]</ref> 87.27% AFMKL <ref type="bibr" target="#b31">[32]</ref> 91.30% Cov3D 93.91% <ref type="table">Table 2</ref>. Average recognition rate on the UCF dataset <ref type="bibr" target="#b23">[24]</ref>. space-time neighbourhood features (HDN) <ref type="bibr" target="#b12">[13]</ref>, and augmented features in conjunction with multiple kernel learning (AFMKL) <ref type="bibr" target="#b31">[32]</ref>. HOG3D is the extension of histogram of oriented gradient descriptor <ref type="bibr" target="#b14">[15]</ref> to the spatiotemporal case. HDN learns shapes of space-time feature neighbourhoods that are most discriminative for a given action category. The idea is to form new features composed of the neighbourhoods around the interest points in a video. AFMKL exploits appearance distribution features and spatio-temporal context features in a learning scheme for action recognition. As shown in <ref type="table">Table 2</ref>, the proposed Cov3D-based approach achieves the highest accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">CK+ facial expression dataset</head><p>The extended Cohn-Kanade (CK+) facial expression database <ref type="bibr" target="#b17">[18]</ref> contains 593 sequences from 123 subjects. We used the sequences with validated emotion labels, among 7 possible emotions. The image sequences vary in duration (i.e. 10 to 60 frames) and incorporate the onset (which is also the neutral frame) to peak formation of the facial expressions.</p><p>We compared the Cov3D approach against active appearance models (AAM), constrained local models (CLM) <ref type="bibr" target="#b2">[3]</ref>, and temporal modelling of shapes (TMS) <ref type="bibr" target="#b8">[9]</ref>. AAM is the baseline approach included with the dataset. It uses active appearance models to track the faces and extract the features, and then uses support vector machines (SVM) to classify the facial expressions. The CLM approach is an improvement on AAM, designed for better generalisation to unseen objects. The TMS approach uses latentdynamic conditional random fields to model temporal variations within shapes.</p><p>We show the performance per emotion in <ref type="table" target="#tab_1">Table 3</ref>, in line with existing literature. The proposed Cov3D approach achieves the highest average recognition accuracy of 92.3% (averaged over the 7 classes). The next best method (TMS) obtained an average accuracy of 87.92%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Cambridge hand gesture dataset</head><p>The Cambridge hand-gesture dataset <ref type="bibr" target="#b11">[12]</ref> consists of 900 image sequences of 9 gesture classes. Each class has 100 image sequences performed by 2 subjects, captured under 5 illuminations and 10 arbitrary motions. The 9 classes are defined by three primitive hand shapes and three primitive motions. Each sequence was recorded with a fixed cam-  <ref type="table">Table 4</ref>. Average recognition rate on the Cambridge dataset <ref type="bibr" target="#b11">[12]</ref>. era having roughly isolated gestures in space and time. We followed the test protocol defined in <ref type="bibr" target="#b11">[12]</ref>. Sequences with normal illumination were considered for training while tests were performed on the remaining sequences. The proposed method was compared against tensor canonical correlation analysis (TCCA) <ref type="bibr" target="#b11">[12]</ref>, product manifolds (PM) <ref type="bibr" target="#b19">[20]</ref> and tangent bundles (TB) <ref type="bibr" target="#b18">[19]</ref>. TCCA is the extension of canonical correlation analysis to multiway data arrays or tensors. Canonical correlation analysis and principal angles are standard methods for measuring the similarity between subspaces. In the PM method a tensor is characterised as a point on a product manifold and classification is performed on this space. The product manifold is created by applying a modified high order singular value decomposition on the tensors and interpreting each factorised space as a Grassmann manifold. In the TB method, video data is represented as a third order tensor and factorised using high order singular value decomposition, where each factor is projected onto a tangent space and the intrinsic distance is computed from a tangent bundle for action classification.</p><p>We report the recognition rates for the four test sets in <ref type="table">Table 4</ref>, where the proposed Cov3D-based approach obtains the highest performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we first extended the flat covariance descriptors proposed in <ref type="bibr" target="#b28">[29]</ref> to spatio-temporal covariance descriptors termed Cov3D, and then showed how they can be computed quickly through the use of integral video representations.</p><p>The proposed Cov3D descriptors belong to the group of symmetric positive definite matrices, which can be formulated as a connected Riemannian manifold. Prior to classification, points on a manifold are generally mapped to an Euclidean space, through a technique such as Riemannian locality preserving projection (RLPP) <ref type="bibr" target="#b7">[8]</ref>.</p><p>The Cov3D descriptors are extracted from spatiotemporal windows inside sample videos, with the number of possible windows being very large. We used a boosting approach to find a subset which is the most useful for classification. In order to take into account the weights of the training samples, we further proposed to extend RLPP by incorporating weighting during the projection. The weighted projection (termed WRLPP) leads to a better representation of the neighbourhoods around the most critical training samples during each boosting iteration.</p><p>Combining the proposed Cov3D descriptors with the classification approach based on WRLPP boosting leads to a state-of-the-art method for action and gesture recognition. The proposed Cov3D-based method performs better than several recent approaches on three benchmark datasets for action and gesture recognition. The method is robust and does not require additional processing of the videos, such as foreground detection, interest-point detection or tracking. To our knowledge, this is the first approach proving to be equally suitable (ie., &gt; 90% recognition accuracy) for both action and gesture recognition.</p><p>Further avenues of research include adapting the method for related tasks, such as anomaly detection in surveillance videos <ref type="bibr" target="#b22">[23]</ref>, where there is often a shortage of positive examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Conceptual demonstration for obtaining a Cov3D spatio-temporal covariance descriptor. A spatio-temporal window R is defined inside the input video. For each pixel in R a feature vector z i is calculated. The feature vectors are then used to compute the covariance matrix Cov3D R .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Two examples of Cov3D windows that, together, can be useful for the recognition of face expressions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2 :- 2 , m = 1 -</head><label>221</label><figDesc>Boosting with WRLPP Input: Training videos with labels {(V i , y i )} N i=1 belonging to Nc classes • For each unique pair of class labels &lt; k, l &gt; train the one-vs-one classifier C &lt;k,l&gt; -Let k be the positive class label and restrict the training set to {(V j , y j )} j=1...N |y j ∈{k,l} -Let either k or l be the positive label y &lt;k,l&gt; -Create binary labels y j ← (y j = y &lt;k,l&gt; ) Start with w j = 1/N , F (V ) = 0, p(V j ) = 1 Repeat while p(Vp) − p(Vn) &lt; margin * Compute the response values and weights</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 -</head><label>1</label><figDesc>) * Sort positive and negative samples according to descending probabilities and find samples at the decision boundaries Vp = (dr · Np)-th V + , Vn = (rr · Nn)-th V − , where dr and rr are the desired detection and false positive rejection rates * m ← m + Store C &lt;k,l&gt; = {(Rm, W RLP Pm, gm)} M m=1 , threshold τ &lt;k,l&gt; = F (Vn) and positive label y &lt;k,l&gt; Output: A set of Nc(Nc−1) 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Performance comparison of various Sym + d → R mapping approaches, used within the classifier framework described in Section 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Overview of the datasets used in the experiments.</figDesc><table><row><cell></cell><cell>sports</cell><cell>facial expressions</cell><cell>hand gestures</cell></row><row><cell>Classes</cell><cell>10</cell><cell>7</cell><cell>9</cell></row><row><cell>Subjects</cell><cell>-</cell><cell>123</cell><cell>2</cell></row><row><cell>Scenarios</cell><cell>-</cell><cell>-</cell><cell>5</cell></row><row><cell>Video samples</cell><cell>150</cell><cell>593</cell><cell>900</cell></row><row><cell>Resolution</cell><cell>variable</cell><cell>640 × 480</cell><cell>320 × 240</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Recognition rate (in %) on the CK+ dataset<ref type="bibr" target="#b17">[18]</ref>.</figDesc><table><row><cell>Method</cell><cell>a n g r y</cell><cell cols="2">c o n t e m p t</cell><cell cols="2">d is g u s t</cell><cell>f e a r</cell><cell>h a p p y</cell><cell>s a d n e s s</cell><cell>s u r p r is e</cell></row><row><cell cols="2">AAM [18] 75.0</cell><cell>84.4</cell><cell></cell><cell cols="2">94.7</cell><cell>65.2</cell><cell>100</cell><cell>68.0</cell><cell>96.0</cell></row><row><cell>CLM [3]</cell><cell>70.1</cell><cell>52.4</cell><cell></cell><cell cols="2">92.5</cell><cell>72.1</cell><cell>94.2</cell><cell>45.9</cell><cell>93.6</cell></row><row><cell>TMS [9]</cell><cell>76.7</cell><cell>-</cell><cell></cell><cell cols="2">81.5</cell><cell>94.4</cell><cell>98.6</cell><cell>77.2</cell><cell>99.1</cell></row><row><cell>Cov3D</cell><cell>94.4</cell><cell>100</cell><cell></cell><cell cols="2">95.5</cell><cell>90.0</cell><cell>96.2</cell><cell>70.0</cell><cell>100</cell></row><row><cell>Method</cell><cell></cell><cell>Set1</cell><cell cols="2">Set2</cell><cell cols="2">Set3</cell><cell>Set4</cell><cell>Overall</cell></row><row><cell cols="2">TCCA [12]</cell><cell>81%</cell><cell cols="2">81%</cell><cell cols="2">78%</cell><cell>86%</cell><cell>82% (±3.5)</cell></row><row><cell>PM [20]</cell><cell></cell><cell>89%</cell><cell cols="2">86%</cell><cell cols="2">89%</cell><cell>87%</cell><cell>88% (±2.1)</cell></row><row><cell>TB [19]</cell><cell></cell><cell>93%</cell><cell cols="2">88%</cell><cell cols="2">90%</cell><cell>91%</cell><cell>91% (±2.4)</cell></row><row><cell>Cov3D</cell><cell></cell><cell>92%</cell><cell cols="6">94% 94% 93% 93% (±1.1)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* Published in: IEEE Workshop on Applications of Computer Vision, pp. 103-110, 2013. http://dx.doi.org/10.1109/WACV.2013.6475006. Acknowledgements: NICTA is funded by the Australian Government via the Department of Broadband, Communications and the Digital Economy, and the Australian Research Council through the ICT Centre of Excellence program.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human action recognition in videos using kinematic features and multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="288" to="303" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Person-independent facial expression detection using constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face Gesture Recognition and Workshops</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="915" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Additive logistic regression: a statistical view of boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="407" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Action recognition using sparse representation on covariance manifolds of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="188" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse coding and dictionary learning for symmetric positive definite matrices: A kernel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7573</biblScope>
			<biblScope unit="page" from="216" to="229" />
		</imprint>
	</monogr>
	<note>LNCS)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kernel analysis over Riemannian manifolds for visual recognition of actions, pedestrians and textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wiliem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on the Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="433" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facial expression recognition with temporal modeling of shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1642" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient visual event detection using volumetric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="166" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human activity recognition using a dynamic texture based method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kellokumpu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis of video volume tensors for action categorization and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1415" to="1428" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a hierarchy of discriminative space-time neighborhood features for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2046" to="2053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Riemannian manifold learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="796" to="809" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tangent bundles on special manifolds for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="930" to="942" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action classification on product manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="833" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatial-temporal words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="318" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A survey on vision-based human action recognition. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="976" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed, size and texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="55" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action MACH a spatiotemporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">K-tangent spaces on Riemannian manifolds for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Procesing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A 3-dimensional SIFT descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pose primitive based human action recognition in videos or still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Machine recognition of human activities: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Subrahmanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Udrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1473" to="1488" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pedestrian detection via classification on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1713" to="1727" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action recognition using context and appearance distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
