<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">75 Languages, 1 Model: Parsing Universal Dependencies Universally</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
							<email>dankondratyuk@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">Saarland University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
							<email>straka@ufal.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">75 Languages, 1 Model: Parsing Universal Dependencies Universally</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can meet or exceed state-of-the-art UPOS, UFeats, Lemmas, (and especially) UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on. Code for UDify is available at https:// github.com/hyperparticle/udify.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the absence of annotated data for a given language, it can be considerably difficult to create models that can parse the language's text accurately. Multilingual modeling presents an attractive way to circumvent this low-resource limitation. In a similar way learning a new language can enhance the proficiency of a speaker's previous languages (Abu-Rabia and Sanitsky, 2010), a model which has access to multilingual information can begin to learn generalizations across languages that would not have been possible through monolingual data alone. Works such as <ref type="bibr">McDonald et al. (2011);</ref><ref type="bibr">Naseem et al. (2012)</ref>; <ref type="bibr">Duong et al. (2015)</ref>; <ref type="bibr" target="#b2">Ammar et al. (2016)</ref>; <ref type="bibr">de Lhoneux et al. (2018)</ref>; <ref type="bibr">Kitaev and Klein (2018)</ref>; <ref type="bibr">Mulcaire et al. (2019)</ref> consistently demonstrate how pairing the "The best optimizer is grad student descent"</p><p>The best op ##timi ##zer is grad student descent ... training data of similar languages can boost evaluation scores of models predicting syntactic information like part-of-speech and dependency trees. Multilinguality not only can improve a model's evaluation performance, but can also reduce the cost of training multiple models for a collection of languages <ref type="bibr">(Johnson et al., 2017;</ref><ref type="bibr">Smith et al., 2018)</ref>. However, scaling to a higher number of languages can often be problematic. Without an ample supply of training data for the considered languages, it can be difficult to form appropriate generalizations and especially difficult if those arXiv:1904.02099v3 [cs.CL] 25 Aug 2019 languages are distant from each other. But recent techniques in language model pretraining can profit from a drastically larger supply of unsupervised text, demonstrating the capability of transferring contextual sentence-level knowledge to boost the parsing accuracy of existing NLP models <ref type="bibr">(Howard and Ruder, 2018;</ref><ref type="bibr">Peters et al., 2018;</ref><ref type="bibr">Devlin et al., 2018)</ref>.</p><p>One such model, <ref type="bibr">BERT (Devlin et al., 2018)</ref>, introduces a self-attention (Transformer) network that results in state-of-the-art parsing performance when fine-tuning its contextual embeddings. And with the release of a multilingual version pretrained on the entirety of the top 104 resourced languages of Wikipedia, BERT is remarkably capable of capturing an enormous collection of cross-lingual syntactic information. Conveniently, these languages nearly completely overlap with languages supported by the Universal Dependencies treebanks, which we will use to demonstrate the ability to scale syntactic parsing up to 75 languages and beyond.</p><p>The Universal Dependencies (UD) framework provides syntactic annotations consistent across a large collection of languages <ref type="bibr">(Nivre et al., 2018;</ref><ref type="bibr">Zeman et al., 2018)</ref>. This makes it an excellent candidate for analyzing syntactic knowledge transfer across multiple languages. UD offers tokenized sentences with annotations ideal for multi-task learning, including lemmas (LEMMAS), treebank-specific part-of-speech tags (XPOS), universal part-of-speech tags (UPOS), morphological features (UFEATS), and dependency edges and labels (DEPS) for each sentence.</p><p>We propose UDify, a semi-supervised multitask self-attention model automatically producing UD annotations in any of the supported UD languages. To accomplish this, we perform the following:</p><p>1. We input all sentences into a pretrained multilingual BERT network to produce contextual embeddings, introduce task-specific layer-wise attention similar to <ref type="bibr">ELMo (Peters et al., 2018)</ref>, and decode each UD task simultaneously using softmax classifiers.</p><p>2. We apply a heavy amount of regularization to BERT, including input masking, increased dropout, weight freezing, discriminative finetuning, and layer dropout.</p><p>3. We train and fine-tune the model on the en-tirety of UD by concatenating all available training sets together.</p><p>We evaluate our model with respect to UDPipe Future, one of the winners of the CoNLL 2018 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies <ref type="bibr">(Straka, 2018;</ref><ref type="bibr">Zeman et al., 2018)</ref>. In addition, we analyze languages that multilingual training benefits prediction the most, and evaluate the model for zeroshot learning, i.e., treebanks which do not have a training set. Finally, we provide evidence from our experiments and other related work to help explain why pretrained self-attention networks excel in multilingual dependency parsing.</p><p>Our work uses the AllenNLP library built for the PyTorch framework. Code for UDify and a release of the fine-tuned BERT weights are available at https://github. com/hyperparticle/udify.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multilingual Multi-Task Learning</head><p>In this section, we detail the multilingual training setup and the UDify multi-task model architecture. See <ref type="figure" target="#fig_0">Figure 1</ref> for an architecture diagram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multilingual Pretraining with BERT</head><p>We leverage the provided BERT base multilingual cased pretrained model 1 , with a self-attention network of 12 layers, 12 attention heads per layer, and hidden dimensions of 768 <ref type="bibr">(Devlin et al., 2018)</ref>. The model was trained by predicting randomly masked input words on the entirety of the top 104 languages with the largest Wikipedias. BERT uses a wordpiece tokenizer <ref type="bibr">(Wu et al., 2016)</ref>, which segments all text into (unnormalized) sub-word units. <ref type="table" target="#tab_1">Table 1</ref> displays a list of vocabulary sizes, indicating that UD treebanks possess nearly 1.6M unique tokens combined. To sidestep the problem of a ballooning vocabulary, we use BERT's wordpiece tokenizer directly for all inputs. UD expects predictions to be along word boundaries, so we take the simple approach of applying the tokenizer to each word using UD's provided segmentation. For prediction, we use the outputs of BERT corresponding to the first wordpiece per word, ignoring  the rest 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-Linguistic Training Issues</head><p>In addition, the XPOS annotations are not universal across languages, or even across treebanks. Because each treebank can possess a different annotation scheme for XPOS which can slow down inference, we omit training and evaluation of XPOS from our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Task Learning with UD</head><p>For predicting UD annotations, we employ a multi-task network based on UDPipe Future <ref type="bibr">(Straka, 2018)</ref>, but with all embedding, encoder, and projection layers replaced with BERT. The remaining components include the prediction layers for each task detailed below, and layer attention (see Section 3.1). Then we compute softmax cross entropy loss on the output logits to train the network. For more details on reasons behind architecture choices, see Appendix A.</p><p>UPOS As is standard for neural sequence tagging, we apply a softmax layer along each word input, computing a probability distribution over the tag vocabulary to predict the annotation string.</p><p>UFeats Identical to UPOS prediction, we treat each UFeats string as a separate token in the vocabulary. We found this to produce higher evaluation accuracy than predicting each morphological feature separately. Only a small subset of the full Cartesian product of morphological features is valid, eliminating invalid combinations.</p><p>Lemmas Similar to <ref type="bibr">Chrupała (2006);</ref><ref type="bibr">Müller et al. (2015)</ref>, we reduce the problem of lemmatization to a sequence tagging problem by predicting a class representing an edit script, i.e., the sequence of character operations to transform the word form to the lemma. To precompute the tags, we first find the longest common substring between the form and the lemma, and then compute the shortest edit script converting the prefix and suffix of the form into the prefix and suffix of the lemma using the Wagner-Fischer algorithm <ref type="bibr">(Wagner and Fischer, 1974)</ref>. Upon predicting a lemma edit script, we apply the edit operations to the word form to produce the final lemma. See also Straka (2018) for more details. We chose this approach over a sequence-to-sequence architecture like <ref type="bibr">Bergmanis and Goldwater (2018)</ref> or <ref type="bibr">Kondratyuk et al. (2018)</ref>, as this significantly reduces training efficiency.</p><p>Deps We use the graph-based biaffine attention parser developed by <ref type="bibr">Dozat and Manning (2016);</ref><ref type="bibr">Dozat et al. (2017)</ref>, replacing the bidirectional LSTM layers with BERT. The final embeddings are projected through arc-head and arc-dep feedforward layers, which are combined using biaffine attention to produce a probability distribution of arc heads for each word. We then decode each tree with the Chu-Liu/Edmonds algorithm <ref type="bibr">(Chu, 1965;</ref><ref type="bibr">Edmonds, 1967)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fine-Tuning BERT on UD Annotations</head><p>We employ several strategies for fine-tuning BERT for UD prediction, finding that regularization is absolutely crucial for producing a highscoring network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Layer Attention</head><p>Empirical results suggest that when fine-tuning BERT, combining the output of the last several layers is more beneficial for the downstream tasks than just using the last layer <ref type="bibr">(Devlin et al., 2018)</ref>. Instead of restricting the model to any subset of layers, we devise a simple layer-wise dot-product attention where the network computes a weighted sum of all intermediate outputs of the 12 BERT layers using the same weights for each token. This is similar to how ELMo mixes the output of multiple recurrent layers <ref type="bibr">(Peters et al., 2018)</ref>.</p><p>More formally, let w i be a trainable scalar for BERT embeddings BERT ij at layer i with a token at position j, and let c be a trainable scalar. We compute contextual embeddings e (task) such that</p><formula xml:id="formula_0">e (task) j = c i BERT ij · softmax(w) i (1)</formula><p>To prevent the UD classifiers from overfitting to the information in any single layer, we devise layer dropout, where at each training step, we set each parameter w i to −∞ with probability 0.1. This effectively redistributes probability mass to all other layers, forcing the network to incorporate the information content of all BERT layers. We compute layer attention per task, using one set of c, w parameters for each of UPOS, UFeats, Lemmas, and Deps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transfer Learning with ULMFiT</head><p>The ULMFiT strategy defines several useful methods for fine-tuning a network on a pretrained language model <ref type="bibr">(Howard and Ruder, 2018)</ref>. We apply the same methods, with a few minor modifications.</p><p>We split the network into two parameter groups, i.e., the parameters of BERT and all other parameters. We apply discriminative fine-tuning, setting the base learning rate of BERT to be 5e −5 and 1e −3 everywhere else. We also freeze the BERT parameters for the first epoch to increase training stability.</p><p>While ULMFiT recommends decaying the learning rate linearly after a linear warmup, we found that this is prone to training divergence in self-attention networks, introducing vanishing gradients and underfitting. Instead, we apply an inverse square root learning rate decay with linear warmup (Noam) seen in training Transformer networks for machine translation <ref type="bibr">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Input Masking</head><p>The authors of BERT recommend not to mask words randomly with [MASK] when fine-tuning the network. However, we discovered that masking often reduces the tendency of the classifiers to overfit to BERT by forcing the network to rely on the context of surrounding words. This word dropout strategy has been observed in other works showing improved test performance on a variety of NLP tasks <ref type="bibr">(Iyyer et al., 2015;</ref><ref type="bibr">Bowman et al., 2016;</ref><ref type="bibr">Clark et al., 2018;</ref><ref type="bibr">Straka, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate UDify with respect to every test set in each treebank. As there are too many results to fit within one page, we display a salient subset of scores and compare them with UDPipe Future. The full results are listed in Appendix A.</p><p>We do not directly reference metrics from other models in the CoNLL 2018 Shared Task, as the tables of results do not assume gold word segmentation and may not provide a fair comparison. Instead, we retrained the open source UDPipe Future model using gold segmentation and report results here due to its architectural similarity to UDify and its strong performance. Note that the UDPipe Future baseline does not itself use BERT. Evaluation of BERT utilization in UDPipe Future can be found in <ref type="bibr">Straka et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>For all experiments, we use the full Universal Dependencies v2.3 corpus available on <ref type="bibr">LINDAT (Nivre et al., 2018)</ref>. We omit the evaluation of datasets that do not have their training annotations freely available, i.e., Arabic NYUAD (ar nyuad), English ESL (en esl), French FTB (fr ftb), Hindi English HEINCS (qhe heincs), and Japanese BC-CWJ (ja bccwj).</p><p>To train the multilingual model, we concatenate all available training sets together, similar to <ref type="bibr">Mc-Donald et al. (2011)</ref>. Before each epoch, we shuffle all sentences and feed mixed batches of sentences to the network, where each batch may contain sentences from any language or treebank, for a total of 80 epochs 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperparameters</head><p>A summary of hyperparameters can be found in <ref type="table" target="#tab_11">Table 6</ref> in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Probing for Syntax</head><p>Hewitt and Manning (2019) introduce a structural probe for identifying dependency structures in contextualized word embeddings. This probe evaluates whether syntax trees (i.e., unlabeled undirected dependency trees) can be easily extracted as a global property of the embedding space using a linear transformation of the network's contextual word embeddings. The probe trains a weighted adjacency matrix on the layers of contextual embeddings produced by BERT, identifying a linear transformation where squared L2 distance between embedding vectors encodes the distance between words in the parse tree. Edges are decoded by computing the minimum spanning tree on the weight matrix (the lowest sum of edge distances).  <ref type="table">Table 2</ref>:</p><p>Test set scores for a subset of highresource (top) and low-resource (bottom) languages in comparison to UDPipe Future without BERT, with 3 UDify configurations: Lang, fine-tune on the treebank. UDify, fine-tune on all UD treebanks combined. UDify+Lang, fine-tune on the treebank using BERT weights saved from fine-tuning on all UD treebanks combined.</p><p>We train the structural probe on unmodified and fine-tuned BERT using the default hyperparameters of Hewitt and Manning <ref type="formula">(2019)</ref>   <ref type="table">Table 3</ref>: Ablation comparing the average of scores over all treebanks: task-specific layer attention (4 sets of c, w computed for the 4 UD tasks), global layer attention (one set of c, w for all tasks), and simple sum of layers (c = 1 and w = ).   ate whether the representations affected by finetuning BERT on dependency trees would more closely match the structure of these trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREEBANK UPOS FEATS LEM UAS LAS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We show scores of UPOS, UFeats (FEATS), and Lemma (LEM) accuracies, along with unlabeled and labeled attachment scores (UAS, LAS) evaluated using the offical CoNLL 2018 Shared Task evaluation script. 4 Results for a salient subset of high-resource and low-resource languages are shown in <ref type="table">Table 2</ref>, with a comparison between UDPipe Future and UDify fine-tuning on all languages. In addition, the table compares UDify with fine-tuning on either a single language or both languages (fine-tuning multilingually, then fine-tuning on the language with the saved multilingual weights) to provide a reference point for multilingual influences on UDify. We provide a full table of scores for all treebanks in Appendix A.4. A more comprehensive overview is shown in <ref type="table">Table 3</ref>, comparing different attention strategies applied to UDify. We display an average of scores over all (89) treebanks with a training set. For zero-shot learning evaluation, <ref type="table" target="#tab_5">Table 4</ref> displays a subset of test set evaluations of treebanks that do not have a training set, i.e., Breton, Tagalog, Faroese, Naija, and Sanskrit. We plot the layer attention weights w after fine-tuning BERT in <ref type="figure" target="#fig_1">Figure 2</ref>, showing a set of weights per task. And Table 5 compares the unlabeled undirected attachment scores (UUAS) of dependency trees produced using a structural probe on both the unmodified multilingual cased BERT model and the extracted BERT model fine-tuned on the English EWT treebank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this section, we discuss the most notable features of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Model Performance</head><p>On average, UDify reveals a strong set of results that are comparable in performance with the state-of-the-art in parsing UD annotations. UDify excels in dependency parsing, exceeding UD-Pipe Future by a large margin especially for lowresource languages. UDify slightly underperforms with respect to Lemmas and Universal Features, likely due to UDPipe Future additionally using character-level embeddings <ref type="bibr">(Santos and Zadrozny, 2014;</ref><ref type="bibr">Ling et al., 2015;</ref><ref type="bibr" target="#b4">Ballesteros et al., 2015;</ref><ref type="bibr">Kim et al., 2016)</ref>, while (for simplicity) UDify does not. Additionally, UDify severely underperforms the baseline on a few low-resource languages, e.g., cop scriptorum. We surmise that this is due to using mixed batches on an unbalanced training set, which skews the model towards predicting larger treebanks more accurately. However, we find that fine-tuning on the treebank individually with BERT weights saved from UDify eliminates most of these gaps in performance.</p><p>Echoing results seen in Smith et al. <ref type="formula">(2018)</ref>, UDify also shows strong improvement leveraging multilingual data from other UD treebanks. In low-resource cases, fine-tuning BERT on all treebanks can be far superior to fine-tuning monolingually. A second round of fine-tuning on an individual treebank using UDify's BERT weights can improve this further, especially for treebanks that underperform the baseline. However, for languages that are already display strong results, we typically notice worse evaluation performance across all the evaluation metrics. This indicates that multilingual fine-tuning really is superior to single language fine-tuning with respect to these high-performing languages, showing improvements of up to 20% reduction in error.</p><p>Interestingly, Slavic languages tend to perform the best with multilingual training. While languages like Czech and Russian possess the largest UD treebanks and do not differ as much in performance from monolingual fine-tuning, evidenced by the improvements over single-language finetuning, we can see a large degree of morphological and syntactic structure has transferred to low-resource Slavic languages like Upper Sorbian, whose treebank contains only 646 sentences. But this is not only true of Slavic languages, as the Turkic language Kazakh (with less than 1,000 training sentences) has also improved significantly.</p><p>The zero-shot results indicate that fine-tuning on BERT can result in reasonably high scores on languages that do not have a training set. It can be seen that a combination of BERT pretraining and multilingual learning can improve predictions for Breton and Tagalog, which implies that the network has learned representations of syntax that cross lingual boundaries. Furthermore, despite the fact that neither BERT nor UDify have directly observed Faroese, Naija, or Sanskrit, we see unusually high performance in these languages. This can be partially attributed to each language closely resembling another: Faroese is very close to Ice- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT</head><p>We land and spill out and go our separate ways .  <ref type="figure">Figure 3</ref>: Examples of minimum spanning trees produced by the syntactic probe are shown below each sentence, evaluated on BERT (left) and on UDify (right). Gold dependency trees are shown above each sentence in black.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UDify</head><p>Matched and unmatched spanning tree edges are shown in blue and red respectively. landic, Naija (Nigerian Pidgin) is a variant of English, and Sanskrit is an ancient Indian language related to Greek, Latin, and Hindi. <ref type="table">Table 3</ref> shows that layer attention on BERT for each task is beneficial for test performance, much more than using a global weighted average. In fact, <ref type="figure" target="#fig_1">Figure 2</ref> shows that each task prefers the layers of BERT differently, uniquely extracting the optimal information for a task. All tasks favor the information content in the last 3 layers, with a tendency to disprefer layers closer to the input. However, an interesting observation is that for Lemmas and UFeats, the classifier prefers to also incorporate the information of the first 3 layers. This meshes well with the linguistic intuition that morphological features are more closely related to the surface form of a word and rely less on context than other syntactic tasks. Curiously enough, the middle layers are highly dispreferred, meaning that the most useful processing for multilingual syntax (tagging, dependency parsing) occurs in the last 3-4 layers. The results released by <ref type="bibr">Tenney et al. (2019)</ref> also agree with the intuition behind the weight distribution above, showing how the different layers of BERT generate hierarchical information like a traditional NLP pipeline, starting with low-level syntax (e.g., POS tagging) and building up to high-level syntactic and semantic dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effect of Syntactic Fine-Tuning on BERT</head><p>Even without any supervised training, BERT encodes its syntax in the embedding's distance close to human-annotated dependencies. But more notably, the results in <ref type="table" target="#tab_6">Table 5</ref> show that fine-tuning BERT on Universal Dependencies significantly boosts UUAS scores when compared to the gold dependency trees, an error reduction of 41%.</p><p>This indicates that the self-attention weights have learned a linearly-transformable representation of its vectors more closely resembling annotated dependency trees defined by linguists. Even with just unsupervised pretraining, a global structural property of the vector space of the BERT weights already produces a decent representation of the dependency tree in the squared L2 distance. Following this, it should be no surprise that training with a non-linear graph-based dependency decoder would produce even higher quality dependency trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Attention Visualization</head><p>We performed a high-level visual analysis of the BERT attention weights to see if they have changed on any discernible level. Our observations reveal something notable: the attention weights tend to be more sparse, and are more often sensitive to constituent boundaries like clauses and prepositions. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates this point, showing the attention weights of a particular attention head on an example sentence. We find similar behavior in 13 additional attention heads for the provided example sentence.</p><p>We see that some of the attention structure remains after fine-tuning. Previously, the attention head was mostly sensitive to previous words and punctuation. But after fine-tuning, it demonstrates more fine-grained attention towards immediate wordpieces, prepositions, articles, and adjectives. We found similar evidence in other attention heads, which implies that fine-tuning on UD produces attention that more closely resembles localized dependencies within constituents. We also find that BERT base heavily preferred to attend to punctuation, while UDify BERT does to a much lesser degree. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Factors that Enable BERT to Excel at Dependency Parsing and Multilinguality</head><p>Goldberg <ref type="formula">(2019)</ref> assesses the syntactic capabilities of BERT and concludes that BERT is remarkably capable of processing syntactic tasks despite not being trained on any supervised data. Conducting similar experiments, Vig (2019) and Sileo (2019) visualize the attention heads within each BERT layer, showing a number of distinct attention patterns, including attending to previous/next words, related words, punctuation, verbs/nouns, and coreference dependencies. This neat delegation of certain low-level information processing tasks to the attention heads hints at why BERT might excel at processing syntax. We see that from the analysis on BERT finetuned with syntax using the syntactic probe and attention visualization, BERT produces a representation that keeps constituents close in its vector space, and improves this representation to more closely resemble human annotated dependency trees when fine-tuned on UD as seen in <ref type="figure">Figure 3</ref>. Furthermore, <ref type="bibr" target="#b1">Ahmad et al. (2018)</ref> provide results consistent with their claim that self-attention networks can be more robust than recurrent networks to the change of word order, observing that selfattention networks capture less word order information in their architecture, which is what allows them to generally perform better at cross-lingual parsing. Wu and Dredze (2019) also analyze multilingual BERT and report that the model retains both language-independent as well as languagespecific information related to each input sentence, and that the shared embedding space with the input wordpieces correlates strongly with crosslingual generalization.</p><p>From the evidence above, we can see that the combination of strong regularization paired with the ability to capture long-range dependencies with self-attention and contextual pretraining on an enormous corpus of raw text are large contributors that enable robust multilingual modeling with respect to dependency parsing. Pretraining self-attention networks introduces a strong syntactic bias that is capable of generalizing across languages. The dependencies seen in the output dependency trees are highly correlated with the implicit dependencies learned by the self-attention, showing that self-attention is remarkably capable of modeling syntax by picking up on common syntactic patterns in text. The introduction of multilingual data also shows that these attention heads provide a surprising amount of capacity that do not degrade the performance considerably when compared to monolingual training. E.g., <ref type="bibr">Devlin et al. (2018)</ref> report that the fine-tuning on the multilingual BERT model results in a small degradation in English fine-tune performance with 104 pretrained languages compared to an equivalent model pretrained only on English. This also hints that the BERT model can be compressed significantly without compromising heavily on evaluation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>This work's main contribution in combining treebanks for multilingual UD parsing is most similar to the Uppsala system for the <ref type="bibr">CoNLL 2018</ref><ref type="bibr">Shared Task (Smith et al., 2018</ref>. Uppsala combines treebanks of one language or closely related languages together over 82 treebanks and parses all UD annotations in a multi-task pipeline architecture for a total of 34 models. This approach reduces the number of models required to parse each language while also showing results that are no worse than training on each treebank individually, and in especially low-resource cases, significantly improved. Combining UD treebanks in a language-agnostic way was first introduced in Vilares et al. <ref type="formula">(2016)</ref>, which train bilingual parsers on pairs of UD treebanks, showing similar improvements.</p><p>Other efforts in training multilingual models include Johnson et al. <ref type="formula">(2017)</ref>, which demonstrate a machine translation model capable of supporting translation between 12 languages. Recurrent models have also shown to be capable of scaling to a larger number of languages as seen in <ref type="bibr" target="#b3">Artetxe and Schwenk (2018)</ref>, which define a scalable approach to train massively multilingual embeddings using recurrent networks on an auxiliary task, e.g., natural language inference. Schuster et al. <ref type="formula">(2019)</ref> produce context-independent multilingual embeddings using a novel embedding alignment strategy to allow models to improve the use of crosslingual information, showing improved results in dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have proposed and evaluated UDify, a multilingual multi-task self-attention network finetuned on BERT pretrained embeddings, capable of producing annotations for any UD treebank, and exceeding the state-of-the-art in UD dependency parsing in a large subset of languages while being comparable in tagging and lemmatization accuracy. Strong regularization and task-specific layer attention are highly beneficial for fine-tuning, and coupled with training multilingually, also reduce the number of required models to train down to one. Multilingual learning is most beneficial for low-resource languages, even ones that do not possess a training set, and can be further improved by fine-tuning monolingually using BERT weights saved from UDify's multilingual training. All these results indicate that self-attention networks are remarkably capable of capturing syntactic patterns, and coupled with unsupervised pretraining are able to scale to a large number of languages without degrading performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work described herein has been supported by OP VVV VI LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project CZ.02.1.01/0.0/0.0/16 013/0001781) and it has been supported and has been using language resources developed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2015071 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>In this section, we detail and explain hyperparameter choices and miscellaneous details related to model training and display the full tables of evaluation results of UDify across all UD languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Hyperparameters</head><p>Upon concatenating all training sets, we shuffle all the sentences, bundle them into batches of 32 sentences each, and train UDify for a total of 80 epochs before stopping. We hold the learning rate constant until we unfreeze BERT in the second epoch, where we and linearly warm up the learning rate for the next 8,000 batches and then apply inverse square root learning rate decay for the remaining epochs. For the dependency parser, we use feedforward tag and arc dimensions of 300  and 800 respectively. We apply a small weight decay penalty of 0.01 to ensure that the weights remain small after each update. For optimization we use the Adam optimizer and we compute softmax cross entropy loss to train the network. We use a default β 1 value of 0.9 and lower the β 2 value from the typical 0.999 to 0.99. The reasoning is to increase the decay rate of the second moment in the Adam optimizer to reduce the chance of the optimizer being too optimistic with respect to the gradient history. We clip the gradient updates to a maximum L2 magnitude of 5.0. A summary of hyperparameters can be found in <ref type="table" target="#tab_11">Table 6</ref>.</p><p>To speed up training, we employ bucketed batching, sorting all sentences by their length and grouping similar length sentences into each batch. However, to ensure that most sentences do not get grouped within the same batch, we fuzz the lengths of each sentence by a maximum of 10% of its true length when grouping sentences together.</p><p>Despite using all the regularization strategies shown previously, we still observe overfitting and must apply more aggressive techniques. To further regularize the network, we also increase the attention and hidden dropout rates of BERT from 0.1 to 0.2, and we also apply a dropout rate of 0.5 to all BERT layers before computing layer attention for each of the four tasks and applying a layer dropout with probability 0.1. We increase the masking probability of each wordpiece from 0.15 to 0.2.</p><p>With all these regularization strategies and hyperparameter choices combined, we are able to fine-tune BERT for far more epochs before the network starts to overfit, i.e., 80 as opposed to around 10. Even so, we believe even more regularization can improve test performance.</p><p>The final multilingual UDify model was trained over approximately 25 days on an NVIDIA GTX 1080 Ti taking an average of 8 hours per epoch. We use half-precision (fp16) training to be able to keep the BERT model in memory. One notable aspect of training is that while we observed the model start to level out in validation performance at around epoch 30, the model continually made small, incremental improvements over each subsequent epoch, resulting in far higher scores than if the model training was terminated early. This can be partially attributed to the decaying inverse square root learning rate.</p><p>Due to the high training times, we are only able to report on a small number of training experiments for the most relevant and useful results. Prior to developing the final model, we conducted fine-tuning experiments on pairs of languages to find a set of hyperparameters that worked best for multilingual learning. After this, we gradually scaled up training to 3 languages, 5 languages, 15 languages, and then finally the model presented above. We had high doubts, and wanted to see where the limit was in multilingual training. We were pleasantly surprised to find that this simple training scheme was able to scale up so well to all UD treebanks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Size Effect on Performance</head><p>To gain a better understanding of where the largest score improvements in UDify occur, we plot the LAS improvement UDify provides over UDPipe Future for each treebank, ordered by the size (number of sentences) of the training set, see ure 5. The results show that the largest improvements tend to occur on small treebanks with less than 3,000 training examples. For absolute LAS values, see <ref type="figure" target="#fig_4">Figure 6</ref>, which indicates that more training resources tend to improve evaluation performance overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Miscellaneous Details</head><p>Our results show that modeling language-specific properties is not strictly necessary to achieve highperforming cross-lingual representations for dependency parsing, though we caution that the model can also likely be improved by these techniques.</p><p>Fine-tuning BERT on UD introduces a syntactic bias in the network, and we are interested in observing any differences in transfer learning by fine-tuning this new "UD-BERT" on other tasks. We leave a comprehensive evaluation of injecting syntactic bias into language models with respect to knowledge transfer for future work.</p><p>We note that saving the weights of BERT and fine-tuning a second round can improve performance as demonstrated in <ref type="bibr">Stickland et al. (2019)</ref>. The improvements of UDify+Lang over just UDify can be partially attributed to this, but we can see that even these improvements can be inferior to fine-tuning on all UD treebanks.</p><p>BERT limits its positional encoding to 512 wordpieces, causing some sentences in UD to be too long to fit into the model. We use a sliding window approach to break up long sentences into windows of 512 wordpieces, overlapping each window by 256 wordpieces. After feeding the windows into BERT, we select the first 256 wordpieces of each window and any remaining wordpieces in the last window to represent the contex-tual embeddings of each word in the original sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Full Results of UD Scores</head><p>We show in <ref type="table" target="#tab_13">Tables 7, 8</ref>, 9, and 10 UDify scores evaluated on all 124 treebanks with the official CoNLL 2018 Shared Task evaluation script. For comparison, we also include the full test evaluation of UDPipe Future on the subset of 89 treebanks with a training set. We also add a column indicating the size of each treebank, i.e., the number of sentences in the training set.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of the UDify network architecture with task-specific layer attention, inputting word tokens and outputting UD annotations for each token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The unnormalized BERT layer attention weights w i contributing to layer i for each task after training. A linear change in weight scales each BERT layer exponentially due to the softmax in Equation 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of BERT attention head 4 at layer 11, comparing the attended words on an English sentence between BERT base and UDify BERT after fine-tuning. The right column indicates the attended words (keys) with respect to the words in the left column (queries). Darker lines indicate stronger attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>A plot of the difference in LAS between UDify and UDPipe Future with respect to the number of training sentences in each treebank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FigFigure 6 :</head><label>6</label><figDesc>A plot of LAS between with respect to the number of training sentences in each treebank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Vocabulary sizes of words and tags over all of</cell></row><row><cell>UD v2.3, with a total of 12,032,309 word tokens and</cell></row><row><cell>668,939 sentences.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>to evalu-MODEL CONFIGURATION UPOS FEATS LEM UAS LAS UDPipe w/o BERT 93.76 91.04 94.63 84.37 79.76 UDify Task Layer Attn 93.40 88.72 90.41 85.69 80.43 UDify Global Layer Attn 93.12 87.53 89.03 85.07 79.49 UDify Sum Layers 93.02 87.20 88.70 84.97 79.33</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>TREEBANK</cell><cell>MODEL</cell><cell>UUAS</cell></row><row><cell cols="2">English EWT (en ewt) BERT</cell><cell>65.48</cell></row><row><cell></cell><cell>BERT+finetune en ewt</cell><cell>79.67</cell></row></table><note>Test set results for zero-shot learning, i.e., no UD training annotations available. Languages that are pretrained with BERT are bolded.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>UUAS test scores calculated on the predic- tions produced by the syntactic structural probe (Hewitt and Manning, 2019) using the English EWT treebank, on the unmodified multilingual cased BERT model and the same BERT model fine-tuned on the treebank.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Moving fields to the category and series areasWe land and spill out and go our separate ways .</figDesc><table><row><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.</cell><cell></cell><cell>. . .</cell><cell>. .</cell><cell></cell><cell cols="2">Moving fields to the category and series areas . . . . . . .</cell></row><row><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell>.</cell><cell>.</cell><cell>.</cell><cell>.</cell><cell>.</cell><cell>.</cell><cell>.</cell><cell>.</cell><cell>.</cell><cell>. .</cell><cell>. .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>.</cell><cell>.</cell><cell>.</cell><cell>.</cell><cell>.</cell><cell></cell><cell>.</cell><cell></cell><cell></cell><cell>. .</cell><cell>.</cell><cell></cell></row><row><cell>.</cell><cell>.</cell><cell></cell><cell></cell><cell>. .</cell><cell>.</cell><cell>.</cell><cell></cell><cell></cell><cell>.</cell><cell>.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Thomas Müller, Ryan Cotterell, Alexander Fraser, and Hinrich Schütze. 2015. Joint lemmatization and morphological tagging with lemming. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2268-2274. Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008.</figDesc><table><row><cell>ods in Natural Language Processing, pages 349-</cell><cell cols="2">Jeremy Howard and Sebastian Ruder. 2018. Universal Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob</cell></row><row><cell>359. Toms Bergmanis and Sharon Goldwater. 2018. Con-text sensitive neural lemmatization with lematus. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-tional Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 1391-Tahira Naseem, Regina Barzilay, and Amir Globerson. 2012. Selective sharing for multilingual dependency parsing. In Proceedings of the 50th Annual Meet-</cell><cell cols="2">language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 328-339. Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daumé III. 2015. Deep unordered com-position rivals syntactic methods for text classifica-Uszkoreit, Jesse Vig. 2019. Visualizing attention in transformer-based language models. arXiv preprint arXiv:1904.02679.</cell></row><row><cell>1400. Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-drew M Dai, Rafal Jozefowicz, and Samy Ben-gio. 2016. Generating sentences from a continuous space. CoNLL 2016, page 10. Grzegorz Chrupała. 2006. Simple data-driven context-sensitive lemmatization. Procesamiento del Lenguaje Natural, 37. Yoeng-Jin Chu. 1965. On the shortest arborescence of a directed graph. Scientia Sinica, 14:1396-1400. ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 629-637. Asso-ciation for Computational Linguistics. Joakim Nivre, Mitchell Abrams,Željko Agić, and Ahrenberg. 2018. Universal dependencies 2.3. LIN-DAT/CLARIN digital library at the Institute of For-mal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep-</cell><cell cols="2">tion. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural David Vilares, Carlos Gómez-Rodríguez, and Miguel A Alonso. 2016. One model, two lan-guages: training bilingual parsers with harmonized Language Processing (Volume 1: Long Papers), vol-ume 1, pages 1681-1691. Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, treebanks. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 425-431. Fernanda Viégas, Martin Wattenberg, Greg Corrado, Robert A Wagner and Michael J Fischer. 1974. The et al. 2017. Googles multilingual neural machine string-to-string correction problem. Journal of the translation system: Enabling zero-shot translation. ACM (JACM), 21(1):168-173. Transactions of the Association for Computational Linguistics, 5:339-351. Shijie Wu and Mark Dredze. 2019. Beto, bentz, be-</cell></row><row><cell>Kevin Clark, Minh-Thang Luong, Christopher D Man-ning, and Quoc V Le. 2018. Semi-supervised se-quence modeling with cross-view training. arXiv preprint arXiv:1809.08370. resentations. In Proc. of NAACL. Cicero D Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-of-speech tagging. In Proceedings of the 31st International</cell><cell cols="2">cas: The surprising cross-lingual effectiveness of Yoon Kim, Yacine Jernite, David Sontag, and Alexan-bert. arXiv preprint arXiv:1904.09077. der M Rush. 2016. Character-aware neural language models. In Thirtieth AAAI Conference on Artificial Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Intelligence. Le, Mohammad Norouzi, Wolfgang Macherey,</cell></row><row><cell>Conference on Machine Learning (ICML-14), pages 1818-1826. Timothy Dozat and Christopher D Manning. 2016. Deep biaffine attention for neural dependency pars-ing. arXiv preprint arXiv:1611.01734. Timothy Dozat, Peng Qi, and Christopher D Manning. 2017. Stanford's graph-based neural dependency parser at the conll 2017 shared task. Proceedings of the CoNLL 2017 Shared Task: Multilingual Pars-ing from Raw Text to Universal Dependencies, pages 20-30. Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Low resource dependency parsing: parser. In Proceedings of the 53rd Annual Meet-2018 Shared Task: Multilingual Parsing from Raw volume 2, pages 845-850. 2018 UD shared task. In Proceedings of the CoNLL ral Language Processing (Volume 2: Short Papers), Milan Straka. 2018. UDPipe 2.0 prototype at CoNLL and the 7th International Joint Conference on Natu-ing of the Association for Computational Linguistics putational Linguistics. 113-123, Brussels, Belgium. Association for Com-Cross-lingual parameter sharing in a neural network Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson. 2019. Cross-lingual alignment of con-textual word embeddings, with applications to zero-shot dependency parsing. arXiv preprint arXiv:1902.09492. Damien Sileo. 2019. Understanding bert transformer: Attention isnt all you need. Towards Data Science. ing from Raw Text to Universal Dependencies, pages the CoNLL 2018 Shared Task: Multilingual Pars-ing with multi-treebank models. In Proceedings of treebanks, 34 models: Universal dependency pars-Aaron Smith, Bernd Bohnet, Miryam de Lhoneux, Joakim Nivre, Yan Shao, and Sara Stymne. 2018. 82</cell><cell cols="2">Maxim Krikun, Yuan Cao, Qin Gao, Klaus ). Macherey, et al. 2016. Google's neural ma-Dan Kondratyuk has been supported by the chine translation system: Bridging the gap between Erasmus Mundus program in Language &amp; Com-munication Technologies (LCT), and by the Ger-man Federal Ministry of Education and Re-search (BMBF) through the project DEEPLEE (01IW17001). Conference on Empirical Methods in Natural Lan-lary word representation. In Proceedings of the 2015 Compositional character models for open vocabu-and Tiago Luis. 2015. Finding function in form: coso, Ramon Fermandez, Silvio Amir, Luis Marujo, Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-human and machine translation. arXiv preprint arXiv:1609.08144. Daniel Kondratyuk, Tomáš Gavenčiak, Milan Straka, and Jan Hajič. 2018. Lemmatag: Jointly tagging and lemmatizing for morphologically rich languages with brnns. In Proceedings of the 2018 Conference Daniel Zeman, Jan Hajič, Martin Popel, Martin Pot-thast, Milan Straka, Filip Ginter, Joakim Nivre, and Slav Petrov. 2018. CoNLL 2018 shared task: Mul-on Empirical Methods in Natural Language Pro-cessing, pages 4921-4928. tilingual parsing from raw text to universal depen-dencies. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Univer-Miryam de Lhoneux, Johannes Bjerva, Isabelle Augen-stein, and Anders Søgaard. 2018. Parameter sharing 4992-4997. cal Methods in Natural Language Processing, pages In Proceedings of the 2018 Conference on Empiri-between dependency parsers for related languages. sal Dependencies, pages 1-21, Brussels, Belgium. Association for Computational Linguistics.</cell></row><row><cell>Text to Universal Dependencies, pages 197-207, Jack Edmonds. 1967. Optimum branchings. Journal Brussels, Belgium. Association for Computational of Research of the national Bureau of Standards B, 71(4):233-240. Linguistics.</cell><cell cols="2">guage Processing, pages 1520-1530. Ryan McDonald, Slav Petrov, and Keith Hall. 2011.</cell></row><row><cell></cell><cell cols="2">Multi-source transfer of delexicalized dependency</cell></row><row><cell>Yoav Goldberg. 2019. Assessing bert's syntactic abili-Milan Straka, Jana Straková, and Jan Hajič. 2019.</cell><cell cols="2">parsers. In Proceedings of the conference on empir-</cell></row><row><cell>ties. arXiv preprint arXiv:1901.05287. Evaluating Contextualized Embeddings on 54 Lan-</cell><cell cols="2">ical methods in natural language processing, pages</cell></row><row><cell>guages in POS Tagging, Lemmatization and Depen-</cell><cell cols="2">62-72. Association for Computational Linguistics.</cell></row><row><cell>dency Parsing. arXiv preprint arXiv:1908.07448.</cell><cell></cell></row><row><cell></cell><cell cols="2">Phoebe Mulcaire, Jungo Kasai, and Noah Smith.</cell></row><row><cell></cell><cell>2019.</cell><cell>Polyglot contextual representations im-</cell></row><row><cell></cell><cell cols="2">prove crosslingual transfer.</cell><cell>arXiv preprint</cell></row><row><cell></cell><cell cols="2">arXiv:1902.09697.</cell></row></table><note>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.John Hewitt and Christopher D Manning. 2019. A structural probe for finding syntax in word represen- tations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics.Nikita Kitaev and Dan Klein. 2018. Multilingual constituency parsing with self-attention and pre- training. arXiv preprint arXiv:1812.11760.Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. Bert rediscovers the classical nlp pipeline. arXiv preprint arXiv:1905.05950.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>A summary of model hyperparameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>The full test results of UDify on 124 treebanks (part 1 of 4). The SIZE column indicates the number of training sentences.</figDesc><table><row><cell>TREEBANK</cell><cell>MODEL</cell><cell>UPOS</cell><cell>UFEATS</cell><cell>LEMMAS</cell><cell>UAS</cell><cell>LAS</cell><cell>CLAS</cell><cell>MLAS</cell><cell>BLEX</cell><cell>SIZE</cell></row><row><cell>English EWT</cell><cell>UDPipe</cell><cell>96.29</cell><cell>97.10</cell><cell cols="3">98.25 89.63 86.97</cell><cell>84.02</cell><cell>79.00</cell><cell cols="2">82.36 12.5k</cell></row><row><cell></cell><cell>UDify</cell><cell>96.21</cell><cell>96.02</cell><cell cols="3">97.28 90.96 88.50</cell><cell>86.25</cell><cell>79.80</cell><cell>83.39</cell><cell>12.5k</cell></row><row><cell>English GUM</cell><cell>UDPipe</cell><cell>96.02</cell><cell>96.82</cell><cell cols="3">96.85 87.27 84.12</cell><cell>78.55</cell><cell>73.51</cell><cell>74.68</cell><cell>2.9k</cell></row><row><cell></cell><cell>UDify</cell><cell>95.44</cell><cell>94.12</cell><cell cols="3">93.15 89.14 85.73</cell><cell>83.03</cell><cell>72.55</cell><cell>74.30</cell><cell>2.9k</cell></row><row><cell>English LinES</cell><cell>UDPipe</cell><cell>96.91</cell><cell>96.31</cell><cell cols="3">96.45 84.15 79.71</cell><cell>77.44</cell><cell>71.38</cell><cell>73.22</cell><cell>2.7k</cell></row><row><cell></cell><cell>UDify</cell><cell>95.31</cell><cell>91.34</cell><cell cols="3">94.50 87.33 83.71</cell><cell>82.95</cell><cell>68.62</cell><cell>76.23</cell><cell>2.7k</cell></row><row><cell>English PUD</cell><cell>UDify</cell><cell>96.18</cell><cell>93.50</cell><cell cols="3">94.20 91.52 88.66</cell><cell>87.83</cell><cell>75.61</cell><cell>80.57</cell><cell>0</cell></row><row><cell>English ParTUT</cell><cell>UDPipe</cell><cell>96.10</cell><cell>95.51</cell><cell cols="3">97.74 90.29 87.27</cell><cell>82.58</cell><cell>76.44</cell><cell>80.33</cell><cell>1.8k</cell></row><row><cell></cell><cell>UDify</cell><cell>96.16</cell><cell>92.61</cell><cell cols="3">96.45 92.84 90.14</cell><cell>86.28</cell><cell>74.59</cell><cell>82.01</cell><cell>1.8k</cell></row><row><cell>Erzya JR</cell><cell>UDify</cell><cell>46.66</cell><cell>31.82</cell><cell cols="3">45.73 31.90 16.38</cell><cell>10.83</cell><cell>0.58</cell><cell>2.83</cell><cell>0</cell></row><row><cell>Estonian EDT</cell><cell>UDPipe</cell><cell>97.64</cell><cell>96.23</cell><cell cols="3">95.30 88.00 85.18</cell><cell>83.62</cell><cell>78.72</cell><cell>78.51</cell><cell>24.4k</cell></row><row><cell></cell><cell>UDify</cell><cell>97.44</cell><cell>95.13</cell><cell cols="3">86.56 89.53 86.67</cell><cell>85.17</cell><cell>79.20</cell><cell cols="2">69.31 24.4k</cell></row><row><cell>Faroese OFT</cell><cell>UDify</cell><cell>77.46</cell><cell>35.20</cell><cell cols="3">51.09 67.24 59.26</cell><cell>51.17</cell><cell>2.39</cell><cell>21.92</cell><cell>0</cell></row><row><cell>Finnish FTB</cell><cell>UDPipe</cell><cell>96.65</cell><cell>96.62</cell><cell cols="3">95.49 90.68 87.89</cell><cell>85.11</cell><cell>80.58</cell><cell cols="2">81.18 15.0k</cell></row><row><cell></cell><cell>UDify</cell><cell>93.80</cell><cell>90.38</cell><cell cols="3">88.80 86.37 81.40</cell><cell>81.01</cell><cell>68.16</cell><cell cols="2">70.15 15.0k</cell></row><row><cell>Finnish PUD</cell><cell>UDify</cell><cell>96.48</cell><cell>93.84</cell><cell cols="3">84.64 89.76 86.58</cell><cell>86.64</cell><cell>77.83</cell><cell>69.12</cell><cell>0</cell></row><row><cell>Finnish TDT</cell><cell>UDPipe</cell><cell>97.45</cell><cell>95.43</cell><cell cols="3">91.45 89.88 87.46</cell><cell>85.87</cell><cell>80.43</cell><cell cols="2">76.64 12.2k</cell></row><row><cell></cell><cell>UDify</cell><cell>94.43</cell><cell>90.48</cell><cell cols="3">82.89 86.42 82.03</cell><cell>82.62</cell><cell>70.89</cell><cell cols="2">63.66 12.2k</cell></row><row><cell>French GSD</cell><cell>UDPipe</cell><cell>97.63</cell><cell>97.13</cell><cell cols="3">98.35 90.65 88.06</cell><cell>84.35</cell><cell>79.76</cell><cell cols="2">82.39 14.5k</cell></row><row><cell></cell><cell>UDify</cell><cell>97.83</cell><cell>96.17</cell><cell cols="3">97.34 93.60 91.45</cell><cell>88.54</cell><cell>81.61</cell><cell>84.51</cell><cell>14.5k</cell></row><row><cell>French PUD</cell><cell>UDify</cell><cell>91.67</cell><cell>59.65</cell><cell cols="3">100.00 88.36 82.76</cell><cell>81.74</cell><cell>25.24</cell><cell>81.74</cell><cell>0</cell></row><row><cell>French ParTUT</cell><cell>UDPipe</cell><cell>96.93</cell><cell>94.43</cell><cell cols="3">95.70 92.17 89.63</cell><cell>84.62</cell><cell>75.22</cell><cell>78.07</cell><cell>804</cell></row><row><cell></cell><cell>UDify</cell><cell>96.12</cell><cell>88.36</cell><cell cols="3">93.97 90.55 88.06</cell><cell>83.19</cell><cell>63.03</cell><cell>74.03</cell><cell>804</cell></row><row><cell>French Sequoia</cell><cell>UDPipe</cell><cell>98.79</cell><cell>98.09</cell><cell cols="2">98.57 92.37</cell><cell>90.73</cell><cell>87.55</cell><cell>84.51</cell><cell>85.93</cell><cell>2.2k</cell></row><row><cell></cell><cell>UDify</cell><cell>97.89</cell><cell>88.97</cell><cell cols="2">97.15 92.53</cell><cell>90.05</cell><cell>86.67</cell><cell>67.98</cell><cell>82.52</cell><cell>2.2k</cell></row><row><cell>French Spoken</cell><cell>UDPipe</cell><cell>95.91</cell><cell>100.00</cell><cell>96.92</cell><cell cols="2">82.90 77.53</cell><cell>71.82</cell><cell>68.24</cell><cell>69.47</cell><cell>1.2k</cell></row><row><cell></cell><cell>UDify</cell><cell>96.23</cell><cell>98.67</cell><cell>96.59</cell><cell cols="2">85.24 80.01</cell><cell>75.40</cell><cell>69.74</cell><cell>72.77</cell><cell>1.2k</cell></row><row><cell>Galician CTG</cell><cell>UDPipe</cell><cell>97.84</cell><cell>99.83</cell><cell cols="3">98.58 86.44 83.82</cell><cell>78.58</cell><cell>72.46</cell><cell>77.21</cell><cell>2.3k</cell></row><row><cell></cell><cell>UDify</cell><cell>96.51</cell><cell>97.10</cell><cell cols="3">97.08 84.75 80.89</cell><cell>74.62</cell><cell>65.86</cell><cell>72.17</cell><cell>2.3k</cell></row><row><cell>Galician TreeGal</cell><cell>UDPipe</cell><cell>95.82</cell><cell>93.96</cell><cell cols="2">97.06 82.72</cell><cell>77.69</cell><cell>71.69</cell><cell>63.73</cell><cell>68.89</cell><cell>601</cell></row><row><cell></cell><cell>UDify</cell><cell>94.59</cell><cell>80.67</cell><cell cols="2">94.93 84.08</cell><cell>76.77</cell><cell>73.06</cell><cell>49.76</cell><cell>66.99</cell><cell>601</cell></row><row><cell>German GSD</cell><cell>UDPipe</cell><cell>94.48</cell><cell>90.68</cell><cell cols="3">96.80 85.53 81.07</cell><cell>76.26</cell><cell>58.82</cell><cell cols="2">72.13 13.8k</cell></row><row><cell></cell><cell>UDify</cell><cell>94.55</cell><cell>90.43</cell><cell cols="3">94.42 87.81 83.59</cell><cell>80.03</cell><cell>61.27</cell><cell>72.48</cell><cell>13.8k</cell></row><row><cell>German PUD</cell><cell>UDify</cell><cell>89.49</cell><cell>30.66</cell><cell cols="3">94.77 89.86 84.46</cell><cell>80.50</cell><cell>2.10</cell><cell>72.95</cell><cell>0</cell></row><row><cell>Gothic PROIEL</cell><cell>UDPipe</cell><cell>96.61</cell><cell>90.73</cell><cell cols="2">94.75 85.28</cell><cell>79.60</cell><cell>76.92</cell><cell>66.70</cell><cell>72.93</cell><cell>3.4k</cell></row><row><cell></cell><cell>UDify</cell><cell>95.55</cell><cell>85.97</cell><cell cols="2">80.57 85.61</cell><cell>79.37</cell><cell>76.26</cell><cell>63.09</cell><cell>58.65</cell><cell>3.4k</cell></row><row><cell>Greek GDT</cell><cell>UDPipe</cell><cell>97.98</cell><cell>94.96</cell><cell cols="3">95.82 92.10 89.79</cell><cell>85.71</cell><cell>78.60</cell><cell>79.72</cell><cell>1.7k</cell></row><row><cell></cell><cell>UDify</cell><cell>97.72</cell><cell>93.29</cell><cell cols="3">89.43 94.33 92.15</cell><cell>88.67</cell><cell>77.89</cell><cell>71.83</cell><cell>1.7k</cell></row><row><cell>Hebrew HTB</cell><cell>UDPipe</cell><cell>97.02</cell><cell>95.87</cell><cell cols="3">97.12 89.70 86.86</cell><cell>81.45</cell><cell>75.52</cell><cell>78.14</cell><cell>5.2k</cell></row><row><cell></cell><cell>UDify</cell><cell>96.94</cell><cell>93.41</cell><cell cols="3">94.15 91.63 88.11</cell><cell>83.04</cell><cell>72.55</cell><cell>74.87</cell><cell>5.2k</cell></row><row><cell>Hindi HDTB</cell><cell>UDPipe</cell><cell>97.52</cell><cell>94.15</cell><cell cols="2">98.67 94.85</cell><cell>91.83</cell><cell>88.21</cell><cell>78.49</cell><cell>86.83</cell><cell>13.3k</cell></row><row><cell></cell><cell>UDify</cell><cell>97.12</cell><cell>92.59</cell><cell cols="2">98.23 95.13</cell><cell>91.46</cell><cell>87.80</cell><cell>75.54</cell><cell cols="2">86.10 13.3k</cell></row><row><cell>Hindi PUD</cell><cell>UDify</cell><cell>87.54</cell><cell>22.81</cell><cell cols="3">100.00 71.64 58.42</cell><cell>53.03</cell><cell>3.32</cell><cell>53.03</cell><cell>0</cell></row><row><cell cols="2">Hungarian Szeged UDPipe</cell><cell>95.76</cell><cell>91.75</cell><cell cols="3">95.05 84.04 79.73</cell><cell>78.65</cell><cell>67.63</cell><cell>73.63</cell><cell>911</cell></row><row><cell></cell><cell>UDify</cell><cell>96.36</cell><cell>86.16</cell><cell cols="3">90.19 89.68 84.88</cell><cell>83.93</cell><cell>64.27</cell><cell>72.21</cell><cell>911</cell></row><row><cell>Indonesian GSD</cell><cell>UDPipe</cell><cell>93.69</cell><cell>95.58</cell><cell cols="3">99.64 85.31 78.99</cell><cell>76.76</cell><cell>67.74</cell><cell>76.38</cell><cell>4.5k</cell></row><row><cell></cell><cell>UDify</cell><cell>93.36</cell><cell>93.32</cell><cell cols="3">98.37 86.45 80.10</cell><cell>78.05</cell><cell>66.93</cell><cell>76.31</cell><cell>4.5k</cell></row><row><cell>Indonesian PUD</cell><cell>UDify</cell><cell>76.10</cell><cell>44.23</cell><cell cols="3">100.00 77.47 56.90</cell><cell>54.88</cell><cell>7.41</cell><cell>54.88</cell><cell>0</cell></row><row><cell>Irish IDT</cell><cell>UDPipe</cell><cell>92.72</cell><cell>82.43</cell><cell cols="3">90.48 80.39 72.34</cell><cell>63.48</cell><cell>46.49</cell><cell>55.32</cell><cell>567</cell></row><row><cell></cell><cell>UDify</cell><cell>90.49</cell><cell>71.84</cell><cell cols="3">81.27 80.05 69.28</cell><cell>60.02</cell><cell>34.39</cell><cell>43.07</cell><cell>567</cell></row><row><cell>Italian ISDT</cell><cell>UDPipe</cell><cell>98.39</cell><cell>98.11</cell><cell cols="3">98.66 93.49 91.54</cell><cell>87.34</cell><cell>84.28</cell><cell cols="2">85.49 13.1k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.51</cell><cell>98.01</cell><cell cols="3">97.72 95.54 93.69</cell><cell>90.40</cell><cell>86.54</cell><cell>86.70</cell><cell>13.1k</cell></row><row><cell>Italian PUD</cell><cell>UDify</cell><cell>94.73</cell><cell>58.16</cell><cell cols="3">96.08 94.18 91.76</cell><cell>90.05</cell><cell>25.55</cell><cell>83.74</cell><cell>0</cell></row><row><cell>Italian ParTUT</cell><cell>UDPipe</cell><cell>98.38</cell><cell>97.77</cell><cell>98.16</cell><cell cols="2">92.64 90.47</cell><cell>85.05</cell><cell>81.87</cell><cell>82.99</cell><cell>1.8k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.21</cell><cell>98.38</cell><cell>97.55</cell><cell cols="2">95.96 93.68</cell><cell>89.83</cell><cell>86.83</cell><cell>86.44</cell><cell>1.8k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>The full test results of UDify on 124 treebanks (part 2 of 4).</figDesc><table><row><cell>TREEBANK</cell><cell>MODEL</cell><cell>UPOS</cell><cell>UFEATS</cell><cell>LEMMAS</cell><cell>UAS</cell><cell>LAS</cell><cell>CLAS</cell><cell>MLAS</cell><cell>BLEX</cell><cell>SIZE</cell></row><row><cell>Japanese GSD</cell><cell>UDPipe</cell><cell>98.13</cell><cell>99.98</cell><cell cols="3">99.52 95.06 93.73</cell><cell>88.35</cell><cell>86.37</cell><cell>88.04</cell><cell>7.1k</cell></row><row><cell></cell><cell>UDify</cell><cell>97.08</cell><cell>99.97</cell><cell cols="3">98.80 94.37 92.08</cell><cell>86.19</cell><cell>82.99</cell><cell>85.12</cell><cell>7.1k</cell></row><row><cell>Japanese Modern</cell><cell>UDify</cell><cell>74.94</cell><cell>96.14</cell><cell cols="3">79.70 74.99 55.62</cell><cell>42.67</cell><cell>30.89</cell><cell>35.47</cell><cell>0</cell></row><row><cell>Japanese PUD</cell><cell>UDify</cell><cell>97.89</cell><cell>99.98</cell><cell cols="3">99.31 94.89 93.62</cell><cell>87.92</cell><cell>84.86</cell><cell>87.15</cell><cell>0</cell></row><row><cell>Kazakh KTB</cell><cell>UDPipe</cell><cell>55.84</cell><cell>40.40</cell><cell cols="3">63.96 53.30 33.38</cell><cell>27.06</cell><cell>4.82</cell><cell>15.10</cell><cell>32</cell></row><row><cell></cell><cell>UDify</cell><cell>85.59</cell><cell>65.49</cell><cell cols="3">77.18 74.77 63.66</cell><cell>61.84</cell><cell>34.23</cell><cell>45.51</cell><cell>32</cell></row><row><cell>Komi Zyrian IKDP</cell><cell>UDify</cell><cell>59.92</cell><cell>39.32</cell><cell cols="3">57.56 36.01 22.12</cell><cell>17.45</cell><cell>1.54</cell><cell>6.80</cell><cell>0</cell></row><row><cell>Komi Zyrian Lattice</cell><cell>UDify</cell><cell>38.57</cell><cell>29.45</cell><cell cols="3">55.33 28.85 12.99</cell><cell>10.79</cell><cell>0.72</cell><cell>3.28</cell><cell>0</cell></row><row><cell>Korean GSD</cell><cell>UDPipe</cell><cell>96.29</cell><cell>99.77</cell><cell cols="3">93.40 87.70 84.24</cell><cell>82.05</cell><cell>79.74</cell><cell>76.35</cell><cell>4.4k</cell></row><row><cell></cell><cell>UDify</cell><cell>90.56</cell><cell>99.63</cell><cell cols="3">82.84 82.74 74.26</cell><cell>71.72</cell><cell>65.94</cell><cell>57.58</cell><cell>4.4k</cell></row><row><cell>Korean Kaist</cell><cell>UDPipe</cell><cell>95.59</cell><cell>100.00</cell><cell cols="3">94.30 88.42 86.48</cell><cell>84.12</cell><cell>80.72</cell><cell>79.22</cell><cell>23.0k</cell></row><row><cell></cell><cell>UDify</cell><cell>94.67</cell><cell>99.98</cell><cell cols="3">85.89 87.57 84.52</cell><cell>82.05</cell><cell>78.27</cell><cell cols="2">68.99 23.0k</cell></row><row><cell>Korean PUD</cell><cell>UDify</cell><cell>64.43</cell><cell>60.47</cell><cell cols="3">70.47 63.57 46.89</cell><cell>45.29</cell><cell>16.26</cell><cell>30.94</cell><cell>0</cell></row><row><cell>Kurmanji MG</cell><cell>UDPipe</cell><cell>53.36</cell><cell>41.54</cell><cell cols="3">69.58 45.23 34.32</cell><cell>29.41</cell><cell>2.74</cell><cell>19.39</cell><cell>21</cell></row><row><cell></cell><cell>UDify</cell><cell>60.23</cell><cell>37.78</cell><cell cols="3">58.08 35.86 20.40</cell><cell>14.75</cell><cell>1.42</cell><cell>7.28</cell><cell>21</cell></row><row><cell>Latin ITTB</cell><cell>UDPipe</cell><cell>98.34</cell><cell>96.97</cell><cell cols="3">98.99 91.06 88.80</cell><cell>86.40</cell><cell>82.35</cell><cell cols="2">85.71 16.8k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.48</cell><cell>95.81</cell><cell cols="3">98.08 92.43 90.12</cell><cell>87.93</cell><cell>82.24</cell><cell>85.97</cell><cell>16.8k</cell></row><row><cell>Latin PROIEL</cell><cell>UDPipe</cell><cell>97.01</cell><cell>91.53</cell><cell cols="3">96.32 83.34 78.66</cell><cell>76.20</cell><cell>67.40</cell><cell>73.65</cell><cell>15.9k</cell></row><row><cell></cell><cell>UDify</cell><cell>96.79</cell><cell>89.49</cell><cell cols="3">91.79 84.85 80.52</cell><cell>77.96</cell><cell>67.18</cell><cell cols="2">71.00 15.9k</cell></row><row><cell>Latin Perseus</cell><cell>UDPipe</cell><cell>88.40</cell><cell>79.10</cell><cell>81.45</cell><cell cols="2">71.20 61.28</cell><cell>56.32</cell><cell>41.58</cell><cell>45.09</cell><cell>1.3k</cell></row><row><cell></cell><cell>UDify</cell><cell>90.96</cell><cell>82.09</cell><cell>81.08</cell><cell cols="2">78.33 69.60</cell><cell>65.95</cell><cell>50.26</cell><cell>51.33</cell><cell>1.3k</cell></row><row><cell>Latvian LVTB</cell><cell>UDPipe</cell><cell>96.11</cell><cell>93.01</cell><cell cols="3">95.46 87.20 83.35</cell><cell>80.90</cell><cell>71.92</cell><cell>76.64</cell><cell>7.2k</cell></row><row><cell></cell><cell>UDify</cell><cell>96.02</cell><cell>89.78</cell><cell cols="3">91.00 89.33 85.09</cell><cell>82.34</cell><cell>69.51</cell><cell>72.58</cell><cell>7.2k</cell></row><row><cell>Lithuanian HSE</cell><cell>UDPipe</cell><cell>81.70</cell><cell>60.47</cell><cell>76.89</cell><cell cols="2">51.98 42.17</cell><cell>38.93</cell><cell>18.17</cell><cell>28.70</cell><cell>154</cell></row><row><cell></cell><cell>UDify</cell><cell>90.47</cell><cell>70.00</cell><cell>67.17</cell><cell cols="2">79.06 69.34</cell><cell>66.00</cell><cell>36.21</cell><cell>36.35</cell><cell>154</cell></row><row><cell>Maltese MUDT</cell><cell>UDPipe</cell><cell>95.99</cell><cell>100.00</cell><cell cols="3">100.00 84.65 79.71</cell><cell>71.49</cell><cell>66.75</cell><cell>71.49</cell><cell>1.1k</cell></row><row><cell></cell><cell>UDify</cell><cell>91.98</cell><cell>99.89</cell><cell cols="3">100.00 83.07 75.56</cell><cell>65.08</cell><cell>58.14</cell><cell>65.08</cell><cell>1.1k</cell></row><row><cell>Marathi UFAL</cell><cell>UDPipe</cell><cell>80.10</cell><cell>67.23</cell><cell cols="3">81.31 70.63 61.41</cell><cell>57.44</cell><cell>29.34</cell><cell>45.87</cell><cell>374</cell></row><row><cell></cell><cell>UDify</cell><cell>88.59</cell><cell>59.22</cell><cell cols="3">72.82 79.37 67.72</cell><cell>60.13</cell><cell>21.71</cell><cell>39.25</cell><cell>374</cell></row><row><cell>Naija NSC</cell><cell>UDify</cell><cell>55.44</cell><cell>51.32</cell><cell cols="3">97.03 45.75 32.16</cell><cell>31.62</cell><cell>4.73</cell><cell>29.33</cell><cell>0</cell></row><row><cell>North Sami Giella</cell><cell>UDPipe</cell><cell>92.54</cell><cell>90.03</cell><cell cols="3">88.31 78.30 73.49</cell><cell>70.94</cell><cell>62.40</cell><cell>61.45</cell><cell>2.3k</cell></row><row><cell></cell><cell>UDify</cell><cell>90.21</cell><cell>83.55</cell><cell cols="3">71.50 74.30 67.13</cell><cell>64.41</cell><cell>51.20</cell><cell>40.63</cell><cell>2.3k</cell></row><row><cell>Norwegian Bokmaal</cell><cell>UDPipe</cell><cell>98.31</cell><cell>97.14</cell><cell cols="3">98.64 92.39 90.49</cell><cell>88.18</cell><cell>84.06</cell><cell cols="2">86.53 15.7k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.18</cell><cell>96.36</cell><cell cols="3">97.33 93.97 92.18</cell><cell>90.40</cell><cell>85.02</cell><cell>87.13</cell><cell>15.7k</cell></row><row><cell>Norwegian Nynorsk</cell><cell>UDPipe</cell><cell>98.14</cell><cell>97.02</cell><cell cols="3">98.18 92.09 90.01</cell><cell>87.68</cell><cell>82.97</cell><cell cols="2">85.47 14.2k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.14</cell><cell>96.55</cell><cell cols="3">97.18 94.34 92.37</cell><cell>90.39</cell><cell>85.01</cell><cell>86.71</cell><cell>14.2k</cell></row><row><cell>Norwegian NynorskLIA</cell><cell>UDPipe</cell><cell>89.59</cell><cell>86.13</cell><cell cols="3">93.93 68.08 60.07</cell><cell>54.89</cell><cell>44.47</cell><cell>50.98</cell><cell>340</cell></row><row><cell></cell><cell>UDify</cell><cell>95.01</cell><cell>93.36</cell><cell cols="3">96.13 75.40 69.60</cell><cell>65.33</cell><cell>56.90</cell><cell>62.27</cell><cell>340</cell></row><row><cell cols="2">Old Church Slavonic PROIEL UDPipe</cell><cell>96.91</cell><cell>90.66</cell><cell cols="3">93.11 89.66 85.04</cell><cell>83.41</cell><cell>73.63</cell><cell>77.81</cell><cell>4.1k</cell></row><row><cell></cell><cell>UDify</cell><cell>84.23</cell><cell>71.30</cell><cell cols="3">65.70 76.71 66.67</cell><cell>64.10</cell><cell>46.25</cell><cell>43.88</cell><cell>4.1k</cell></row><row><cell>Old French SRCMF</cell><cell>UDPipe</cell><cell>96.09</cell><cell>97.81</cell><cell cols="3">100.00 91.74 86.83</cell><cell>83.85</cell><cell>79.91</cell><cell>83.85</cell><cell>13.9k</cell></row><row><cell></cell><cell>UDify</cell><cell>95.73</cell><cell>96.98</cell><cell cols="2">100.00 91.74</cell><cell>86.65</cell><cell>83.49</cell><cell>78.85</cell><cell cols="2">83.49 13.9k</cell></row><row><cell>Persian Seraji</cell><cell>UDPipe</cell><cell>97.75</cell><cell>97.78</cell><cell cols="3">97.44 90.05 86.66</cell><cell>83.26</cell><cell>81.23</cell><cell>80.93</cell><cell>4.8k</cell></row><row><cell></cell><cell>UDify</cell><cell>96.22</cell><cell>94.73</cell><cell cols="3">92.55 89.59 85.84</cell><cell>81.98</cell><cell>76.65</cell><cell>74.74</cell><cell>4.8k</cell></row><row><cell>Polish LFG</cell><cell>UDPipe</cell><cell>98.80</cell><cell>95.49</cell><cell cols="2">97.54 96.58</cell><cell>94.76</cell><cell>93.01</cell><cell>87.04</cell><cell>90.26</cell><cell>13.8k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.80</cell><cell>87.71</cell><cell cols="2">94.04 96.67</cell><cell>94.58</cell><cell>93.03</cell><cell>76.50</cell><cell cols="2">85.15 13.8k</cell></row><row><cell>Polish SZ</cell><cell>UDPipe</cell><cell>98.34</cell><cell>93.04</cell><cell cols="2">97.16 93.39</cell><cell>91.24</cell><cell>89.39</cell><cell>81.06</cell><cell>85.99</cell><cell>6.1k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.36</cell><cell>67.11</cell><cell cols="2">93.92 93.67</cell><cell>89.20</cell><cell>87.31</cell><cell>48.47</cell><cell>80.24</cell><cell>6.1k</cell></row><row><cell>Portuguese Bosque</cell><cell>UDPipe</cell><cell>97.07</cell><cell>96.40</cell><cell cols="2">98.46 91.36</cell><cell>89.04</cell><cell>85.19</cell><cell>76.67</cell><cell>83.06</cell><cell>8.3k</cell></row><row><cell></cell><cell>UDify</cell><cell>97.10</cell><cell>89.70</cell><cell cols="2">91.60 91.37</cell><cell>87.84</cell><cell>84.13</cell><cell>69.09</cell><cell>78.64</cell><cell>8.3k</cell></row><row><cell>Portuguese GSD</cell><cell>UDPipe</cell><cell>98.31</cell><cell>99.92</cell><cell cols="3">99.30 93.01 91.63</cell><cell>87.67</cell><cell>85.96</cell><cell>86.94</cell><cell>9.7k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.04</cell><cell>95.75</cell><cell cols="3">98.95 94.22 92.54</cell><cell>89.37</cell><cell>82.32</cell><cell>87.90</cell><cell>9.7k</cell></row><row><cell>Portuguese PUD</cell><cell>UDify</cell><cell>90.14</cell><cell>51.16</cell><cell cols="3">99.79 87.02 80.17</cell><cell>74.10</cell><cell>17.51</cell><cell>74.10</cell><cell>0</cell></row><row><cell>Romanian Nonstandard</cell><cell>UDPipe</cell><cell>96.68</cell><cell>90.88</cell><cell cols="3">94.78 89.12 84.20</cell><cell>78.91</cell><cell>65.93</cell><cell>73.44</cell><cell>8.0k</cell></row><row><cell></cell><cell>UDify</cell><cell>96.83</cell><cell>88.89</cell><cell cols="3">89.33 90.36 85.26</cell><cell>80.41</cell><cell>64.68</cell><cell>68.11</cell><cell>8.0k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :Table 10 :</head><label>910</label><figDesc>The full test results of UDify on 124 treebanks (part 3 of 4). The full test results of UDify on 124 treebanks (part 4 of 4).</figDesc><table><row><cell>TREEBANK</cell><cell>MODEL</cell><cell>UPOS</cell><cell>UFEATS</cell><cell>LEMMAS</cell><cell>UAS</cell><cell>LAS</cell><cell>CLAS</cell><cell>MLAS</cell><cell>BLEX</cell><cell>SIZE</cell></row><row><cell>Romanian RRT</cell><cell>UDPipe</cell><cell>97.96</cell><cell>97.53</cell><cell cols="3">98.41 91.31 86.74</cell><cell>82.57</cell><cell>79.02</cell><cell>81.09</cell><cell>8.0k</cell></row><row><cell></cell><cell>UDify</cell><cell>97.73</cell><cell>96.12</cell><cell cols="3">95.84 93.16 88.56</cell><cell>84.87</cell><cell>79.20</cell><cell>79.92</cell><cell>8.0k</cell></row><row><cell>Russian GSD</cell><cell>UDPipe</cell><cell>97.10</cell><cell>92.66</cell><cell cols="3">97.37 88.15 84.37</cell><cell>82.66</cell><cell>74.07</cell><cell>80.03</cell><cell>3.9k</cell></row><row><cell></cell><cell>UDify</cell><cell>96.91</cell><cell>87.45</cell><cell cols="3">77.73 90.71 86.03</cell><cell>84.51</cell><cell>67.24</cell><cell>62.08</cell><cell>3.9k</cell></row><row><cell>Russian PUD</cell><cell>UDify</cell><cell>93.06</cell><cell>63.60</cell><cell cols="3">77.93 93.51 87.14</cell><cell>83.96</cell><cell>37.25</cell><cell>61.86</cell><cell>0</cell></row><row><cell>Russian SynTagRus</cell><cell>UDPipe</cell><cell>99.12</cell><cell>97.57</cell><cell cols="3">98.53 93.80 92.32</cell><cell>90.85</cell><cell>87.91</cell><cell>89.17</cell><cell>48.8k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.97</cell><cell>96.29</cell><cell cols="3">94.47 94.83 93.13</cell><cell>91.87</cell><cell>86.91</cell><cell cols="2">85.44 48.8k</cell></row><row><cell>Russian Taiga</cell><cell>UDPipe</cell><cell>93.18</cell><cell>82.87</cell><cell cols="3">89.99 75.45 69.11</cell><cell>65.31</cell><cell>48.81</cell><cell>57.21</cell><cell>881</cell></row><row><cell></cell><cell>UDify</cell><cell>95.39</cell><cell>88.47</cell><cell cols="3">90.19 84.02 77.80</cell><cell>75.12</cell><cell>59.71</cell><cell>65.15</cell><cell>881</cell></row><row><cell>Sanskrit UFAL</cell><cell>UDify</cell><cell>37.33</cell><cell>17.63</cell><cell cols="3">37.38 40.21 18.56</cell><cell>15.38</cell><cell>0.85</cell><cell>4.12</cell><cell>0</cell></row><row><cell>Serbian SET</cell><cell>UDPipe</cell><cell>98.33</cell><cell>94.35</cell><cell cols="3">97.36 92.70 89.27</cell><cell>87.08</cell><cell>79.14</cell><cell>84.18</cell><cell>2.9k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.30</cell><cell>92.22</cell><cell cols="3">95.86 95.68 91.95</cell><cell>90.30</cell><cell>78.45</cell><cell>84.93</cell><cell>2.9k</cell></row><row><cell>Slovak SNK</cell><cell>UDPipe</cell><cell>96.83</cell><cell>90.82</cell><cell cols="3">96.40 89.82 86.90</cell><cell>84.81</cell><cell>74.00</cell><cell>81.37</cell><cell>8.5k</cell></row><row><cell></cell><cell>UDify</cell><cell>97.46</cell><cell>89.30</cell><cell cols="3">93.80 95.92 93.87</cell><cell>92.86</cell><cell>77.33</cell><cell>85.12</cell><cell>8.5k</cell></row><row><cell>Slovenian SSJ</cell><cell>UDPipe</cell><cell>98.61</cell><cell>95.92</cell><cell cols="3">98.25 92.96 91.16</cell><cell>88.76</cell><cell>83.85</cell><cell>86.89</cell><cell>6.5k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.73</cell><cell>93.44</cell><cell cols="3">96.50 94.74 93.07</cell><cell>90.94</cell><cell>81.55</cell><cell>86.38</cell><cell>6.5k</cell></row><row><cell>Slovenian SST</cell><cell>UDPipe</cell><cell>93.79</cell><cell>86.28</cell><cell>95.17</cell><cell cols="2">73.51 67.51</cell><cell>63.46</cell><cell>52.67</cell><cell>60.32</cell><cell>2.1k</cell></row><row><cell></cell><cell>UDify</cell><cell>95.40</cell><cell>89.81</cell><cell>95.15</cell><cell cols="2">80.37 75.03</cell><cell>71.19</cell><cell>61.32</cell><cell>67.24</cell><cell>2.1k</cell></row><row><cell>Spanish AnCora</cell><cell>UDPipe</cell><cell>98.91</cell><cell>98.49</cell><cell cols="3">99.17 92.34 90.26</cell><cell>86.39</cell><cell>83.97</cell><cell>85.51</cell><cell>14.3k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.53</cell><cell>97.89</cell><cell cols="3">98.07 92.99 90.50</cell><cell>87.26</cell><cell>83.43</cell><cell cols="2">84.85 14.3k</cell></row><row><cell>Spanish GSD</cell><cell>UDPipe</cell><cell>96.85</cell><cell>97.09</cell><cell cols="2">98.97 90.71</cell><cell>88.03</cell><cell>82.85</cell><cell>75.98</cell><cell>81.47</cell><cell>14.2k</cell></row><row><cell></cell><cell>UDify</cell><cell>95.91</cell><cell>95.08</cell><cell cols="2">96.52 90.82</cell><cell>87.23</cell><cell>82.83</cell><cell>72.47</cell><cell cols="2">78.08 14.2k</cell></row><row><cell>Spanish PUD</cell><cell>UDify</cell><cell>88.98</cell><cell>54.58</cell><cell cols="3">100.00 90.45 83.08</cell><cell>77.42</cell><cell>18.06</cell><cell>77.42</cell><cell>0</cell></row><row><cell>Swedish LinES</cell><cell>UDPipe</cell><cell>96.78</cell><cell>89.43</cell><cell cols="3">97.03 86.07 81.86</cell><cell>80.32</cell><cell>66.48</cell><cell>77.38</cell><cell>2.7k</cell></row><row><cell></cell><cell>UDify</cell><cell>96.85</cell><cell>87.24</cell><cell cols="3">92.70 88.77 85.49</cell><cell>85.61</cell><cell>66.99</cell><cell>77.62</cell><cell>2.7k</cell></row><row><cell>Swedish PUD</cell><cell>UDify</cell><cell>96.36</cell><cell>80.04</cell><cell cols="3">88.81 89.17 86.10</cell><cell>85.25</cell><cell>57.12</cell><cell>72.92</cell><cell>0</cell></row><row><cell cols="2">Swedish Sign Language SSLC UDPipe</cell><cell>68.09</cell><cell>100.00</cell><cell cols="3">100.00 50.35 37.94</cell><cell>39.51</cell><cell>30.96</cell><cell>39.51</cell><cell>88</cell></row><row><cell></cell><cell>UDify</cell><cell>63.48</cell><cell>96.10</cell><cell cols="3">100.00 40.43 26.95</cell><cell>30.12</cell><cell>23.29</cell><cell>30.12</cell><cell>88</cell></row><row><cell>Swedish Talbanken</cell><cell>UDPipe</cell><cell>97.94</cell><cell>96.86</cell><cell cols="3">98.01 89.63 86.61</cell><cell>84.45</cell><cell>79.67</cell><cell>82.26</cell><cell>4.3k</cell></row><row><cell></cell><cell>UDify</cell><cell>98.11</cell><cell>95.92</cell><cell cols="3">95.50 91.91 89.03</cell><cell>87.26</cell><cell>80.72</cell><cell>81.31</cell><cell>4.3k</cell></row><row><cell>Tagalog TRG</cell><cell>UDify</cell><cell>60.62</cell><cell>35.62</cell><cell cols="3">73.63 64.04 40.07</cell><cell>36.84</cell><cell>0.00</cell><cell>13.16</cell><cell>0</cell></row><row><cell>Tamil TTB</cell><cell>UDPipe</cell><cell>91.05</cell><cell>87.28</cell><cell cols="3">93.92 74.11 66.37</cell><cell>63.71</cell><cell>55.31</cell><cell>59.58</cell><cell>401</cell></row><row><cell></cell><cell>UDify</cell><cell>91.50</cell><cell>83.21</cell><cell cols="3">80.84 79.34 71.29</cell><cell>69.10</cell><cell>53.62</cell><cell>54.84</cell><cell>401</cell></row><row><cell>Telugu MTG</cell><cell>UDPipe</cell><cell>93.07</cell><cell>99.03</cell><cell>100.00</cell><cell>91.26</cell><cell>85.02</cell><cell>81.76</cell><cell>77.75</cell><cell>81.76</cell><cell>1.1k</cell></row><row><cell></cell><cell>UDify</cell><cell>93.48</cell><cell>99.31</cell><cell cols="2">100.00 92.23</cell><cell>83.91</cell><cell>79.92</cell><cell>76.10</cell><cell>79.92</cell><cell>1.1k</cell></row><row><cell>Thai PUD</cell><cell>UDify</cell><cell>56.78</cell><cell>62.48</cell><cell cols="3">100.00 49.05 26.06</cell><cell>18.42</cell><cell>3.77</cell><cell>18.42</cell><cell>0</cell></row><row><cell>Turkish IMST</cell><cell>UDPipe</cell><cell>96.01</cell><cell>92.55</cell><cell cols="2">96.01 74.19</cell><cell>67.56</cell><cell>63.83</cell><cell>56.96</cell><cell>61.37</cell><cell>3.7k</cell></row><row><cell></cell><cell>UDify</cell><cell>94.29</cell><cell>84.49</cell><cell cols="2">87.71 74.56</cell><cell>67.44</cell><cell>63.87</cell><cell>49.42</cell><cell>54.10</cell><cell>3.7k</cell></row><row><cell>Turkish PUD</cell><cell>UDify</cell><cell>77.34</cell><cell>24.59</cell><cell cols="3">84.31 67.68 46.07</cell><cell>39.95</cell><cell>2.61</cell><cell>32.50</cell><cell>0</cell></row><row><cell>Ukrainian IU</cell><cell>UDPipe</cell><cell>97.59</cell><cell>92.66</cell><cell cols="3">97.23 88.29 85.25</cell><cell>81.90</cell><cell>73.81</cell><cell>79.10</cell><cell>5.3k</cell></row><row><cell></cell><cell>UDify</cell><cell>97.71</cell><cell>88.63</cell><cell cols="3">94.00 92.83 90.30</cell><cell>88.15</cell><cell>72.93</cell><cell>81.04</cell><cell>5.3k</cell></row><row><cell>Upper Sorbian UFAL</cell><cell>UDPipe</cell><cell>62.93</cell><cell>41.10</cell><cell cols="3">68.68 45.58 34.54</cell><cell>27.18</cell><cell>3.37</cell><cell>16.65</cell><cell>24</cell></row><row><cell></cell><cell>UDify</cell><cell>84.87</cell><cell>48.84</cell><cell cols="3">72.68 71.55 62.82</cell><cell>56.04</cell><cell>16.19</cell><cell>37.89</cell><cell>24</cell></row><row><cell>Urdu UDTB</cell><cell>UDPipe</cell><cell>93.66</cell><cell>81.92</cell><cell>97.40</cell><cell cols="2">87.50 81.62</cell><cell>75.20</cell><cell>55.02</cell><cell>73.07</cell><cell>4.0k</cell></row><row><cell></cell><cell>UDify</cell><cell>94.37</cell><cell>82.80</cell><cell>96.68</cell><cell cols="2">88.43 82.84</cell><cell>77.00</cell><cell>56.70</cell><cell>73.97</cell><cell>4.0k</cell></row><row><cell>Uyghur UDT</cell><cell>UDPipe</cell><cell>89.87</cell><cell>88.30</cell><cell cols="3">95.31 78.46 67.09</cell><cell>60.85</cell><cell>47.84</cell><cell>57.08</cell><cell>1.7k</cell></row><row><cell></cell><cell>UDify</cell><cell>75.88</cell><cell>70.80</cell><cell cols="3">79.70 65.89 48.80</cell><cell>38.95</cell><cell>21.75</cell><cell>31.31</cell><cell>1.7k</cell></row><row><cell>Vietnamese VTB</cell><cell>UDPipe</cell><cell>89.68</cell><cell>99.72</cell><cell cols="3">99.55 70.38 62.56</cell><cell>60.03</cell><cell>55.56</cell><cell>59.54</cell><cell>1.4k</cell></row><row><cell></cell><cell>UDify</cell><cell>91.29</cell><cell>99.58</cell><cell cols="3">99.21 74.11 66.00</cell><cell>63.34</cell><cell>58.71</cell><cell>62.61</cell><cell>1.4k</cell></row><row><cell>Warlpiri UFAL</cell><cell>UDify</cell><cell>33.44</cell><cell>18.15</cell><cell cols="2">39.17 21.66</cell><cell>7.96</cell><cell>7.49</cell><cell>0.00</cell><cell>0.88</cell><cell>0</cell></row><row><cell>Yoruba YTB</cell><cell>UDify</cell><cell>50.86</cell><cell>78.32</cell><cell cols="3">85.56 37.62 19.09</cell><cell>16.56</cell><cell>6.30</cell><cell>12.15</cell><cell>0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/google-research/ bert/blob/master/multilingual.md</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We found last, max, or average pooling of the wordpieces were not any better or worse for evaluation. Kitaev and Klein (2018) report similar results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We train on a GTX 1080 Ti for approximately 25 days. See Appendix A.1 for more details</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://universaldependencies.org/ conll18/evaluation.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Advantages of bilinguals over monolinguals in learning a third language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Abu-Rabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Sanitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bilingual Research Journal</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="199" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Wasi Uddin Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00570</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Many languages, one parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="431" to="444" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10464</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Meth</title>
		<meeting>the 2015 Conference on Empirical Meth</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
