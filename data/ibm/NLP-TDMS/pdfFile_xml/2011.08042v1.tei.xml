<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mixing ADAM and SGD: a Combined Optimization Method</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Landro</surname></persName>
							<email>nlandro@uninsubria.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Insubria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignazio</forename><surname>Gallo</surname></persName>
							<email>ignazio.gallo@uninsubria.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Insubria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><forename type="middle">La</forename><surname>Grassa</surname></persName>
							<email>rlagrassa@uninsubria.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Insubria</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mixing ADAM and SGD: a Combined Optimization Method</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optimization methods (optimizers) get special attention for the efficient training of neural networks in the field of deep learning. In literature there are many papers that compare neural models trained with the use of different optimizers. Each paper demonstrates that for a particular problem an optimizer is better than the others but as the problem changes this type of result is no longer valid and we have to start from scratch. In our paper we propose to use the combination of two very different optimizers but when used simultaneously they can overcome the performances of the single optimizers in very different problems. We propose a new optimizer called MAS (Mixing ADAM and SGD) that integrates SGD and ADAM simultaneously by weighing the contributions of both through the assignment of constant weights. Rather than trying to improve SGD or ADAM we exploit both at the same time by taking the best of both. We have conducted several experiments on images and text document classification, using various CNNs, and we demonstrated by experiments that the proposed MAS optimizer produces better performance than the single SGD or ADAM optimizers. The source code and all the results of the experiments are available online at the following link</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Stochastic Gradient Descent <ref type="bibr" target="#b0">[1]</ref> (SGD) is the dominant method for solving optimization problems. SGD iteratively updates the model parameters by moving them in the direction of the negative gradient calculated on a mini-batch scaled by the step length, typically referred to as the learning rate. It is necessary to decay this learning rate as the algorithm proceeds to ensure convergence. Manually adjusting the learning rate decay in SGD is not easy. To address this problem, several methods have been proposed that automatically reduce the learning rate. The basic intuition behind these approaches is to adaptively tune the learning rate based only on recent gradients; therefore, limiting the reliance on the update to only a few past gradients. ADAptive Moment estimation <ref type="bibr" target="#b1">[2]</ref> (ADAM) is one of several methods based on this update mechanism <ref type="bibr" target="#b2">[3]</ref>. On the other hand, adaptive optimization methods such as ADAM, even though they have been proposed to achieve a rapid training process, are observed to generalize poorly with respect to SGD or even fail to converge due to unstable and extreme learning rates <ref type="bibr" target="#b3">[4]</ref>. To try to overcome the problems of both of these types of optimizers and at the same time try to exploit their advantages, we propose an optimizer that combines them in a new optimizer. As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, the basic idea of the MAS optimizer here proposed, is to combine two different known optimizers and automatically go quickly towards the direction of both on the surface of the loss function when the two optimizers agree (see geometric example in <ref type="figure">Figure 2a</ref>). While when the two optimizers used in the combination do not agree, our solution always goes towards the predominant direction between the two but slowing down the speed (see example of <ref type="figure">Figure 2b</ref>).</p><p>Analyzing the literature there are many papers that compare neural models trained with the use of different optimizers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> or that propose modifications for existing optimizers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, always aimed at improving the results on a subset of problems. Each paper demonstrates that an optimizer is better than the others but as the problem changes this type of result is no longer valid and we have to start from scratch. In our paper we propose to combine simultaneously two different optimizers like SGD and ADAM, to overcome the performances of the single optimizers in very different problems.</p><p>Contributions. In light of the above, we set out the main contributions of our paper.</p><p>• We demonstrate experimentally that the combination of different optimizers in a new optimizer that incorporates them leads to a better generalization capacity in different contexts.</p><p>• We apply the proposed solution to well-known computer vision and text analysis problems and in both these domains, we obtain excellent results, demonstrating that our solution exceeds the generalization capacity of the starting ADAM and SGD optimizers.</p><p>• We open the way to this new type of optimizers with which it will be possible to exploit the positive aspects of many existing optimizers, combining them with each others to build new and more efficient optimizers.</p><p>• To facilitate the understanding of the MAS optimizer and to allow other researchers to be able to run the experiments and extend this idea, we release the source code and setups of the experiments at the following URL <ref type="bibr" target="#b11">[12]</ref> 2 Related Work</p><p>In the literature, there aren't many papers that try to combine different optimizers together. In this section, we report some of the more recent papers that in some ways use different optimizers in the same learning process.</p><p>In <ref type="bibr" target="#b12">[13]</ref> the authors investigate a hybrid strategy, called SWATS, which starts training with an adaptive optimization method and switches to SGD when appropriate. This idea starts from the observation that despite superior training results, adaptive optimization methods such as ADAM generalize poorly compared to SGD because they tend to work well in the early part of the training, but are overtaken by SGD in the later stages of training. In concrete terms, SWATS is a simple strategy that goes from Adam to SGD when an activation condition is met. The experimental results obtained in this paper are not so different from ADAM or SGD used individually, so the authors concluded that using SGD with perfect parameters is the best idea. In our proposal, we want to combine two well-known optimizers to create a new one that uses simultaneously two different optimizers from the beginning to the end of the training process.</p><p>ESGD is a population-based Evolutionary Stochastic Gradient Descent framework for optimizing deep neural networks <ref type="bibr" target="#b13">[14]</ref>. In this approach, individuals in the population optimized with various SGD-based optimizers using distinct hyper-parameters are considered competing species in a context of coevolution. The authors experimented with optimizer pools consisting of SGD and ADAM variants where it is often observed that ADAM tends to be aggressive early on, but stabilizes quickly, while SGD starts slowly but can reach a better local minimum. ESGD can automatically choose the appropriate optimizers and their hyper-parameters based on the fitness value during the evolution process so that the merits of SGD and ADAM can be combined to seek a better local optimal solution to the problem of interest. In the method we propose, we do not need another approach, such as the evolutionary one, to decide which optimizer to use and with which hyper-parameters, but it is the same approach that decides at each step what is the contribution of SGD and that of ADAM.</p><p>In this paper, we also compare our MAS optimizer with ADAMW [15] <ref type="bibr" target="#b15">[16]</ref>, which is a version of ADAM in which <ref type="figure">Figure 2</ref>: Graphical representation of the basic idea for the proposed MAS optimizer. In (a), if the two translations w 1 and w 2 obtained from two different optimizers are similar, then the resulting translation w 1 + w 2 is boosted. In (b), if the translations w 1 and w 2 go in two different directions, then the resulting translation is smaller. We also use two hyper-parameters λ 1 and λ 2 to weigh the contribution of the two optimizers.</p><p>weight decay is decoupled from L 2 regularization. This optimizer offers good generalization performance, especially for text analysis, and since we also perform some experimental tests on text classification, then we also compare our optimizer with ADAMW. In fact, ADAMW is often used with BERT <ref type="bibr" target="#b16">[17]</ref> applied to well-known datasets for text classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>return v k 15: end function 16: for batches do 17:</p><formula xml:id="formula_0">w k+1 = w k − ηs · ∆SGD(w k , ∇, γ, µ, d, nesterov)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18: end for 3 Preliminaries</head><p>Training neural networks is equivalent to solving the following optimization problem:</p><formula xml:id="formula_1">min w∈R n L(w)<label>(1)</label></formula><p>where L is a loss function and w are the weights. The iterations of an SGD [1] optimizer can be described as:</p><formula xml:id="formula_2">w k+1 = w k − η · ∇L(w)<label>(2)</label></formula><p>where w k denotes the weights w at the k-th iteration, η denote the learning rate and ∇L(w) denotes the stochastic gradient calculated at w k . To propose a stochastic gradient calculated as generic as possible, we introduce the weight decay <ref type="bibr" target="#b17">[18]</ref> strategy, often used in many SGD implementations. The weight decay can be seen as a modification of the ∇L(w) gradient, and in particular, we can describe it as follows:</p><formula xml:id="formula_3">∇L(w k ) = ∇L(w k ) + w k · γ (3)</formula><p>where γ is a small scalar called weight decay. We can observe that if the weight decay γ is equal to zero then ∇L(w) = ∇L(w). Based on the above, we can generalize the Eq. 2 to the following one that include weight decay:</p><formula xml:id="formula_4">w k+1 = w k − η · ∇L(w)<label>(4)</label></formula><p>The SGD algorithm described up to here is usually used in combination with momentum, and in this case, we refer to it as SGD(M) <ref type="bibr" target="#b18">[19]</ref> (Stochastic Gradient Descend with Momentum). SGD(M) almost always works better and faster than SGD because the momentum helps accelerate the gradient vectors in the right direction, thus leading to faster convergence. The iterations of SGD(M) can be described as follows:</p><formula xml:id="formula_5">v k = µ · v k−1 + ∇L(w) (5) w k+1 = w k − η · v k<label>(6)</label></formula><p>where µ ∈ [0, 1) is the momentum parameter and for k = 0, v 0 is initialized to 0. The simpler methods of momentum have an associated damping coefficient <ref type="bibr" target="#b19">[20]</ref>, which controls the rate at which the momentum vector decays. The dampening coefficient changes the momentum as follow:</p><formula xml:id="formula_6">v d k = m · v k−1 + ∇L(w) · (1 − d)<label>(7)</label></formula><p>where 0 ≤ d &lt; 1, is the dampening value, so the final SGD with momentum and dampening coefficients can be seen as follow:</p><formula xml:id="formula_7">w k+1 = w k − η · v d k<label>(8)</label></formula><p>Nesterov momentum <ref type="bibr" target="#b20">[21]</ref> is an extension of the moment method that approximates the future position of the parameters that takes into account the movement. The SGD with nesterov transforms again the v k of Eq. 5, more precisely:</p><formula xml:id="formula_8">v n k = ∇L(w) + v d k · m (9) w k+1 = w k − η · v n k<label>(10)</label></formula><p>The complete SGD algorithm, used in this paper, is showed in Alg. 1. ADAM <ref type="bibr" target="#b1">[2]</ref> (ADAptive Moment estimation) optimization algorithm is an extension to SGD that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. ADAM's equation for updating the weights of a neural network by iterating over the training data can be represented as follows:</p><formula xml:id="formula_9">m k = β 1 · m k−1 + (1 − β 1 ) · ∇L(w k ) (11) v a k = β 2 · v k−1 + (1 − β 2 ) · ∇L(w k ) 2<label>(12)</label></formula><formula xml:id="formula_10">w k+1 = w k − η · √ 1 − β 2 1 − β 1 · m k v a k +<label>(13)</label></formula><p>where m k and v a k are estimates of the first moment (the mean) and the second moment (the non-centered variance) of the gradients respectively, hence the name of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 ADaptive Moment Estimation (ADAM)</head><p>Input: the weights w k , learing rate η, weight decay γ, β1, β2, , boolean amsgrad </p><formula xml:id="formula_11">1: m0 = 0 2: v a 0 = 0 3: v0 = 0 4: function ∆ADAM(w k , ∇, η, γ, β1, β2, , amsgrad) 5: ∇ = ∇ + w k · γ 6: m k = m k−1 · β1 + ∇ · (1 − β1) 7: v a k = v a k−1 · β2 + ∇ · ∇ · (1 − β2) 8: if amsgrad = T rue then 9: v k = max( v k−1 , v a k ) 10: denom = √ v k √ 1−β 2 + 11: else 12: denom = v</formula><formula xml:id="formula_12">d k , ηa = ∆ADAM(w k , ∇, η, γ, β1, β2, , amsgrad) 20: w k+1 = w k − ηa · d k 21</formula><p>: end for β 1 , β 2 and are three new introduced hyper-parameters of the algorithm. AMSGrad <ref type="bibr" target="#b21">[22]</ref> is a stochastic optimization method that seeks to fix a convergence issue with Adam based optimizers. AMSGrad uses the maximum of past squared gradients v k−1 rather than the exponential average to update the parameters:</p><formula xml:id="formula_13">v k = max( v k−1 , v a k )<label>(14)</label></formula><formula xml:id="formula_14">w k+1 = w k − η · √ 1 − β 2 1 − β 1 · m k √ v k +<label>(15)</label></formula><p>The complete ADAM algorithm, used in this paper, is showed in Alg. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Approach</head><p>In this section, we develop the proposed new optimization method called MAS. Our goal is to propose a strategy that automatically combines the advantages of an adaptive method like ADAM, with the advantages of SGD, throughout the entire learning process. This combination of optimizers is summed as shown in <ref type="figure">Fig. 2</ref> where w 1 and w 2 represent the displacements on the ADAM and SGD on the surface of the loss function, while w1 + w2 represents the displacement obtained thanks to our optimizer. Below we explain each line of the MAS algorithm represented in Alg. 3.</p><p>The MAS optimizer has only two hyper-parameters which are λ a and λ s used to balance the contribution of ADAM and SGD respectively. In our experiments, we use only one learning rate η for both ADAM and SGD, but it is still possible to differentiate between the two learning rates. In addition to the hyper-parameters typical of the MAS optimizer, all the hyper-parameters of SGD and ADAM are also needed. In this paper, we assume to use the most common implementation of gradient descent used in the field of deep learning, namely the mini-batch gradient descent which divides the training dataset into small batches that are used to calculate the model error and update the model coefficients w k . For each mini-batch, we calculate the contribution derived from the two components ADAM and SGD and then update all the coefficients as described in the three following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ADAM component</head><p>The complete ADAM algorithm is defined in Alg. 2. In order to use ADAM in our optimizer, we have extracted the ∆ ADAM function which calculates and returns the increments d k for the coefficients w k , as defined in Eq. 16.</p><formula xml:id="formula_15">d k = √ 1 − β 2 · m k √ v k +<label>(16)</label></formula><p>The same ∆ ADAM function also returns the new learning rate η a defined in Eq. 17, useful when a variable learning rate is used. In this last case, MAS uses η a to calculate a new learning rate at each step.</p><formula xml:id="formula_16">η a = η 1 − β 1<label>(17)</label></formula><p>Now, having η a and d k , we can directly modify the weights w k exactly as done in the ADAM optimizer and described in Eq. 18.</p><formula xml:id="formula_17">w k+1 = w k − η a · d k<label>(18)</label></formula><p>However, we just skip this last step and use eta a and d k for our MAS optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SGD component</head><p>As for the ADAM component, also the SGD component, defined in Alg. 1, has been divided into two parts: the ∆ SGD function which returns the increment to be given to the weight w k , and the formula to update the weights as defined in Eq. 10. The v n k value returned by the ∆ SGD function is exactly the value defined in Eq. 9, which we will use directly for our MAS optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The MAS optimizer</head><p>The proposed approach can be summarized with the following Eq. 19</p><formula xml:id="formula_18">w k+1 = w k − (λ s · η + λ a · η a ) · (λ s · v n k + λ a · d k ) (19)</formula><p>where λ s is a scalar for the SGD component and λ a is another scalar for the ADAM component used for balancing the two contribution of the two optimizers. η is the learning rate of the proposed MAS optimizer, while η a is the learning rate of ADAM defined in Eq. 17. d k and v n k are the two increments define in Eq. 16 and Eq. 9 respectively. Eq. 19 can be expanded in the following Eq. 20 to make explicit what are the elements involved in the weights update step used by our MAS optimizer.</p><formula xml:id="formula_19">w k+1 = w k − (λ s · η + λ a · η 1 − β 1 )· ·(λ s · v n k + λ a · √ 1 − β 2 · m k √ v k + )<label>(20)</label></formula><p>where β 1 and β 2 are two parameters of the ADAM optimizer, v a k is defined in Eq. 12, and m k is defined in Eq. 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Mixing ADAM and SGD (MAS)</head><p>Input: the weights w k , λa, λs, learing rate η, weight decay γ, other SGD and ADAM paramiters . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1:</head><p>for batches do 2:</p><p>d k , ηa = ∆ADAM(w k , ∇, η, γ, . . . )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>vn k = ∆SGD(w k , ∇, γ, . . . )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>merged = λs · vn k + λa · d k 5:</p><formula xml:id="formula_20">ηm = λs · η + λa · ηa 6: w k+1 = w k − ηm · merged 7: end for</formula><p>The MAS algorithm can be easily implemented by following the pseudo code defined in Alg. 3 and by calling the two functions ∆ ADAM defined in Alg. 2 and ∆ SGD defined in Alg. 1. We can also show that convergence is guaranteed for the MAS optimizer if we assume that convergence has been guaranteed for the two optimizers SGD and ADAM.</p><p>Theorem 4.1 (MAS Cauchy necessary convergence condition). If ADAM and SGD are two optimizers whose convergence is guaranteed then the Cauchy necessary convergence condition is true also for MAS.</p><p>Proof. Under the conditions in which the convergence of ADAM and SGD is guaranteed <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, we can say that p k=0 η · v n k and p k=0 η a · d k converge at ∞ . That imply the following:</p><formula xml:id="formula_21">lim p→∞ η · v np = lim p→∞ η a · d p = 0<label>(21)</label></formula><p>We can observe that lim p→∞ p k=0 η = lim p→∞ p k=0 η a = ∞ so we can obtain the following:</p><formula xml:id="formula_22">lim p→∞ v np = lim p→∞ d p = 0<label>(22)</label></formula><p>The thesis is that p k=0 (λ s · η + λ a · η a ) · (λ s · v n k + λ a · d k ) respect the Cauchy necessary convergence condition, so: lim p→∞ (λ s · η + λ a · η a ) · (λ s · v np + λ a · d p ) = 0 and for Eq. 22, this last equality is trivially true:</p><formula xml:id="formula_23">lim p→∞ (λ s · η + λ a · η a ) · (λ s · v np + λ a · d p ) = (λ s · η + λ a · η a ) · lim p→∞ (λ s · 0 + λ a · 0) = 0 (23) Theorem 4.2. (MAS convercence) If for p → ∞ is valid that p k=0 η · v n k = η · m 1 and p k=0 η a · d k = η a · m 2 where m 1 ∈ R and m 2 ∈ R are two finite real values, then M AS = p k=0 (λ s · η + λ a · η a ) · (λ s · v n k + λ a · d k ) = λ 2 s · η · m 1 + λ 2 a · η a · m 2 + λ s · λ a · η a · m 1 + λ s · λ a · η · m 2 Proof.</formula><p>We can write MAS series as:</p><formula xml:id="formula_24">M AS = λ 2 s · η · p k=0 v n k + λ 2 a · η a · p k=0 d k + +λ s · λ a · η a · p k=0 v n k + λ s · λ a · η · p k=0 d k<label>(24)</label></formula><p>This can be rewritten for p → ∞ as:</p><p>M AS = λ 2 s ·η·m 1 +λ 2 a ·η a ·m 2 +λ s ·λ a ·η a ·m 1 +λ s ·λ a ·η·m 2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Geometric explanation</head><p>We can see optimizers as two explorers w 1 and w 2 who want to explore an environment (the surface of a loss function). If the two explorers agree to go in a similar direction, then they quickly go in that direction (w 1 + w 2 ). Otherwise, if they disagree and each prefers a different direction than the other, then they proceed more cautiously and slower (w 1 + w 2 ). As we can see in <ref type="figure">Fig. 2a</ref>, if the directions of the displacement of w 1 and w 2 are similar then the amplitude of the resulting new displacement w 1 + w 2 is increased, however, as shown in <ref type="figure">Fig. 2b</ref>, if the directions of the two displacements w 1 and w 2 are not similar then the amplitude of the new displacement w 1 + w 2 has decreased. In our approach, the sum w 1 + w 2 is weighted (see red vectors in <ref type="figure">Fig. 2a</ref>) so one of the two optimizers SGD or ADAM can become more relevant than the other in the choice of direction for MAS, hence the direction resultant may tend towards one of the two. In MAS we set the weight of the two contributions so as to have a sum λ 1 + λ 2 = 1 in order to maintain a learning rate of the same order of magnitude.</p><p>Another important component that greatly affects the MAS shift module at each training step is its learning rate defined in Eq. 19 which combines η and η a . The shifts are scaled using the learning rate, so there is a situation where MAS gets more thrust than the ADAM and SGD starting shifts. In particular, we can imagine that the displacement vector of ADAM has a greater magnitude than SGD and the learning rate of SGD is greater than that of ADAM. In this case, the MAS shift has a greater vector magnitude than SGD and a higher ADAM learning rate which can cause a large increase in the MAS shift towards the search of a minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Toy examples</head><p>To better understand our proposal, we built a toy example where we highlight the main behaviour of MAS. More precisely we consider the following example:    </p><formula xml:id="formula_26">p i = w 1 · (w 2 · x i ) (27) L(w 1 , w 2 ) = 1 i=0 (p i − y 2 i )<label>(28)</label></formula><p>We set β 1 = 0.9, β 2 = 0.999, = 10 −8 , amsgrad = F alse, dampening d = 0, nesterov = F alse and µ = 0. As we can see in <ref type="figure" target="#fig_2">Fig. 3</ref> our MAS optimizer goes faster towards the minimum value after only two epochs, SGD is fast at the first epoch, however, it decreases its speed soon after and comes close to the minimum after 100 epochs, ADAM instead reaches its minimum after 25 epochs. Our approach can be fast when it gets a large v k from SGD and a large η a from ADAM. Another toy example can be done with the benchmark Rosenbrook <ref type="bibr" target="#b24">[25]</ref> function:</p><formula xml:id="formula_27">z = (a − y) 2 + b · (y − x 2 ) 2<label>(29)</label></formula><p>We set a = 1 and b = 100, weight x = 3 and weight y = 1, lr = 0.0001, epochs = 1000, and default paramiter for ADAM and SGD. The MAS optimizer sets λ s = λ a = 0.5. The comparative result for the minimization of this function is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. In this experiment, we can see how by combining the two optimizers ADAM and SGD we can obtain a better result than the single optimizers. For this function, going from the starting point towards the direction of the maximum slope means moving away from the minimum, and therefore it takes a lot of training epochs to approach the minimum. Let's use a final toy example to highlight the behavior of the MAS optimizer. In this case we look for the minimum of the function z = |x| 10 + |y|. We set the weights x = 3 and y = 2, lr = 0.01, epochs = 400 and use all the default parameters for ADAM and SGD. MAS assigns the same value 0.5 for the two lamdas hyper-parameters. In <ref type="figure" target="#fig_4">Fig. 5</ref> we can see how not all the paths between the paths of ADAM and SGD are the best choice. Since MAS, as shown in <ref type="figure">Fig.2</ref>, goes towards an average direction with respect to that of ADAM and SGD, then in this case ADAM arrives first at the minimum point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets</head><p>In this section, we briefly describe the datasets used in the experimental phase. The Cifar100 <ref type="bibr" target="#b25">[26]</ref> dataset consist of 60000 images divided in 100 classes (600 per classes) with a training set size and test set size of 50000 and 10000 respectively. Each input sample is a 32 × 32 colour images with a low resolution. In <ref type="figure" target="#fig_5">Fig. 6</ref> we report some representative examples of the two datasets, extracted from a subset of classes.</p><p>The Corpus of Linguistic Acceptability (CoLA) <ref type="bibr" target="#b26">[27]</ref> is another dataset which contains 9594 sentences belonging to training and validation sets, and excludes 1063 sentences belonging to a set of tests kept out. In our experiment, we only used the training set and the test set.</p><p>The AG's news corpus <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> is the last dataset used in our experiments. It is a dataset that contains news articles from the web subdivided into four classes. It has 30,000 training samples and 1900 test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>The optimizer MAS proposed is a generic solution not oriented exclusively to image analysis, so we conduct experiments on both image classification and text document classification. By doing so, we are able to give a clear indication of the behavior of the proposed optimizer in different contexts, also bearing in mind that many problems, such as audio recognition, can be traced back to image analysis. In all the experiments β 1 = 0.9, β 2 = 0.999, = 10 −8 , amsgrad = F alse, dampening d = 0 and nesterov = F alse. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiments with images</head><p>In this first group of experiments we use two well-known image datasets for: <ref type="bibr" target="#b0">(1)</ref> conduct an analysis of the two main parameters of MAS, λ a and λ s ; (2) compare the performance of MAS with respect to the two starting optimizers SGD and ADAM; (3) analyze the behavior of MAS with different neural models. The datasets used in this first group of experiments are Cifar10 and Cifar100. The neural models compared are two ResNet and in particular, we use Resnet18 and Resnet34 <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. We analyzed λ a and λ s when they assume values from the set {0.3, 0.4, 0.5, 0.6, 0.7} so that λ a + λ s = 1. The numerical results are all grouped in the two Tabs. 1 and 2.</p><p>For Cifar10 we report the average accuracies calculated on 6 runs of each experiment and for 200 epochs. We set η = 0.001, momentum µ = 0.95, and batch size equal to 1024. From the results reported in Tab. 1, we can observe that the best average results are obtained in correspondence with λ s = 0.6 and λ a = 0.4 but we can also note that all the other settings used for MAS allow obtaining better results than ADAM and SGD for both Resnet18 and Resnet34. To better understand what happens during the training phase, in <ref type="figure">Fig. 7</ref> we represent the accuracy of the test and the corresponding loss values of the experiments that produced the best results with Resnet18 on Cifar10. As we can see even though the best execution of ADAM has the lowest loss value, our optimizer with λ a = 0.3 and λ s = 0.7 offers better testing accuracy. The other combination represented with λ a = 0.5 and λ s = 0.5 is also very similar to the better accuracy of ADAM and in any case always better than the average accuracy produced by ADAM. So in general we can say that the MAS optimizer leads to a better generalize than the other optimizers used.</p><p>For Cifar100 we use the same settings as the Cifar10 experiment except for η = 0.008 and batch size set to 512. We can see the results in Tab. 2. Also for this dataset, we can see that for Resnet18 all MAS configurations work better than SGD and ADAM. Instead, looking at the results obtained with the Resnet34, we can say that only one configuration of MAS exceeds the average accuracies of SGD and ADAM, but if we look at the maximum accuracy values, more than one configuration of MAS is better than the results available with ADAM and SGD.</p><p>In conclusion, as we have seen from the results shown in Tab. 1 and Tab. 2, the proposed method leads to a better generalization than the other optimizers used in each experiment. We get better results both by setting λ a and λ s well, and also even when we don't use the best set of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments with text documents</head><p>In this last group of experiments, we use the two datasets of text documents: CoLA and AG's News. As neural model, we use a model based on BERT <ref type="bibr" target="#b16">[17]</ref> which is one of the best techniques for working with text documents. To run fewer epochs we use a pre-trained version <ref type="bibr" target="#b31">[32]</ref> of BERT. In these experiments, we also introduce the comparison with the AdamW optimizer which is usually the optimizer used in BERT-based models.</p><p>For the CoLA dataset we set η = 0.0002, momentum µ = 0.95, and batch size equal to 100. We ran the experiments 5 times for 50 epochs. For the AG's News dataset we set the same parameters used for CoLA, but we only run it for 10 epochs because it gets good results in the firsts epochs and also because the dataset is very large and there- fore takes more time.</p><p>We can see all the results in Tab. 3. Even for text analysis problems, we can confirm the results of the experiments done on images: although AdamW sometimes has better performances than ADAM, our proposed optimizer performs better than other optimizers used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we introduced MAS (Mixing ADAM and SGD) a new Combined Optimization Method that combines the capability of two different optimizers into one. We demonstrate by experiments the capability of our proposal to overcome the single optimizers used in our experiments and achieve better performance. To balance the contribution of the optimizers used within MAS, we introduce two new hyperparameters λ a , λ s and show experimentally that in almost all configurations of these parameters, the results are better than the results obtained with the other single optimizers. In future work, it is possible to change ADAM and SGD and try to mix different optimizers also without the limitations of using only two optimizers. Another significant future work is to try to change dynamically during the training the influence (lambda hyper-parameters) of the two combined optimizers, in the hypothesis that this can improve further the generalization performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Intuitive representation of the idea behind the proposed MAS (Mixing ADAM and SGD) optimizer: the weights are modified simultaneously by both the optimizers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 8 :</head><label>18</label><figDesc>Stochastic Gradient Descent (SGD)Input: the weights w k , learing rate η, weight decay γ, dampening d, boolean nesterov 1: v0 = 0 2: function ∆SGD(w k , ∇, γ, µ, d, nesterov) 3:∇ = ∇ + w k · γ v k = v k−1 · µ + ∇ · (1 − d) if nesterov = T rue then 11: v k = ∇ + v k · µ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Behavior of the three optimizers MAS, ADAM and SGD on the surface defined in Eq.28. For better visualization the SGD was shifted on X axis of 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Behavior of the three optimizers MAS, ADAM and SGD on the Rosenbrook's surface with a = 1 and b = 100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Behavior of the three optimizers MAS, ADAM and SGD on the surface z = |x| 10 + |y|</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>On the left some sample images for each of the 10 classes of CIFAR-10 (one class for each row). On the right 10 classes randomly selected from the set of 100 classes of CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5 Figure 7 :</head><label>57</label><figDesc>a = 0.3, s = 0.7 MAS: a = 0.5, s = 0.Resnet18 test accuracies and test loss of the best results obtained on Cifar10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Accuracy results on Cifar10, after 6 runs of 200 epochs.The Cifar10<ref type="bibr" target="#b25">[26]</ref> dataset consists of 60,000 images divided into 10 classes (6000 per class) with a training set size and test set size of 50000 and 10000 respectively. Each input sample is a low-resolution color image of size 32 × 32. The 10 classes are airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.</figDesc><table><row><cell>Name</cell><cell>λ a</cell><cell cols="3">λ s avg. acc acc max</cell></row><row><cell>Resnet18</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adam</cell><cell>1</cell><cell>0</cell><cell>84.68</cell><cell>86.24</cell></row><row><cell>SGD</cell><cell>0</cell><cell>1</cell><cell>78.87</cell><cell>79.19</cell></row><row><cell>MAS</cell><cell cols="2">0.5 0.5</cell><cell>85.36</cell><cell>85.80</cell></row><row><cell>MAS</cell><cell cols="2">0.4 0.6</cell><cell>85.64</cell><cell>86.59</cell></row><row><cell>MAS</cell><cell cols="2">0.6 0.4</cell><cell>85.89</cell><cell>86.56</cell></row><row><cell>MAS</cell><cell cols="2">0.7 0.3</cell><cell>85.39</cell><cell>86.09</cell></row><row><cell>MAS</cell><cell cols="2">0.3 0.7</cell><cell>85.57</cell><cell>86.85</cell></row><row><cell>Resnet34</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adam</cell><cell>1</cell><cell>0</cell><cell>82.98</cell><cell>83.52</cell></row><row><cell>SGD</cell><cell>0</cell><cell>1</cell><cell>82.92</cell><cell>83.25</cell></row><row><cell>MAS</cell><cell cols="2">0.5 0.5</cell><cell>84.99</cell><cell>85.69</cell></row><row><cell>MAS</cell><cell cols="2">0.4 0.6</cell><cell>85.75</cell><cell>86.12</cell></row><row><cell>MAS</cell><cell cols="2">0.6 0.4</cell><cell>84.63</cell><cell>85.27</cell></row><row><cell>MAS</cell><cell cols="2">0.7 0.3</cell><cell>84.46</cell><cell>84.80</cell></row><row><cell>MAS</cell><cell cols="2">0.3 0.7</cell><cell>85.71</cell><cell>86.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracy results on Cifar100, after 7 runs of 200 epochs.</figDesc><table><row><cell>Name</cell><cell>λ a</cell><cell cols="3">λ s avg acc. acc max</cell></row><row><cell>Resnet18</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adam</cell><cell>1</cell><cell>0</cell><cell>49.56</cell><cell>50.28</cell></row><row><cell>SGD</cell><cell>0</cell><cell>1</cell><cell>49.48</cell><cell>50.43</cell></row><row><cell>MAS</cell><cell cols="2">0.5 0.5</cell><cell>55.08</cell><cell>56.68</cell></row><row><cell>MAS</cell><cell cols="2">0.4 0.6</cell><cell>56.23</cell><cell>56.83</cell></row><row><cell>MAS</cell><cell cols="2">0.6 0.4</cell><cell>53.82</cell><cell>54.44</cell></row><row><cell>MAS</cell><cell cols="2">0.7 0.3</cell><cell>52.92</cell><cell>54.07</cell></row><row><cell>MAS</cell><cell cols="2">0.3 0.7</cell><cell>58.01</cell><cell>58.48</cell></row><row><cell>Resnet34</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adam</cell><cell>1</cell><cell>0</cell><cell>50.66</cell><cell>51.92</cell></row><row><cell>SGD</cell><cell>0</cell><cell>1</cell><cell>52.81</cell><cell>53.45</cell></row><row><cell>MAS</cell><cell cols="2">0.5 0.5</cell><cell>51.52</cell><cell>53.26</cell></row><row><cell>MAS</cell><cell cols="2">0.4 0.6</cell><cell>51.73</cell><cell>53.48</cell></row><row><cell>MAS</cell><cell cols="2">0.6 0.4</cell><cell>52.15</cell><cell>53.95</cell></row><row><cell>MAS</cell><cell cols="2">0.7 0.3</cell><cell>53.06</cell><cell>54.50</cell></row><row><cell>MAS</cell><cell cols="2">0.3 0.7</cell><cell>51.96</cell><cell>53.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy results of BERT pre-trained on CoLA (50 epochs) and AG's news (10 epochs), after 5 runs.</figDesc><table><row><cell>Name</cell><cell>λ a</cell><cell cols="3">λ s avg acc. acc max</cell></row><row><cell>CoLA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AdamW</cell><cell>-</cell><cell>-</cell><cell>78.59</cell><cell>85.96</cell></row><row><cell>Adam</cell><cell>1</cell><cell>0</cell><cell>79.85</cell><cell>83.30</cell></row><row><cell>SGD</cell><cell>0</cell><cell>1</cell><cell>81.48</cell><cell>81.78</cell></row><row><cell>MAS</cell><cell cols="2">0.5 0.5</cell><cell>85.92</cell><cell>86.72</cell></row><row><cell>MAS</cell><cell cols="2">0.4 0.6</cell><cell>86.18</cell><cell>87.66</cell></row><row><cell>MAS</cell><cell cols="2">0.6 0.4</cell><cell>85.45</cell><cell>86.34</cell></row><row><cell>MAS</cell><cell cols="2">0.7 0.3</cell><cell>84.66</cell><cell>85.78</cell></row><row><cell>MAS</cell><cell cols="2">0.3 0.7</cell><cell>86.34</cell><cell>86.91</cell></row><row><cell>AG's News</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AdamW</cell><cell>-</cell><cell>-</cell><cell>92.62</cell><cell>92.93</cell></row><row><cell>Adam</cell><cell>1</cell><cell>0</cell><cell>92.55</cell><cell>92.67</cell></row><row><cell>SGD</cell><cell>0</cell><cell>1</cell><cell>91.28</cell><cell>91.39</cell></row><row><cell>MAS</cell><cell cols="2">0.5 0.5</cell><cell>93.72</cell><cell>93.80</cell></row><row><cell>MAS</cell><cell cols="2">0.4 0.6</cell><cell>93.82</cell><cell>93.98</cell></row><row><cell>MAS</cell><cell cols="2">0.6 0.4</cell><cell>93.55</cell><cell>93.67</cell></row><row><cell>MAS</cell><cell cols="2">0.7 0.3</cell><cell>93.19</cell><cell>93.32</cell></row><row><cell>MAS</cell><cell cols="2">0.3 0.7</cell><cell>93.86</cell><cell>93.99</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
	<note>A stochastic approximation method</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive methods for nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9793" to="9803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adaptive gradient methods with dynamic bound of learning rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09843</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis of various optimizers on deep convolutional neural network model in the application of hyperspectral remote sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2664" to="2683" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive gradient methods with dynamic bound of learning rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scw-sgd: Stochastically confidenceweighted sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1746" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved adam optimizer for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mixing adam and sgd: a combined optimization method with pythorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Landro</surname></persName>
		</author>
		<ptr target="https://gitlab.com/nicolalandro/multioptimizer.2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving generalization performance by switching from adam to sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07628</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evolutionary stochastic gradient descent for optimization of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6048" to="6058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="950" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Asynchronous byzantine machine learning (the case of sgd)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Damaskinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M E</forename><surname>Mhamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taziki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07928</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Accelerating sgd with momentum for over-parameterized learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13395</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Closing the generalization gap of adaptive gradient methods in training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06763</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09237</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradient descent only converges to minimizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simchowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on learning theory</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1246" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An automatic method for finding the greatest or least value of a function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rosenbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Ag&apos;s corpus of news articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulli</surname></persName>
		</author>
		<ptr target="http://groups.di.unipi.it/gulli/AGcorpusofnewsarticles.html.6" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Targ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lyman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08029</idno>
		<title level="m">Resnet in resnet: Generalizing residual architectures</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bert base uncased pre-trained model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Co</surname></persName>
		</author>
		<ptr target="https://huggingface.co/bert-base-uncased.7" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
