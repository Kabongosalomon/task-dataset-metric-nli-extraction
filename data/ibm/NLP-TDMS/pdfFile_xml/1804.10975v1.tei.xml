<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">U</forename><surname>Darmstadt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Exemplary shape reconstructions from a single image by our Matryoshka network based on nested shape layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper, we develop novel, efficient 2D encodings for 3D geometry, which enable reconstructing full 3D shapes from a single image at high resolution. The key idea is to pose 3D shape reconstruction as a 2D prediction problem. To that end, we first develop a simple baseline network that predicts entire voxel tubes at each pixel of a reference view. By leveraging well-proven architectures for 2D pixelprediction tasks, we attain state-of-the-art results, clearly outperforming purely voxel-based approaches. We scale this baseline to higher resolutions by proposing a memoryefficient shape encoding, which recursively decomposes a 3D shape into nested shape layers, similar to the pieces of a Matryoshka doll. This allows reconstructing highly detailed shapes with complex topology, as demonstrated in extensive experiments; we clearly outperform previous octreebased approaches despite having a much simpler architecture using standard network components. Our Matryoshka networks further enable reconstructing shapes from IDs or shape similarity, as well as shape sampling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Being able to reason about the 3D shape of objects, even when presented with only a single monocular image, is one of the remarkable abilities of the human visual system. In * This work was carried out while at TU Darmstadt. the absence of geometric cues from stereopsis or motion, our visual system is still able to infer detailed surfaces or plausibly complete hidden parts.</p><p>The advent of large-scale shape collections <ref type="bibr" target="#b3">[4]</ref> and advances in data-driven approaches, especially convolutional neural networks (CNNs), have sparked new interest in developing approaches that mimic the human visual system in its ability to reconstruct 3D shapes from a single image, e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. The predominant structure of CNNs employed for this task is an hourglass shape with an encoder, which transforms a single image into a shape code, and a decoder finally producing a 3D shape <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>. Interpreting the shape code as a multi-dimensional tensor with spatial and feature dimensions, the decoder successively increases the spatial resolution of the shape code while reducing the number of feature dimensions. The output of the decoder is a volumetric binary occupancy map. The overall down-sampling and up-sampling of representations in this hourglass architecture facilitates the accumulation of shape information from the whole image and propagating it to all parts of the reconstructed shape. Higher resolutions of the input and/or output require more levels of scaling, which results in deeper networks. The network depth is in turn bound by the available GPU memory, impeding CNNs with volumetric decoders in their ability to reconstruct shapes at high resolution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>More efficient encodings, for example based on octrees <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>, alleviate this problem, but require sophisticated structures and mechanisms for feature propagation To appear in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, June 2018. c 2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. through the decoder, impeding portability across deep leaning frameworks and exploration of alternative architectures. Alternatively, view-based reconstructions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref> can encode highly detailed shape information, but cannot represent shapes with a significant level of self-occlusion.</p><p>In this paper, we develop a novel, efficient 2D encoding for 3D geometry, which enables reconstructing full 3D shapes from a single image at high resolution. We begin by developing a new architecture for dense 3D shape reconstruction at low resolutions. Its key feature is that we pose reconstructing 3D voxel occupancy as a 2D prediction problem by directly predicting whole voxel tubes at every pixel of a reference view. This allows us to use a wide range of standard networks for 2D pixel-prediction tasks, which enables this simple baseline to attain state-of-the-art accuracy, clearly outperforming previous purely voxel-based approaches. Another factor in reaching such high accuracy levels is using a structured loss function.</p><p>We then scale this baseline to higher resolutions by proposing an efficient shape encoding based on the idea of nested shape layers. That is, the object shape is recursively decomposed into nested layers, similar to the pieces of a Matryoshka doll, see <ref type="figure">Fig. 4</ref>. This has several key advantages: (1) it allows for a highly detailed reconstruction of shapes with complex topology, including self-occlusions;</p><p>(2) each shape layer can be represented through a set of six depth maps, which is memory efficient and allows the use of standard network architectures; (3) nested shape layers lead to more detailed reconstructions than octree-based architectures despite being much simpler. We further demonstrate the capabilities of the proposed encoding and decoder architecture in reconstructing shapes from IDs or shape similarity, as well as in shape sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work</head><p>With the advent of large-scale shape collections <ref type="bibr" target="#b3">[4]</ref>, data-driven methods, and especially CNNs, have become the method of choice for predicting 3D shapes. Insprired by the success of CNNs for dense 2D prediction tasks, Wu et al. <ref type="bibr" target="#b30">[31]</ref> adapted CNNs to volumetric outputs. Yan et al. <ref type="bibr" target="#b32">[33]</ref> and Zhu et al. <ref type="bibr" target="#b33">[34]</ref> showed that optimizing projections of the predicted shape benefits the reconstruction. Choy et al. <ref type="bibr" target="#b4">[5]</ref> developed a joint approach for shape reconstruction from one or multiple views. Girdhar et al. <ref type="bibr" target="#b9">[10]</ref> combined an autoencoder and a convolutional network to learn an embedding of images and 3D shapes. Wu et al. <ref type="bibr" target="#b29">[30]</ref> trained a generative adversarial network to synthesize 3D shapes. Tulsiani et al. <ref type="bibr" target="#b27">[28]</ref> learned a shape decoder from 2D supervision. Wu et al. <ref type="bibr" target="#b28">[29]</ref> used intermediate 2.5D shape representations in order to decouple image encoding and 3D shape decoding. All have in common that they model 3D shapes as binary occupancy maps. This allows for casting shape estimation as a classification problem at the voxel level and benefitting from the extraordinary performance of CNNs in classification tasks. Representing each voxel separately to facilitate the classification task comes at a price, however, as the memory requirements scale cubically with the resolution of shapes. Consequently, the output resolution is usually limited to 32 voxels along each side.</p><p>Riegler et al. <ref type="bibr" target="#b21">[22]</ref> addressed the memory requirements of predicting high-resolution occupancy maps by adapting CNNs to operate on octrees. However, their method requires the tree structure to be known ahead of time, which limits its applicability for 3D reconstruction. The works of Tatarchenko et al. <ref type="bibr" target="#b26">[27]</ref> and Häne et al. <ref type="bibr" target="#b10">[11]</ref> alleviate this problem by also predicting the tree structure. Besides commonly requiring custom network layers <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, which impedes porting the approaches to other deep learning libraries, the sparse structure of octrees complicates feature propagation to neighboring cells. This is in contrast to the proposed method, which only requires network layers that are standard in all common deep learning frameworks and, by building on 2D convolutions, facilitates the easy exploration of recent advances in network architectures.</p><p>Fan et al. <ref type="bibr" target="#b7">[8]</ref> addressed the sparse structure of shapes within a 3D volume by explicitly predicting a point cloud. Their method demonstrated impressive results at low resolution, but it has yet to be seen if and how well this approach scales to higher resolutions.</p><p>As an alternative to a volumetric representation, Tatarchenko et al. <ref type="bibr" target="#b25">[26]</ref> trained a CNN to generate RGB-D images from arbitrary views of an object. In a postprocessing step, the different views are merged into a consistent shape. Following this approach for the generation of shapes, Soltani et al. <ref type="bibr" target="#b24">[25]</ref> predicted pairs of silhouettes and depth images for a fixed set of views, and Lun et al. <ref type="bibr" target="#b17">[18]</ref> additionally predicted surface normal maps. The final fusion of views has been addressed by merging them into a point cloud and pruning outliers using the predicted silhouettes <ref type="bibr" target="#b24">[25]</ref>, registering views and solving an optimization problem <ref type="bibr" target="#b17">[18]</ref>, as well as learning a differentiable depth map renderer to produce consistent projections <ref type="bibr" target="#b16">[17]</ref>. In general, view-based methods are able to generate shapes at high resolutions, but occasionally suffer from noisy estimates, which need to be addressed in the fusion step. Furthermore, view-based methods cannot handle large self-occlusions. Our proposed method addresses the fusion step and handling of occlusions in a simple, but efficient formulation.</p><p>Sinha et al. <ref type="bibr" target="#b23">[24]</ref> projected object surfaces to geometry images in order to build on image-based CNN architectures. This shape representation allows for a very memoryefficient encoding, but requires additional care for handling different mesh topologies and projective distortions produce a non-uniformly distributed level of detail. Zou et al. <ref type="bibr" target="#b34">[35]</ref> assembled 3D shapes from volumetric primitives. Our work, in contrast, is inspired by depth peeling <ref type="bibr" target="#b6">[7]</ref> and constructive solid geometry <ref type="bibr" target="#b12">[13]</ref>. Gallup et al. <ref type="bibr" target="#b8">[9]</ref> used a nlayer heightmap representation to constrain the reconstructions of buildings from occupancy grids. Our nested shape layers can be seen as a generalization of this representation as our Matryoshka networks effectively estimate 6 overlapping heightmaps per layer, which are fused together.</p><p>In concurrent work, Delanoy et al. <ref type="bibr" target="#b5">[6]</ref> explored the prediction of multi-channel depth maps in the context of reconstructing aligned shapes from sketches, which allows for exploiting additional projective constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Formulation</head><p>We develop a framework for memory-efficient prediction of 3D shapes in two stages. First, we encode 3D shapes as n-channel images, where each pixel represents a tube of n voxels in a 3-dimensional tensor (a fiber along the z-axis). This leads to a more memory-efficient intermediate representation, since features are shared across entire fibers instead of a single voxel (cell). To that end, we adapt network architectures for dense pixel-prediction tasks to predicting voxel tubes. This reduces the memory footprint of the network, but still produces a dense binary occupancy map in the last network layer. Second, we further compress the output by predicting nested shape layers that can encode shapes with arbitrary amounts of self-occlusion. Each shape layer consists of 6 depth maps, c.f . <ref type="figure">Fig. 4</ref>. Through careful alignment of the depth maps and an appropriate loss function, we avoid noisy estimates, a costly fusion via optimization, and minimize the dimensionality of the final network layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Standard voxel decoder</head><p>To ground our discussions, let us begin by describing a simple standard architecture for predicting volumetric binary occupancy maps, as it is common to a wide range of previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33]</ref>. We here focus our discussion on the decoder and assume that its input, a shape code S with a spatial resolution of n s × n s × n s and n f features, is provided by an image encoder. Each layer of the voxel decoder then up-samples the shape code, i.e. it increases the spatial resolution while decreasing the number of features until the full spatial resolution n o ×n o ×n o has been reached and only one feature dimension is left. The resulting 4-dimensional tensor is then interpreted as a 3D binary occupancy map. Intuitively, the voxel decoder transforms a 4-dimensional tensor into 3-dimensional tensor by iteratively lowering the feature resolution. <ref type="figure" target="#fig_0">Fig. 2</ref> (left) shows an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Predicting voxel tubes</head><p>To address the memory inefficiency of such a standard voxel decoder, we here propose to predict entire voxel tubes. The key idea is that we interpret the shape code as a 3-dimensional tensor with spatial dimensions n t × n t and one feature dimension. Analogously to the voxel decoder, we up-sample the spatial resolution while down-sampling the number of features. Different to the voxel decoder, however, we reduce the number of features until it equals n o . Hence, the output of our decoder is a 3-dimensional tensor with resolution of n o × n o × n o . With this simple change in representation, a fiber of features no longer encodes a single voxel, but a complete tube of voxels jointly. Therefore, we term the resulting tensor a voxel tube image. <ref type="figure" target="#fig_0">Fig. 2</ref> (middle) illustrates the architecture. As the proposed decoder now operates on images instead of voxel grids, we can employ standard 2D network components for designing the decoder and take full advantage of recent advances in architectures for 2D prediction tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Shape layers: Learning to compress voxel tubes</head><p>Although sharing features across voxel tubes reduces the space requirements of the decoder, it is insufficient for scaling the output resolution by multiple octaves with currently available GPU architectures. To scale our approach to higher resolutions, we compress shapes by constructing them from multiple shape layers, each of which requires only n o × n o × 6 activations in the network output. Each shape layer S ∈ S is the product of fusing 6 depth maps</p><formula xml:id="formula_0">d = (d −x , d +x , d −y , d +y , d −z , d +z ) ∈ D,</formula><p>which represent a shape as depicted in <ref type="figure">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(left).</head><p>Specifically, each depth map d i is an orthogonal projection imaged from view position v i , which is located at the center of side i of an axis-aligned unit cube. We assume the cube to be at the origin, and all views to face the origin. Shape from depth maps. For each of the three axes, we now define shapes as</p><formula xml:id="formula_1">S x ≡ {(i, j, k) | d −x (j, k) ≤ i ≤ n o − d +x (j, k)} (1a) S y ≡ {(i, j, k) | d −y (i, k) ≤ j ≤ n o − d +y (i, k)} (1b) S z ≡ {(i, j, k) | d −z (i, j) ≤ k ≤ n o − d +z (i, j)} , (1c)</formula><p>where the tuple (i, j, k) indexes a cell in a binary occupancy map of size n o ×n o ×n o . That is, the shape S x , for example,  <ref type="figure">Figure 3</ref>. Fusion of depth maps. We interpret pairs of depth maps taken along three view axes (left) as run-length encoding of geometry and fuse them to shapes Sx (right, green), Sy (red), and Sz (yellow). Noisy predictions cause a smearing at the shape silhouettes. By intersecting Sx, Sy, and Sz, we obtain a shape S (white) with outliers removed.</p><p>can be thought of as being represented by sending x-axisaligned rays through it and recording where they enter the shape and exit again. Put differently, the pairs (d −x , d +x ), (d −y , d +y ), and (d −z , d +z ) are effectively run-length encodings of the shapes S x , S y , and S z . The colored car shapes in <ref type="figure">Fig. 3</ref> (right) illustrate this. Note how a single shape, say S z , is not sufficient to represent the car (shown in yellow), since the geometry of the wheels cannot be recovered correctly from this view due to self-occlusion.</p><p>Shape fusion. We address this by fusing the three shapes via their intersection as</p><formula xml:id="formula_2">S = φ(d) ≡ S x ∩ S y ∩ S z with φ : D → S.<label>(2)</label></formula><p>The result is shown as the white car shape in <ref type="figure">Fig. 3</ref> (right). This fusion process and the placement of the three orthogonal views v i is motivated by our observation that depth map predictions are often less accurate near the silhouette of an object. This is intuitive as the decision whether to assign a pixel to foreground or background is less certain close to the silhouette. If we cast the occupancy prediction as a classification problem (c.f . Sec. 3.2), we can assess the uncertainty through the softmax predictions. Depth map prediction is cast as regression problem here, however, and the network tends to average multiple plausible predictions. This has been observed also for point clouds <ref type="bibr" target="#b7">[8]</ref>. In regions around the silhouette, this averaging causes noisy estimates and a smearing of the shape as can be seen in the colored shapes in <ref type="figure">Fig. 3 (right)</ref>. By placing orthogonal views v i at the sides of a unit cube, we ensure that regions of high uncertainty in one view are complemented by regions of low uncertainty in another. Fusing the shapes S x , S y , and S z through their intersection thus allows to remove outliers reliably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Nested shape layers: Recovering occluded parts</head><p>Representing shapes through a single set of 6 depth maps cannot possibly recover parts that are occluded from all three view axes. We address this by building up a shape S 1:L from L nested shape layers by iteratively adding and subtracting shapes φ(d i ). This process is inspired by constructive solid geometry <ref type="bibr" target="#b12">[13]</ref>. Let φ : D → S be the fusion of a shape from the set of depth maps as defined in Eq. (2). We then compose shapes via the recursion</p><formula xml:id="formula_3">S 1 ≡ φ(d 1 ) (3a) S 1:2n ≡ S 1:2n−1 \ φ(d 2n ) (3b) S 1:2n+1 ≡ S 1:2n ∪ φ(d 2n+1 ),<label>(3c)</label></formula><p>where n ∈ N + . We begin the recursion by only fusing depth maps in the first layer (Eq. 3a), then we subtract shapes in even layers (Eq. 3b), and add shapes in odd layers (Eq. 3c). This process allows us to encode complex geometries; <ref type="figure">Fig.  4</ref> shows an exemplary encoding of a shape into multiple nested shape layers. Note that the nesting of the shape layers is akin to Matryoshka dolls ( <ref type="figure">Fig. 4, right)</ref>, i.e. the first two shape layers encode the outermost doll, the next two layers describe the second doll inside the first, and so on. Learning. To learn to predict nested shape layers, we need to define the appropriate ground truth depth maps.</p><p>To that end, let T 1:L ∈ S be the (true) target shape and π : S → φ(D) be the projection from an arbitrary shape to the space of shapes that can be represented by the depth map fusion process φ from Eq. (2). To compute the projection, we greedily apply a simple ray casting and store the depth of the first intersection with the shape. The ground truth is then given by the recursion</p><formula xml:id="formula_4">T 1 ≡ π(T 1:L ) (4a) T 1:2n ≡ π(T 1:2n−1 \ T 1:L ) (4b) T 1:2n+1 ≡ π(T 1:L \ T 1:2n ).<label>(4c)</label></formula><p>Note that although the shapes are encoded recursively, we train a single network to predict all layers jointly. How many shape layers do we need? To answer this question, we encode shapes from the ShapeNet-core dataset <ref type="bibr" target="#b3">[4]</ref> with a varying number L of layers and compute the intersection over union between shapes before encoding ( <ref type="figure">Figure 4</ref>. Composing shapes from nested shape layers. The proposed method reconstructs a shape S1:5 by iteratively adding (S1,S3,S5) and subtracting (S2,S4) shape layers built from fused depth maps (a). This is akin to the layers of a Matryoshka doll (b). and after decoding (S 1:L ). We report results in Tab. 1. We find that 94.8% of shapes at a low resolution (32 3 , provided by Choy et al. <ref type="bibr" target="#b4">[5]</ref>) can be completely encoded with a single shape layer and only 4 shapes require more than 2 layers. Evaluating shapes from ShapeNet-cars at 128 3 (from Tatarchenko et al. <ref type="bibr" target="#b26">[27]</ref>), we find that only 2.6% of shapes can be completely encoded with just a single shape layer. This demonstrates the need for a nested representation to accurately represent shapes at high resolution.</p><formula xml:id="formula_5">T 1:L ) (a) S 1:5 = (((S 1 \S 2 ) ∪S 3 ) \S 4 ) ∪S 5 . (b) Matryoshka doll</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss functions for dense and sparse prediction</head><p>As a voxel can either be occupied or empty, the prediction of occupancies within a voxel grid is often cast as binary classification, minimizing the binary cross entropy <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>. This is in contrast to the metrics commonly used for evaluating predictions <ref type="bibr" target="#b1">[2]</ref>. Most common is the intersection over union (IoU), or Jaccard index</p><formula xml:id="formula_6">IoU(A, B) = |A ∩ B| |A ∪ B| .<label>(5)</label></formula><p>The IoU divides number of true positives (the intersection) by the sum of true positives, false positives, and false negatives (the union). In the context of segmenting a 3D object, correct foreground predictions are thus effectively weighted by the size of the true object and the prediction. Consequently, the contribution of a single voxel toward the overall loss depends on the remaining predictions within the voxel grid. This is in contrast to the binary cross entropy or typical regression losses, e.g. L 1 or L 2 , which decompose into losses of individual voxels (or pixels). Alternatively, we also consider the cosine similarity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of shape layers</head><formula xml:id="formula_7">C(A, B) = A · B A 2 B 2 ,<label>(6)</label></formula><p>which has been used for learning embeddings, e.g. <ref type="bibr" target="#b2">[3]</ref>, but as far as we know not in a reconstruction setting. To adapt cosine similarity and IoU to our setting, c.f . <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, we define</p><formula xml:id="formula_8">L C (x,ȳ) = 1 − x,ȳ<label>(7)</label></formula><formula xml:id="formula_9">L IoU (x,ȳ) = 1 − x,ȳ ix i +ȳ i −x iȳi ,<label>(8)</label></formula><p>wherex,ȳ are predicted and ground truth shape each stacked into a vector and normalized to unit norm. Note thatx is based directly on the softmax outputs. Loss functions for shape layers. Employing L C and L IoU (Eqs. 7 and 8) for predicting (nested) shape layers would require decoding the representation into a voxel grid during training, thus counteracting the efficiency gains from the compact representation. We hence opt for a different loss function for training the prediction of (nested) shape layers. Estimating depth (or run-lengths) is naturally a regression task, which we address via a robust L 1 -penalty. Applying a regression loss naïvely to the full depth map, however, will bias the network toward background pixels. This has been addressed in the literature by predicting separate foreground masks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>, requiring an additional channel per depth map. Avoiding auxiliary outputs, we modify the employed loss to adaptively weigh foreground and background regions by computing the average loss separately for foreground and background regions. We further refrain from forcing background pixels to equal any specific value, as this would unnecessarily bind model capacity. Thus, we require background pixels to take on values less than a margin m, instead. Our modified loss for each pixel then becomes</p><formula xml:id="formula_10">L 1 (x, y) = |x − y| , if y &gt; 0 max(0, x + m), otherwise,<label>(9)</label></formula><p>where x and y are prediction and label for a pixel. We also experimented with the L 2 -norm as basis for our modification, but observed significantly worse reconstructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Implementation details</head><p>Our networks can be structured into an encoder and a decoder with a bottleneck in the middle <ref type="figure" target="#fig_0">(Fig. 2</ref> right). The encoder starts with 2 convolution layers with interleaved batch normalization and ReLU nonlinearity to produce 8 initial feature channels while keeping the input resolution. It is further composed of residual modules <ref type="bibr" target="#b11">[12]</ref> that down-sample the input image to a resolution of 4 × 4 while linearly increasing the number of feature channels to 512 (257 for experiments on shapes of a single category and small resolution, i.e. our ablation study). Each residual module contains two 3 × 3 convolutions, with batch normalization and a ReLU nonlinearity before each convolution. Downsampling while increasing the number of feature channels is done in the first convolution layer of each residual module. Modules altering the spatial resolution alternate with modules operating at the same resolution. The decoder upsamples again until the desired output resolution is reached. Mirroring the encoder, the decoder is also composed of residual modules and decreases the number of feature channels linearly. For upsampling, the first convolution is replaced with a transposed convolution. In all our experiments, we trained using Adam <ref type="bibr" target="#b15">[16]</ref> using an initial learning rate of 0.001 and β 1 = 0.9, β 2 = 0.999, and varied the schedule for dropping the learning rate with the dataset size. We refer to the supplemental for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>To assess the performance of our geometry prediction approaches at different tasks, compare them to prior work, and study the influence of loss functions and network architectures, we a use a common subset <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27]</ref> of ShapeNetcore <ref type="bibr" target="#b3">[4]</ref>. The subset consists of nearly 50000 3D shapes divided into 13 major categories. For all experiments, we report the intersection over union (IoU) in %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Reconstruction from a single view</head><p>Comparison to prior work. For evaluating the performance of our networks in reconstructing 3D shape from a single RGB image, we compare to 3 recent approaches on the ShapeNet-core dataset using the renderings, dataset split, and ground truth voxel representations provided by Choy et al. <ref type="bibr" target="#b4">[5]</ref>. The renderings feature images of size 137×137 and a uniform sampling of viewpoints. The voxel representations are of size 32 × 32 × 32. As preprocessing step, we crop input images to 128 × 128 and shuffle color channels randomly during training. We train a single network for all shape categories. We compare to different representative approaches for predicting 3D shapes: (1) 3D-R2N2 <ref type="bibr" target="#b4">[5]</ref> features a dense 3D voxel decoder and an LSTM to enable reconstruction from one or multiple views; (2) Octree Generating Networks (OGN) <ref type="bibr" target="#b26">[27]</ref> operate on octrees to exploit sparsity of occupancy maps; (3) Point Set Generation Networks (PSGN) <ref type="bibr" target="#b7">[8]</ref> predict a point cloud using a stacked hourglass network, a volume prediction network, and a voxel-based post-processing network.</p><p>We show results in Tab. 2. Although conceptually simpler, the dense voxel tube image version of our network outperforms all voxel decoder-based approaches and is on par with PSGN, which uses a more complex multi-stage (multinetwork) architecture. Moreover, it is not clear if PSGNs scale to higher resolutions, whereas this is easily possibly for our networks (see below). Interestingly, the sparse Matryoshka version of our network, which predicts nested shape layers, performs only slightly worse than its dense counterpart and clearly outperforms all voxel decoder baselines. This demonstrates the power of our compact imagebased representation for 3D shape.  <ref type="table">Table 3</ref>. Single image 3D shape reconstruction for high resolutions. We report IoU (in %) between predictions at several resolutions and ground truth shapes at 256 3 . Predictions at lower resolution are up-sampled to 256 3 .</p><p>Reconstructing higher resolutions. Low-resolution occupancy maps are naturally limited to a low level of detail they can represent. To assess the performance of our Matryoshka network at reconstructing shapes at high resolution, we compared it to Octree Generating Networks <ref type="bibr" target="#b26">[27]</ref>, which are representative for Octree-based approaches. We follow the experimental setup of Tatarchenko et al. <ref type="bibr" target="#b26">[27]</ref> and predict 3D shapes from ShapeNet-cars at resolutions of 32 3 , 64 3 , 128 3 , and 256 3 given a single RGB input image. We then up-sample the predictions to a resolution of 256 3 voxels and compute the intersection over union with the ground truth shapes at that resolution. For fair comparison, we use dataset split and ground truth shapes provided by Tatarchenko et al. <ref type="bibr" target="#b26">[27]</ref>. Furthermore, we provide results for 2 additional classes from ShapeNet-core, which pose different challenges; while the airplane class consists of shapes with intricate structure, the table class contains the most samples. We report quantitative results in Tab. 3 and show qualitative examples in <ref type="figure">Fig. 5</ref>. We find that both methods predict more accurate shapes at higher resolutions. However, OGN's performance saturates at 128 3 due to the high complexity of the car class with 7496 samples. For our method we only observe this effect in the even more complex table category (8509 samples). For all resolutions, the proposed method clearly outperforms the octree-based approach despite being based on standard 2D networks, which can be easily implemented in all popular frameworks. The benefits of higher resolutions are observed best for the airplane class, which shows the biggest relative improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation studies</head><p>To better understand the contribution of individual components to the overall performance of our networks, we examine different base architectures and loss functions. For our ablation study we use the dataset split and renderings (64 × 64 pixels) from Yan et al. <ref type="bibr" target="#b32">[33]</ref>, taken from the same 24 viewpoints for each object. For the study of loss functions we train one network per class and for the study of network architectures, we train one network for all classes. Network architectures. We investigate several network architectures that are known to perform well for dense 2D pre- diction tasks. As memory consumption is a dominating factor in the choice of a suitable architecture, we modified all architectures to fit within 3 GB of GPU memory when predicting shapes at a resolution of 32 3 with a mini-batch size of 128. This leaves sufficient memory budget for scaling up any architecture to higher output resolutions on a single GPU. Since some of the base architectures (ResNet <ref type="bibr" target="#b11">[12]</ref>, DenseNet <ref type="bibr" target="#b13">[14]</ref>) are designed to operate on images, but to produce a single class label, they require adaptation to generate dense output. In the interest of space, we defer architectural details to the supplementary material. We take our voxel tube network as ResNet-inspired baseline. Removing residual connections yields an Encoder/Decoder or Deconvolution Network <ref type="bibr" target="#b18">[19]</ref>; the introduction of connections between layers of the same spatial resolution to skip varying sequences of down-and up-sampling forms a U-Net inspired network, c.f . <ref type="bibr" target="#b22">[23]</ref>. We report results in Tab. 4. Across all categories, the ResNet-inspired architecture outperforms all other networks with a significant margin. Note that, e.g., octree-based decoders, in contrast to our approach, cannot take advantage of this as easily.</p><p>Loss functions. To assess how specific loss functions affect the reconstruction quality for our voxel tube networks, we evaluate the binary cross entropy, L 1 -norm, L 2 -norm, the negative cosine similarity (Eq. 7), and the negative intersection over union (Eq. 8). We report results in Tab. 5. We find that the binary cross entropy is a strong baseline, but performs worse than all other evaluated loss functions except for the L 1 -norm, which consistently performs worst. Since the evaluated architecture constrains activations in the final layer between 0 and 1, a robust loss is less important. For all categories, the two proposed losses perform best. <ref type="figure">Figure 6</ref>. Sampling shapes. By supplying the SfSS-decoder with Gaussian noise, we can draw varied samples from the car distribution. <ref type="figure">Figure 7</ref>. Shape interpolation. Linearly interpolating the descriptors we feed to the SfSS-decoder produces plausible interpolations of the generated shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Other applications</head><p>Shape from ID. To assess the ability of our method to represent highly complex datasets, we follow Tatarchenko et al. <ref type="bibr" target="#b26">[27]</ref> and predict shapes from their Blendswap dataset at 512 3 voxels from a high-level shape ID. We find that our method is able to reconstruct the dataset at a similar quality level (97.8% IoU) as OGN <ref type="bibr" target="#b26">[27]</ref> (96.9% IoU), but in contrast to <ref type="bibr" target="#b26">[27]</ref> using a 2D representation alone.</p><p>Shape from shape similarity and shape generation. We aim to assess our model's ability to reconstruct shapes from high-level information without relying on a specific image encoder architecture. To that end, we train our network to generate shapes from a high-dimensional descriptor that captures shape similarities within a semantic category. We construct the descriptor by computing a pairwise similarity matrix of 3D models such that an entry at (i, j) represents the intersection over union between models i and j of resolution 32 3 in the ShapeNet-cars dataset. Reducing the dimensionality of the matrix with PCA while retaining 95% of the variance and removing duplicates yields 2424-dimensional descriptors for 7426 remaining shapes. Trained on 80% of the descriptors to generate shapes at 128 3 voxels resolution, our Matryoshka network reaches a mean intersection over union of 81.1% on the held out shapes. This Shape-from-Shape-Similarity (SfSS) decoder can also be used for interpolating between shapes ( <ref type="figure">Fig. 7)</ref> and to synthesize new shapes by supplying a random noise vector. As shown in <ref type="figure">Fig. 6</ref>, samples drawn from the model are quite diverse (c.f . <ref type="figure">Fig. 5</ref> of <ref type="bibr" target="#b24">[25]</ref> in contrast).</p><p>Reconstruction from real images. To show the applicability of our method to real-world images, we give a qualitative example in <ref type="figure" target="#fig_3">Fig. 8</ref>, c.f . supplemental for more examples.</p><p>Shape from silhouette. In the supplemental material, we additionally study the ability of our Matryoshka networks to reconstruct a 3D shape from a single silhouette image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we posed 3D shape reconstruction as a 2D prediction problem, allowing us to leverage well-proven architectures for 2D pixel-prediction. Both proposed networks clearly outperform dense voxel-based approaches at low resolutions. Our novel efficient encoding based on nested shape layers, furthermore, allows to scale our Matryoshka networks to handle reconstruction of shapes at a high resolution, while outperforming octree-based decoder architectures with a considerable margin, despite being based only on standard network layers. Applications to shape from ID and shape similarity, as well as shape sampling demonstrated the broad applicability of our approach.</p><p>The proposed shape layer encoding requires fewer than 5 layers even for high-resolution shapes. We consequently fix the maximum number of components in our experiments. To encode arbitrarily complex objects without requiring retraining, the required number of components could be predicted per individual object in a recursive formulation. We leave this and learning the shape fusion <ref type="bibr" target="#b20">[21]</ref> for future work.</p><formula xml:id="formula_11">∆f ↑ = f inner −f out d o + d .<label>(12)</label></formula><p>To obtain predictions with the desired number of output channels (equaling s out for voxel tube networks and the number of shape layers ×6 for Matryoshka networks) we simply add a 2D convolution with kernel size 1 as final layer to our networks. We summarize the architectures and training schedules used in the individual experiments in Tab. 7. For a batch size of 128, we start with a learning rate of 0.001 and reduce it by a factor of 10 after drop epochs. For any different batch size, we scale the learning rate accordingly. All models were trained on a single GPU.</p><p>For the ablation studies, we used a voxel tube network as summarized in the penultimate row of Tab. 7. Since the networks for the shape-from-silhouette task and the ablation studies were trained on renderings of smaller resolution and on a smaller number of categories, we roughly halved the number of feature channels at the bottleneck (setting it to 257 for an integer ∆f ↑ ).</p><p>For the study on network architectures, we refer to the voxel tube network as described above as ResNet-based network. We remove the identity pathways from all residual modules to obtain an Encoder/decoder network, and add skip connections between layers of same spatial resolution to obtain a U-Net, c.f . <ref type="bibr" target="#b22">[23]</ref>. To adapt the number of feature channels for the skip connections, we use a 1 × 1 2D convolution akin to the identity path of the upsampling module in <ref type="figure">Fig. 9</ref>. The DenseNet-inspired version (c.f . <ref type="bibr" target="#b13">[14]</ref>) of our voxel-tube network consists of 7 dense blocks (B), 2 up-transitions (U) and 3 down-transitions (D) arranged as BDBDBDBBUBUB. Each dense block contains 2 dense layers with an expansion factor of 16. For the down-transitions we halve the spatial resolution while keeping the number of feature channels constant. For the up-transitions we double the spatial resolution and halve the number of feature channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Results</head><p>Shape from silhouette. We investigate the performance of our Matryoshka network on the task of reconstructing a 3D shape from a single silhouette image. To that end, we reconstruct the shapes of the 3 categories with the most examples (chair, car, table) from ShapeNet-core with the dataset split and shapes from Choy et al. <ref type="bibr" target="#b4">[5]</ref>. We obtain silhouettes from the alpha-channels of the renderings of Choy et al. As can be seen in Tab. 8, the network performs much better on cars than on tables or chairs. This can be attributed to the approximately convex shape of cars, which makes their silhouette a very effective cue for the overall shape. Compared to the easier setting of reconstructing shapes from a color image, the network performs remarkably well. Note, however, that the network for predicting shapes from color images was trained in a category-agnostic way, making the prediction considerably harder.   <ref type="table">Table 8</ref>: Shape from silhouette on ShapeNet-core.</p><p>Real-world images. To assess the performance of our proposed network for real world examples, we tested it on images from the Stanford Products Dataset <ref type="bibr" target="#b36">[37]</ref> (chairs) and the web (cars). Qualitative examples are shown in <ref type="figure" target="#fig_2">Fig. 10</ref> (chairs) and <ref type="figure" target="#fig_2">Fig. 11 (cars)</ref>. In both cases, we trained a category-specific Matryoshka network to predict 3D shapes at 128 3 resolution from a single image. For predicting chairs, we took the renderings of Choy et al. <ref type="bibr" target="#b4">[5]</ref> and created ground truth shapes of higher resolution from the corresponding ShapeNet <ref type="bibr" target="#b3">[4]</ref> models using binvox <ref type="bibr" target="#b35">[36]</ref>. Since most images of cars found on the web are recorded from different camera positions than the renderings of Choy et al., we re-rendered the car shapes from ShapeNet with random camera positions (focal length ∈ [40mm, 90mm), azimuth ∈ [0 • , 360 • ), elevation ∈ [0 • , 25 • ]) and environment maps collected from the web 1,2 . We find that Matryoshka networks generalize well to real-world imagery even when only trained on synthetic images. They are able to reconstruct thin structures (e.g., the legs of the right-most chair in <ref type="figure" target="#fig_2">Fig. 10</ref>) and a wide variety of shapes (both Figs. 10 and 11). Synthetic images. We show more results for predicting 3D shapes of high resolution in Figs. 13 (airplanes), 14 (chairs), and 12 (cars). The input images are renderings from Choy et al. <ref type="bibr" target="#b4">[5]</ref> and the shapes have been converted to binary voxel grids using binvox <ref type="bibr" target="#b35">[36]</ref>. The ground truth car shapes have  been provided by Tatarchenko et al. <ref type="bibr" target="#b26">[27]</ref>. Supporting the quantitative results from the main paper, learning to reconstruct 3D shapes at higher resolution produces much more accurate predictions, as can be seen for different resolutions in <ref type="figure" target="#fig_0">Fig. 12</ref>. Even for highly varied classes such as airplanes or chairs, Matryoshka networks produce high-quality reconstructions. Finally, we show qualitative examples of reconstructed shapes at low resolution from a voxel tube network and a Matryoshka network in <ref type="figure" target="#fig_2">Fig. 15</ref>. Both networks were trained on 13 categories from ShapeNet-core. Quantitative results for this experiment can be found in Tab. 2 in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Matryoshka network at 128 3 Ground truth <ref type="figure" target="#fig_2">Figure 13</ref>: Qualitative results at high resolution (128 3 ) for airplane images rendered from ShapeNet models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Matryoshka network at 128 3 Ground truth <ref type="figure" target="#fig_2">Figure 14</ref>: Qualitative results at high resolution (128 3 ) for chair images rendered from ShapeNet models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Voxel tube network at 32 3 Matryoshka network at 32 3 Ground truth <ref type="figure" target="#fig_2">Figure 15</ref>: Qualitative results at low resolution (32 3 ) for images rendered from ShapeNet models. For input images (left-most row), we predict 3D shapes using a voxel tube network (2 nd column) and a Matryoshka network (3 rd column). Ground truth shapes are shown in the right-most column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Voxel tube network at 32 3 Matryoshka network at 32 3 Ground truth <ref type="figure" target="#fig_2">Figure 15</ref>: Qualitative results at low resolution (32 3 ) for images rendered from ShapeNet models (continued).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Voxel tube network at 32 3 Matryoshka network at 32 3 Ground truth <ref type="figure" target="#fig_2">Figure 15</ref>: Qualitative results at low resolution (32 3 ) for images rendered from ShapeNet models (continued).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Memory-efficient geometry decoders. Encoding features jointly per voxel tube turns a standard voxel decoder (left) into a voxel tube image decoder (middle). Predicting shape layers (right) allows for reconstructing shapes at higher resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Table 1 .</head><label>1</label><figDesc>IoU of reconstruction 97.8 99.9 100.0 100.0 100.0 Modeling power of nested shape layers. Percentage of ShapeNet-core/cars shapes completely reconstructed with given number of shape layers. Higher resolutions require more layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative result for 3D shape reconstruction from real-world images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative results at high resolution (128 3 ) for real-world images of chairs. For a given input image (top row), our Matryoshka network predicts a 3D shape (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>Qualitative results at high resolution (128 3 ) for real-world images of cars. For a given input image (top row), our Matryoshka network predicts a 3D shape (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Shapes reconstructed from a single image by our Matryoshka network at different resolutions.</figDesc><table><row><cell>, middle &amp;</cell></row></table><note>. Single image 3D shape reconstruction on ShapeNet-core at 32 3 resolution. We report the mean IoU (%) per category, and the average IoU over all categories. Our networks outperform all voxel decoder baselines and are competitive with the more complex PSGN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Influence of loss functions. We report the IoU for our voxel tube network trained with several loss functions on the car, chair, and table categories.</figDesc><table><row><cell>Base architecture</cell><cell>car</cell><cell>chair table</cell><cell>mean</cell></row><row><cell>Encoder/decoder [19]</cell><cell cols="2">73.0 52.5 57.0</cell><cell>60.8</cell></row><row><cell>U-Net [23] ResNet [12] DenseNet [14]</cell><cell cols="2">74.2 54.8 58.8 75.6 56.8 59.1 72.3 49.4 55.8</cell><cell>62.6 63.8 59.2</cell></row><row><cell cols="4">Table 4. Evaluation of base architectures. Across all categories, the ResNet-inspired architecture outperforms all other networks</cell></row><row><cell>with a significant margin.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Loss function</cell><cell>car</cell><cell>chair table</cell><cell>mean</cell></row><row><cell>Binary cross-entropy</cell><cell cols="2">75.9 57.8 58.2</cell><cell>64.0</cell></row><row><cell>L1 L2 Cosine similarity LC Approx. IoU LIoU</cell><cell cols="2">73.6 57.2 57.4 76.4 58.0 58.7 75.7 58.4 59.3 76.3 58.3 59.5</cell><cell>62.7 64.4 64.5 64.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Residual modules. Each residual module consists of Batch normalization (BN) and Rectified Linear Unit (ReLU) layers followed by a 2D convolution, except for the up-sampling module where we replace the first convolution with a transposed convolution. The number of feature channels is denoted as f . Moreover, k denotes the filter size and s the stride.</figDesc><table><row><cell>Constant size</cell><cell cols="2">Down-sampling</cell><cell></cell><cell></cell><cell cols="3">Up-sampling</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BN, ReLU</cell><cell>BN, ReLU</cell><cell></cell><cell></cell><cell></cell><cell>BN, ReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv2D f, k=3, s=1</cell><cell>Conv2D f, k=3, s=2</cell><cell cols="2">Downsample s=2</cell><cell cols="3">Conv2D T f, k=4, s=2</cell><cell cols="3">Upsample s=2</cell><cell></cell></row><row><cell>BN, ReLU</cell><cell>BN, ReLU</cell><cell>Pad 0</cell><cell></cell><cell></cell><cell>BN, ReLU</cell><cell></cell><cell cols="3">Conv2D f, k=1, s=1</cell><cell></cell></row><row><cell>Conv2D f, k=3, s=1</cell><cell>Conv2D f, k=3, s=1</cell><cell></cell><cell></cell><cell cols="3">Conv2D f, k=3, s=1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 9: Network</cell><cell cols="10">sin sout batch size epochs dropfin d finner bfout</cell></row><row><cell>ShapeNet-all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Voxel tube</cell><cell>128</cell><cell>32</cell><cell>128</cell><cell>45</cell><cell>15</cell><cell>8</cell><cell>3</cell><cell>512</cell><cell>1</cell><cell>32</cell></row><row><cell>Matryoshka</cell><cell>128</cell><cell>32</cell><cell>128</cell><cell>40</cell><cell>20</cell><cell>8</cell><cell>3</cell><cell>512</cell><cell cols="2">2 128</cell></row><row><cell>High resolution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Matryoshka</cell><cell>128</cell><cell>32</cell><cell>128</cell><cell>30</cell><cell>20</cell><cell>8</cell><cell>3</cell><cell>512</cell><cell cols="2">0 128</cell></row><row><cell>Matryoshka</cell><cell>128</cell><cell>64</cell><cell>128</cell><cell>30</cell><cell>20</cell><cell>8</cell><cell>4</cell><cell>512</cell><cell cols="2">3 128</cell></row><row><cell>Matryoshka</cell><cell cols="2">128 128</cell><cell>32</cell><cell>30</cell><cell>20</cell><cell>8</cell><cell>5</cell><cell>512</cell><cell cols="2">1 128</cell></row><row><cell>Matryoshka</cell><cell cols="2">128 256</cell><cell>8</cell><cell>30</cell><cell>20</cell><cell>8</cell><cell>6</cell><cell>512</cell><cell cols="2">0 128</cell></row><row><cell>Shape from Silhouette</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Matryoshka</cell><cell>64</cell><cell>32</cell><cell>128</cell><cell>40</cell><cell>15</cell><cell>8</cell><cell>3</cell><cell>257</cell><cell>2</cell><cell>32</cell></row><row><cell>Shape from ID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Matryoshka</cell><cell cols="2">2 512</cell><cell>4</cell><cell>28K</cell><cell>12K</cell><cell>-</cell><cell>8</cell><cell>1</cell><cell cols="2">0 196</cell></row><row><cell>Ablation studies</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Voxel tube</cell><cell>64</cell><cell>32</cell><cell>128</cell><cell>40</cell><cell>15</cell><cell>8</cell><cell>3</cell><cell>257</cell><cell>2</cell><cell>32</cell></row><row><cell>Shape from Similarity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Matryoshka</cell><cell cols="2">1 128</cell><cell>8</cell><cell>60</cell><cell>25</cell><cell>-</cell><cell>7</cell><cell>2424</cell><cell cols="2">0 128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc>Network architectures for individual experiments. See text for a description of the network parameters.</figDesc><table><row><cell>Category</cell><cell>car chair table</cell><cell>mean</cell></row><row><cell>Shape from silhouette</cell><cell>86.7 53.2 58.8</cell><cell>66.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.hdrlabs.com/sibl/archive.html 2 https://hdrihaven.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>Our networks consist of multiple residual modules (c.f . <ref type="bibr" target="#b11">[12]</ref>) as shown in <ref type="figure">Fig. 9</ref>. Across all modules, we keep the kernel size k and the stride s constant as depicted. For all convolutions with a kernel size k &gt; 1, we use reflective padding of size 1. Altering the spatial resolution and number of feature channels requires special handling of the identity pathway of the residual modules. For downsampling ( <ref type="figure">Fig. 9</ref>, middle), we simply drop every other pixel and initialize the added feature channels as zero using zeropadding. For up-sampling ( <ref type="figure">Fig. 9</ref>, right), we use nearest neighbor interpolation and a 1 × 1 convolution to project the feature dimension. We experimented with more sophisticated up-and down-sampling alternatives, but found no significant benefits.</p><p>In all experiments with images as inputs, processing in our networks begins with a feature generation module, which produces an initial representation with f in feature channels. This module is equivalent to the residual module operating at constant resolution ( <ref type="figure">Fig. 9, left)</ref>, but with the first rectified linear unit and identity pathway removed. Each module is only parametrized by the number of feature channels added during down-sampling ∆f ↓ or removed during up-sampling ∆f ↑ , and we pair each up-sampling and down-sampling module with a subsequent module of same resolution to form one residual block. Thus, we specify network architectures by a desired number of initial featureŝ f in , output featuresf out , features at the bottleneck f inner , number of desired down-sampling blocks d in the decoder, and residual blocks at the bottleneck b. We match the number of down-sampling blocks in the encoder with the number of up-sampling blocks in the decoder. We set it to 3 for an output resolution s out = 32 and increase it by 1 for every doubling of s out . If input and output resolutions are different, we add d i = log 2 s in − log 2 s out down-sampling blocks or d o = log 2 s out − log 2 s in up-sampling blocks accordingly. For all networks, we scale input images to powers of 2. We compute the number of feature channels to add for each down-sampling block as</p><p>and adjust the number of initially generated features as</p><p>to obtain integral numbers for the number of feature channels. Analogously, we compute the number of feature channels added per up-sampling block as</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimizing expected intersection-over-union with candidate-constrained CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optimization of the Jaccard index for image segmentation with the Lovász hinge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<idno>abs/1705.08790</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
		<title level="m">ShapeNet: An information-rich 3D model repository. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D-R2N2: A unified approach for single and multi-view 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What you sketch is what you get: 3D sketching using multi-view deep volumetric prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delanoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>abs/1707.08390</idno>
	</analytic>
	<monogr>
		<title level="m">I3D</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Interactive order-independent transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Everitt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Nvidia</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d reconstruction using an n-layer heightmap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hierarchical surface prediction for 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Geometric and Solid Modeling: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Hoffmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Categoryspecific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning efficient point cloud generation for dense 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">3D shape reconstruction from sketches via multi-view convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative shape from shading in uncalibrated illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Octnetfusion: Learning depth fusion from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SurfNet: Generating 3D shape surfaces using deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Unmesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Synthesizing 3D shapes via modeling multiview depth maps and silhouettes with deep generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Soltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-view 3D models from single images with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3D outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Marrnet: 3d shape reconstruction via 2.5 d sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning single-view 3D object reconstruction without 3D supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking reprojection: Closing the loop for pose-aware shape reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D-PRNN: Generating shape primitives with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Min</surname></persName>
		</author>
		<idno>2017-11-22. 2</idno>
		<ptr target="http://www.patrickmin.com/binvox" />
		<imprint>
			<biblScope unit="page" from="2004" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Qualitative results at varying resolution. We train Matryoshka networks to reconstruct 3D shapes from a single image rendered from ShapeNet models (top row) for output resolutions 32 3 , 64 3 , 128 3 , and 256 3 . The last row shows the ground truth shapes</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
