<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TP-LSD: Tri-Points Based Line Segment Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbo</forename><surname>Qin</surname></persName>
							<email>qinfangbo2013@ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
							<email>xiongpengfei@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TP-LSD: Tri-Points Based Line Segment Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Line Segment Detection</term>
					<term>Low-level vision</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel deep convolutional model, Tri-Points Based Line Segment Detector (TP-LSD), to detect line segments in an image at real-time speed. The previous related methods typically use the two-step strategy, relying on either heuristic post-process or extra classifier. To realize one-step detection with a faster and more compact model, we introduce the tri-points representation, converting the line segment detection to the end-to-end prediction of a root-point and two endpoints for each line segment. TP-LSD has two branches: tri-points extraction branch and line segmentation branch. The former predicts the heat map of root-points and the two displacement maps of endpoints. The latter segments the pixels on straight lines out from background. Moreover, the line segmentation map is reused in the first branch as structural prior. We propose an additional novel evaluation metric and evaluate our method on Wireframe and YorkUrban datasets, demonstrating not only the competitive accuracy compared to the most recent methods, but also the real-time run speed up to 78 FPS with the 320 × 320 input.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Compact environment description is an important issue in visual perception. For man-made environments with various flat surfaces, line segments can encode the environment structure, providing fundamental information to the upstream vision tasks, such as vanishing point estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, 3D structure reconstruction <ref type="bibr" target="#b15">[16]</ref>, distortion correction <ref type="bibr" target="#b24">[24]</ref>, and pose estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>With the rapid advance of deep learning, deep neural networks are applied to line segment detection. As shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>, the existing methods have two steps. With the top-down strategy it first detects the region of a line and then squeezes the region into a line segment <ref type="bibr" target="#b22">[22]</ref>, which might be affected by regional textures and does not have an explicit definition of endpoints. With the bottomup strategy it first detect junctions and then organize them to line segments using grouping algorithm <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, or extra classifier <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28]</ref>, which might be prone to the inaccurate junction predictions caused by local ambiguity. The two-step strategy might also limit the inference speed in real-time applications.</p><p>Considering the above problems, we propose the tri-points (TP) representation, which uses a root-point as the unique identity to localize a line segment, and the two corresponding end-points are represented by their displacements w.r.t the root-point. Thus a TP encodes the length, orientation and location of a line segment. Moreover, inspired by that human perceive line segments according to straight lines, we leverage the straight line segmentation map as structural prior to guide the inference of TPs, by embedding feature aggregation modules which fuse the line-map with TP related features. Accordingly, Tri-Points Based Line Segment Detector (TP-LSD) is designed, which has three parts: feature extraction backbone, TP extraction branch, and line segmentation branch.</p><p>As to the evaluation of line segment detection, the current metrics either treat a line segment as a set of pixels, or use squared euclidean distance to judge the matching degree, which cannot reflect the various relationships between line segments such as intersection and overlapping. Therefore we propose a new metric named line matching average precision from a camera model perspective.</p><p>In summary, the main contributions of this paper are as follows:</p><p>-We utilize the TP representation to encode line segment, based on which TP-LSD is proposed to realize the real-time and compact one-step detection pipeline. The synthesis of local root-point detection and global shape inference makes the detection more robust to various textures and spatialdistributions. -A novel evaluation metric is designed based on the spatial imaging geometry, so that the relative spatial relationship between line segments is reflected more distinctively.</p><p>-Our proposed method obtains the state-of-the-art performance on two public LSD benchmarks. The average inference speed achieves up to 78 FPS, which significantly promotes the LSD applications in real-time tasks.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hand-crafted Feature Based Methods</head><p>Line segment detection is a long-standing task in computer vision. Traditional methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12</ref>] usually depend on low-level cues like image gradients, which are used to construct line segments with predefined rules. However, the handcrafted line segment detectors are sensitive to the threshold settings and image noise. Another way to detect line segments applies Hough transform <ref type="bibr" target="#b21">[21]</ref>, which is able to use the entire image's information but difficult to identify the endpoints of line segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Edge and Line Segment Detection</head><p>In the past few years, CNN-based methods have been introduced to solve the edge detection problem. HED <ref type="bibr" target="#b19">[20]</ref> treats edge detection problem as pixel-wise binary classification, and achieves significant performance improvement compared to traditional methods. Following this breakthrough, numerous methods for edge detection have been proposed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>. However, edge maps lack explicit geometric information for compact environment representation. Most recently, CNN-based method has been realized for line segment detection. Huang et al. <ref type="bibr" target="#b7">[8]</ref> proposed DWP, which includes two parallel branches to predict junction map and line heatmap in an image, then merges them as line segments. Zhang et al. <ref type="bibr" target="#b25">[25]</ref> and Zhou et al. <ref type="bibr" target="#b28">[28]</ref> utilize a point-pair graph representation for line segments. Their methods (PPGNet and L-CNN) first detect junctions, then use an extra classifier to create an adjacency matrix to identify whether a point-pair belongs to the same line segment. Xue e al. <ref type="bibr" target="#b22">[22]</ref> creatively presented regional attraction of line segment maps, and proposed AFM to predict attraction field maps from raw images, followed by a squeeze module to produce line segments. Furthermore, Xue et al. <ref type="bibr" target="#b23">[23]</ref> proposed a 4-D holistic attraction field map (H-AFM) to better parameterize line segments, and proposed HAWP with L-CNN pipeline. Though learning-based methods have significant advantages over the hand-crafted ones. However, their two-step strategy might limit their real-time performance, and rely on extra classifier or heuristic post-process. Moreover, the relationship between line-map and line segments is under-utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Object Detection</head><p>Current object detectors represent each object by an axis-aligned bounding box and classify whether its content is a specific object or background <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>. Recently, keypoint estimation has been introduced to object detection to avoid the  dependence on generating boxes. CornerNet <ref type="bibr" target="#b8">[9]</ref> detects two bounding box corners as keypoints, while ExtremeNet <ref type="bibr" target="#b27">[27]</ref> detects the top-, left-, bottom-, right-most and center points of all objects. These two models both require a grouping stage to form objects based on the extracted keypoints. CenterNet <ref type="bibr" target="#b26">[26]</ref> represents objects by the center of bounding boxes, and regresses other properties directly from image features around the center location. Such anchor-free based methods have achieved good detection accuracy with briefer structure, motivated by which we adopt a similar strategy to detect line segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tri-Points representation</head><p>The Tri-Points (TP) representation is inspired by how people model a long narrow object. Intuitively, we usually find a root point on a line, then extend it from the root-point to two opposite directions and determine the endpoints. TP contains three key-points and their spatial relationship to encode a line segment. The root-point localizes the center of a line segment. The two end-points are represented by two displacement vectors w.r.t the root point, as illustrated in <ref type="figure" target="#fig_1">Fig. 2c, 2d</ref>. It is similar to SPM <ref type="bibr" target="#b12">[13]</ref> used in human pose estimation. The conversion from a TP to a vectorized line segment, which is denotes as TP generation opperation, is expressed by, (x s , y s ) =(x r , y r ) + d s (x r , y r ) (x e , y e ) =(x r , y r ) + d e (x r , y r )</p><p>where (x r , y r ) denotes the root-point of a line segment. (x s , y s ) and (x e , y e ) represent its start-point and end-point, respectively. Generally, the most left point is the start-point. Specially, if line segment is vertical, the upper point is the start-point. d s (x r , y r ) and d e (x r , y r ) denote the predicted 2D displacements from root-point to its corresponding start-point and end-point, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>Based on the proposed Tri-Points, a one-step model TP-LSD is proposed for line segment detection, whose architecture is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>  used to generate shared features, which are then fed to two branches: 1) TP extraction branch, which contains a root-point detection task and a displacement regression task; 2)line segmentation branch, which generates a pixel-wise line-map. These two branches are bridged by feature aggregation modules. Finally, after processed by point filter module, the filtered TPs are transformed to vectorized line segment instances with TP generation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TP Extraction Branch</head><p>Root-Point Detection The first task in TP extraction branch is to detect root points. Similar to CenterNet <ref type="bibr" target="#b26">[26]</ref>, each pixel is classified to discriminate whether it is a root-point. The output activation function is sigmoid function. MCM. Because of the narrow and even long shape of line segment, it requires a large receptive field to classify the center of line segment. Therefore, a mixture convolution module (MCM) is introduced to provide the adaptive and expanded reception field, by cascading three convolution layers, a 3×3 deformable convolutional layer, and two 3 × 3 atrous convolutional layers with dilation rate= 2, whose strides are all set as 1.</p><p>Displacement Regression The second task in TP extraction branch is to regress the two displacements of the start and end points w.r.t a root-point in the continuous domain. The sparse maps for the displacements are inferred by one 3×3 deformable convolutional, two 3×3 convolutional and a 1×1 convolutional layers, whose strides are all set as 1. With the output maps, we can index the related displacements by positions. Given a root point (x r , y r ), the corresponding displacements are indexed as d s (x r , y r ) and d e (x r , y r ). Then the coordinates of the start-and end-points can be obtained by Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Line Segmentation Branch</head><p>Pixel-wise map of straight lines is easier to obtain because the precise determination of end-points is not required. Based on the idea that line segment is highly related to straight line, we use a straight line segmentation branch to provide prior knowledge for line segment detection. First, straight line can serve as spatial attention cue. Second, a root-point must be localized on a straight line. As is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the line segmentation branch has two 3×3 convolutional layers with the stride 1. The output activation function is sigmoid function, so that the line-map P (L) has the pixel values ranging within (0, 1).</p><p>FAM. From the multi-modal feature fusion prospective, we present a feature aggregating module (FAM) to aggregate the structural prior of line-map with the TP extraction branch. Given a line-map P (L) from the line segmentation branch, the straight line activation map A l is obtained by tanh(w×P (L)+b) where w, b denotes the parameters of a 1 × 1 convolutional layer, and the tanh gating function indicates whether a pixel is activated or suppressed according to its relative position to a straight line. The shared feature is firstly aggregated with the straight line activation map A l by concatenation, and then fed to the root-point detection sub-branch, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. For the displacement regression sub-branch, similarly, the straight line activation map and the root-point activation map are obtained by 1×1 conv and tanh, then fused with the shared feature map by concatenation, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Thus the prior knowledge of straight line and root point can benifit the displacement regression.</p><p>PFM. The line-map can also be leveraged to filter the noisy root-points that lies out of line. We consider the root-point confidence map as a probability distribution P (R|L) conditioned on line existence. Thus the root-point confidence map P (R|L) can be refined by the multiplication with the line confidence map P (L), which is called point filter module (PFM), as given bỹ</p><formula xml:id="formula_1">P (R) =P (R|L) ×P (L) α<label>(2)</label></formula><p>where the power coefficient α ∈ (0, 1) is to adjust the contribution of line-map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training and Inference</head><p>Feature extractor. A U-shape network is used as the feature extractor. After a backbone encoder, there are four decoder blocks. Each decoder block is formed by a bi-linear interpolation based up-sampling and a residual block. Skip connection is used to aggregate multi-scale features by concatenating the low level features with the high level features. The output of the feature extractor is a 64-channel feature map, whose size is the same with the input image, or optionally half of the input size for faster inference. This feature map is used as the shared features for the following branches.</p><p>Loss. In training stage, the input image is resized to 320 × 320, and the outputs include a line-map, a root-point confidence map, and four displacement maps, whose ground truths are generated from the raw line segment labels. The three tasks' losses are combined as Eq. <ref type="formula" target="#formula_2">(3)</ref>, where λ root,disp,line = {50, 1, 20}.</p><formula xml:id="formula_2">L total = λ root L root + λ disp L disp + λ line L line<label>(3)</label></formula><p>The ground truth of root-point confidence map is constructed by marking the root-point positions on a zero-map and then smoothed by a scaled 2D Gaussian kernel truncated by a 5 × 5 window, so that the root-point has the highest confidence 1, and its nearby pixels have lower confidence. A weighted binary cross-entropy loss L root is used to supervise this task. The ground truths of the displacement maps are constructed by assigning displacement values at the root-point positions on the zero-maps. For each ground truth line segment, its mid-point is considered as the root point. For the pixels within a 5 × 5 window centered at the mid-point, we calculate the displacements from it to the startand end-points, then assigned the displacement values to these pixels. After all the ground truth line segments are visited, the final displacements maps are used for smoothed L1 loss L disp based regression learning. Note that only the rootpoints and its 5 × 5 neighbourhood window are involved in the loss calculation. As to the line segmentation sub-task, the ground truth of line segmentation map are constructed by simply draw the line segments on a zero map and the learning is supervised by the weighted binary cross entropy loss L line .</p><p>In the inference stage, after the root-point confidence map is produced, the non-maximum suppression is operated to extract the exact root-point positions. Afterwards, we use the extracted root-points and their corresponding displacements to generate line segments from TPs with Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Metrics</head><p>In this section, we briefly introduce two existed evaluation metrics: pixel based metric and structural average precision, and then design a novel metric, line matching average precision.</p><p>Pixel based metric: For a pixel on a detected line segment, if its minimum distance to all the ground truth pixels is within the 1 percent of the image diagonal size, it is regarded as true positive. After evaluating all the pixels on the detected line segments, the F-score F H can be calculated <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b28">28]</ref>. The limitation is that it cannot reveal the continuity of line segment. For example, if a long line segment is broken into several short ones, the F-score is high but these split line segments is not suitable for 3D reconstruction or wireframe parsing.</p><p>Structural Average Precision: The structural average precision (sAP) <ref type="bibr" target="#b28">[28]</ref> uses the sum of squared error (SSE) between the predicted end-points and their ground truths as evaluation metric. The predicted line segment will be counted as a true positive detection when its SSE is less than a threshold, such as = 5, 10, 15. However, line segment matching could be more complicated than point pair correspondence. For example, in <ref type="figure" target="#fig_4">Fig. 4b, 4c</ref>, it is shown that sAP is not discriminative enough for some different matching situations.</p><p>Line Matching Average Precision: To better reflect the various line segment matching situations in term of direction and position as well as length, the Line Matching Score (LMS) is proposed. LMS contains two parts: Score θ denotes the differences in angle and position, and Score l denotes the matching degree in length. The LMS is calculated by  Inspired by 3D line reconstruction, Score θ is calculated in the 3D camera frame as shown in <ref type="figure" target="#fig_4">Fig. 4a</ref>. A line segment and the camera's optical center jointly determine a unique plane whose normal vector is n. Thus, given a predicted and a ground truth line segments, they determine two 3D planes, and the angle between their normal vectors is used to measure the directional matching degree. The angle is equal to 0 if and only if the two line segments are collinear. To calculate Score θ , Firstly, a ground truth line segment is aligned to the center of the image plane by subtracting the coordinates of the midpoint l m = (x m , y m ) . The endpoints of detected line segment is also subtracted by l m . Then, the endpoints are projected from the 2D image plane l i = (x i , y i ) , i = s, e onto the 3D normalized image plane by dividing the camera focal length, i.e.l i = xi f , yi f , 1 . Finally, the normal vectors n gt and n pred are obtained by crossmultiplying their endpointl s ×l e , respectively. Score θ is given by,</p><formula xml:id="formula_3">LM S = Score θ × Score l<label>(4)</label></formula><formula xml:id="formula_4">Score θ = 1 − θ(ngt,n pred ) η θ , if θ(n gt , n pred ) &lt; η θ 0, otherwise<label>(5)</label></formula><p>where θ () is to calculate the angle between two vectors with the unit degree. η θ is a minimum threshold. Score l demonstrates the overlap degree of two line segment. The ratio of overlap length against the ground truth length is η 1 . The ratio of overlap length against the projection length is η 2 .</p><formula xml:id="formula_5">η 1 = L pred ∩ L gt L gt , η 2 = L pred ∩ L gt L pred |cos(α)|<label>(6)</label></formula><p>where L is the length of line segment and L pred ∩ L gt is the overlap length of the predicted line segment projected to the ground truth line segment. α is the angle between the two line segments in 2D image. Then Score l is calculated by,</p><formula xml:id="formula_6">Score l = η1+η2 2 , if η 1 ≥ η l , and η 2 ≥ η l 0, otherwise<label>(7)</label></formula><p>where η l denotes a minimum threshold. Since the focal length of a camera might be unknown for public data sets, to make a fair comparison, we firstly re-scale the detected line segments with the same ratio of resizing the original image to the resolution 128 × 128, and set a virtual focal length f = 24. Besides we set η θ = 10 • and η l = 0.5 in this work.</p><p>Using LMS to determine true positive, i.e. a detected line segment is considered to be true positive if LMS&gt; 0.5, we can calculate the Line Matching Average Precision (LAP) on the entire test set.LAP is defined as the area under the precision recall curve.</p><p>Analysis of metric on real image. We compare the line matching evaluation results between SSE used in sAP and LMS used in LAP on a real image, as shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. Comparing the areas labeled by yellow boxes in <ref type="figure" target="#fig_5">Fig. 5b</ref> and <ref type="figure" target="#fig_5">Fig. 5a</ref>, the detected line segments have obvious error direction compared to ground truth. However, SSE gives the same tolerance for line segments with different lengths, and accepts them as true positive matches. In contrast, as shown in <ref type="figure" target="#fig_5">Fig. 5c</ref>, LMS could better capture the direction errors and give the correct judgement. As shown by the green boxes in <ref type="figure" target="#fig_5">Fig. 5a and Fig. 5c</ref>, for the line segment with the correct direction but the slightly shorter length compared with the ground truth, namely, whose Score l is lower than 1 but greater than η l , LMS will accept it while SSE would not. Considering that the direction of line segments are more important in upper-level applications such as SLAM, this deviation can be acceptable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Experiments are conducted on Wireframe dataset <ref type="bibr" target="#b7">[8]</ref> and YorkUrban dataset <ref type="bibr" target="#b2">[3]</ref>. Wireframe contains 5462 images of indoor and outdoor man-made environments, among which 5000 images are used for training. To validate the generalization ability, we also evaluate on YorkUrban Dataset <ref type="bibr" target="#b2">[3]</ref>, which has 102 test images. We use the standard data augmentation procedure to expand the diversity of training samples, including horizontal and vertical flip, rotation and scaling. The hardware configuration includes four NVIDIA RTX 2080Ti GPUs and an Intel Xeon Gold 6130 2.10 GHz CPU. We use the ADAM optimizer with an initial learning rate of 1 × 10 −3 , which is divided by 10 at the 150th, 250th, and 350th epoch. The total training epoch is 400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Analysis of TP-LSD</head><p>We run a series of ablation experiments to study our proposed TP-LSD on Wireframe dataset. The evaluation results are shown in <ref type="table">Table 1</ref>. F H refers to pixel based metric <ref type="bibr" target="#b7">[8]</ref>. sAP 10 is the structural average precision <ref type="bibr" target="#b28">[28]</ref> with threshold of 10. LAP is the proposed metric. As presented in <ref type="table">Table 1</ref>, all the proposed modules present contributions to the performance improvements.</p><p>LSB. After integrating the line-map segmentation branch with the TP extraction branch without cross-branch guidance, the multi-task learning improves the performance from 0.782 to 0.808, because the line segmentation learning can guide the model to learn more line-awareness features.</p><p>FAM. FAM combines the cross-branch guidance with the line-map segmentation branch. Although the F H metric increases indicating the better pixel localization accuracy, sAP 10 and LAP are slightly decreased, because of the a larger number of line segments are detected.</p><p>MCM. Mixture Convolution Module is applied in root-point detection subbranch. Compared to the standard convolution layers, MCM improves the LAP scores significantly, showing a better matching degree. <ref type="table">Table 1</ref>: Ablation study of TP-LSD on Wireframe dataset. "LSB", "FAM" and "MCM" refer to line segmentation branch, feature aggregate module and mixture convolution module, respectively. α is the contribution ratio of line-map as Eq. (2). "R/S" refers to the rotation and scale data augmentation strategy. "Avg. line Num." means the average number of detected line segments whose confidence are greater than 0.2. PFM. With PFM and the contribution ratio of α = 0.5, the precision is increased while the recall slightly decreased, which lead to a better overall accuracy. The decrease in sAP 10 and LAP is due to the reduced confidence of the root-points after PFM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. LSB FAM MCM PFM R/S F</head><p>Augmentation. The 7 th row in <ref type="table">Table 1</ref> shows the data augmentation with only horizontal and vertical flip. Compared to the result in 6 th row, the lower performance shows that the rotation and scaling based data augmentation can further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretability.</head><p>To explore what the network learned from the line segment detection task, we use Guided Backpropogation <ref type="bibr" target="#b17">[18]</ref> to visualize which pixels are important for the root-point detection. Guided Backpropogation interprets the pixels' importance degree on the input image, by calculating the gradient flow from the output layer to the input images. The gradients flowed to the input images from the three specific detected root-point are visualized in <ref type="figure">Fig. 6</ref>. We find that the network automatically learns to localize the saliency region w.r.t a root-point, which is along a complete line segment. It shows that the root point detection task is mainly based on the line feature.</p><p>Furthermore, the integration of LSB lead to the higher influence of on-line pixels to root point prediction. Comparing <ref type="figure">Fig. 6c</ref> to <ref type="figure">Fig. 6b</ref>, the former presents higher gradient values along the line. The saliency maps obtained by model No. <ref type="bibr">3 and model No. 4</ref> are cleaner, and the saliency regions are more concentrated on specific line segments. With the introduction of MCM in model No. 4, the response of long line segment could be improved with a lager receptive field, which can be shown by the comparison between <ref type="figure">Fig. 6d</ref> and <ref type="figure">Fig. 6e</ref>. <ref type="table">Table 2</ref>: Evaluation results of different line segment detection methods. "/" means that the score is too slow to be meaningful. The best two scores are shown in red and blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Input Size</p><p>Wireframe dataset YorkUrban dataset FPS F H sAP 5 sAP 10 LAP F H sAP 5 sAP 10 LAP LSD <ref type="bibr" target="#b5">[6]</ref> 320 0.641 6.7 8. <ref type="bibr" target="#b7">8</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison with other methods</head><p>We compare our proposed TP-LSD with LSD 1 <ref type="bibr" target="#b5">[6]</ref>, DWP 2 <ref type="bibr" target="#b7">[8]</ref>, AFM 3 <ref type="bibr" target="#b22">[22]</ref>, L-CNN 4 and L-CNN with post-process (L-CNN(P)) <ref type="bibr" target="#b28">[28]</ref>. The source codes and their model weights provided by the authors are available online, except that we reproduced DWP by ourselves. F H , sAP and LAP are used to evaluate those methods quantitatively. For TP-LSD, we tried a series of minimum thresholds of the root-point detection confidence, ranging within (0.1, 0.8) with the step γ = 0.05. LSD is evaluated with − log(NFA) in 0.01×{1.75 0 , ..., 1.75 19 }, where NFA is the number of false positive detections. For other methods, we use the author recommended threshold array listed in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b28">28]</ref>.</p><p>We evaluate the methods on Wireframe and YorkUrban dataset. We use the model No. 6 in Section 6.1 as the representative model, named as TP-LSD-Res34. Furthermore, we alter the backbone with Hourglass used in L-CNN <ref type="bibr" target="#b28">[28]</ref> to form TP-LSD-HG. To achieve a faster speed, TP-LSD-Lite is realized by using the output of the last second layer of the decoder as the shared feature. Thus the input to the task branches has the smaller size of 160×160. And the final output of the task branches are upsampled back to 320×320 with the bi-linear interpolation.</p><p>The precision-recall curves are depicted in <ref type="figure" target="#fig_7">Fig. 7</ref> and the detection performances are reported in <ref type="table">Table 2</ref>. <ref type="figure" target="#fig_7">Fig. 7a</ref> and <ref type="figure" target="#fig_7">Fig. 7b</ref> show that TP-LSD outperforms other line segment detection methods, according to the pixel based PR curves. In addition, our one-step method provides the comparable detection performance compared to the two-step L-CNN that requires post-processing. We then evaluate the methods with the sAP and the proposed LAP. The precision recall curve of LAP in two datasets are drawn in <ref type="figure" target="#fig_7">Fig. 7c and Fig. 7d</ref>. The performances of AFM and LSD are limited by length prediction of line segments. As to DWP, the inaccurate direction prediction might affect the detection. Our method and L-CNN present the higher scores, which shows that these two methods perform better not only in detection but also in alignment. Moreover, our method has the better precision than L-CNN in higher recall region. Though the higher F H is obtained by TP-LSD-HG, the decreases of sAP and LAP were caused by the lower recall rate due to the lower feature map resolution. TP-LSD-Lite gets comparable generalization performance on both dataset. YorkUrban dataset is more challenging because only the line segments which satisfy the Manhattan World assumption are labeled out as ground truth, which causes lower precision.</p><p>Visualization and Discussion In <ref type="figure" target="#fig_8">Fig. 8</ref>, several results of line segment detection are visualized. LSD detected some noisy local textures without semantic meaning. Recent CNN-based methods have shown good noise-suppression ability because they obtain high-level semantics. AFM does not have explicit endpoint definition, limiting the accuracy of end-points localization. It also presented many short line segments. DWP gives a relatively cleaner detection result, but there exist some incorrectly connected junction pairs, caused by inaccurate junctions predictions and sub-optimal heuristic combination algorithm. L-CNN, which has a junction detector and an extra line segment classifier, has good visualization results. However, its line segment detection result rely on the junction detection and line feature sampling, which might be prone to missed junction and nearby texture variation. In comparison, the proposed TP-LSD method is capable to detect line segments in complicated even low-contrast environments as is shown on the first and the sixth rows in <ref type="figure" target="#fig_8">Fig. 8</ref>.</p><p>Inference Speed Based on NVIDIA RTX2080Ti GPU and Intel Xeon Gold 6130 2.10 GHz CPU, the inference speed is reported in <ref type="table">Table 2</ref>. With the image size of 320 × 320, the proposed TP-LSD achieve the real-time speed up to 78 FPS, offering the potential to be used in real-time applications like SLAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper proposes a faster and more compact model TP-LSD for line segment detection with the one-step strategy. Tri-points representation is used to encodes a line segment with three keypoints, based on which the line-segment detection is realized by end-to-end inference. Furthermore, the straight line-map is produced based on segmentation task, and is used as structural prior cues to guide the extraction of TPs. Both quantitatively and qualitatively, TP-LSD shows the improved performances compared to the existing models. Besides, our method achieves 78 FPS speed, showing potential to be integrated with realtime applications, such as vanishing point estimation, 3D reconstruction and pose estimation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Y.Fig. 1 :</head><label>1</label><figDesc>He is the corresponding author (heyijia2016@gmail.com). S. Huang and N. Ding contribution was made when they were interns at Megvii Research Beijing, Megvii Technology, China. arXiv:2009.05505v1 [cs.CV] 11 Sep 2020 Overview. (a) Compared to the existing two-step methods, TP-LSD detects multiple line segments simultaneously in one step, providing better efficiency and compactness. (b) Inference speed and F-score on Wireframe test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Line segment representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>An overview of our network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Evaluation metrics for line segment detection. (a) The geometric explanation of the proposed line matching score (LMS). The blue and red line segments on the normalized image plane correspond to the detection and ground truth, respectively, which determine two planes together with the optical center C. In (b) and (c), the different matching situations could have the same SSE score 8 with sAP metric. In contrast, the LMS gives the discriminative scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of line matching evaluation results using different metrics. (a) The ground truth line segments marked by red. (b) Line matching result using sAP 10 metric. (c) Line matching result using proposed LAP metric. In (b) and (c), the mismatched and matched line segments are marked by blue and red, respectively. The endpoints are marked by cyan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4 Fig. 6 :</head><label>46</label><figDesc>Gradient based interpretation of root-point detection. (a) Raw image and the root points (white dots) of three line segments. (b-e)The gradient saliency maps of the input layer backpropogated from the three root points detected by the four different models, based on Guided Back-propogation method<ref type="bibr" target="#b16">[17]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Precision-recall curves of line segment detection. The models are trained on Wireframe dataset and tested on both Wireframe and YorkUrban datasets. Scores below 0.1 are not plotted. The PR curves for LAP of DWP are not ploted for its lower score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Quanlitative evaluation of line detection methods on Wireframe dataset and YorkUrban dataset. The line segments and their end-points are marked by orange and cyan colors, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. A U-shape network is</figDesc><table><row><cell></cell><cell cols="3">Line Segmentation Branch</cell><cell>Line Map</cell><cell></cell></row><row><cell></cell><cell>Conv. 3*3</cell><cell cols="2">Conv. 3*3</cell><cell></cell><cell></cell></row><row><cell>Feature Extractor</cell><cell>Shared Features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Mixture Conv. Module</cell><cell></cell><cell></cell><cell>Root Point Map</cell></row><row><cell></cell><cell>FAM</cell><cell>Deform. Conv. 3*3</cell><cell>Atrous Conv. 3*3</cell><cell cols="2">Atrous Conv. 3*3</cell><cell>Conv. 3*3</cell><cell>PFM</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Displacement Maps</cell><cell>TP Generat.</cell></row><row><cell></cell><cell></cell><cell>FAM</cell><cell>Deform. Conv. 3*3</cell><cell>Conv. 3*3</cell><cell>×2</cell><cell>Conv. 1*1</cell><cell>Line Segments</cell></row><row><cell></cell><cell cols="3">TP Extraction Branch</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.ipol.im/pub/art/2012/gjmr-lsd/ 2 https://github.com/huangkuns/wireframe 3 https://github.com/cherubicXN/afm-cvpr2019 4 https://github.com/zhou13/lcnn</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Edlines: Real-time line segment detection by edge drawing (ed)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Akinlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Topal</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2011.6116138</idno>
		<ptr target="https://doi.org/10.1109/ICIP.2011.6116138" />
	</analytic>
	<monogr>
		<title level="m">18th IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="2837" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A novel linelet-based representation for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2703841</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2703841" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1195" to="1208" />
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<editor>Forsyth, D., Torr, P., Zisserman, A.</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="197" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Line-based relative pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elqursh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2011.5995512</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2011.5995512" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3049" to="3056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lsd: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grompone Von Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2008.300</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2008.300" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="722" to="732" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wireframe parsing with guidance of distance map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2943885</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2019.2943885" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="141036" to="141044" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00072</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00072" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="626" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5872" to="5881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cannylines: A parameter-free line segment detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2015.7350850</idno>
		<ptr target="https://doi.org/10.1109/ICIP.2015.7350850" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="507" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6950" to="6959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contour primitives of interest extraction method for microscopic images and its application on pose measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMC.2017.2669219</idno>
		<ptr target="https://doi.org/10.1109/TSMC.2017.2669219" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1348" to="1359" />
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jägersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7471" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lifting 3d manhattan lines from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new approach to vanishing point detection in architectural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="647" to="655" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (workshop track</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using line segment clustering to detect vanishing point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Materials Research</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="1553" to="1558" />
			<date type="published" when="2011" />
			<publisher>Trans Tech Publ</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2015.164</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.164" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate and robust line segment extraction using minimum entropy with hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2014.2387020</idno>
		<ptr target="https://doi.org/10.1109/TIP.2014.2387020" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="822" />
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning attraction field representation for robust line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Holistically-Attracted Wireframe Parsing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to calibrate straight lines for fisheye image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ppgnet: Learning point-pair graph for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7098" to="7107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno>ArXiv abs/1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/iccv.2019.00105</idno>
		<ptr target="https://doi.org/10.1109/iccv.2019.00105" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
