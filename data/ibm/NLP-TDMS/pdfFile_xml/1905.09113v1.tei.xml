<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comprehensive Study of ImageNet Pre-Training for Historical Document Image Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Studer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Alberti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinaychandran</forename><surname>Pondenkandath</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Goktepe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kolonko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Fischer</surname></persName>
							<email>andreas.fischer@hefr.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
							<email>marcus.liwicki@ltu.se</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Machine Learning Group Lule√• University of Technology</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Ingold</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Document Image and Voice Analysis Group (DIVA</orgName>
								<orgName type="institution">University of Fribourg</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Complex Systems (iCoSys) University of Applied Sciences and Arts Western</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comprehensive Study of ImageNet Pre-Training for Historical Document Image Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic analysis of scanned historical documents comprises a wide range of image analysis tasks, which are often challenging for machine learning due to a lack of humanannotated learning samples. With the advent of deep neural networks, a promising way to cope with the lack of training data is to pre-train models on images from a different domain and then fine-tune them on historical documents. In the current research, a typical example of such cross-domain transfer learning is the use of neural networks that have been pre-trained on the ImageNet database for object recognition. It remains a mostly open question whether or not this pre-training helps to analyse historical documents, which have fundamentally different image properties when compared with ImageNet. In this paper, we present a comprehensive empirical survey on the effect of ImageNet pretraining for diverse historical document analysis tasks, including character recognition, style classification, manuscript dating, semantic segmentation, and content-based retrieval. While we obtain mixed results for semantic segmentation at pixel-level, we observe a clear trend across different network architectures that ImageNet pre-training has a positive effect on classification as well as content-based retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Historical documents span centuries of different writing supports (including stone, palm leaf, papyrus, parchment, and paper in different states of decay), writing instruments, languages, scripts, fonts, ornaments, illustrations, and so on. Furthermore, the image acquisition methods may vary substantially depending on the type of document. When performing automatic image analysis for a specific type of document using machine learning, one of the main challenges is to collect a sufficient amount of representative learning samples. In the case of ancient languages and scripts, such annotations often can only be provided by experts in the respective field and are thus costly to obtain. * These authors contributed equally to this work.</p><p>In recent years, the use of deep neural networks has strongly influenced the state of the art for historical document analysis. However, deep neural network models have millions of parameters to fine-tune and a random initialization <ref type="bibr" target="#b0">[1]</ref> may not be the best option when facing a lack of annotated training data. Several promising alternatives have been suggested including layer-wise pre-training <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and transfer learning <ref type="bibr" target="#b3">[4]</ref>. The latter is the main focus of the present paper. Transfer learning aims to fine-tune network parameters with respect to another image analysis task -which features a large amount of annotated training data -and then use these parameters as an initialisation for the image analysis task at hand.</p><p>Transfer learning is a widespread technique in computer vision <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Since the publication of large datasets such as ImageNet <ref type="bibr" target="#b6">[7]</ref>, CIFAR-10 <ref type="bibr" target="#b7">[8]</ref>, PASCAL <ref type="bibr" target="#b8">[9]</ref>, and COCO <ref type="bibr" target="#b9">[10]</ref>, many architectures have been trained on them and their weights made publicly available to be used for transfer learning. Although transfer learning has been around for the last two decades <ref type="bibr" target="#b10">[11]</ref>, it has only become popular in the last years with the breakthrough of Convolutional Neural Networks (CNNs) architectures consistently winning the Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" target="#b6">[7]</ref> competition since 2012 <ref type="bibr" target="#b11">[12]</ref>. Pre-training on ImageNet and successive fine-tuning on another dataset has become a widely used practice <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. It is generally believed that this approach helps to learn good and general features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution</head><p>Previous studies mostly focus on fine-tuning on datasets similar to the dataset used for pre-training. Moreover, they only explore a small set of tasks and neural network architectures. Historical documents have very different image properties when compared to the natural images found in the ImageNet dataset. It is therefore not immediately intuitive that pretraining a model on ImageNet for historical document analysis  The Kuzushiji-MNIST (KMNIST) dataset contains different Hiragana (cursive Japanese) characters, here depicted is the character for "o". The expected output is the character label of the image. The Classification of Medieval Handwritings in Latin Scripts (CLaMM) dataset is annotated for style classification and manuscript dating. The expected output is the style or date label for a given image. The Historical Manuscript Database DIVA-HisDB is annotated for semantic segmentation at pixel level. The example shown here is from manuscript CB55. The output is a segmentation label for each pixel, here shown with different colours. The Historical Writer Identification (Historical-WI) dataset consists of images from different writers. Here a section from one of the pages from writer 100 is depicted. The output of the network is a ranking of the most similar writers, based on the input image. will have the same benefits.</p><p>In this paper, we provide a comprehensive empirical study of the impact of transfer learning from ImageNet pre-trained models to historical document analysis. A variety of different applications, datasets and network architectures are taken into account. The applications can be grouped into three categories: classification, semantic segmentation at pixel level and contentbased image retrieval. Most of the datasets we use were published as part of previous competitions at the International Conference on Document Analysis and Recognition (ICDAR). Note that the main aim of our study is to investigate the effect of pre-training on relevant and high-quality datasets, and not to outperform the winners of the competitions by optimizing task-specific pre-and post-processing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>ImageNet is the most widely used dataset for pre-training and transfer learning. Popular beliefs as to why ImageNet is particularly suited for this task are its large size, the high number of distinct classes and the close similarity of many of the classes, e.g. a number of different dog breeds.</p><p>Huh et al. <ref type="bibr" target="#b12">[13]</ref> examined the impact of various aspects of ImageNet pre-training and successive fine-tuning on the PASCAL <ref type="bibr" target="#b8">[9]</ref> dataset, such as dataset size, number of classes, using fine-grained versus coarser class labels and the ratio of images per class. Additionally, they showed that the aforementioned commonly held beliefs are not accurate, and that transfer learning still works well with restrictions, such as only using half of the dataset.</p><p>He et al. <ref type="bibr" target="#b14">[15]</ref> showed that pre-training on ImageNet speeds up convergence early in training, but that training from scratch will eventually catch up and sometimes even surpass the pre-trained and fine-tuned accuracy. They further argue that ImageNet pre-training does not automatically give better regularisation and that it shows no benefit when the target tasks or metrics are more sensitive to spatially localised predictions. Training from scratch requires different normalisation and regularisation methods as compared to transfer learning. This can skew results, benefiting the pre-trained paradigm over learning from scratch.</p><p>Similarly, Kornblith et al. <ref type="bibr" target="#b15">[16]</ref> showed that although Ima-geNet pre-training accelerates convergence, it does not necessarily lead to a better performance if run long enough. They also investigated how transfer learning relates to the architecture used in the context of image classification. Their findings suggest that the accuracy increase from ImageNet pretraining fades quickly as the size of the dataset for the task at hand grows larger. They conclude that pre-training is and will remain an essential tool in the near future but also highlight clearly that it has limitations.</p><p>When it comes to cross-domain transfer learning, its usefulness -especially from natural images such as ImageNet -is subject of an open discussion. There are cases where transfer learning across domains has been proven to be successful <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. In contrast, there is literature suggesting that this technique is harmful to the final performance of the networks. Tensmeyer et al. <ref type="bibr" target="#b18">[19]</ref> specifically question the usefulness of transfer learning from ImageNet (natural images of 3D objects) on document analysis, which are 2D entities. They argue that feature mappings for natural images fundamentally differ from feature mappings for documents. They also question whether architectures optimized for natural image classification are a good fit for historical document analysis. In their work, they do not provide conclusive results, focus only on the AlexNet <ref type="bibr" target="#b11">[12]</ref> architecture and lack a thorough comparative study. The impact of transfer learning from ImageNet was also a topic of discussion at ICDAR 2017. The organizers of the Competition on Historical Document Writer Identification <ref type="bibr" target="#b19">[20]</ref> speculated that the competition participant who used a deep learningbased method, may have performed relatively poorly due to their network being initialized with ImageNet pre-trained weights.</p><p>Therefore, a conclusive answer on what the real reasons behind the benefit of transfer learning are and whether these can be harnessed in a cross-domain scenario is yet to be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. STUDY DESIGN</head><p>In this section, we present the details of our empirical study, namely the tasks we choose to perform, the datasets and network architectures we use and finally the training procedure for each specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tasks</head><p>In this work, we choose three tasks as representatives of some of the most common challenges in the field of historical document analysis. Specifically, our use cases include image classification, semantic segmentation at pixel level and contentbased image retrieval. We believe that their radically different natures will give a robust estimation of the generality of our findings. <ref type="table" target="#tab_0">Table I</ref> gives an overview of the input and output of the different tasks.</p><p>1) Classification: This task is well known in the computer vision community and consists of producing one or more descriptive labels for a given input image. In the context of historical image document analysis this task can be, for example, formulated as character recognition <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, style/script classification <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> or manuscript dating <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. We train the networks to minimize the cross-entropy loss function shown below:</p><formula xml:id="formula_0">L( x, y) = ‚àílog e xy n i=0 e xi<label>(1)</label></formula><p>where n is the number of classes, x is a vector of size n representing the output of the network, and y = {1..n} is a scalar representing the class label, e.g. style of the document.</p><p>2) Semantic Segmentation at Pixel Level: Semantic segmentation at pixel level is a specific case of a classification task. In this task, each pixel of an input image has to be assigned a label. This is often performed to analyse the layout of historical documents <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, or as a form of pre-processing for further tasks, e.g. line segmentation <ref type="bibr" target="#b27">[28]</ref>. Neural networks for semantic segmentation are trained similarly to the ones for classification, but the architectures employed are different (see section III-C).</p><p>3) Content-based Image Retrieval: Image similarity (or content-based image retrieval) is another typical scenario found in computer vision. In the context of historical document image analysis, this can be seen in the form of writer identification <ref type="bibr" target="#b19">[20]</ref>, signature verification <ref type="bibr" target="#b28">[29]</ref> or watermark recognition <ref type="bibr" target="#b17">[18]</ref>. To train the networks for this task, we use the triplet loss approach <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. The triplet loss operates on a tuple of three images {a, p, n} where a is the anchor (reference), p is the positive sample (an image of the same class as the reference), and n is the negative sample (an image of another class). The loss function is then defined as:</p><formula xml:id="formula_1">L(Œ¥ + , Œ¥ ‚àí ) = max(Œ¥ + ‚àí Œ¥ ‚àí + ¬µ, 0)<label>(2)</label></formula><p>where Œ¥ + and Œ¥ ‚àí are the Euclidean distances between the anchor-positive and anchor-negative pairs in the high dimensional output space of the network and ¬µ is the margin used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>The datasets are available for download through Deep-DIVA 1 . The image input size depends on the architecture and is described in section III-D. <ref type="table" target="#tab_0">Table I</ref> shows an example image for each dataset.</p><p>1) Kuzushiji-MNIST: The KMNIST dataset <ref type="bibr" target="#b20">[21]</ref> contains grayscale images of ten different Hiragama characters written in Kuzushiji (cursive Japanese). It is a curated subset of the full Kuzushiji dataset, which was created during the digitisation of around 300'000 old Japanese books. The images in KMNIST are from 35 classic books printed in the 18th century. The training set contains 7'000 and the test set 1'000 images per class, each of size 28 √ó 28 pixels.</p><p>2) ICDAR2017 Classification of Medieval Handwritings in Latin Scripts: This dataset was published for the Classification of Medieval Handwritings in Latin Scripts (CLaMM) competition <ref type="bibr" target="#b23">[24]</ref> at the ICDAR 2017 conference and includes 3'540 images annotated for style classification and manuscript dating. The test set contains 2'000 images. The dataset is divided into 12 classes for style classification (Caroline, Cursiva, Half-Uncial, Humanistic, Humanistic Cursive, Hybrida, Praegothica, Semihybrida, Semitextualis, Southern Textualis, Textualis, Uncial). Each of the 15 classes provided for manuscript dating corresponds to a specific time interval, ranging from 500 C.E. to 1600 C.E. The competition provided two variations of the dataset, we use the subset that contains images of mixed resolutions and colour representations.</p><p>3) ICDAR2017 Competition on Layout Analysis for Challenging Medieval Manuscripts: The DIVA-HisDB dataset <ref type="bibr" target="#b31">[32]</ref> consists of three different medieval manuscripts (CB55, CSG18, CSG863), each containing 50 pixel-wise annotated pages with a size of approximately 4k √ó 5.5k pixels. The manuscripts have a challenging layout with four different classes (main text body, decoration, comment and background). There is also an additional label for boundary pixels. These pixels originate from the labelling process and are background pixels along the border of the text, which are labelled as text. For our purposes, we relabelled these pixels as background. The same training and test dataset split is used as in the ICDAR 2017 Competition on Layout Analysis for Challenging Medieval Manuscripts <ref type="bibr" target="#b32">[33]</ref>. 4) ICDAR2017 Historical Writer Identification: The Historical-WI dataset <ref type="bibr" target="#b19">[20]</ref> consists of colored and binarized versions of handwritten historical documents. The training dataset consists of 394 writers with three pages per writer, which gives a total of 1'182 pages. The dataset for the competition contains 720 writers and five pages per writer, which makes 3'600 instances in total. We use the coloured version of the dataset for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Architectures</head><p>For the classification and content-based image retrieval experiments, we investigate four well-known architectures: VGG19 with batch normalisation (VGG19 BN), Inception V3, ResNet152 and DenseNet121. A simple architecture with only three layers is also trained for each task to give a baseline for the performance. VGG19 uses batch normalisation <ref type="bibr" target="#b33">[34]</ref> and consists of alternating blocks of convolutional and maxpooling layers. The Inception architecture <ref type="bibr" target="#b34">[35]</ref> introduced Inception blocks, which combine different convolutional filters and layers into one block by concatenation. A second classification head further back in the architecture is added to counteract the vanishing gradient. Another way to combat the vanishing gradient was introduced with the ResNet architecture <ref type="bibr" target="#b35">[36]</ref>. The layers in these type of networks contain direct, additive connections from one layer to a next one, so-called skip connections. We use the ResNet152 architecture. The idea of skip connections has also been extended to not only connect one layer to the next one, but to have blocks of denselyconnected layers. Unlike for the skip connections, the output is not added but concatenated. These dense blocks are alternated with 1 √ó 1 convolutions and max-pooling layers in order to reduce the number of parameters in the model. Here, we use the DenseNet121 <ref type="bibr" target="#b36">[37]</ref>, which features such dense blocks.</p><p>For the segmentation experiments, we use two different architectures: SegNet and DeepLabV3. SegNet <ref type="bibr" target="#b37">[38]</ref> can use any VGG architecture as an encoder, we use VGG19 BN. For each encoder layer, there is a decoder layer which uses the max-pooling indices from the respective encoder layer to perform non-linear up-sampling. The DeepLabV3 architecture <ref type="bibr" target="#b38">[39]</ref> uses a ResNet as the encoder, ResNet18 in our case, and an Atrous Spatial Pyramid Pooling (ASPP) as the decoder. A simple architecture with five layers is also trained on the dataset to give a baseline for the performance.</p><p>All the architectures are available in DeepDIVA 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training Procedure</head><p>All experiments are run using the DeepDIVA framework <ref type="bibr" target="#b39">[40]</ref>. The models are trained long enough to reach convergence on the training data. Each architecture is trained from scratch as well as with ImageNet <ref type="bibr" target="#b6">[7]</ref> pre-training to evaluate the effect of pre-training. All hyper-parameters can be found in our fork of DeepDIVA 3 .</p><p>1) Classification: The architectures described in section III-C are trained with data balancing. Three different classification tasks are performed. For the character recognition task on the KMNIST dataset, the models are trained for 35 epochs. The input images are resized to match the required input size of the respective network. On the CLaMM dataset, the models are trained for 50 epochs for manuscript dating and style classification. 10 random sections of the required input size of the respective network are sampled from each input image, evaluated and their output is averaged.</p><p>2) Semantic Segmentation at Pixel Level: The architectures described in section III-C are trained for 50 epochs. Since the images are very large, using the whole image as an input for the network is not feasible. Instead, a total of 60 000 sections of size 256 √ó 256 are randomly sampled per training epoch. In the test phase, crops of size 256 √ó 256 are sampled as a sliding window (with 50% overlap) to segment the full image. Both architectures used for this task feature encoders, for which ImageNet pre-trained weights are available. For the experiments that use pre-training, we initialise only the encoder using these weights, the weights of the decoder are initialised randomly.</p><p>3) Content-based Image Retrieval: The architectures, as described in section III-C, are all trained for ten epochs with 1.5 million triplets. All the networks are designed to embed the input images in a 128-dimensional space. The images are randomly cropped to match the required input size of the respective network. During training one random section per page is fed to the network. During the test phase, ten random sections of the input image are evaluated, and their output is averaged. The triplets are regenerated after every epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>In the following, results are presented for each of the three chosen tasks individually, i.e. classification, semantic segmentation at pixel level and content-based image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Classification</head><p>Table II reports the accuracies achieved by the different architectures trained from scratch and with ImageNet pretraining on the KMNIST and CLaMM datasets. In general, the network architectures clearly profit from pre-training.. ResNet152 shows the largest increase in performance while DenseNet121 benefits the least.</p><p>1) Optical Character Recognition: For this task, we report the mean accuracy along with the standard deviation computed over five runs of each experiment. Comparing the mean performance of each model using the t-test, the improvement from trained from scratch to pre-trained is statistically significant for VGG19 BN, InceptionV3 and ResNet152. DenseNet121 shows a small decrease of 0.08% with pre-training. The ResNet152 benefits the most from pre-training with a delta of 1.42%, which also makes it the best performing model.</p><p>2) Style Classification: Pre-training leads to a higher accuracy for all architectures with an average increase of 8.1%. VGG19 BN, Inception V3 and ResNet152 profit the most from pre-training with an increase in accuracy of around 9.5%.</p><p>3) Manuscript Dating: Pre-training improves the performance of all the architectures with an average increase of 11.4%. ResNet152 shows the largest increase in accuracy with 17.3%, which also makes it the best performing model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semantic Segmentation at Pixel Level: DIVA-HisDB</head><p>The performance of the segmentation models are evaluated with the layout analysis tool <ref type="bibr" target="#b40">[41]</ref> used in the ICDAR 2017 competition <ref type="bibr" target="#b32">[33]</ref>. The tool computes the mean Intersection   over Union (mean IU) between the predicted results and the ground truth. <ref type="table" target="#tab_0">Table III</ref> shows the mean IU achieved by the models on the test set of the three different manuscripts. The results regarding the effect of transfer-learning from ImageNet are mixed for both architectures. On some manuscripts pretraining on ImageNet increases the performance, but on others, the pre-trained network performs much worse. In terms of dataset size, semantic segmentation outnumbers the classification and content-based image retrieval by far, as every pixel is a data point. Kornblith et al. <ref type="bibr" target="#b15">[16]</ref> have found that the impact of ImageNet pre-training on the performance of the model becomes smaller the larger the dataset gets. This could explain why pre-training is not beneficial for this task. <ref type="table" target="#tab_0">Table IV</ref> reports the mean average precision achieved by the different architectures trained from scratch and with pretraining on the Historical-WI dataset. Pre-training improves the performance of all models except ResNet152.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Content-based Image Retrieval: Writer Identification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>For the classification and content-based image retrieval tasks we find a clear trend that cross-domain transfer learning from ImageNet pre-training leads to an improved performance. For semantic segmentation at pixel level we obtain mixed results. In some instances pre-training is beneficial but harmed the performance in others. We speculate that this could be possibly attributed to the larger amount of training data available for semantic segmentation, as each pixel can be considered an individual data point.</p><p>Overall, in historical document image analysis, the lack of annotated training data is often one of the most limiting factors for machine learning. Facing this restriction, ImageNet pretraining can significantly help to improve the performance of deep learning models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:1905.09113v1 [cs.CV] 22 May 2019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>This table gives an overview of the different tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Accuracy (%) on the test set for the different architectures trained for the different classification tasks. Character recognition is performed on the KMNIST dataset. The reported accuracy is the average along with the standard deviation from five runs. Style classification and manuscript dating are preformed on the CLaMM dataset.</figDesc><table><row><cell></cell><cell cols="3">CHARACTER RECOGNITION</cell><cell cols="3">STYLE CLASSIFICATION</cell><cell cols="2">MANUSCRIPT DATING</cell><cell></cell></row><row><cell></cell><cell>SCRATCH</cell><cell>PRE-TRAINED</cell><cell>‚àÜ</cell><cell>SCRATCH</cell><cell>PRE-TRAINED</cell><cell>‚àÜ</cell><cell>SCRATCH</cell><cell>PRE-TRAINED</cell><cell>‚àÜ</cell></row><row><cell cols="2">3-LAYER CNN 92.98¬±0.22</cell><cell>N/A</cell><cell>-</cell><cell>12.4</cell><cell>N/A</cell><cell>-</cell><cell>11.7</cell><cell>N/A</cell><cell>-</cell></row><row><cell>VGG19 BN</cell><cell>98.17¬±0.18</cell><cell>98.35¬±0.15</cell><cell>+0.18</cell><cell>42.5</cell><cell>52.1</cell><cell>+9.6</cell><cell>24.0</cell><cell>36.1</cell><cell>+12.1</cell></row><row><cell>INCEPTION V3</cell><cell>97.82¬±0.11</cell><cell>98.51¬±0.11</cell><cell>+0.69</cell><cell>46.5</cell><cell>55.5</cell><cell>+9.0</cell><cell>24.8</cell><cell>35.4</cell><cell>+10.6</cell></row><row><cell>RESNET152</cell><cell>97.27¬±0.26</cell><cell>98.69¬±0.10</cell><cell>+1.42</cell><cell>39.1</cell><cell>49.3</cell><cell>+10.2</cell><cell>20.6</cell><cell>37.9</cell><cell>+17.3</cell></row><row><cell>DENSENET121</cell><cell>98.64¬±0.06</cell><cell>98.56¬±0.06</cell><cell>-0.08</cell><cell>47.3</cell><cell>50.9</cell><cell>+3.6</cell><cell>30.7</cell><cell>36.4</cell><cell>+5.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Mean IU (%) on the test set of the different architectures trained on the three manuscripts of the DIVA-HisDB dataset. For pre-training, only the encoder is initialized with the pre-trained weights from ImageNet.</figDesc><table><row><cell></cell><cell cols="2">MANUSCRIPT CB55</cell><cell></cell><cell cols="2">MANUSCRIPT CSG18</cell><cell></cell><cell cols="2">MANUSCRIPT CSG863</cell><cell></cell></row><row><cell></cell><cell>SCRATCH</cell><cell>PRE-TRAINED</cell><cell>‚àÜ</cell><cell>SCRATCH</cell><cell>PRE-TRAINED</cell><cell>‚àÜ</cell><cell>SCRATCH</cell><cell>PRE-TRAINED</cell><cell>‚àÜ</cell></row><row><cell>5-LAYER CNN</cell><cell>55.7</cell><cell>N/A</cell><cell>-</cell><cell>56.8</cell><cell>N/A</cell><cell>-</cell><cell>45.6</cell><cell>N/A</cell><cell>-</cell></row><row><cell>SEGNET</cell><cell>86.9</cell><cell>72.9</cell><cell>-14.0</cell><cell>73.0</cell><cell>75.3</cell><cell>+2.3</cell><cell>81.6</cell><cell>61.9</cell><cell>-19.7</cell></row><row><cell>DEEPLABV3</cell><cell>92.9</cell><cell>91.4</cell><cell>-1.5</cell><cell>69.8</cell><cell>73.1</cell><cell>+3.3</cell><cell>85.5</cell><cell>86.7</cell><cell>+1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Mean average precision (%) achieved on the test set by the different architectures trained on the Historical-WI dataset for writer identification.</figDesc><table><row><cell></cell><cell cols="3">WRITER IDENTIFICATION</cell></row><row><cell></cell><cell>SCRATCH</cell><cell>PRE-TRAINED</cell><cell>‚àÜ</cell></row><row><cell>3-LAYER CNN</cell><cell>11.4</cell><cell>N/A</cell><cell>-</cell></row><row><cell>VGG19 BN</cell><cell>14.6</cell><cell>24.0</cell><cell>+9.4</cell></row><row><cell>INCEPTION V3</cell><cell>9.1</cell><cell>26.1</cell><cell>+17.0</cell></row><row><cell>RESNET152</cell><cell>24.7</cell><cell>22.1</cell><cell>-2.6</cell></row><row><cell>DENSENET121</cell><cell>27.2</cell><cell>34.6</cell><cell>+8.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://bit.ly/DeepDIVA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://bit.ly/2VXDIoo 3 https://bit.ly/2I8c3dX</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The work presented in this paper has been partially supported by the HisDoc III project funded by the Swiss National Science Foundation with the grant number 205120 169618 and by the Rising Tide foundation with the grant number CCR-18-130.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pondenkandath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Historical Document Imaging and Processing -HIP2017</title>
		<meeting>the 4th International Workshop on Historical Document Imaging and Processing -HIP2017<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The cifar-10 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning -Special Issue on Inductive Transfer</title>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">What makes imagenet good for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<idno>abs/1811.08883</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08974</idno>
		<title level="m">Do better imagenet models transfer better?</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transforming sensor data to the image domain for deep learning-an application to footstep detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pondenkandath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwickit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2665" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identifying Cross-Depicted Historical Motifs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pondenkandath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Eichenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR)</title>
		<meeting><address><addrLine>Niagara Falls, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for font classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="985" to="990" />
		</imprint>
	</monogr>
	<note>14th IAPR International Conference on</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Icdar2017 competition on historical document writer identification (historical-wi)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kleber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Christlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Louloudis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nikos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1377" to="1382" />
		</imprint>
	</monogr>
	<note>14th IAPR International Conference on</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep learning for classical japanese literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A complete optical character recognition methodology for historical documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vamvakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Stamatopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Perantonis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 The Eighth IAPR International Workshop on Document Analysis Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="525" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognition for old arabic manuscripts using spatial gray level dependence (sgld)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M A</forename><surname>Al-Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gheith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Sayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Egyptian Informatics Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="43" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Icdar2017 competition on the classification of medieval handwritings in latin script</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cloppet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Helias-Baron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stutzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1371" to="1376" />
		</imprint>
	</monogr>
	<note>14th IAPR International Conference on</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Historical manuscript production date estimation using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2016 15th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Icdar 2009 page segmentation competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antonacopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pletschacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bridson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Papadopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 10th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1370" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Icdar 2013 competition on historical newspaper layout analysis (hnla 2013)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antonacopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clausner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pletschacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1454" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Anonymous Submission</title>
	</analytic>
	<monogr>
		<title level="m">2019 CoRR at 15th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
	<note>AnonymousSubmission</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Offline Signature Verification by Combining Graph Edit Distance and Triplet Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maergner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pondenkandath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-08" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="470" to="480" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9370</biblScope>
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning local feature descriptors with triplets and shallow convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="119" to="120" />
		</imprint>
	</monogr>
	<note type="report_type">Bmvc</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Diva-hisdb: A precisely annotated large dataset of challenging medieval manuscripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Simistira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Eichenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2016 15th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="471" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Icdar2017 competition on layout analysis for challenging medieval manuscripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Simistira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wursch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1361" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1608.06993</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1505.07293</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deepdiva: A highly-functional python framework for reproducible experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pondenkandath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>W√ºrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00329</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Open evaluation tool for layout analysis of document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="43" to="47" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
