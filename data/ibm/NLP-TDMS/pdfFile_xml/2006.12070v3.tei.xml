<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIPSCHITZ RECURRENT NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Benjamin</forename><surname>Erichson</surname></persName>
							<email>erichson@berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Azencot</surname></persName>
							<email>azencot@cs.bgu.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Queiruga</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Hodgkinson</surname></persName>
							<email>liam.hodgkinson@berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
							<email>mmahoney@stat.berkeley.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">ICSI and UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Gurion University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">ICSI and UC</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">ICSI and UC</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LIPSCHITZ RECURRENT NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Viewing recurrent neural networks (RNNs) as continuous-time dynamical systems, we propose a recurrent unit that describes the hidden state's evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. This particular functional form facilitates stability analysis of the long-term behavior of the recurrent unit using tools from nonlinear systems theory. In turn, this enables architectural design decisions before experimentation. Sufficient conditions for global stability of the recurrent unit are obtained, motivating a novel scheme for constructing hidden-to-hidden matrices. Our experiments demonstrate that the Lipschitz RNN can outperform existing recurrent units on a range of benchmark tasks, including computer vision, language modeling and speech prediction tasks. Finally, through Hessian-based analysis we demonstrate that our Lipschitz recurrent unit is more robust with respect to input and parameter perturbations as compared to other continuous-time RNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many interesting problems exhibit temporal structures that can be modeled with recurrent neural networks (RNNs), including problems in robotics, system identification, natural language processing, and machine learning control. In contrast to feed-forward neural networks, RNNs consist of one or more recurrent units that are designed to have dynamical (recurrent) properties, thereby enabling them to acquire some form of internal memory. This equips RNNs with the ability to discover and exploit spatiotemporal patterns, such as symmetries and periodic structures <ref type="bibr" target="#b18">(Hinton, 1986)</ref>. However, RNNs are known to have stability issues and are notoriously difficult to train, most notably due to the vanishing and exploding gradients problem <ref type="bibr" target="#b1">(Bengio et al., 1994;</ref><ref type="bibr" target="#b42">Pascanu et al., 2013)</ref>.</p><p>Several recurrent models deal with the vanishing and exploding gradients issue by restricting the hidden-to-hidden weight matrix to be an element of the orthogonal group <ref type="bibr" target="#b0">(Arjovsky et al., 2016;</ref><ref type="bibr" target="#b50">Wisdom et al., 2016;</ref><ref type="bibr" target="#b37">Mhammedi et al., 2017;</ref><ref type="bibr" target="#b49">Vorontsov et al., 2017;</ref><ref type="bibr" target="#b33">Lezcano-Casado &amp; Martinez-Rubio, 2019)</ref>. While such an approach is advantageous in maintaining long-range memory, it limits the expressivity of the model. To address this issue, recent work suggested to construct hidden-tohidden weights which have unit norm eigenvalues and can be nonnormal <ref type="bibr" target="#b26">(Kerg et al., 2019)</ref>. Another approach for resolving the exploding/vanishing gradient problem has recently been proposed by <ref type="bibr" target="#b25">Kag et al. (2020)</ref>, who formulate the recurrent units as a differential equation and update the hidden states based on the difference between predicted and previous states.</p><p>In this work, we address these challenges by viewing RNNs as dynamical systems whose temporal evolution is governed by an abstract system of differential equations with an external input. The data are formulated in continuous-time where the external input is defined by the function x = x(t) ∈ R p , and the target signal is defined as y = y(t) ∈ R d . Based on insights from dynamical systems theory, we propose a continuous-time Lipschitz recurrent neural network with the functional form</p><formula xml:id="formula_0">ḣ = A β A ,γ A h + tanh(W β W ,γ W h + U x + b) , y = Dh ,<label>(1a)</label></formula><p>Published as a conference paper at ICLR 2021 where the hidden-to-hidden matrices A β,γ ∈ R N ×N and W β,γ ∈ R N ×N are of the form</p><formula xml:id="formula_2">A β A ,γ A = (1 − β A )(M A + M T A ) + β A (M A − M T A ) − γ A I W β W ,γ W = (1 − β W )(M W + M T W ) + β W (M W − M T W ) − γ W I,<label>(2a)</label></formula><p>where β A , β W ∈ [0, 1], γ A , γ W &gt; 0 are tunable parameters and M A , M W ∈ R N ×N are trainable matrices. Here, h = h(t) ∈ R N is a function of time t that represents an internal (hidden) state, anḋ h = ∂h(t) ∂t is its time derivative. The hidden state represents the memory that the system has of its past. The function in Eq. (1) is parameterized by the hidden-to-hidden weight matrices A ∈ R N ×N and W ∈ R N ×N , the input-to-hidden encoder matrix U ∈ R N ×p , and an offset b. The function in Eq. (1b) is parameterized by the hidden-to-output decoder matrix D ∈ R d×N . Nonlinearity is introduced via the 1-Lipschitz tanh activation function. While RNNs that are governed by differential equations with an additive structure have been studied before <ref type="bibr" target="#b52">(Zhang et al., 2014)</ref>, the specific formulation that we propose in (1) and our theoretical analysis are distinct.</p><p>Treating RNNs as dynamical systems enables studying the long-term behavior of the hidden state with tools from stability analysis. From this point of view, an unstable unit presents an exploding gradient problem, while a stable unit has well-behaved gradients over time <ref type="bibr" target="#b38">(Miller &amp; Hardt, 2019)</ref>. However, a stable recurrent unit can suffer from vanishing gradients, leading to catastrophic forgetting <ref type="bibr" target="#b20">(Hochreiter &amp; Schmidhuber, 1997b)</ref>. Thus, we opt for a stable model whose dynamics do not (or only slowly do) decay over time. Importantly, stability is also a statement about the robustness of neural units with respect to input perturbations, i.e., stable models are less sensitive to small perturbations compared to unstable models. Recently, <ref type="bibr" target="#b4">Chang et al. (2019)</ref> explored the stability of linearized RNNs and provided a local stability guarantee based on the Jacobian. In contrast, the particular structure of our unit (1) allows us to obtain guarantees of global exponential stability using control theoretical arguments. In turn, the sufficient conditions for global stability motivate a novel symmetric-skew decomposition based scheme for constructing hidden-to-hidden matrices. This scheme alleviates exploding and vanishing gradients, while remaining highly expressive.</p><p>In summary, the main contributions of this work are as follows:</p><p>• First, in Section 3, using control theoretical arguments in a direct Lyapunov approach, we provide sufficient conditions for global exponential stability of the Lipschitz RNN unit (Theorem 1). Global stability is advantageous over local stability results since it guarantees non-exploding gradients regardless of the state. In the special case where A is symmetric, we find that these conditions agree with those in classical theoretical analyses (Lemma 1). • Next, in Section 4, drawing from our stability analysis, we propose a novel scheme based on the symmetric-skew decomposition for constructing hidden-to-hidden matrices. This scheme mitigates the vanishing and exploding gradients problem, while obtaining highly expressive hidden-to-hidden matrices. • In Section 6, we show that our Lipschitz RNN has the ability to outperform state-of-theart recurrent units on computer vision, language modeling and speech prediction tasks. Further, our results show that the higher-order explicit midpoint time integrator improves the predictive accuracy as compared to using the simpler one-step forward Euler scheme. • Finally, in Section 7), we study our Lipschitz RNN via the lens of the Hessian and show that it is robust with respect to parameter perturbations; we also show that our model is more robust with respect to input perturbations, compared to other continuous-time RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The problem of vanishing and exploding gradients (and stability) have a storied history in the study of RNNs. Below, we summarize two particular approaches to the problem (constructing unitary/orthogonal RNNs and the dynamical systems viewpoint) that have gained significant attention.</p><p>Unitary and orthogonal RNNs. Unitary recurrent units have received attention recently, largely due to <ref type="bibr" target="#b0">Arjovsky et al. (2016)</ref> showing that unitary hidden-to-hidden matrices alleviate the vanishing and exploding gradients problem. Several other unitary and orthogonal models have also been proposed <ref type="bibr" target="#b50">(Wisdom et al., 2016;</ref><ref type="bibr" target="#b37">Mhammedi et al., 2017;</ref><ref type="bibr" target="#b23">Jing et al., 2017;</ref><ref type="bibr" target="#b49">Vorontsov et al., 2017;</ref><ref type="bibr" target="#b24">Jose et al., 2018)</ref>. While these approaches stabilize the training process of RNNs considerably, they also limit their expressivity and their prediction accuracy. Further, unitary RNNs are expensive to train, as they typically involve the computation of a matrix inverse at each step of training. Recent work by <ref type="bibr" target="#b33">Lezcano-Casado &amp; Martinez-Rubio (2019)</ref> overcame some of these limitations. By leveraging concepts from Riemannian geometry and Lie group theory, their recurrent unit exhibits improved expressivity and predictive accuracy on a range of benchmark tasks while also being efficient to train. Another competitive recurrent design was recently proposed by <ref type="bibr" target="#b26">Kerg et al. (2019)</ref>. Their approach is based on the Schur decomposition, and it enables the construction of general nonnormal hidden-to-hidden matrices with unit-norm eigenvalues.</p><p>Dynamical systems inspired RNNs. The continuous time view of RNNs has a long history in the neurodynamics community as it provides higher flexibility and increased interpretability <ref type="bibr" target="#b44">(Pineda, 1988;</ref><ref type="bibr" target="#b43">Pearlmutter, 1995;</ref><ref type="bibr" target="#b52">Zhang et al., 2014)</ref>. In particular, RNNs that are governed by differential equations with an additive structure have been extensively studied from a theoretical point of view <ref type="bibr" target="#b11">(Funahashi &amp; Nakamura, 1993;</ref><ref type="bibr" target="#b28">Kim et al., 1996;</ref><ref type="bibr" target="#b8">Chow &amp; Li, 2000;</ref><ref type="bibr" target="#b22">Hu &amp; Wang, 2002;</ref><ref type="bibr" target="#b34">Li et al., 2005;</ref><ref type="bibr" target="#b48">Trischler &amp; D'Eleuterio, 2016)</ref>. See <ref type="bibr" target="#b52">Zhang et al. (2014)</ref> for a comprehensive survey of continuous-time RNNs and their stability properties.</p><p>Recently, several works have adopted the dynamical systems perspective to alleviate the challenges of training RNNs which are related to the vanishing and exploding gradients problem. For nonsequential data, <ref type="bibr" target="#b9">Ciccone et al. (2018)</ref> proposed a negative-definite parameterization for enforcing stability in the RNN during training. <ref type="bibr" target="#b4">Chang et al. (2019)</ref> introduced an antisymmetric hidden-tohidden weight matrix and provided guarantees for local stability. <ref type="bibr" target="#b25">Kag et al. (2020)</ref> have proposed a differential equation based formulation for resolving the exploding/vanishing gradients problem by updating the hidden states based on the difference between predicted and previous states. Niu et al.</p><p>(2019) employed numerical methods for differential equations to study the stability of RNNs.</p><p>Another line of recent work has focused on continuous-time models that deal with irregular sampled time-series, missing values and multidimensional time series. <ref type="bibr" target="#b45">Rubanova et al. (2019) and</ref><ref type="bibr" target="#b10">De Brouwer et al. (2019)</ref> formulated novel recurrent models based on the theory of differential equations and their discrete integration. <ref type="bibr" target="#b31">Lechner &amp; Hasani (2020)</ref> extended these ordinary differential equation (ODE) based models and addresses the issue of vanishing and exploding gradients by designing an ODE-model that is based on the idea of long short-term memory (LSTM). This ODE-LSTM outperforms the continuous-time LSTM <ref type="bibr" target="#b36">(Mei &amp; Eisner, 2017)</ref> as well as the GRU-D model <ref type="bibr" target="#b5">(Che et al., 2018)</ref> that is based on a gated recurrent unit (GRU).</p><p>The link between dynamical systems and models for forecasting sequential data also provides the opportunity to incorporate physical knowledge into the learning process which improves the generalization performance, robustness, and ability to learn with limited data .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STABILITY ANALYSIS OF LIPSCHITZ RECURRENT UNITS</head><p>One of the key contributions in this work is that we prove that model (1) is globally exponentially stable under some mild conditions on A and W . Namely, for any initial hidden state we can guarantee that our Lipschitz unit converges to an equilibrium if it exists, and therefore, gradients can never explode. We improve upon recent work on stability in recurrent models, which provide only a local analysis, see e.g., <ref type="bibr" target="#b4">(Chang et al., 2019)</ref>. In fact, global exponential stability is among the strongest notions of stability in nonlinear systems theory, implying all other forms of Lyapunov stability about the equilibrium h * <ref type="figure">(</ref> <ref type="bibr">Khalil, 2002, Definitions 4.4 and 4.5)</ref>.</p><formula xml:id="formula_4">Definition 1. A point h * is an equilibrium point ofḣ = f (h, t) if f (h * , t) = 0 for all t.</formula><p>Such a point is globally exponentially stable if there exists some C &gt; 0 and λ &gt; 0 such that for any choice of initial values h(0) ∈ R N ,</p><formula xml:id="formula_5">h(t) − h * ≤ Ce −λt h(0) − h * , for any t ≥ 0.<label>(3)</label></formula><p>The presence of a Lipschitz nonlinearity in (1) plays an important role in our analysis. While we focus on tanh in our experiments, our proof is more general and is applicable to models whose nonlinearity σ(·) is an M -Lipschitz function. Specifically, we consider the general model</p><formula xml:id="formula_6">h = Ah + σ(W h + U x + b) ,<label>(4)</label></formula><p>for which we have the following stability result. In the following, we let σ min and σ max denote the smallest and largest singular values of the hidden-to-hidden matrices, respectively. Theorem 1. Let h * be an equilibrium point of a differential equation of the form (4) for some x ∈ R p . The point h * is globally exponentially stable if the eigenvalues of A sym := 1 2 (A + A T ) are strictly negative, W is non-singular, and either (a) σ min (A sym ) &gt; M σ max (W ); or (b) σ is monotone non-decreasing, W + W T is negative definite, and A T W + W T A is positive definite.</p><p>The two cases show that global exponential stability is guaranteed if either (a) the matrix A has eigenvalues with real parts sufficiently negative to counteract expanding trajectories in the nonlinearity; or (b) the nonlinearity is monotone, both A and W yield stable linear systemsu = Au, v = W v, and A, W have sufficiently similar eigenvectors. In practice, case (b) occasionally holds, but is challenging to ensure without assuming specific structure on A, W . Because such assumptions could limit the expressiveness of the model, the next section will develop a tunable formulation for A and W with the capacity to ensure that case (a) holds.</p><p>In Appendix A.1, we provide a proof of Theorem 1 using a direct Lyapunov approach. One advantage of this approach is that the driving input x is permitted to evolve in time arbitrarily in the analysis. The proof relies on the classical Kalman-Yakubovich-Popov lemma and circle criterion from control theory -to our knowledge, these tools have not been applied in the modern RNN literature, and we hope our proof can illustrate their value to the community.</p><p>In the special case where A is symmetric and x(t) constant, we show that we can also inherit criteria for both local and global stability from a class of well-studied Cohen-Grossberg-Hopfield models. Lemma 1. Suppose that A is symmetric and W is nonsingular. There exists a diagonal matrix D ∈ R N ×N , and nonsingular matrices L, V ∈ R N ×N such that an equilibrium of (4) is (globally exponentially) stable if and only if there is a corresponding (globally exponentially) stable equilibrium for the systemż</p><formula xml:id="formula_7">= Dz + Lσ(V z + U x + b).<label>(5)</label></formula><p>For a thorough review of analyses of (5), see <ref type="bibr" target="#b52">(Zhang et al., 2014)</ref>. In this special case, the criteria in Theorem 1 coincide with those obtained for the corresponding model (5). However, in practice, we will not choose A to be symmetric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYMMETRIC-SKEW HIDDEN-TO-HIDDEN MATRICES</head><p>In this section we propose a novel scheme for constructing hidden-to-hidden matrices. Specifically, based on the successful application of skew-symmetric hidden-to-hidden weights in several recent recurrent architectures, and our stability criteria in Theorem 1, we propose an effective symmetricskew decomposition for hidden matrices. Our decomposition allows for a simple control of the matrix spectrum while retaining its wide expressive range, enabling us to satisfy the spectral constraints derived in the previous section on both A and W . The proposed scheme also accounts for the issue of vanishing gradients by reducing the magnitude of large negative eigenvalues.</p><p>Recently, several methods used skew-symmetric matrices, i.e., S + S T = 0 to parameterize the recurrent weights W ∈ R N ×N , see e.g., <ref type="bibr" target="#b50">(Wisdom et al., 2016;</ref><ref type="bibr" target="#b4">Chang et al., 2019)</ref>. From a stability analysis viewpoint, there are two main advantages for using skew-symmetric weights: these matrices generate the orthogonal group whose elements are isometric maps and thus preserve norms <ref type="bibr" target="#b33">(Lezcano-Casado &amp; Martinez-Rubio, 2019)</ref>; and the spectrum of skew-symmetric matrices is purely imaginary which simplifies stability analysis <ref type="bibr" target="#b4">(Chang et al., 2019)</ref>. The main shortcoming of this parametrization is its reduced expressivity, as these matrices have fewer than half of the parameters of a full matrix <ref type="bibr" target="#b26">(Kerg et al., 2019)</ref>. The latter limiting aspect can be explained from a dynamical systems perspective: skew-symmetric matrices can only describe oscillatory behavior, whereas a matrix whose eigenvalues have nonzero real parts can also encode viable growth and decay information.</p><p>To address the expressivity issue, we aim for hidden matrices which on the one hand, allow to control the expansion and shrinkage of their associated trajectories, and on the other hand, will be sampled from a superset of the skew-symmetric matrices. Our analysis in Theorem 1 guarantees that Lipschitz recurrent units maintain non-expanding trajectories under mild conditions on A and W . Unfortunately, this proposition does not provide any information with respect to the shrinkage of paths.</p><p>Here, we opt for a system whose expansion and shrinkage can be easily controlled. Formally, the latter requirement is equivalent to designing hidden weights S with small Rλ i (S), i = 1, 2, . . . , N , where R(z) denotes the real part of z. A system of the form (4) whose matrices A and W exhibit</p><formula xml:id="formula_8">(a) β = 0.5, γ = 0 (b) β = 0.75, γ = 0.01 (c) β = 0.85, γ = 0.01 (d) β = 1.0, γ = 0</formula><p>Figure 1: Vector fields of hidden states that are governed by Eq. (1) trained for simple pendulum dynamics. In (a), an unstable model is shown. In (b) and (c), it can be seen that we yield models that are asymptotically stable,i.e., all trajectories are attracted by an equilibrium point. In contrast, in (d), a skew-symmetric parameterization leads to a stable model without an attracting equilibrium.</p><p>small spectra and satisfy the conditions of Theorem 1, will exhibit dynamics with moderate decay and growth behavior and alleviate the problem of exploding and vanishing gradients. To this end, we propose the following symmetric-skew decomposition for constructing hidden matrices:</p><formula xml:id="formula_9">S β,γ := (1 − β) · (M + M T ) + β · (M − M T ) − γI,<label>(6)</label></formula><p>where M is a weight matrix, and β ∈ [0.5, 1], γ &gt; 0 are tuning parameters. In the case (β, γ) = (1, 0), we recover a skew-symmetric matrix, i.e., S 1,0 + S T 1,0 = 0. The construction S β,γ is useful as we can easily bound its spectrum via the parameters β and γ, as we show in the next proposition.</p><formula xml:id="formula_10">Proposition 1. Let S β,γ satisfy (6), and let M sym = 1 2 (M + M T ). The real parts λ i (S β,γ ) of the eigenvalues of S β,γ , as well as the eigenvalues of S sym β,γ = S β,γ + S T β,γ , lie in the interval [(1 − β)λ min (M sym ) − γ, (1 − β)λ max (M sym ) − γ].</formula><p>A proof is provided in Appendix A.2. We infer that β controls the width of the spectrum, while increasing γ shifts the spectrum to the left along the real axis, thus enforcing eigenvalues with nonpositive real parts. Choosing our hidden-to-hidden matrices to be A β A ,γ A and W β W ,γ W of the form (6) for different values of β A , β W and γ A , γ W , we can ensure small spectra and satisfy the conditions of Theorem 1 as desired. Note, that different tuning parameters β and γ affect the stability behavior of the Lipschitz recurrent unit. This is illustrated in <ref type="figure">Figure 1</ref>, where different values for β and γ are used to construct both A β,γ and W β,γ and applied to learning simple pendulum dynamics.</p><p>One cannot guarantee that model parameters will remain in the stability region during training. However, we can show that when β is taken to be close to one, the eigenvalues of A sym β,γ and W sym β,γ (which dictate the stability of the RNN) change slowly during training. Let ∆ δ F denote the change in a function F depending on the parameters of the RNN (1) after one step of gradient descent with step size δ with respect to some loss L(y). For a matrix A, we let λ k (A) denote the k-th singular value of A. We have the following lemma.</p><formula xml:id="formula_11">Lemma 2. As β → 1 − , max k |∆ δ λ k (A sym β,γ )| + max k |∆ δ λ k (W sym β,γ )| = O(δ(1 − β) 2 ).</formula><p>Therefore, provided both the initial and optimal parameters lie within the stability region, the model parameters will remain in the stability region for longer periods of time with high probability as β → 1. Further empirical evidence of parameters often remaining in the stability region during training are provided alongside the proof of Lemma 2 in the Appendix (see <ref type="figure" target="#fig_7">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TRAINING CONTINUOUS-TIME LIPSCHITZ RECURRENT UNITS</head><p>ODEs such as Eq.</p><p>(1) can be approximately solved by employing numerical integrators. In scientific computing, numerical integration is a well studied field that provides well understood techniques <ref type="bibr" target="#b32">(LeVeque, 2007)</ref>. Recent literature has also introduced new approaches which are designed with neural network frameworks in mind <ref type="bibr" target="#b6">(Chen et al., 2018)</ref>.</p><p>To learn the weights A, W, U and b, we discretize the continuous model using one step of a numerical integrator between sequence entries. In what follows, a subscript t denotes discrete time indices, ∆t represents the time difference between a pair of consecutive data points.</p><formula xml:id="formula_12">Letting f (h, t) = Ah + tanh(W h + U x(s) + b) so thatḣ(t) = f (h, t)</formula><p>, the exact and approximate solutions for h t+1 given h t are given by</p><formula xml:id="formula_13">h t+1 = h t + t+∆t t f (h(s), s)ds := h t + t+∆t t Ah(s) + tanh(W h(s) + U x(s) + b) ds (7) ≈ h t + ∆t · scheme [f, h t , ∆t] ,<label>(8)</label></formula><p>where scheme represents one step of a numerical integration scheme whose application yields an approximate solution for 1 ∆t t+∆t t f (h(s), s)ds given h t using one or more evaluations of f .</p><p>We consider both the explicit (forward) Euler scheme,</p><formula xml:id="formula_14">h t+1 = h t + ∆t · Ah t + ∆t · tanh(z t ),<label>(9)</label></formula><p>as well as the midpoint method which is a two-stage explicit Runge-Kutta scheme (RK2), </p><formula xml:id="formula_15">h t+1 = h t + ∆t · Ah + ∆t · tanh(Wh + U x t + b),<label>(10)</label></formula><formula xml:id="formula_16">whereh = h t + ∆t/2 · Ah t + ∆t/2 · tanh(z t ) is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EMPIRICAL EVALUATION</head><p>In this section, we evaluate the performance of the Lipschitz RNN and compare it to other state-ofthe-art methods. The model is applied to ordered and permuted pixel-by-pixel MNIST classification, as well as to audio data using the TIMIT dataset. We show the sensitivity with respect to to random initialization in Appendix B. Appendix B also contains additional results for: pixel-by-pixel CIFAR-10 and a noise-padded version of CIFAR-10; as well as for character level and word level prediction using the Penn Tree Bank (PTB) dataset. All of these tasks require that the recurrent unit learns long-term dependencies: that is, the hidden-to-hidden matrices need to have sufficient memory to remember information from far in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">ORDERED AND PERMUTED PIXEL-BY-PIXEL MNIST</head><p>The pixel-by-pixel MNIST task tests long range dependency by sequentially presenting 784 pixels to the recurrent unit, i.e., the RNN processes one pixel at a time <ref type="bibr" target="#b30">(Le et al., 2015)</ref>. At the end of the  sequence, the learned hidden state is used to predict the class membership probability of the input image. This task requires that the RNN has a sufficient long-term memory in order to discriminate between different classes. A more challenging variation to this task is to operate on a fixed random permutation of the input sequence. Hence, our Lipschitz recurrent unit outperforms state-of-the-art RNNs on both tasks and is competitive even when a hidden dimension of N = 64 is used, however, it can be seen that a larger unit with more capacity is advantageous for the permuted task. Our results show that we significantly outperform the Antisymmetric RNN <ref type="bibr" target="#b4">(Chang et al., 2019)</ref> on the ordered tasks, while using fewer weights. That shows that the antisymmetric weight paramterization is limiting the expressivity of the recurrent unit. The exponential RNN is the next most competitive model, yet this model requires a larger hidden-to-hidden unit to perform well on the two considered tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">TIMIT</head><p>Next, we consider the TIMIT dataset <ref type="bibr" target="#b12">(Garofolo, 1993)</ref> to study the capabilities of the Lipschitz RNN for speech prediction using audio data. For our experiments, we used the publicly available implementation of this task by <ref type="bibr" target="#b33">Lezcano-Casado &amp; Martinez-Rubio (2019)</ref>. This implementation applies the preprocessing steps suggested by <ref type="bibr" target="#b50">Wisdom et al. (2016)</ref>: (i) downsample each audio sequence to 8kHz; (ii) process the downsampled sequences with a short-time Fourier transform using a Hann window of 256 samples and a window hop of 128 samples; and (iii) normalize the logmagnitude of the Fourier amplitudes. We obtain a set of frames that each have 129 complex-valued Fourier amplitudes and the task is to predict the log-magnitude of future frames. To compare our results with those of other models, we used the common train / validation / test split: 3690 utterances from 462 speakers for training, 192 utterances for validation, and 400 utterances for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ROBUSTNESS WITH RESPECT TO PERTURBATIONS</head><p>An important consideration beyond accuracy is robustness with respect to input and parameter perturbations. We consider a Hessian-based analysis and noise-response analysis of different continuous-time recurrent units and train the models on MNIST. Here, we reshape each MNIST thumbnail into sequences of length 98 so that each input has dimension x ∈ R 8 . We consider this simpler problem so that all models obtain roughly the same training loss. Here we use stochastic gradient decent (SGD) with momentum to train the models.</p><p>Eigenanalysis of the Hessian provides a tool for studying various aspects of neural networks <ref type="bibr" target="#b19">(Hochreiter &amp; Schmidhuber, 1997a;</ref><ref type="bibr" target="#b46">Sagun et al., 2017;</ref><ref type="bibr" target="#b13">Ghorbani et al., 2019)</ref>. Here, we study the Hessian H spectrum with respect to the model parameters of the recurrent unit using Py-Hessian <ref type="bibr" target="#b51">(Yao et al., 2019)</ref>. The Hessian provides us with insights about the curvature of the loss function L. This is because the Hessian is defined as the derivatives of the gradients, and thus the Hessian eigenvalues describe the change in the gradient of L as we take an infinitesimal step into a given direction. The eigenvectors span the (local) surface of the loss function at a given point, and the corresponding eigenvalue determines the curvature in the direction of the eigenvectors. This means that larger eigenvalues indicate a larger curvature, i.e., greater sensitivity, and the sign of the eigenvalues determines whether the curvature will be positive or negative.</p><p>To demonstrate the advantage of the additional linear term and our weight parameterization, we compare the Lipschitz RNN to two other continuous-time recurrent units. First, we consider a simple neural ODE RNN <ref type="bibr" target="#b45">(Rubanova et al., 2019)</ref> that takes the forṁ</p><formula xml:id="formula_17">h = tanh(W h + U x + b), y = Dh,<label>(11)</label></formula><p>where W is a simple hidden-to-hidden matrix. As a second model we consider the antisymmetric RNN <ref type="bibr" target="#b4">(Chang et al., 2019)</ref>, that takes the same form as <ref type="formula" target="#formula_17">(11)</ref>, but uses a skew-symmetric scheme to parameterize the hidden-to-hidden matrix as W := (M − M T ) − γI, where M is a trainable weight matrix and γ is a tunable parameter. <ref type="table" target="#tab_5">Table 3</ref> reports the largest eigenvalue λ max (H) and the trace of the Hessian tr(H).The largest eigenvalue being smaller indicates that our Lipschitz RNN found a flatter minimum, as compared to the simple neural ODE and Antisymmetric RNN. It is known that such flat minima can be perturbed without significantly changing the loss value <ref type="bibr" target="#b19">(Hochreiter &amp; Schmidhuber, 1997a)</ref>. <ref type="table" target="#tab_5">Table 3</ref> also reports the condition number κ(H) := λmax(H) λmin(H) of the Hessian. The condition number κ(H) provides a measure for the spread of the eigenvalues of the Hessian. It is known that first-order methods can slow down in situations where κ is large <ref type="bibr" target="#b3">(Bottou &amp; Bousquet, 2008)</ref>. The condition number and trace of our Lipshitz RNN being smaller also indicates improved robustness properties.</p><p>Next, we study the sensitivity of the response y T at time T in terms of the test accuracy with respect to a sequence of perturbed inputs {x 1 , . . . ,x T } ∈ R 8 . We consider three different perturbations. The results for the artificially constructed perturbations are presented in <ref type="table" target="#tab_5">Table 3</ref>, showing that the Lipschitz RNN is more resilient to adversarial perturbation. Here, we have considered the projected gradient decent (PGD) <ref type="bibr" target="#b14">(Goodfellow et al., 2014)</ref> method with l ∞ , and the DeepFool    method <ref type="bibr" target="#b39">(Moosavi-Dezfooli et al., 2016)</ref> with l 2 and l ∞ norm ball perturbations. We construct the adversarial examples with full access to the models, using 7 iterations. The step size for PGD is set to 0.01.</p><p>Further, <ref type="figure" target="#fig_1">Figure 2</ref> shows the results for white noise and salt and pepper noise. It can be seen that the Lipschitz unit is less sensitive to input perturbations, as compared to the simple neural ODE RNN, and the antisymmetric RNN. In addition, we also show the results for an unitary RNN here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">ABLATION STUDY</head><p>The performance of the Lipschitz recurrent unit is due to two main innovations: (i) the additional linear term; and (ii) the scheme for constructing the hidden-to-hidden matrices A and W in Eq. (6). Thus, we investigate the effect of both innovations, while keeping all other conditions fixed. More concretely, we consider the following ablation recurrent unit</p><formula xml:id="formula_18">h t+1 = h t + α · · Ah t + · tanh(z t ), with z t = W h t + U x t + b,<label>(12)</label></formula><p>where α controls the effect of the linear hidden unit. Both A and W depend on the parameters β, γ. <ref type="figure" target="#fig_2">Figure 3a</ref> studies the effect of the linear hidden unit, with β = 0.65 for the ordered task and β = 0.8 for the permuted task. In both cases we use γ = 0.001. It can be seen that the test accuracies of both the ordered and permuted pixel-by-pixel MNIST tasks clearly depend on the linear hidden unit. For α = 0, our models reduces to simple neural ODE recurrent units (Eq. (11)). The recurrent unit degenerates for α &gt; 1.6, since the external input is superimposed by the hidden state. <ref type="figure" target="#fig_2">Figure 3b</ref> studies the effect of the hidden-to-hidden matrices with respect to β. It can be seen that β = {0.65, 0.70} achieves peak performance for the ordered task, and β = {0.8, 0.85} does so for the permuted task. Note that β = 1.0 recovers an skew-symmetric hidden-to-hidden matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Viewing RNNs as continuous-time dynamical systems with input, we have proposed a new Lipschitz recurrent unit that excels on a range of benchmark tasks. The special structure of the recurrent unit allows us to obtain guarantees of global exponential stability using control theoretical arguments. In turn, the insights from this analysis motivated the symmetric-skew decomposition scheme for constructing hidden-to-hidden matrices, which mitigates the vanishing and exploding gradients problem. Due to the nice stability properties of the Lipschitz recurrent unit, we also obtain a model that is more robust with respect to input and parameter perturbations as compared to other continuoustime units. This behavior is also reflected by the Hessian analysis of the model. We expect that the improved robustness will make Lipschitz RNNs more reliable for sensitive applications. The theoretical results for our symmetric-skew decomposition of parameterizing hidden-to-hidden matrices also directly extend to the convolutional setting. Future work will explore this extension and study the potential advantages of these more parsimonious hidden-to-hidden matrices in combination with our parameterization in practice. Research code is shared via github.com/erichson/LipschitzRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS</head><p>A.1 PROOFS OF THEOREM 1 AND LEMMA 1</p><p>There are numerous ways that one can analyze the global stability of (4) through the related model (5), many of which are discussed in <ref type="bibr" target="#b52">Zhang et al. (2014)</ref>. Instead, here we shall conduct a direct approach and avoid appealing to diagonalization in order to obtain cleaner conditions, and a more straightforward proof that readily applies in the time-inhomogeneous setting.</p><p>Our method of choice relies on Lyapunov arguments summarized in the following theorem, which can be found as <ref type="bibr">(Khalil, 2002, Theorem 4.10)</ref>. For more details on related Lyapunov theory, see also <ref type="bibr" target="#b15">Hahn (1967)</ref>; <ref type="bibr" target="#b47">Sastry (2013)</ref>.</p><formula xml:id="formula_19">Theorem 2. An equilibrium h * forḣ = f (t, h) is globally exponentially stable if there exists a continuously differentiable function V : [0, ∞) × R N → [0, ∞) such that for all h ∈ R N and t ≥ 0, k 1 h − h * α ≤ V (t, h) ≤ k 2 h − h * α , and ∂V ∂t + ∂V ∂h ≤ −k 3 h − h * α , for some constants k 1 , k 2 , k 3 , α &gt; 0. andV (h) &lt; 0 for h = h * .</formula><p>To simplify matters, we shall choose a Lyapunov function V : (iii) Either G(∞) + G(∞) T is positive definite or it is positive semidefinite and</p><formula xml:id="formula_20">R N → [0, ∞) that is independent of time.</formula><formula xml:id="formula_21">lim ω→∞ ω 2 M T [G(iω) + G(−iω) T ]M is positive definite for any N × (N − q) full-rank matrix M such that M T [G(∞) + G(∞) T ]M = 0, where q = rank[G(∞) + G(∞) T ].</formula><p>The following is presented in <ref type="bibr">(Khalil, 2002, Lemma 6.3)</ref>. Lemma 3 (Kalman-Yakubovich-Popov). Let A, W : R N → R N be full-rank square matrices. There exists a symmetric positive-definite matrix P and matrices L, U and a constant &gt; 0 such that</p><formula xml:id="formula_22">P A + A T P = −L T L − P P = L T U − W T U T U = 0,</formula><p>if and only if the transfer function G(s) = W (sI − A) −1 is strictly positive real. In this case, we may take = 2µ, where µ &gt; 0 is chosen so that G(s − µ) remains strictly positive real.</p><p>A shorter proof for case (a) is available to us through the (multivariable) circle criterion -the following theorem is a corollary of (Khalil, 2002, Theorem 7.1) suitable for our purposes. Theorem 3 (Circle Criterion). The system of differential equationṡ Both the Kalman-Yakubovich-Popov lemma and the circle criterion are classical results in control theory, and are typically discussed in the setting of feedback systems <ref type="bibr">(Khalil, 2002, Chapter 6, 7)</ref>. Our presentation here is less general than the complete formulation, but makes clearer the connection to RNNs. With these tools, we state our proof of Theorem 1.</p><formula xml:id="formula_23">h = Ah + ψ(t, W h)</formula><p>Proof of Theorem 1. To begin, we shall center the differential equation about the equilibrium. By assumption, there exists h * such that</p><formula xml:id="formula_24">Ah * = −σ(W h * + U x(t) + b). Lettingh = h − h * , we find thatḣ = Ah + σ(W h + U x(t) + b) = Ah + Ah * + σ(Wh + W h * + U x(t) + b) = Ah + σ(Wh + W h * + U x(t) + b) − σ(W h * + U x(t) + b).<label>(13)</label></formula><p>It will suffice to show that (13) is globally exponentially stable at the origin.</p><p>Let us begin with case (a). The proof follows arguments analogous to <ref type="bibr">(Khalil, 2002, Example 7.1)</ref>. Let G(s) = W (A − sI) −1 denote the transfer function for the system (13). Letting</p><formula xml:id="formula_25">ψ(t, x) = σ(x + W h * + U x(t) + b) − σ(W h * + U x(t) + b), since σ is M -Lipschitz, we know that ψ(t, x) ≤ M x for any x ∈ R N . Therefore, let Z(s) = [I + M G(s)][I − M G(s)]</formula><p>−1 denote the transfer function in the circle criterion. Our objective is to show that Z(s) is strictly positive real -by Theorem 3, this will guarantee the desired global exponential stability of (4). First, we need to show that the poles of Z(s) have negative real parts. This can only occur when G(s) itself has poles or I − M G(s) is singular. The former case occurs precisely where A − sI is singular, which occurs when s is an eigenvalue of A. Since A + A T is assumed to be negative definite, A must have eigenvalues with negative real part by Lemma 4, and so the poles of G(s) also have negative real parts. The latter case is more difficult to treat. First,</p><formula xml:id="formula_26">since σ max (AB) ≤ σ max (A)σ max (B) and σ max (B −1 ) = σ min (B) −1 , σ max (G(s)) ≤ σ max (W ) σ min (A − sI) .<label>(14)</label></formula><p>Therefore, we observe that</p><formula xml:id="formula_27">σ min (I − M G(s)) ≥ 1 − σ max (M G(s)) ≥ 1 − M σ max (G(s)) ≥ 1 − M σ max (W ) σ min (A − sI) .</formula><p>From the Fan-Hoffman inequality <ref type="bibr">(Bhatia, 2013, Proposition III.5</ref>.1), we have that</p><formula xml:id="formula_28">σ min (A − sI) = σ min (sI − A) ≥ λ min (s)I − A + A T 2 = (s) + λ min − A + A T 2 ,</formula><p>and since A + A T is negative definite, for any s with (s) ≥ 0,</p><formula xml:id="formula_29">σ min (A − sI) ≥ (s) + σ min A + A T 2 ≥ σ min (A sym ).<label>(15)</label></formula><p>Since σ min (A sym ) &gt; M σ max (W ), it follows that σ min (I − M G(s)) &gt; 0 whenever s has nonnegative real part, and so the poles of Z(s) must have negative real parts.</p><p>Next, we need to show that Z(iω) + Z(−iω) T is positive definite for all ω ∈ R. Observe that</p><formula xml:id="formula_30">Z(iω) + Z(−iω) T = [I + M G(iω)][I − M G(iω)] −1 + [I − M G(−iω) T ] −1 [I + M G(−iω) T ] = 2[I − M G(−iω) T ] −1 [I − M 2 G(−iω) T G(iω)][I − M G(iω)] −1 .</formula><p>From Sylvester's law of inertia, we may infer that Z(iω) + Z(−iω) T is positive definite if and only if I + Y ω is positive definite, where Y ω = M 2 G(−iω) T G(iω). If we can show that the eigenvalues of Y ω lie strictly within the unit circle, that is, σ max (Y ω ) &lt; 1 for all ω ∈ R, then I + Y ω will necessarily be positive definite. From <ref type="formula" target="#formula_6">(14)</ref> and <ref type="formula" target="#formula_7">(15)</ref>, we may verify that</p><formula xml:id="formula_31">sup ω∈R σ max (G(iω)) ≤ sup ω∈R σ max (W ) σ min (A − iωI) ≤ σ max (W ) σ min (A sym ) . σ max (Y ω ) ≤ M 2 σ max (G(−iω) T )σ max (G(iω)) ≤ M σ max (W ) σ min (A sym ) 2 &lt; 1,</formula><p>by assumption. Finally, since Z(∞) + Z(∞) T = 2I is positive definite, Z(s) is strictly positive real and Theorem 3 applies. Now, consider case (b). The proof proceeds in two steps. First, we verify that the transfer function G(s) = W (A − sI) −1 satisfies the conditions of the Kalman-Yakubovich-Popov lemma. Then, using the matrices P , L, U , and the constant inferred from the lemma, a Lyapunov function is constructed which satisfies the conditions of Theorem 2, guaranteeing global exponential stability. Once again, condition (i) of Lemma 3 is straightforward to verify: G(s) exhibits poles when s is an eigenvalue of A, and so the poles of G(s) also have negative real parts. Furthermore, condition (iii) is easily satisfied with M = I since G(∞) + G(∞) T = 0. To show that condition (ii) holds, observe that for any ω ∈ R, letting A −T = (A −1 ) T for brevity,</p><formula xml:id="formula_32">G(iω) + G(−iω) T = W (A − iωI) −1 + (A + iωI) −T W T = (A + iωI) −T [(A + iωI) T W + W T (A − iωI)](A − iωI) −1 .</formula><p>Since the inner matrix factor is Hermitian, Sylvester's law of inertia implies that G(iω) + G(−iω) T is positive definite if and only if</p><formula xml:id="formula_33">B ω := (A + iωI) T W + W T (A − iωI).</formula><p>is positive definite. Since B ω is a Hermitian matrix, it has real eigenvalues, with minimal eigenvalue given by the infimum of the Rayleigh quotient:</p><formula xml:id="formula_34">λ min (B ω ) = inf v =1 v T B ω v = inf v =1 v T (A T W + W T A)v + iωv T (W − W T )v = inf v =1 v T (A T W + W T A)v = λ min (A T W + W T A).</formula><p>By assumption, A T W + W T A has strictly positive eigenvalues, and hence B ω and G(iω) + G(−iω) T are positive definite. Therefore, Lemma 3 applies, and we obtain matrices P, L, U and a constant &gt; 0 with the corresponding properties. Now we may construct our Lyapunov function V . Let v = Wh and</p><formula xml:id="formula_35">u(t) = σ(v(t) + W h * + U x(t) + b) − σ(W h * + U x(t) + b),</formula><p>so thatḣ = Ah + u. Since σ is monotone non-decreasing, σ(x) − σ(y) ≥ 0 for any x ≥ y. This implies that for each i = 1, . . . , N , v i and u i have the same sign. In particular, v T u ≥ 0. Now, let V (h) = h T P h be our Lyapunov function, noting that V is independent of t. Taking the derivative of the Lyapunov function over (13) and using the properties of P, L, U, ,</p><formula xml:id="formula_36">V (h) =h T Pḣ +ḣ T Ph =h T (P A + A T P )h + 2h T P u =h T (−L T L − P )h + 2h T (L T U − W T )u = −(Lh) T (Lh) + (Lh) T U u + (U u) T (Lh) − u T U T U u − 2v T u = −(Lh + U u) T (Lh + U u) − h T Ph − 2v T u.</formula><p>Since v T u ≥ 0 and (Lh + U u) T (Lh + U u) ≥ 0, it follows thatV (h) ≤ − λ min (P ) h 2 , and hence global exponential stability follows from Theorem 2 and positive-definiteness of P .</p><p>To finish off discussion regarding the results from Sec. 3, we provide a quick proof of Lemma 1 using a simple diagonalization argument.</p><p>Proof of Lemma 1. Since A is symmetric and real-valued, by (Horn &amp; Johnson, 2012, Theorem 4.1.5), there exists an orthogonal matrix P and a real diagonal matrix D such that A = P DP T . Letting z = P T h where h satisfies (4), since h = P z, we see thaṫ</p><formula xml:id="formula_37">z = P T P DP T h + P T σ(W h + U x + b) = Dz + P T σ(W P z + U x + b).</formula><p>Therefore, z satisfies (5) with L = P T and V = W P , both of which are nonsingular by orthogonality of P . By the same argument, for any equilibrium h * , taking z * = P T h * ,</p><formula xml:id="formula_38">Dz * + P T σ(W P z * + U x + b) = P T (P DP T h * + σ(W h * + U x + b))</formula><p>= P T (Ah * + σ(W h * + U x + b)) = 0, implying that z * is an equilibrium of (5). Furthermore, since</p><formula xml:id="formula_39">z − z * 2 = (P T h − P T h * ) T (P T h − P T h * ) = (h − h * ) T P P T (h − h * ) = h − h * 2</formula><p>, from orthogonality of P . Because every form of Lyapunov stability, both local and global, including global exponential stability, depend only on the norm h − h * <ref type="bibr">(Khalil, 2002, Definitions 4.4 and 4.5)</ref>, h * is stable under any of these forms if and only if z * is also stable.</p><p>We remark that the proof of Lemma 1 can extend to matrices A which have real eigenvalues and are diagonalizable. These attributes are implied for symmetric matrices. However, they can be difficult to ensure in practice for nonsymmetric matrices without imposing difficult structural constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PROOF OF PROPOSITION 1</head><p>The proof of Proposition 1 relies on the following lemma, which we also have made use of several times throughout this work. Proof. Recall by the min-max theorem, for u, v = u * v, where u * is the conjugate transpose of u, the upper and lower eigenvalues of A + A T satisfy</p><formula xml:id="formula_40">λ min (A + A T ) = inf v∈C N , v =1 v, (A + A T )v = inf v∈C N , v =1 v, Av + Av, v , λ max (A + A T ) = sup v∈C N , v =1 v, (A + A T )v = sup v∈C N , v =1 v, Av + Av, v .</formula><p>Let λ i (A) = u + iω be an eigenvalue of A with corresponding eigenvector v satisfying v = 1.</p><formula xml:id="formula_41">Since Av = (u + iω)v, v, Av + Av, v = v, Av + v, Av = 2 v, Av = 2u v 2 = 2u. Hence, λ min (A + A T ) ≤ u ≤ λ max (A + A T ).</formula><p>Proof of Proposition 1. By construction, S sym β,γ = S β,γ + S T β,γ = (1 − β)M sym − γI, and so from Lemma 4, both the real parts λ i (S β,γ ) of the eigenvalues of S β,γ as well as the eigenvalues of S sym β,γ lie in the interval</p><formula xml:id="formula_42">[λ min (S sym β,γ ), λ max (S sym β,γ )] = [λ min ((1 − β)M sym − γI), λ max ((1 − β)M sym − γI)</formula><p>]. If β &lt; 1, for any eigenvalue λ of S sym β,γ with corresponding eigenvector v,</p><formula xml:id="formula_43">(1 − β)M sym v − γv = λv, and so M sym v = λ + γ 1 − β v implying that λ+γ 1−β</formula><p>is an eigenvalue of M sym , and therefore contained in [λ min (M sym ), λ max (M sym )]. In particular, we find that  <ref type="figure" target="#fig_5">Figure 4</ref> illustrates the effect of β onto the eigenvalues of A β,γ with the largest and smallest real parts. It can be seen, both empirically and theoretically, that the real part of the eigenvalues converges towards zero as β tends towards one, i.e., we yield a skew-symmetric matrix with purely imaginary eigenvalues in the limit. Thus, for a sufficiently large parameter β we yield a system that approximately preserves an "energy" for a limited time-horizon Rλ i (A β,γ ) ≈ 0, for i = 1, 2, . . . , N.</p><formula xml:id="formula_44">[λ min (S sym β,γ ), λ max (S sym β,γ )] ⊆ [(1 − β)λ min (M sym ) − γ, (1 − β)λ max (M sym )],<label>(16)</label></formula><p>A.3 PROOF OF LEMMA 2</p><p>First, it follows from Gronwall's inequality that the norm of the final hidden state h(T ) is bounded uniformly in β. From Weyl's inequalities and the definition of A β,γ , max</p><formula xml:id="formula_46">k |∆ δ λ k (A sym β,γ )| ≤ ∆ δ A sym β,γ = (1 − β) ∆ δ M sym A .</formula><p>By the chain rule, for each element M ij A of the matrix M A , ∂L</p><formula xml:id="formula_47">∂M ij A = ∂L ∂y(T ) ∂y(T ) ∂h(T ) ∂h(T ) ∂M ij A = ∂L ∂y(T ) D ∂h(T ) ∂M ij A .</formula><p>Now, for any collection of parameters θ i ,</p><formula xml:id="formula_48">d dt i ∂h ∂θ i = A i ∂h ∂θ i + i ∂A ∂θ i h + sech 2 (W h + U x + b) W i ∂h ∂θ i + i ∂W ∂θ i h ,</formula><p>and from Gronwall's inequality,</p><formula xml:id="formula_49">i ∂h(T ) ∂θ i ≤ i ∂A β,γ ∂θ i + i ∂W β,γ ∂θ i h e ( A β,γ + W β,γ )T . Since ∆ δ M sym A = δ ∂L ∂M A + δ ∂L ∂M A T , ∆ δ M sym A ≤ ∆ δ M sym A F ≤ δ i,j ∂L ∂M ij A + ∂L ∂M ij A 2 ≤ δ ∂L ∂y D h e ( A β,γ + W β,γ )T i,j ∂A β,γ ∂M ij A + ∂A β,γ ∂M ji A 2 . Since ∂(M A h) ∂M ij A = ∂(M T A h) ∂M ji A , it follows that ∂A β,γ ∂M ij A + ∂A β,γ ∂M ji A = 2(1 − β) ∂(M A h) ∂M ij A + ∂(M T A h) ∂M ji A , and so ∆ δ M sym A = O(δ(1 − β)), and therefore max k |∆ δ σ k (A sym β,γ )| = O(δ(1 − β) 2 ). Similarly, for the matrix M W , max k ∆ δ λ k (W sym β,γ ) ≤ (1 − β) ∆ δ M sym W ≤ δ(1 − β) ∂L ∂y D h e ( A β,γ + W β,γ )T i,j ∂W β,γ ∂M ij W + ∂W β,γ ∂M ji W 2 = 2δ(1 − β) 2 ∂L ∂y D h e ( A β,γ + W β,γ )T i,j ∂(M W h) ∂M ij W + ∂(M T W h) ∂M ji W 2 , and hence max k |∆ δ λ k (W sym β,γ )| = O(δ(1 − β) 2 )</formula><p>. In <ref type="figure" target="#fig_7">Figure 5</ref>, we plot the most positive real part of the eigenvalues of A β,γ and W β,γ during training for the ordered MNIST task. As β increases, the eigenvalues change less during training, remaining in the stability region provided by case (b) of Theorem 1 for more of the training time. The hidden matrices are initialized by sampling weights from the normal distribution N (0, σ), where σ is the variance, which can be treated as a tuning parameter. In our experiments we typically chose a small σ; see the <ref type="table" target="#tab_11">Table 8</ref> for details. To show that the Lipschitz RNN is insensitive to random initialization, we have trained each model with 10 different seeds. <ref type="table" target="#tab_6">Table 4</ref> shows the maximum, average and minimum values obtained for each task. Note that higher values indicate better performance on the ordered and permuted MNIST tasks, while lower values indicate better performance on the TIMIT task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ORDERED PIXEL-BY-PIXEL AND NOISE-PADDED CIFAR-10</head><p>The pixel-by-pixel CIFAR-10 benchmark problem that has recently been proposed by <ref type="bibr" target="#b4">(Chang et al., 2019)</ref>. This task is similar to the pixel-by-pixel MNIST task, yet more challenging due to the increased sequence length and the more difficult classification problem. Similar to MNIST, we flatten the CIFAR-10 images to construct a sequence of length 1024 in scanline order, where each element of the sequence consists of three pixels (one from each channel).</p><p>A variation of this problem is the noise-padded CIFAR-10 problem <ref type="bibr" target="#b4">(Chang et al., 2019)</ref>, where we consider each row of an image as input at time step t. The rows from each channel are stacked so that we obtain an input of dimension x ∈ R 96 . Then, after the 32 time step which process the 32  row, we start to feed the recurrent unit with independent standard Gaussian noise for 968 time steps. At the final point in T = 1000, we use the learned hidden state for classification. This problem is challenging because only the first 32 time steps contain signals. Thus, the recurrent unit needs to recall information from the beginning of the process.  <ref type="bibr" target="#b26">Kerg et al. (2019)</ref>, which computes the performance in terms of mean bits per character (BPC). <ref type="table" target="#tab_9">Table 6</ref> shows the results for back-propagation through time (BPTT) over 150 and 300 time steps, respectively. The Lipschitz RNN performs slightly better then the exponential RNN and the nonnormal RNN on this task. <ref type="bibr" target="#b26">(Kerg et al., 2019)</ref> notes that orthogonal hidden-to-hidden matrices are not particular well-suited for this task. Thus, it is not surprising that the Lipschitz unit has a small advantage here.</p><p>For comparison, we have also tested the Antisymmetric RNN <ref type="bibr" target="#b4">(Chang et al., 2019)</ref> on this task. The performance of this unit is considerably weaker as compared to our Lipschitz unit. This suggests that the Lipschitz RNN is more expressive and improves the propagation of meaningful signals over longer time scales. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.2 WORD-LEVEL PREDICTION</head><p>In addition to character-level prediction, we also consider word-level prediction using the PTB corpus. For comparison with other state-of-the-art units, we consider the setup by <ref type="bibr" target="#b29">Kusupati et al. (2018)</ref>, who use a sequence length of 300. <ref type="table" target="#tab_10">Table 7</ref> shows results for back-propagation through time (BPTT) over 300 time steps. The Lipschitz RNN performs slightly better than the other RNNs on this task and the baseline LSTM for the test perplexity metric reported by <ref type="bibr" target="#b29">Kusupati et al. (2018)</ref>. C TUNING PARAMETERS For tuning we utilized a standard training procedure using a non-exhaustive random search within the following plausible ranges for the our weight parameterization β = 0.65, 0.7, 0.75, 0.8, γ = [0.001, 1.0]. For Adam we explored learning rates between 0.001 and 0.005, and for SGD we considered 0.1. For the step size we explored values in the range 0.001 to 1.0. We did not perform an automated grid search and thus expect that the models can be further fine-tuned.</p><p>The tuning parameters for the different tasks that we have considered are summarized in <ref type="table" target="#tab_11">Table 8</ref>.</p><p>For pixel-by-pixel MNIST and CIFAR-10, we use Adam for minimizing the objective. We train all our models for 100 epochs, with scheduled learning rate decays at epochs {90}. We do not use gradient clipping during training. <ref type="figure" target="#fig_8">Figure 6</ref> shows the test accuracy curves for our Lipschitz RNN for the ordered and permuted MNIST classification tasks.</p><p>For TIMIT we use Adam with default parameters for minimizing the objective. We also tried Adam using betas (0.0, 0.9) as well as RMSprop with α = 0.9, however, Adam with default values worked best in our experiments. We train the model for 1200 epochs without learning-rate decay. Similar to <ref type="bibr" target="#b26">Kerg et al. (2019)</ref> we train our model with gradient clipping, however, we observed that the performance of our model is relatively insensitive to the clipping value.</p><p>For the character level prediction task, we use Adam with default parameters for minimizing the objective, while we use RMSprop with α = 0.9 for the word level prediction task. We train the model for 200 epochs for the character-level task, and for 500 epochs for the word-level task.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) Salt and pepper perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sensitivity with respect to different input perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The ablation study examines the effect of the linear term Ah (in (a)) and the importance of the Skew-Symmetric Decomposition for constructing the hidden-to-hidden matrices (in (b)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The most common type of Lyapunov function satisfying the conditions of Theorem 2 is of the formV (h) = (h − h * ) T P (h − h * ), where P is a positive definite matrix. One need only show thatV (h) ≤ −(h − h * ) T Q(h − h * )for some other positive definite matrix Q to guarantee global exponential stability.The construction of the Lyapunov function V that satisfies the conditions of Theorem 2 is accomplished using the Kalman-Yakubovich-Popov lemma, which is a statement regarding strictly positive real transfer functions. We use the following definition, equivalent to other standard definitions by (Khalil, 2002, Lemma 6.1). Definition 2. A function G : C → C N ×N is strictly positive real if it satisfies the following:(i) The poles of G(s) have negative real parts.(ii) G(iω) + G(−iω) T is positive definite for all ω ∈ R, where i = √ −1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>is globally exponentially stable towards an equilibrium at the origin if ψ(t, y) ≤ M y for some M &gt; 0 and Z(s) = [I + M G(s)][I − M G(s)] −1 is strictly positive real, where G(s) = W (sI − A) −1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 4 .</head><label>4</label><figDesc>For any matrix A ∈ R N ×N , the real parts of the eigenvalues λ i (A) are contained in the interval [λ min (A sym ), λ max (A sym )], where A sym = 1 2 (A + A T ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>as required. Finally, if β = 1, then (16) still holds, since both intervals collapse to the single point {−γ}. Empirical evaluation of the theoretical bounds (16). The red lines track the largest real part and the blue lines track the smallest real part of the eigenvalues of the hidden-to-hidden matrix A β . Each line corresponds to a different hidden-to-hidden matrix of dimension N = 64 in (a) and N = 128 in (b). The dashed black lines indicate the theoretical bound for each trial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>The red lines track the largest real part of the eigenvalues of the hidden-to-hidden matrix A β,γ and the blue lines track the largest real part of the eigenvalues of W β,γ . We show results for two models trained on the ordered MNIST task with varying β.B ADDITIONAL EXPERIMENTS B.1 SENSITIVITY TO RANDOM INITIALIZATION FOR MNIST AND TIMIT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Test accuracy for the Lipschitz RNN for different classification tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>an intermediate hidden state. The RK2 scheme can potentially improve the performance since the scheme is more accurate, however, this scheme also requires twice as many function evaluations as compared to the forward Euler scheme. Given a β and γ that yields a globally exponentially stable continuous model, ∆t can always be chosen so that the model remains in the stability region of forward Euler and RK2<ref type="bibr" target="#b32">(LeVeque, 2007)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation accuracy on ordered and permuted pixel-by-pixel MNIST.</figDesc><table><row><cell>Name</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on TIMIT using 1 layer models. The mean squared error (MSE) is computes the distance between the predicted and actual log-magnitudes of each predicted frame in the sequence.</figDesc><table><row><cell>Name</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>provides a summary of our results. The Lipschitz RNN, with hidden dimension of N = 128 and trained with the forward Euler and RK2 scheme, achieves 99.4% and 99.3% accuracy on the ordered pixel-by-pixel MNIST task. For the permuted task, the model trained with forward Euler achieves 96.3% accuracy, whereas the model trained with RK2 achieves 96.2% accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>lists the results for the Lipschitz recurrent unit as well as for several benchmark models. It can be seen that the Lipschitz RNN outperforms other state-of-the-art models for a fixed number of parameters (≈ 200K). In particular, LSTMs do not perform well on this task, however, the recently proposed momentum based LSTMs<ref type="bibr" target="#b40">(Nguyen et al., 2020)</ref> have improvemed performance. Interestingly, the RK2 scheme leads to a better performance since this scheme provides more accurate approximations for the intermediate states.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Summary of Hessian-based robustness metrics and resilience to adversarial attacks.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Model</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PGD</cell><cell>DF 2</cell><cell>DF ∞ λ max (H) tr(H) κ(H)</cell></row><row><cell></cell><cell></cell><cell cols="4">Neural ODE RNN</cell><cell></cell><cell cols="3">88.5% 69.6% 44.5%</cell><cell>0.30</cell><cell>4.7</cell><cell>37.6</cell></row><row><cell></cell><cell></cell><cell cols="5">Antisymmetric RNN</cell><cell cols="3">84.7% 83.4% 44.3%</cell><cell>0.24</cell><cell>4.8</cell><cell>35.5</cell></row><row><cell></cell><cell></cell><cell cols="8">Lipschitz RNN (ours) 93.0% 89.2% 54.1%</cell><cell>0.14</cell><cell>3.1</cell><cell>23.2</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Neural ODE RNN</cell></row><row><cell>test accuracy</cell><cell>0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Antisymmetric RNN Lipschitz RNN Unitary RNN</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>0.5</cell><cell>1.0</cell><cell>1.5</cell><cell>2.0</cell><cell>2.5</cell><cell>3.0</cell><cell>3.5</cell><cell>4.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">amount of noise</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">(a) White noise perturbations.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Sensitivity to random initialization evaluated over 10 runs.</figDesc><table><row><cell>Solver</cell><cell>Task</cell><cell cols="3">Minimum Average Maximum</cell><cell>N</cell><cell># params</cell></row><row><cell>Euler</cell><cell>ordered MNIST</cell><cell>98.9%</cell><cell>99.0%</cell><cell>99.0%</cell><cell>64</cell><cell>≈9K</cell></row><row><cell>RK2</cell><cell>ordered MNIST</cell><cell>98.9%</cell><cell>99.0%</cell><cell>99.1%</cell><cell>64</cell><cell>≈9K</cell></row><row><cell>Euler</cell><cell>ordered MNIST</cell><cell>99.0%</cell><cell>99.2%</cell><cell>99.4%</cell><cell>128</cell><cell>≈34K</cell></row><row><cell>RK2</cell><cell>ordered MNIST</cell><cell>98.9%</cell><cell>99.1%</cell><cell>99.3%</cell><cell>128</cell><cell>≈34K</cell></row><row><cell>Euler</cell><cell>permuted MNIST</cell><cell>93.5%</cell><cell>93.8%</cell><cell>94.2%</cell><cell>64</cell><cell>≈9K</cell></row><row><cell>RK2</cell><cell>permuted MNIST</cell><cell>93.5%</cell><cell>93.9%</cell><cell>94.2%</cell><cell>64</cell><cell>≈9K</cell></row><row><cell>Euler</cell><cell>permuted MNIST</cell><cell>95.6%</cell><cell>95.9%</cell><cell>96.3%</cell><cell>128</cell><cell>≈34K</cell></row><row><cell>RK2</cell><cell>permuted MNIST</cell><cell>95.4%</cell><cell>95.8%</cell><cell>96.2%</cell><cell>128</cell><cell>≈34K</cell></row><row><cell>Euler</cell><cell>TIMIT (test MSE)</cell><cell>2.82</cell><cell>2.98</cell><cell>3.10</cell><cell>256</cell><cell>≈198K</cell></row><row><cell>RK2</cell><cell>TIMIT (test MSE)</cell><cell>2.76</cell><cell>2.81</cell><cell>2.84</cell><cell>256</cell><cell>≈198K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Evaluation accuracy on pixel-by-pixel CIFAR-10 and noise padded CIFAR-10.</figDesc><table><row><cell>Name</cell><cell cols="2">ordered noise padded</cell><cell>N</cell><cell># params</cell></row><row><cell>LSTM baseline by (Chang et al., 2019)</cell><cell>59.7%</cell><cell>11.6%</cell><cell>128</cell><cell>69K</cell></row><row><cell cols="2">Antisymmetric RNN (Chang et al., 2019) 58.7%</cell><cell>48.3%</cell><cell>256</cell><cell>36K</cell></row><row><cell>Incremental RNN (Kag et al., 2020)</cell><cell>-</cell><cell>54.5%</cell><cell>128</cell><cell>-</cell></row><row><cell>Lipschitz RNN using Euler (ours)</cell><cell>60.5%</cell><cell>57.4%</cell><cell>128</cell><cell>34K/46K</cell></row><row><cell>Lipschitz RNN using RK2 (ours)</cell><cell>60.3%</cell><cell>57.3%</cell><cell>128</cell><cell>34K/46K</cell></row><row><cell>Lipschitz RNN using Euler (ours)</cell><cell>64.2%</cell><cell>59.0%</cell><cell cols="2">256 134K/158K</cell></row><row><cell>Lipschitz RNN using RK2 (ours)</cell><cell>64.2%</cell><cell>58.9%</cell><cell cols="2">256 134K/158K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>provides a summary of our results. Our Lipschitz recurrent unit outperforms both the incremental RNN<ref type="bibr" target="#b25">(Kag et al., 2020)</ref> and the antisymmetric RNN<ref type="bibr" target="#b4">(Chang et al., 2019)</ref> by a significant margin. This impressively demonstrates that the Lipschitz unit enables the stable propagation of signals over long time horizons.<ref type="bibr" target="#b35">Marcus et al., 1993</ref>). Specifically, this task studies how well a model can predict the next character in a sequence of text. The dataset is composed of a train / validation / test set, where 5017K characters are used for training, 393K characters are used for validation and 442K characters are used for testing. For our experiments, we used the publicly available implementation of this task by</figDesc><table><row><cell>B.3 PENN TREE BANK (PTB)</cell></row><row><cell>B.3.1 CHARACTER LEVEL PREDICTION</cell></row><row><cell>Next, we consider a character level language modeling task using the Penn Treebank Corpus</cell></row><row><cell>(PTB) (</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Evaluation accuracy on PTB for character-level prediction for different sequence lengths T . The * indicate results that were adopted from<ref type="bibr" target="#b26">Kerg et al. (2019)</ref>.NameT P T B = 150 T P T B = 300 # params RNN baseline by(Arjovsky et al.</figDesc><table><row><cell>, 2016)</cell><cell>2.89</cell><cell>2.90</cell><cell>≈1.32M</cell></row><row><cell>RNN-orth (Henaff et al., 2016) (*)</cell><cell>1.62</cell><cell>1.66</cell><cell>≈1.32M</cell></row><row><cell>EURNN (Jing et al., 2017) (*)</cell><cell>1.61</cell><cell>1.62</cell><cell>≈1.32M</cell></row><row><cell>Exponential RNN (Lezcano-Casado &amp; Martinez-Rubio, 2019) (*)</cell><cell>1.49</cell><cell>1.52</cell><cell>≈1.32M</cell></row><row><cell>Non-normal RNN (Kerg et al., 2019)</cell><cell>1.47</cell><cell>1.49</cell><cell>≈1.32M</cell></row><row><cell>Antisymmteric RNN</cell><cell>1.60</cell><cell>1.64</cell><cell>≈1.32M</cell></row><row><cell>Lipschitz RNN using Euler (ours)</cell><cell>1.43</cell><cell>1.46</cell><cell>≈1.32M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Evaluation accuracy on PTB for word-level prediction. The * indicate results adopted from<ref type="bibr" target="#b29">Kusupati et al. (2018)</ref>. Note that here the parameters for the hidden-to-hidden units are reported.</figDesc><table><row><cell>Name</cell><cell cols="2">validation perplexity test perplexity</cell><cell>N</cell><cell># params</cell></row><row><cell>LSTM (*)</cell><cell>-</cell><cell>117.41</cell><cell>-</cell><cell>210K</cell></row><row><cell>SpectralRNN (*)</cell><cell>-</cell><cell>130.20</cell><cell>-</cell><cell>24.8K</cell></row><row><cell>FastRNN (*)</cell><cell>-</cell><cell>127.76</cell><cell>-</cell><cell>52.5K</cell></row><row><cell>FastGRNN-LSQ (*)</cell><cell>-</cell><cell>115.92</cell><cell>-</cell><cell>52.5K</cell></row><row><cell>FastGRNN (*)</cell><cell>-</cell><cell>116.11</cell><cell>-</cell><cell>52.5K</cell></row><row><cell>Incremental RNN (Kag et al., 2020)</cell><cell>-</cell><cell>115.71</cell><cell>-</cell><cell>29.5K</cell></row><row><cell>Lipschitz RNN using Euler (ours)</cell><cell>124.55</cell><cell>115.36</cell><cell>160</cell><cell>50K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Tuning parameters used for our experimental results and the performance evaluated with 12 different seed values for the parameter initialization of the model.</figDesc><table><row><cell>Name</cell><cell>N</cell><cell>lr</cell><cell>decay</cell><cell>β</cell><cell>γ a</cell><cell>γ w</cell><cell>σ</cell></row><row><cell>Ordered MNIST</cell><cell>64</cell><cell>0.003</cell><cell>0.1</cell><cell cols="4">0.75 0.001 0.001 0.03 0.1/64</cell></row><row><cell>Ordered MNIST</cell><cell cols="2">128 0.003</cell><cell>0.1</cell><cell cols="4">0.75 0.001 0.001 0.03 0.1/128</cell></row><row><cell>Permuted MNIST</cell><cell cols="2">64 0.0035</cell><cell>0.1</cell><cell cols="4">0.75 0.001 0.001 0.03 0.1/128</cell></row><row><cell>Permuted MNIST</cell><cell cols="2">128 0.0035</cell><cell>0.1</cell><cell cols="4">0.75 0.001 0.001 0.03 0.1/128</cell></row><row><cell>Ordered CIFAR10</cell><cell>256</cell><cell>0.1</cell><cell>0.2</cell><cell cols="3">0.65 0.001 0.001 0.01</cell><cell>6/256</cell></row><row><cell cols="2">Noise-padded CIFAR10 256</cell><cell>0.1</cell><cell>0.2</cell><cell cols="3">0.75 0.001 0.001 0.01</cell><cell>6/256</cell></row><row><cell>TIMIT</cell><cell cols="2">256 0.001</cell><cell>-</cell><cell>0.8</cell><cell>0.8</cell><cell>0.001 0.9</cell><cell>12/256</cell></row><row><cell cols="3">PTB character-level 150 750 0.005</cell><cell>-</cell><cell>0.8</cell><cell>0.5</cell><cell>0.001 0.1</cell><cell>12/256</cell></row><row><cell cols="3">PTB character-level 300 750 0.005</cell><cell>-</cell><cell>0.8</cell><cell>0.5</cell><cell>0.001 0.1</cell><cell>12/256</cell></row><row><cell>PTB word-level</cell><cell>160</cell><cell>0.1</cell><cell>-</cell><cell>0.8</cell><cell>0.9</cell><cell cols="2">0.001 0.01 10/256</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Ed H. Chi for fruitful discussions about physics-informed machine learning and the Antisymmetric RNN. We are grateful to the generous support from Amazon AWS and Google Cloud. NBE and MWM would like to acknowledge IARPA (contract W911NF20C0035), NSF, ONR and CLTC for providing partial support of this work. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Matrix analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajendra</forename><surname>Bhatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">169</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The tradeoffs of large scale learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AntisymmetricRNN: A dynamical system view on recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Symplectic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling of continuous time dynamical systems with input by recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Tommy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Dong</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="575" to="578" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nais-net: Stable deep networks from non-autonomous differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gallieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/file/7bd28f15a49d5e5848d6ec70e584e625-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3025" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaak</forename><surname>Edward De Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7379" to="7390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximation of dynamical systems by continuous time recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Ken-Ichi Funahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="801" to="806" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">TIMIT acoustic phonetic continuous speech corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An investigation into neural net optimization via Hessian eigenvalue density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2232" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Stability of motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Hahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Orthogonal recurrent neural networks with scaled Cayley transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Helfrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Willmott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1969" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent orthogonal networks and long-memory tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/henaff16.html" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="20" to="22" />
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Cognitive Science Society</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Global stability of a class of continuous-time recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1334" to="1347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tunable efficient unitary neural networks (EUNN) and their application to RNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tena</forename><surname>Dubcek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Peurifoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Skirlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Soljačić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1733" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kronecker recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cijo</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2380" to="2389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RNNs incrementally evolving on an equilibrium manifold: A panacea for vanishing and exploding gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-normal recurrent neural network (nnRNN): Learning long time dependencies while improving expressivity with transient dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Kerg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Goyette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><forename type="middle">Puelma</forename><surname>Touzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gauthier</forename><surname>Gidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lajoie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13591" to="13601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Nonlinear Systems. Pearson Education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khalil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nonlinear observer design using dynamic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaouki</forename><forename type="middle">T</forename><surname>Abdallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 35th IEEE Conference on Decision and Control</title>
		<meeting>35th IEEE Conference on Decision and Control</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="949" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9017" to="9028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning long-term dependencies in irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04418</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Finite Difference Methods for Ordinary and Partial Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><forename type="middle">J</forename><surname>Leveque</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9780898717839</idno>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lezcano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Casado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martinez-Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3794" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Approximation of dynamical time-variant systems by continuous-time recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K L</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommy</forename><forename type="middle">W S</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems II: Express Briefs</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="656" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Building a large annotated corpus of English: the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The neural hawkes process: A neurally self-modulating multivariate point process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6754" to="6764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient orthogonal parametrisation of recurrent neural networks using Householder reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zakaria</forename><surname>Mhammedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hellicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashfaqur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2401" to="2409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stable recurrent models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deepfool: A simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">L</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">J</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06919</idno>
		<title level="m">Momentumrnn: Integrating momentum into recurrent neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Murphy Yuezhen Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Horesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12933</idno>
		<title level="m">Recurrent neural networks in the eye of differential equations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gradient calculations for dynamic recurrent neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1212" to="1228" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamics and architecture for neural computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">J</forename><surname>Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complexity</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="216" to="245" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5321" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Empirical analysis of the Hessian of over-parametrized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Ugur</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04454</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Nonlinear systems: Analysis, stability, and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science</publisher>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Synthesis of recurrent neural networks for dynamical system simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><forename type="middle">M T</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>D&amp;apos;eleuterio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On orthogonality and learning recurrent networks with long term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheb</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3570" to="3578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Full-capacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pyhessian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07145</idno>
		<title level="m">Neural networks through the lens of the Hessian</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A comprehensive review of stability analysis of continuous-time recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1229" to="1262" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
