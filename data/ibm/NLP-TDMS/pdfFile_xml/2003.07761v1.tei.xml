<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CycleISP: Real Image Restoration via Improved Data Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syed</forename><surname>Waqas Zamir</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Arora</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Fahad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CycleISP: Real Image Restoration via Improved Data Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The availability of large-scale datasets has helped unleash the true potential of deep convolutional neural networks (CNNs). However, for the single-image denoising problem, capturing a real dataset is an unacceptably expensive and cumbersome procedure. Consequently, image denoising algorithms are mostly developed and evaluated on synthetic data that is usually generated with a widespread assumption of additive white Gaussian noise (AWGN). While the CNNs achieve impressive results on these synthetic datasets, they do not perform well when applied on real camera images, as reported in recent benchmark datasets. This is mainly because the AWGN is not adequate for modeling the real camera noise which is signaldependent and heavily transformed by the camera imaging pipeline. In this paper, we present a framework that models camera imaging pipeline in forward and reverse directions. It allows us to produce any number of realistic image pairs for denoising both in RAW and sRGB spaces. By training a new image denoising network on realistic synthetic data, we achieve the state-of-the-art performance on real camera benchmark datasets. The parameters in our model are ∼5 times lesser than the previous best method for RAW denoising. Furthermore, we demonstrate that the proposed framework generalizes beyond image denoising problem e.g., for color matching in stereoscopic cinema. The source code and pre-trained models are available at https://github.com/swz30/CycleISP. arXiv:2003.07761v1 [eess.IV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>High-level computer vision tasks, such as image classification, object detection and segmentation have witnessed significant progress due to deep CNNs <ref type="bibr" target="#b31">[33]</ref>. The major driving force behind the success of CNNs is the availability of large-scale datasets <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b36">38]</ref>, containing hundreds of thousands of annotated images. However, for low-level vision problems (image denoising, super-resolution, deblurring, etc.), collecting even small datasets is extremely challenging and non-trivial. For instance, the typical procedure to acquire noisy paired data is to take multiple noisy images (a) Noisy Input (b) N3NET <ref type="bibr" target="#b43">[45]</ref> PSNR(RAW) / PSNR(sRGB) 38.24 dB / 32.42 dB (c) UPI <ref type="bibr" target="#b6">[7]</ref> (d) Ours 37.37 dB / 35.49 dB 40.44 dB / 36.16 dB <ref type="figure">Figure 1</ref>: Denoising a real camera image from DND dataset <ref type="bibr" target="#b42">[44]</ref>. Our model is effective in removing real noise, especially the low-frequency chroma and defective pixel noise. of the same scene and generate clean ground-truth image by pixel-wise averaging. In practice, spatial pixels misalignment, color and brightness mismatch is inevitable due to changes in lighting conditions and camera/object motion. Moreover, this expensive and cumbersome exercise of acquiring image pairs needs to be repeated with different camera sensors, as they exhibit different noise characteristics.</p><p>Consequently, single image denoising is mostly per-formed in synthetic settings: take a large set of clean sRGB images and add synthetic noise to generate their noisy versions. On synthetic datasets, existing deep learning based denoising models yield impressive results, but they exhibit poor generalization to real camera data as compared to conventional methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. This trend is also demonstrated in recent benchmarks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">44]</ref>. Such behavior stems from the fact that deep CNNs are trained on synthetic data that is usually generated with the Additive White Gaussian Noise (AWGN) assumption. Real camera noise is fundamentally different from AWGN, thereby causing a major challenge for deep CNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b22">24]</ref>. In this paper, we propose a synthetic data generation approach that can produce realistic noisy images both in RAW and sRGB spaces. The main idea is to inject noise in the RAW images obtained with our learned device-agnostic transformation rather than in the sRGB images directly. The key insight behind our framework is that the real noise present in sRGB images is convoluted by the series of steps performed in a regular image signal processing (ISP) pipeline <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b44">46]</ref>. Therefore, modeling real camera noise in sRGB is an inherently difficult task as compared to RAW sensor data <ref type="bibr" target="#b33">[35]</ref>. As an example, noise at the RAW sensor space is signal-dependent; after demosaicking, it becomes spatio-chromatically correlated; and after passing through the rest of the pipeline, its probability distribution not necessarily remains Gaussian <ref type="bibr" target="#b50">[52]</ref>. This implies that the camera ISP heavily transforms the sensor noise, and therefore more sophisticated models that take into account the influence of imaging pipeline are needed to synthesize realistic noise than uniform AWGN model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b42">44]</ref>.</p><p>In order to exploit the abundance and diversity of sRGB photos available on the Internet, the main challenge with the proposed synthesis approach is how to transform them back to RAW measurements. Brooks et al. <ref type="bibr" target="#b6">[7]</ref> present a technique that inverts the camera ISP, step-by-step, and thereby allows conversion from sRGB to RAW data. However, this approach requires prior information about the target camera device (e.g., color correction matrices and white balance gains), which makes it specific to a given device and therefore lacks in generalizability. Furthermore, several operations in a camera pipeline are proprietary and such black boxes are very difficult to reverse engineer. To address these challenges, in this paper we propose a CycleISP framework that converts sRGB images to RAW data, and then back to sRGB images, without requiring any knowledge of camera parameters. This property allows us to synthesize any number of clean and realistic noisy image pairs in both RAW and sRGB spaces. Our main contributions are:</p><p>• Learning a device-agnostic transformation, called Cy-cleISP, that allows us to move back and forth between sRGB and RAW image spaces. • Real image noise synthesizer for generating clean/noisy paired data in RAW and sRGB spaces.</p><p>• A deep CNN with dual attention mechanism that is effective in a variety of tasks: learning CycleISP, synthesizing realistic noise, and image denoising. • Algorithms to remove noise from RAW and sRGB images, setting new state-of-the-art on real noise benchmarks of DND <ref type="bibr" target="#b42">[44]</ref> and SIDD <ref type="bibr" target="#b0">[1]</ref> (see <ref type="figure">Fig. 1</ref>). Moreover, our denoising network has much fewer parameters (2.6M) than the previous best model (11.8M) <ref type="bibr" target="#b6">[7]</ref>. • CycleISP framework generalizes beyond denoising, we demonstrate this via an additional application i.e., color matching in stereoscopic cinema.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The presence of noise in images is inevitable, irrespective of the acquisition method; now more than ever, when majority of images come from smartphone cameras having small sensor size but large resolution. Single-image denoising is a vastly researched problem in the computer vision and image processing community, with early works dating back to 1960's <ref type="bibr" target="#b5">[6]</ref>. Classic methods on denoising are mainly based on the following two principles. (1) Modifying transform coefficients using the DCT <ref type="bibr" target="#b59">[61]</ref>, wavelets <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b52">54]</ref>, etc. (2) Averaging neighborhood values: in all directions using Gaussian kernel, in all directions only if pixels have similar values <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b55">57]</ref> and along contours <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b48">50]</ref>.</p><p>While these aforementioned methods provide satisfactory results in terms of image fidelity metrics and visual quality, the Non-local Means (NLM) algorithm of Buades et al. <ref type="bibr" target="#b7">[8]</ref> makes significant advances in denoising. The NLM method exploits the redundancy, or self-similarity <ref type="bibr" target="#b18">[20]</ref> present in natural images. For many years the patch-based methods yielded comparable results, thus prompting studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">37]</ref> to investigate whether we reached the theoretical limits of denoising performance. Subsequently, Burger et al. <ref type="bibr" target="#b8">[9]</ref> train a simple Multi-Layer Perceptron (MLP) on a large synthetic noise dataset. This method performs well against previous sophisticated algorithms. Several recent methods use deep CNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b1">2]</ref> and demonstrate promising denoising performance.</p><p>Image denoising can be applied to RAW or sRGB data. However, capturing diverse large-scale real noise data is a prohibitively expensive and tedious procedure, consequently leaving us to study denoising in synthetic settings. The most commonly used noise model for developing and evaluating image denoising is AWGN. As such, algorithms that are designed for AWGN cannot effectively remove noise from real images, as reported in recent benchmarks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">44]</ref>. A more accurate model for real RAW sensor noise contains both the signal-dependent noise component (the shot noise), and the signal-independent additive Gaussian component (the read noise) <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b22">24]</ref>. The camera ISP transforms RAW sensor noise into a complicated form (spatio-chromatically correlated and not necessarily Gaussian). Therefore, estimating a noise model for denoising in sRGB space requires careful consideration of the influence of ISP. In this paper, we present a framework that is capable of synthesizing realistic noise data for training CNNs to effectively remove noise from RAW as well as sRGB images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CycleISP</head><p>To synthesize realistic noise datasets, we use a two-stage scheme in this work. First, we develop a framework that models the camera ISP both in forward and reverse directions, hence the name CycleISP. Second, using CycleISP, we synthesize realistic noise datasets for the tasks of RAW denoising and sRGB image denoising. In this section, we only describe our CycleISP framework that models the camera ISP as a deep CNN system. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the modules of the CycleISP model: (a) RGB2RAW network branch, and (b) RAW2RGB network branch. In addition, we introduce an auxiliary color correction network branch that provides explicit color attention to the RAW2RGB network in order to correctly recover the original sRGB image.</p><p>The noise injection module in <ref type="figure" target="#fig_0">Fig. 2</ref> is only required when synthesizing noisy data (Section 4), and thus we keep it in the 'OFF' state while learning CycleISP. The training process of CycleISP is divided in two steps: the RGB2RAW and RAW2RGB networks are first independently trained, and then joint fine-tuning is performed. Next, we present details of different branches of CycleISP. Note that we use RGB instead of sRGB to avoid notation clutter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">RGB2RAW Network Branch</head><p>Digital cameras apply a series of operations on RAW sensor data in order to generate the monitor-ready sRGB images <ref type="bibr" target="#b44">[46]</ref>. Our RGB2RAW network branch aims to invert the effect of camera ISP. In contrast to the unprocessing technique of <ref type="bibr" target="#b6">[7]</ref>, the RGB2RAW branch does not require any camera parameters.</p><p>Given an input RGB image I rgb ∈ R H×W ×3 , the RGB2RAW network first extracts low-level features T 0 ∈ R H×W ×C using a convolutional layer M 0 as: T 0 = M 0 (I rgb ). Next, we pass the low-level feature maps T 0 through N recursive residual groups (RRGs) to extract deep features T d ∈ R H×W ×C as:</p><formula xml:id="formula_0">T d = RRG N (...(RRG 1 (T 0 ))) ,<label>(1)</label></formula><p>where each RRG contains multiple dual attention blocks, as we shall see in Section 3.3. We then apply the final convolution operation M 1 to the features T d and obtain the demosaicked imageÎ dem ∈ R H×W ×3 . We deliberately set the number of output channels of M 1 layer to three rather than one in order to preserve as much structural information of the original image as possible. Moreover, we empirically found that it helps the network to learn the mapping from sRGB to RAW faster and more accurately. At this point, the network is able to invert the effects of tone mapping, gamma correction, color correction, white balance, and other transformations, and provide us with the imageÎ dem whose values are linearly related to the scene radiance. Finally, in order to generate the mosaicked RAW outputÎ raw ∈ R H×W ×1 , the Bayer sampling function f Bayer is applied toÎ dem that omits two color channels per pixel according to the Bayer pattern:</p><formula xml:id="formula_1">I raw = f bayer (M 1 (T d )).</formula><p>(</p><p>The RGB2RAW network is optimized using the L 1 loss in linear and log domains as:</p><formula xml:id="formula_3">L s→r (Î raw , I raw ) = Î raw − I raw 1 + log(max(Î raw , )) − log(max(I raw , )) 1 ,<label>(3)</label></formula><p>where is a small constant for numerical stability, and I raw is the ground-truth RAW image. Similar to <ref type="bibr" target="#b19">[21]</ref>, the log loss term is added to enforce approximately equal treatment for all the image values; otherwise the network dedicates more attention to recovering the highlight regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RAW2RGB Network Branch</head><p>While the ultimate goal of RAW2RGB network is to generate synthetic realistic noise data for the sRGB image denoising problem, in this section we first describe how we can map clean RAW images to clean sRGB images (leaving the noise injection module 'OFF' in <ref type="figure" target="#fig_0">Fig. 2</ref>).</p><p>Let I raw andÎ rgb be the input and output of the RAW2RGB network. First, in order to restore translation invariance and reduce computational cost, we pack the 2×2 blocks of I raw into four channels (RGGB) and thus reduce the image resolution by half <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">25]</ref>. Since the input RAW data may come from different cameras having different Bayer patterns, we ensure the channel order of the packed image to be RGGB by applying the Bayer pattern unification technique <ref type="bibr" target="#b37">[39]</ref>. Next, a convolutional layer</p><formula xml:id="formula_4">M 2 followed by K − 1 RRG modules encode the packed RAW image I pack ∈ R H 2 × W 2 ×4 into a deep feature tensor T d ∈ R H 2 × W 2 ×C as: T d = RRG K−1 (...(RRG 1 (M 2 (Pack(I raw ))))). (4)</formula><p>Note that I raw is the original camera RAW image (not the output of RGB2RAW network) because our objective here is to first learn RAW to sRGB mapping, independently.</p><p>Color attention unit. To train the CycleISP, we use the MIT-Adobe FiveK dataset <ref type="bibr" target="#b9">[10]</ref> that contains images from several different cameras having diverse and complex ISP systems. It is extremely difficult for a CNN to accurately learn a RAW to sRGB mapping function for all different types of cameras (as one RAW image can potentially map to many sRGB images). One solution is to train one network for each camera ISP <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b60">62]</ref>. However, such solutions are not scalable and the performance may not generalize to other cameras. To address this issue, we propose to include a color attention unit in the RAW2RGB network that provides explicit color attention via a color correction branch.</p><p>The color correction branch is a CNN that takes as input an sRGB image I rgb and generates a color-encoded deep feature tensor T color ∈ R H 2 × W 2 ×C . In the color correction branch, we first apply Gaussian blur to I rgb , followed by a convolutional layer M 3 , two RRGs and a gating mechanism with sigmoid activation σ:</p><formula xml:id="formula_5">T color = σ(M 4 (RRG 2 (RRG 1 (M 3 (K * I rgb ))))), (5)</formula><p>where * denotes convolution, and K is the Gaussian kernel with standard deviation empirically set to 12. This strong blurring operation ensures that only the color information flows through this branch, whereas the structural content and fine texture comes from the main RAW2RGB network. Using weaker blurring will undermine the effectiveness of the feature tensor T d of Eq. (4). The overall color attention unit process becomes:</p><formula xml:id="formula_6">T atten = T d + (T d ⊗ T color ),<label>(6)</label></formula><p>where, ⊗ is Hadamard product. To obtain the final sRGB imageÎ rgb , the output features T atten from the color attention unit are passed through a RRG module, a convolutional layer M 4 and an upscaling layer M up <ref type="bibr" target="#b51">[53]</ref>, respectively:</p><formula xml:id="formula_7">I rgb = M up (M 5 (RRG K (T atten ))).<label>(7)</label></formula><p>For optimizing RAW2RGB network, we use the L 1 loss:</p><formula xml:id="formula_8">L r→s (Î rgb , I rgb ) = Î rgb − I rgb 1 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">RRG: Recursive Residual Group</head><p>Motivated by the advances of recent low-level vision methods <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b64">66]</ref> based on the residual learning framework <ref type="bibr" target="#b27">[29]</ref>, we propose the RRG module, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. The RRG contains P dual attention blocks (DAB). The goal of each DAB is to suppress the less useful features and only allow the propagation of more informative ones. The DAB performs this feature recalibration by using two attention mechanisms: (1) channel attention (CA) <ref type="bibr" target="#b28">[30]</ref>, and (2) spatial attention (SA) <ref type="bibr" target="#b56">[58]</ref>. The overall process is:</p><formula xml:id="formula_9">T DAB = T in + M c ([CA(U ), SA(U )]),<label>(9)</label></formula><p>where U ∈ R H×W ×C denotes features maps that are obtained by applying two convolutions on input tensor T in ∈ R H×W ×C at the beginning of the DAB, and M c is the last convolutional layer with filter size 1 × 1.</p><p>Channel attention. This branch is designed to exploit the inter-channel dependencies of convolutional features. It first performs a squeeze operation in order to encode the spatially global context, which is then followed by an excitation operation to fully capture channel-wise relationships <ref type="bibr" target="#b28">[30]</ref>. The squeeze operation is realized by applying global average pooling (GAP) on feature maps U , thus yielding a descriptor z ∈ R 1×1×C . The excitation operator recalibrates the descriptor z using two convolutional layers followed by the sigmoid activation and results in activations s ∈ R 1×1×C . Finally, the output of CA branch is obtained by rescaling U with the activations s.</p><p>Spatial attention. This branch exploits the inter-spatial relationships of features and computes a spatial attention map that is then used to rescale the incoming features U . To generate the spatial attention map, we first independently apply global average pooling and max pooling operations on features U along the channel dimensions and concatenate the output maps to form a spatial feature descriptor d ∈ R H×W ×2 . This is followed by a convolution and sigmoid activation to obtain the spatial attention map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Joint Fine-tuning of CycleISP</head><p>Since the RGB2RAW and RAW2RGB networks are initially trained independently, they may not provide the optimal-quality images due to the disconnection between them. Therefore, we perform joint fine-tuning in which the output of RGB2RAW becomes the input of RAW2RGB. The loss function for the joint optimization is:</p><formula xml:id="formula_10">L joint = βL s→r (Î raw , I raw ) + (1−β)L r→s (Î rgb , I rgb ),</formula><p>where β is a positive constant. Note that the RAW2RGB network receives gradients from the RAW2RGB sub-loss (only the second term). Whereas, the RGB2RAW network receives gradients from both sub-losses, thereby effectively contributing to the reconstruction of the final sRGB image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Synthetic Realistic Noise Data Generation</head><p>Capturing perfectly-aligned real noise data pairs is extremely difficult. Consequently, image denoising is mostly studied in artificial settings where Gaussian noise is added to the clean images. While the state-of-the-art image denoising methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b62">64]</ref> have shown promising performance on these synthetic datasets, they do not perform well when applied on real camera images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">44]</ref>. This is because the synthetic noise data differs fundamentally from real camera data. In this section, we describe the process of synthesizing realistic noise image pairs for denoising both in RAW <ref type="figure">Figure 5</ref>: Proposed denoising network. It has the same network structure for denoising both RAW images and sRGB images, except in the handling of input and output. and sRGB space using the proposed CycleISP method.</p><p>Data for RAW denoising. The RGB2RAW network branch of the CycleISP method takes as input a clean sRGB image and converts it to a clean RAW image (top branch, <ref type="figure" target="#fig_0">Fig. 2</ref>). The noise injection module, which we kept off while training CycleISP, is now turned to the 'ON' state. The noise injection module adds shot and read noise of different levels to the output of RGB2RAW network. We use the same procedure for sampling shot/read noise factors as in <ref type="bibr" target="#b6">[7]</ref>. As such, we can generate clean and its corresponding noisy image pairs {RAW clean , RAW noisy } from any sRGB image.</p><p>Data for sRGB denoising. Given a synthetic RAW noisy image as input, the RAW2RGB network maps it to a noisy sRGB image (bottom branch, <ref type="figure" target="#fig_0">Fig. 2</ref>); hence we are able to generate an image pair {sRGB clean ,sRGB noisy } for the sRGB denoising problem. While these synthetic image pairs are already adequate for training the denoising networks, we can further improve their quality with the following procedure. We fine-tune the CycleISP model (Section 3.4) using the SIDD dataset <ref type="bibr" target="#b0">[1]</ref> that is captured with real cameras. For each static scene, SIDD contains clean and noisy image pairs in both RAW and sRGB spaces. The fine-tuning process is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Notice that the noise injection module which adds random noise is replaced by (only for fine-tuning) per-pixel noise residue that is obtained by subtracting the real RAW clean image from the real RAW noisy image. Once the fine-tuning procedure is complete, we can synthesize realistic noisy images by feeding clean sRGB images to the CycleISP model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Denoising Architecture</head><p>As illustrated in <ref type="figure">Fig. 5</ref>, we propose an image denoising network by employing multiple RRGs. Our aim is to apply the proposed network in two different settings: (1) denoising RAW images, and (2) denoising sRGB data. We use the same network structure under both settings, with the only difference being in the handling of input and output. For denoising in the sRGB space, the input and output of the network are the 3-channel sRGB images. For denoising the RAW images, our network takes as input a 4channel noisy packed image concatenated with a 4-channel noise level map, and provides us with a 4-channel packed denoised output. The noise level map provides an estimate of the standard deviation of noise present in the input image, based on its shot and read noise parameters <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Real Image Datasets</head><p>DND <ref type="bibr" target="#b42">[44]</ref>. This dataset consists of 50 pairs of noisy and (nearly) noise-free images captured with four consumer cameras. Since the images are of very high-resolution, the providers extract 20 crops of size 512 × 512 from each image, thus yielding a total of 1000 patches. The complete dataset is used for testing because the ground-truth noisefree images are not publicly available. The data is provided for two evaluation tracks: RAW space and sRGB space. Quantitative evaluation in terms of PSNR and SSIM can only be performed through an online server <ref type="bibr">[16]</ref>.</p><p>SIDD <ref type="bibr" target="#b0">[1]</ref>. Due to the small sensor size and high-resolution, smartphone images are much more noisy than those of DSLRs. This dataset is collected using five smartphone cameras. There are 320 image pairs available for training and 1280 image pairs for validation. This dataset also provides images both in RAW format and in sRGB space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Implementation Details</head><p>All the models presented in this paper are trained with Adam optimizer (β 1 = 0.9, and β 2 = 0.999) and image crops of 128 × 128. Using the Bayer unification and augmentation technique <ref type="bibr" target="#b37">[39]</ref>, we randomly perform horizontal and vertical flips. We set a filter size of 3 × 3 for all convolutional layers of the DAB except the last for which we use 1 × 1.</p><p>Initial training of CycleISP. To train the CycleISP model, we use the MIT-Adobe FiveK dataset <ref type="bibr" target="#b9">[10]</ref>, which contains 5000 RAW images. We process these RAW images using the LibRaw library and generate sRGB images. From this dataset, 4850 images are used for training and 150 for validation. We use 3 RRGs and 5 DABs for both RGB2RAW and RAW2RGB networks, and 2 RRGs and 3 DABs for the color correction network. The RGB2RAW and RAW2RGB branches of CycleISP are independently trained for 1200 epochs with a batch size of 4. The initial learning rate is 10 −4 , which is decreased to 10 −5 after 800 epochs.</p><p>Fine-tuning CycleISP. This process is performed twice: first with the procedure presented in Section 3.4, and then with the method of Section 4. In the former case, the output of the CycleISP model is noise-free, and in the latter case, the output is noisy. For each fine-tuning stage, we use 600 epochs, batch size of 1 and learning rate of 10 −5 .</p><p>Training denoising networks. We train four networks to perform denoising on: (1) DND RAW data, (2) DND sRGB images, (3) SIDD RAW data, and (4) SIDD sRGB images. For all four networks, we use 4 RRGs and 8 DABs, 65 epochs, batch size of 16, and initial learning rate of 10 −4 which is decreased by a factor of 10 after every 25 epochs. We take 1 million images from the MIR flickr extended <ref type="table">Table 1</ref>: RAW denoising results on the DND benchmark dataset <ref type="bibr" target="#b42">[44]</ref>. * denotes that these methods use variance stabilizing transform (VST) <ref type="bibr" target="#b38">[40]</ref> to provide their best results.  dataset <ref type="bibr" target="#b29">[31]</ref> and split them into a ratio of 90:5:5 for training, validation and testing. All the images are preprocessed with the Gaussian kernel (σ = 1) to reduce the effect of noise, and other artifacts. Next, we synthesize clean/noisy paired training data (both for RAW and sRGB denoising) using the procedure described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results for RAW Denoising</head><p>In this section, we evaluate the denoising results of the proposed CycleISP model with existing state-of-theart methods on the RAW data from DND <ref type="bibr" target="#b42">[44]</ref> and SIDD <ref type="bibr" target="#b0">[1]</ref> benchmarks. <ref type="table">Table 1</ref> shows the quantitative results (PSNR/SSIM) of all competing methods on the DND dataset obtained from the website of the evaluation server <ref type="bibr">[16]</ref>. Note that there are two super columns in the table listing the values of image quality metrics. The numbers in the sRGB super column are provided by the server after passing the denoised RAW images through the camera imaging pipeline <ref type="bibr" target="#b30">[32]</ref> using image metadata. Our model consistently performs better against the learning-based as well as conventional denoising algorithms. Furthermore, the proposed model has ∼5× lesser parameters than previous best method <ref type="bibr" target="#b6">[7]</ref>. The trend is similar for the SIDD dataset, as shown in <ref type="table" target="#tab_1">Table 2</ref>. Our algorithm achieves 6.89 dB improvement in PSNR over the BM3D algorithm <ref type="bibr" target="#b14">[15]</ref>.</p><p>A visual comparison of our result against the state-of-   <ref type="bibr" target="#b26">[28]</ref> RIDNet <ref type="bibr" target="#b3">[4]</ref> Ours <ref type="figure">Figure 7</ref>: Denoising results of different methods on a challenging sRGB image from the SIDD dataset <ref type="bibr" target="#b0">[1]</ref>.</p><p>the-art algorithms is presented in <ref type="figure">Fig. 1</ref>. Our model is very effective in removing real noise, especially the lowfrequency chroma noise and defective pixel noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Results for sRGB Denoising</head><p>While it is recommended to apply denoising on RAW data (where noise is uncorrelated and less complex) <ref type="bibr" target="#b24">[26]</ref>, denoising is commonly studied in the sRGB domain. We compare the denoising results of different methods on sRGB images from the DND and SIDD datasets. <ref type="table" target="#tab_2">Table 3</ref> and 4 show the scores of image quality metrics. Overall, the proposed model performs favorably against the state-ofthe-art. Compared to the recent best algorithm RIDNet <ref type="bibr" target="#b3">[4]</ref>, our approach demonstrates the performance gain of 0.33 dB and 0.81 dB on DND and SIDD datasets, respectively. <ref type="figure">Fig. 6 and 7</ref> illustrate the sRGB denoising results on DND and SIDD, respectively. To remove noise, most of the evaluated algorithms either produce over-smooth images (and sacrifice image details) or generate images with splotchy texture and chroma artifacts. In contrast, our method generates clean and artifact-free results, while faithfully preserving image details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Generalization Test</head><p>To compare the generalization capability of the denoising model trained on the synthetic data generated by our method and that of <ref type="bibr" target="#b6">[7]</ref>, we perform the following experiments. We take the (publicly available) denoising model of <ref type="bibr" target="#b6">[7]</ref> trained for DND, and directly evaluate it on the RAW images from the SIDD dataset. We repeat the same procedure for our denoising model as well. For a fair comparison, we use the same network architecture (U-Net) and noise model as of <ref type="bibr" target="#b6">[7]</ref>. The only difference is data conver-   sion from sRGB to RAW. The results in <ref type="table" target="#tab_4">Table 5</ref> show that the denoising network trained with our method not only performs well on the DND dataset but also shows promising generalization to the SIDD set (a gain of ∼ 1 dB over <ref type="bibr" target="#b6">[7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Ablations</head><p>We study the impact of individual contributions by progressively integrating them to our model. To this end, we use the RAW2RGB network that maps clean RAW image to clean sRGB image. <ref type="table" target="#tab_5">Table 6</ref> shows that the skip connections cause the largest performance drop, followed by the color correction branch. Furthermore, it is evident that the presence of both CA and SA is important, as well as their configuration (see <ref type="table" target="#tab_6">Table 7</ref>), for the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.">Color Matching For Stereoscopic Cinema</head><p>In professional 3D cinema, stereo pairs for each frame are acquired using a stereo camera setup, with two cameras mounted on a rig either side-by-side or (more commonly) in a beam splitter formation <ref type="bibr" target="#b4">[5]</ref>. During movie production, meticulous efforts are required to ensure that the twin cameras perform in exactly the same manner. However, oftentimes visible color discrepancies between the two views are inevitable because of the imperfect camera adjustments and impossibility of manufacturing identical lens systems. In movie post-production, color mismatch is corrected by a skilled technician, which is an expensive and highly involved procedure <ref type="bibr" target="#b39">[41]</ref>.</p><p>With the proposed CycleISP model, we can perform the color matching task, as shown in <ref type="figure">Fig. 8</ref>. Given a stereo pair, we first choose one view as the target and apply morphing to fully register it with the source view. Next, we pass the source RGB image through RGB2RAW model and obtain the source RAW image. Finally, we map back the source RAW image to the sRGB space using the RAW2RGB net- work, but with the color correction branch providing the color information from the 'target' RGB image (rather than the source RGB). <ref type="figure">Fig. 9</ref> compares our method with three other color matching techniques <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b45">47]</ref>. The proposed method generates results that are perceptually more faithful to the target views than other competing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>sIn this work, we propose a data-driven CycleISP framework that is capable of converting sRGB images to RAW data and back to sRGB images. The CycleISP model allows us to synthesize realistic clean/noisy paired training data both in RAW and sRGB spaces. By training a novel network for the tasks of denoising the RAW and sRGB images, we achieve state-of-the-art performance on real noise benchmark datasets (DND <ref type="bibr" target="#b42">[44]</ref> and SIDD <ref type="bibr" target="#b0">[1]</ref>). Furthermore, we demonstrate that the CycleISP model can be applied to the color matching problem in stereoscopic cinema. Our future work includes exploring and extending the Cy-cleISP model for other low-level vision problems such as super-resolution and deblurring.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Our CycleISP models the camera imaging pipeline in both directions. It comprises two main branches: RGB2RAW and RAW2RGB. The RGB2RAW branch converts sRGB images to RAW measurements, whereas the RAW2RGB branch transforms RAW data to sRGB images. The auxiliary color correction branch provides explicit color attention to RAW2RGB network. The noise injection module is switched OFF while training the CycleISP (Section 3), and switched ON when synthesizing noise data (Section 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Recursive residual group (RRG) contains multiple dual attention blocks (DAB). Each DAB contains spatial attention and channel attention modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Fine-tuning CycleISP to synthesize realistic sRGB noise data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Scheme for color matching 3D pairs. (a) Target view. (PSNR) (b) Source view. 32.17 dB (c) Reinhard et al. [47]. 18.38 dB (d) Kotera [34]. 32.80 dB (e) Pitié et al. [43]. 33.38 dB (f) Ours. 36.60 dB Example of color correction for 3D cinema. Compare the colors of the ground and side of the car in zoomedin crops. Images are property of Mammoth HD Inc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>RAW denoising results on the SIDD dataset<ref type="bibr" target="#b0">[1]</ref>.</figDesc><table><row><cell></cell><cell>RAW</cell><cell></cell><cell>sRGB</cell><cell></cell></row><row><cell>Method</cell><cell>PSNR ↑</cell><cell>SSIM ↑</cell><cell>PSNR ↑</cell><cell>SSIM ↑</cell></row><row><cell>EPLL [67]</cell><cell>40.73</cell><cell>0.935</cell><cell>25.19</cell><cell>0.842</cell></row><row><cell>GLIDE [56]</cell><cell>41.87</cell><cell>0.949</cell><cell>25.98</cell><cell>0.816</cell></row><row><cell>TNRD [14]</cell><cell>42.77</cell><cell>0.945</cell><cell>26.99</cell><cell>0.744</cell></row><row><cell>FoE [49]</cell><cell>43.13</cell><cell>0.969</cell><cell>27.18</cell><cell>0.812</cell></row><row><cell>MLP [9]</cell><cell>43.17</cell><cell>0.965</cell><cell>27.52</cell><cell>0.788</cell></row><row><cell>KSVD [3]</cell><cell>43.26</cell><cell>0.969</cell><cell>27.41</cell><cell>0.832</cell></row><row><cell>DnCNN [64]</cell><cell>43.30</cell><cell>0.965</cell><cell>28.24</cell><cell>0.829</cell></row><row><cell>NLM [8]</cell><cell>44.06</cell><cell>0.971</cell><cell>29.39</cell><cell>0.846</cell></row><row><cell>WNNM [27]</cell><cell>44.85</cell><cell>0.975</cell><cell>29.54</cell><cell>0.888</cell></row><row><cell>BM3D [15]</cell><cell>45.52</cell><cell>0.980</cell><cell>30.95</cell><cell>0.863</cell></row><row><cell>Ours</cell><cell>52.41</cell><cell>0.993</cell><cell>39.47</cell><cell>0.918</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Denoising sRGB images of the DND benchmark dataset<ref type="bibr" target="#b42">[44]</ref>.</figDesc><table><row><cell>Method</cell><cell>EPLL</cell><cell>TNRD</cell><cell>NCSR</cell><cell>MLP</cell><cell>BM3D</cell><cell>FoE</cell><cell>WNNM</cell><cell>KSVD</cell><cell>MCWNNM</cell><cell>FFDNet+</cell><cell>TWSC</cell><cell>CBDNet</cell><cell>RIDNet</cell><cell>Ours</cell></row><row><cell></cell><cell>[67]</cell><cell>[14]</cell><cell>[18]</cell><cell>[9]</cell><cell>[15]</cell><cell>[49]</cell><cell>[27]</cell><cell>[3]</cell><cell>[60]</cell><cell>[65]</cell><cell>[59]</cell><cell>[28]</cell><cell>[4]</cell><cell></cell></row><row><cell>PSNR ↑</cell><cell>33.51</cell><cell>33.65</cell><cell>34.05</cell><cell>34.23</cell><cell>34.51</cell><cell>34.62</cell><cell>34.67</cell><cell>36.49</cell><cell>37.38</cell><cell>37.61</cell><cell>37.94</cell><cell>38.06</cell><cell>39.23</cell><cell>39.56</cell></row><row><cell>SSIM ↑</cell><cell>0.824</cell><cell>0.831</cell><cell>0.835</cell><cell>0.833</cell><cell>0.851</cell><cell>0.885</cell><cell>0.865</cell><cell>0.898</cell><cell>0.929</cell><cell>0.942</cell><cell>0.940</cell><cell>0.942</cell><cell>0.953</cell><cell>0.956</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Denoising sRGB images of the SIDD benchmark dataset<ref type="bibr" target="#b0">[1]</ref>. Denoising sRGB image from DND<ref type="bibr" target="#b42">[44]</ref>. Our method preserves better structural content than other algorithms.</figDesc><table><row><cell>Method</cell><cell cols="2">DnCNN</cell><cell>MLP</cell><cell>GLIDE</cell><cell>TNRD</cell><cell>FoE</cell><cell>BM3D</cell><cell>WNNM</cell><cell>NLM</cell><cell>KSVD</cell><cell>EPLL</cell><cell>CBDNet</cell><cell>RIDNet</cell><cell>Ours</cell></row><row><cell></cell><cell>[64]</cell><cell></cell><cell>[9]</cell><cell>[56]</cell><cell>[14]</cell><cell>[49]</cell><cell>[15]</cell><cell>[27]</cell><cell>[8]</cell><cell>[3]</cell><cell>[67]</cell><cell>[28]</cell><cell>[4]</cell></row><row><cell>PSNR ↑</cell><cell>23.66</cell><cell></cell><cell>24.71</cell><cell>24.71</cell><cell>24.73</cell><cell>25.58</cell><cell>25.65</cell><cell>25.78</cell><cell>26.76</cell><cell>26.88</cell><cell>27.11</cell><cell>30.78</cell><cell>38.71</cell><cell>39.52</cell></row><row><cell>SSIM ↑</cell><cell>0.583</cell><cell></cell><cell>0.641</cell><cell>0.774</cell><cell>0.643</cell><cell>0.792</cell><cell>0.685</cell><cell>0.809</cell><cell>0.699</cell><cell>0.842</cell><cell>0.870</cell><cell>0.754</cell><cell>0.914</cell><cell>0.957</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">26.90 dB</cell><cell cols="2">30.91 dB</cell><cell cols="2">32.47 dB</cell><cell cols="2">32.50 dB</cell><cell cols="2">32.74 dB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Noisy</cell><cell cols="2">BM3D [15]</cell><cell cols="2">NC [36]</cell><cell cols="4">TWSC [59] MCWNNM [60]</cell></row><row><cell></cell><cell cols="3">26.90 dB</cell><cell></cell><cell cols="2">33.05 dB</cell><cell cols="2">33.29 dB</cell><cell cols="2">33.62 dB</cell><cell cols="2">34.09 dB</cell><cell cols="2">34.32 dB</cell></row><row><cell></cell><cell cols="3">Noisy Image</cell><cell></cell><cell cols="6">FDDNet [65] DnCNN [64] CBDNet [28]</cell><cell cols="2">RIDNet [4]</cell><cell>Ours</cell></row><row><cell cols="4">Figure 6: 18.25 dB</cell><cell cols="2">19.70 dB</cell><cell>20.76 dB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Reference</cell><cell></cell><cell>Noisy</cell><cell cols="4">FFDNet [65] DnCNN [64]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">25.75 dB</cell><cell cols="2">28.84 dB</cell><cell cols="2">35.57 dB</cell><cell>36.75 dB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">BM3D [15] CBDNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Generalization Test. U-Net model is trained only for DND<ref type="bibr" target="#b42">[44]</ref> with our technique and with the UPI<ref type="bibr" target="#b6">[7]</ref> method, and directly evaluated on the SIDD dataset<ref type="bibr" target="#b0">[1]</ref>.</figDesc><table><row><cell></cell><cell cols="2">DND [44]</cell><cell cols="2">SIDD [1]</cell></row><row><cell>Method</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>UPI [7]</cell><cell>48.89</cell><cell>0.9824</cell><cell>49.17</cell><cell>0.9741</cell></row><row><cell>Ours</cell><cell>49.00</cell><cell>0.9827</cell><cell>50.14</cell><cell>0.9758</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study: RAW2RGB branch.</figDesc><table><row><cell>Short skip connections</cell><cell></cell></row><row><cell>Color correction branch</cell><cell></cell></row><row><cell>Channel Attention (CA)</cell><cell></cell></row><row><cell>Spatial attention (SA)</cell><cell></cell></row><row><cell>PSNR (in dB)</cell><cell>23.22 42.96 33.58 44.67 45.08 45.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Layout of SA and CA in DAB.</figDesc><table><row><cell>Layout</cell><cell>CA + SA</cell><cell>SA + CA</cell><cell>CA &amp; SA in parallel</cell></row><row><cell>PSNR (in dB)</cell><cell>45.17</cell><cell>45.16</cell><cell>45.41</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. Ming-Hsuan Yang is supported by the NSF CAREER Grant 149783.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on real image denoising: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">K-SVD: an algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real image denoising with feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Image Processing for Cinema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmío</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Denoising of Photographic Images and Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmío</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with BM3D?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input/output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Is denoising dead?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyam</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fundamental limits of image denoising: are we there yet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyam</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On learning optimized reaction diffusion processes for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transformdomain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">De-noising by soft-thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on information theory</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alexei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas K</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">HDR image reconstruction from a single exposure using deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyorgy</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rafał</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unger</surname></persName>
		</author>
		<idno>2017. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clipped noisy images: Heteroskedastic modeling and practical denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noise measurement for raw-data of digital imaging sensors by automatic segmentation of nonuniform targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakari</forename><surname>Alenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Practical poissonian-gaussian noise modeling and fitting for single-image raw-data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mejdi</forename><surname>Trimeche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep joint demosaicking and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Local denoising applied to raw images may outperform non-local patch-based methods applied to the camera output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Ghimpeţeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Batard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmío</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Imaging</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shi Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">New trends and ideas in visual concept detection: the MIR flickr retrieval evaluation initiative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MIR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A software platform for manipulating the camera imaging pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Hakki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Karaimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A guide to convolutional neural networks for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syed Afaq Ali</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="207" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A scene-referred color transfer for pleasant imaging on display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Kotera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Secrets of image denoising cuisine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The noise clinic: a blind image denoising algorithm. Image Processing On Line</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Natural image denoising: Optimality and inherent bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Nadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning raw image denoising with bayer pattern unification and bayer preserving augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimal inversion of the generalized anscombe transformation for poissongaussian noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markku</forename><surname>Makitalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">3D Movie Making: Stereoscopic Digital Cinema from Script to Screen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Mendiburu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Focal Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automated colour grading using colour distribution transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Pitié</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rozenn</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahyot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on CVIU</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural nearest neighbors networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Color image processing pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on Computer graphics and applications</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Progressive image deraining networks: a better and simpler baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Dongwei Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Nonlinear total variation based noise removal algorithms. Physica D: nonlinear phenomena</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Leonid I Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">DeepISP: Towards learning an end-to-end image processing pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Noise characteristics of a single sensor camera in digital color image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Özlem</forename><surname>Cakmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Keimel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Stechele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Noise removal via bayesian wavelet coring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward H Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SUSANa new approach to low level image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Global image denoising. TIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A trilateral weighted sparse coding scheme for real-world image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-channel weighted nuclear norm minimization for real color image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Local adaptive image restoration and enhancement with the use of DFT and DCT in a running window</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Leonid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yaroslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelet Applications in Signal and Image Processing IV</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning digital camera pipeline for extreme low-light imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05939</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multiscale single image dehazing using perceptual pyramid deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwanath</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">FFDNet: Toward a fast and flexible solution for CNN-based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
