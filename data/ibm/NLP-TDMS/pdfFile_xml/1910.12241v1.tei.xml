<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-train and Learn: Preserve Global Information for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danhao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jiangsu Police Institute Nanjing 210093</orgName>
								<address>
									<settlement>Jiangsu</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Jiangsu Police Institute</orgName>
								<address>
									<postCode>210031</postCode>
									<settlement>Nanjing, Jiangsu</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Jiangsu Police Institute</orgName>
								<address>
									<postCode>210031</postCode>
									<settlement>Nanjing, Jiangsu</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Jiangsu Police Institute</orgName>
								<address>
									<postCode>210031</postCode>
									<settlement>Nanjing, Jiangsu</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-train and Learn: Preserve Global Information for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have shown great power in learning on attributed graphs. However, it is still a challenge for GNNs to utilize information faraway from the source node. Moreover, general GNNs require graph attributes as input, so they cannot be appled to plain graphs. In the paper, we propose new models named G-GNNs (Global information for GNNs) to address the above limitations. First, the global structure and attribute features for each node are obtained via unsupervised pre-training, which preserve the global information associated to the node. Then, using the global features and the raw network attributes, we propose a parallel framework of GNNs to learn different aspects from these features. The proposed learning methods can be applied to both plain graphs and attributed graphs. Extensive experiments have shown that G-GNNs can outperform other state-of-theart models on three standard evaluation graphs. Specially, our methods establish new benchmark records on Cora (84.31%) and Pubmed (80.95%) when learning on attributed graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semi-supervised learning on graphs aims to recover the labels for all nodes, while only very small proportion of node labels are given. The setting is popular in various of real world applications, since many labels are often expensive and difficult to collect. In the recent years, Graph Neural Networks (GNNs) have shown great power in the semi-supervised learning on attributed graphs. GNN often contains multiple layers. The nodes collect information from the neighborhood iteratively, layer by layer. The representative methods include Graph convolutional network (GCN) <ref type="bibr" target="#b7">(Kipf and Welling 2016)</ref>, Graphsage <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec 2017)</ref>, Graph attention networks <ref type="bibr" target="#b14">(Veličković et al. 2017</ref>) and so on.</p><p>Due to the multi-layer message aggregation scheme, GNN is easy to be over-smoothing after a few propagation steps <ref type="bibr" target="#b9">(Li, Han, and Wu 2018)</ref>. That is to say, the node representations tend to be identical and lack of distinction. In general, GNNs can afford only 2 layers to collection information within 2 hops of neighbor-Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. hood, otherwise the over-smoothing problem will deteriorate the performance. Several previous works aimed to address the problem and expand the size of utilized neighborhood. One major line of works considered to improve the information aggregation scheme. For example, N-GCN <ref type="bibr" target="#b1">(Abu-El-Haija et al. 2018</ref>) trained multiple instances of GCNs over node pairs discovered at different distances in random walks. PPNP/APPNP <ref type="bibr" target="#b8">(Klicpera, Bojchevski, and Günnemann 2018)</ref> introduced the teleport probability of personalized PageRank.</p><p>These solutions are purely based on semi-supervised learning, which have their natural bottleneck in modeling global information. As the reception field is getting larger and more nodes are involved, more powerful models are required to explore the complex relationships among nodes. However, the labels for training is quite sparse in the semisupervised learning, and hence cannot afford to train models with high complexity well. Hence, the existing works often have to restrain the complexity of models, where limits their ability in learning global information.</p><p>The unsupervised learning methods based on random walk, e.g. Deepwalk <ref type="bibr" target="#b12">(Perozzi, Al-Rfou, and Skiena 2014)</ref> and Node2vec <ref type="bibr" target="#b5">(Grover and Leskovec 2016)</ref>, can be used to obtain the global structure information of each node. These methods first sample node sequences that contain the structure regularity of the network, then try to maximize the likelihood of neighborhood node pairs within certain distance, e.g. 10 hops. Hence, they can capture global structure features without label information.</p><p>In this paper, we propose a new learning schema of pretraining and learning to address the global information preserving problem in semi-supervised learning. First, instead of improving the aggregation function via semi-supervision, we obtain the global structure and attribute features by pretraining the graph with random walk strategy in the unsupervised learning. Second, we design a GNN based method to conduct semi-supervised classification by learning from the pre-trained features and the orginal graph. The two-stage schema of pretraining-and-learning has several advantages. First, the global information modeling procedure is decoupled with the subsequent semi-supervised learning method. Therefore, the modeling of the global information no longer suffers from the sparse supervision or over-smoothing problem. Second, the framework marries two lines of work and benefits from their advantages: the global information preserving ability brought by random walk and the local information aggregation ability induced by GNN. Moreover, the method allows GNN to be applied to plain graphs without attributes, since the unsupervised structure features can be used as graph attributes.</p><p>In all, our contributions are as follows.</p><p>• We propose a method named Global Information for Graph Neural Network (G-GNN) for semi-supervised classification on graphs. The proposed pretraining-andlearning framework allows GNN models to use global information for learning. Moreover, the schema enables GNN to be applied to plain graphs.</p><p>• We design the global information as the global structure and attribute features to each node, and propose a parallel GNN based model to learn different aspects from the pretrained global features and the original graph.</p><p>• Our method achieves state-of-the-art results in semisupervised learning on both plain and attributed graphs. Specially, our results of attributed graph learning on Cora (84.31%) and Pubmed (80.95%) are the new benchmark results.</p><p>The rest of the paper is organized as follows. In section 2, The preliminaries are given. We introduce our method in section 3. Section 4 presents the experiments. Section 5 briefly summarizes related work. Finally, we conclude our work in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definition</head><p>First, we will give the formal definition of attributed/plain graph, and the problem we are going to solve.</p><p>Let G = (V, E) be a graph, where V = {v 1 , v 2 ...v n } denotes the node set, E denotes the edges with the adjacency matrix A ∈ R n×n . If A ij is not equal to 0, there is a link from v i to v j with weight A ij . If G is an attributed graph, there is a corresponding attribute matrix X ∈ R n×f , where the ith row denotes v i 's attributes and f denotes the total amount of attributes. If G is a plain network, no X is provided. The graph contains label information Y ∈ R n×c , where the ith row denotes v i 's one hot label vector. The amount of labels is c.</p><p>During the training stage, the provided data is the entire adjacency matrix A and the node attributes X. Only labels of the training nodes V train ⊂ V are given. The task of the semi-supervised learning in attributed graph is to predict the rest of the node labels Y Vtrain . X is not provided for plain graph learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>We will introduce how general GNNs solve the semisupervised learning problem. Note that current GNNs can only be applied to attributed graphs. Therefore, we assume X is given here.</p><p>Among the huge family of GNNs, Graph convolutional network(GCN) <ref type="bibr" target="#b7">(Kipf and Welling 2016)</ref> is a simple and far-reaching method that motivates many of the following works. LetÂ = A + I n be the adjacency matrix with self-connections, where I n is an identity matrix. The selfloops will allow GCN to consider attributes of the represented nodes when aggregating the neigbhood's attributes.</p><p>LetÂ =D −1/2ÂD−1/2 be the nomalized adjacency matrix, whereD denotes the diagonal degree matrix wherê D ii = jÂ ij . The two-layer GCN produces the following hidden states by aggregating neighborhood attributes iteratively:</p><formula xml:id="formula_0">H GCN =ÂRelu(ÂXW 0 )W 1 (1) where H GCN ∈ R n×c .</formula><p>Each row in H GCN denotes the final hidden states of a node, and each row corresponds to a prediction catagory. W 0 and W 1 are the trainable weight matrices. After that, a softmax function is used to produce the classification probability on each class.</p><p>Z GCN = sof tmax(H GCN ) (2) Finally, a loss function is applied to measure the difference between the predict probability and the ground truth labels.</p><p>Many of the following studies aimed to improve the aggregation function, such as assigning alternative weights to the neighborhood nodes (Veličković et al. 2017), adding skip connections (Hamilton, Ying, and Leskovec 2017), introducing teleport probability <ref type="bibr" target="#b8">(Klicpera, Bojchevski, and Günnemann 2018)</ref> and so on. Abstract from the specific designs, these methods can be viewed as a transform from the original X and A to the final hidden states H:</p><formula xml:id="formula_1">GNN(X, A) : X, A → H</formula><p>(3) From Equation 1, we can see only 2-hops of local information can be used. The over-smoothing problem prevents to add more layers, so the global information is difficult to be integrated in. The input attributes X are necessary in the general learning framework of GNN. Hence, GNN cannot be applied to plain graphs directly. In the next section, we will show to solve these problems with the proposed pretrainingand-learning schema.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In the section, we first give an overview of G-GNN within the context of attributed graph. Second, the method to obtain the global features are introduced. Third, a parallel GNN based method is proposed to learn from all these features. Finally, we show how to extend G-GNN to plain network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The overview of the G-GNN method is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. First, the global structure feature matrix X (s) and attribute feature matrix X (a) are learned in an unsupervised way. Next, X (s) , X (a) and the original attribute matrix X are fed to a parallel GNN based model, to learn their corresponding hidden states. Finally, the final hidden states are the weighted sum of the 3 hidden states H (s) , X (a) and H (o) . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised Learning of Global Features</head><p>Herein, we propose to learn the unsupervised features of graphs based on random walk strategy. Each node can utilize information within k steps of random walk, where k is often set to 10. Small world phenomenon suggests that the average distance between nodes will grow logarithmically with the size of the graph nodes <ref type="bibr" target="#b2">(Albert, Jeong, and Barabási 1999)</ref>, and the undirected average distance of a very large Web graph is only 6.83 <ref type="bibr" target="#b3">(Broder et al. 2000)</ref>. Hence, 10 steps of random walk can already capture the global information of the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Global Structure Features</head><p>Similar to Deepwalk <ref type="bibr" target="#b12">(Perozzi, Al-Rfou, and Skiena 2014)</ref>, the structure features are learned by first sampling the context-source node pairs, and then maximizing their co-occurrence probability. Note that graph attributes X are not used here. We apply random walks to the graph G to obtain short truncated node sequences. These sequences contain the structure regularity of the original graph. In the node sequences, the neighborhood nodes within certain distance to the source node v are considered as v's context nodes, which are denoted as</p><formula xml:id="formula_2">N (v) ⊂ V .</formula><p>To maximize the likelihood of the observed sourcecontext pairs, we try to minimize the following objective function:</p><formula xml:id="formula_3">v∈V u∈N (v) exp(X (s) v · X (s) u ) k∈V exp(X (s) v · X (s) k )<label>(4)</label></formula><p>X (s) ∈ R n×ds denotes the global structure feature matrix, where the ith row denotes v i 's global structure feature vector and d s denotes the dimension of the vectors. The calculation of the denominator is computational expensive since it is required to traverse the entire node set. We approximate it using negative sampling <ref type="bibr" target="#b11">(Mikolov et al. 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Global Attribute Features</head><p>The global attribute features are obtained by maximizing the likelihood of the context attributes. The underlying idea is that if the context attributes can be recovered from the source node, it has already been preserved by the learning model. For each sampled context node u ∈ N (v), some attributes of u are sampled as the context attributes of v. In this paper, we sample one attribute for one context node. Let CA(v) be the sampled context attributes of v, and T be the set of all attributes where |T | = shape(X) <ref type="bibr">[1]</ref>. We try to minimize the following objective.</p><formula xml:id="formula_4">v∈V a∈CA(v) exp(X (a) v · S a ) k∈T exp(X (a) v · S k )</formula><p>where X (a) ∈ R n×da denotes the global attribute feature matrix, S ∈ R da×|T | denotes the parameters to predict the attributes. The ith row denotes v i 's global attribute feature vector and d a denotes the dimension of the vectors. <ref type="bibr" target="#b15">(Zhu et al. 2019)</ref> proposed an unsupervised graph learning method that utilize the context attributes. They learned the node representations by jointly optimizing two objective functions that preserved the neighborhood nodes and attributes. The mainly difference of our work is that we learn two feature vectors for each node separately, which provides richer information for the following learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parallel Graph Neural Networks</head><p>As is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we will propose a parallel model with kernels of GNN to learn from these input matrices of X (s) , X (a) and X. The learning is semi-supervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Learning from the Heterogeneous Features</head><p>The motivation of applying multiple parallel GNN kernels to these feature matrices is as follows. First, the features are quite heterogeneous, especially when some of them are obtained via pre-training. The parallel kernels can learn dif-ferent aspects from these features respectively. Second, the three feature matrices are highly correlated. For example, X (a) is obtained partly based on X. X (s) and X (a) are sampled based on the identical random walk method. It makes the model difficult to learn the complex relationships among them. The parallel setting can effectively give some implicit prior knowledge to the model that these features are different, which will make the optimization easier. Indeed, the parallel schema are successful in some previous works, such as multi-head attention <ref type="bibr" target="#b14">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b14">Veličković et al. 2017</ref>) and N-GCN <ref type="bibr" target="#b1">(Abu-El-Haija et al. 2018)</ref>.</p><p>First, because the amplitude at each dimension of the pretrained H (s) and H (a) often varies a lot, it is better to make a normalization. For each row h in H (s) or H (a) , we makes the following transformation:</p><formula xml:id="formula_5">h = h − mean(h) std(h)</formula><p>Then, several kernels of GNN are proposed to learn from the three feature matrices.</p><formula xml:id="formula_6">H (s) = GNN(X (s) , A)<label>(5)</label></formula><formula xml:id="formula_7">H (a) = GNN(X (a) , A)<label>(6)</label></formula><formula xml:id="formula_8">H (o) = GNN(X, A)<label>(7)</label></formula><p>The GNN( * ) is the learning kernel of the G-GNNs. Common GNN based model that fulfill Equation 3 can be used as the kernel, such as GCN <ref type="bibr" target="#b7">(Kipf and Welling 2016)</ref>, Graphsage (Hamilton, Ying, and Leskovec 2017), APPNP <ref type="bibr" target="#b8">(Klicpera, Bojchevski, and Günnemann 2018)</ref> and so on. Hence, G-GNNs can easily benefit from the strong learning capacities of these kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Combine the Hidden States</head><p>A simple way to obtain the final hidden state matrix is to linear combine the 3 obtained hidden state matrices, where α and β are coefficients between 0 and 1.</p><formula xml:id="formula_9">H = αH (s) + βH (a) + H (o)<label>(8)</label></formula><p>Then a softmax function is applied to H, as in Equation 2, to get the prediction probability matrix Z, where Z ij denotes the probability that node v i 's label is the class j. The coefficients of α and β are used to turn down the effect of the pre-trained features, which are essential for optimization. <ref type="bibr" target="#b2">Bengio et al.( 2007)</ref> suggested the pre-training is useful for initializing the network in a region of the parameter space where optimization is easier. In training, easy samples often contribute more to the loss and dominate the gradient updating <ref type="bibr" target="#b10">(Lin et al. 2017)</ref>. Similarly, we found the easytrained component of GNN(X (s) ) and GNN(X (a) ) also dominate the learning procedure. If no weight strategy is used, GNN(X) merely contribute to the results and hence the performance is far from promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Training</head><p>We minimize the cross-entropy loss function between Z and the ground-truth labels Y to train the model <ref type="bibr" target="#b1">(Abu-El-Haija et al. 2018)</ref>.</p><formula xml:id="formula_10">min diag(V train )[Y • log Z]</formula><p>where • denotes Hadamard product, and diag(V train ) denotes a diagonal matrix, with entry at (i, i) set to 1 if v i ∈ V train and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning on Plain Graphs</head><p>Plain Graphs contain no attributes X. It does not affect the obtaining of the global structure feature matrix X (s) . Then the final hidden states is as:</p><formula xml:id="formula_11">H = H (s) = GNN(X (s) , A)</formula><p>Hence, learning on plain graphs also follows the pretraining-and-learning schema, where some components that depend on the graph attributes are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we want to address the following questions 1 :</p><p>• Q1: How do G-GNNs perform in comparison to state-ofthe-art GNN kernels on attributed graphs? • Q2: Are all the designed components in G-GNN helpful for achieving stronger learning ability? • Q2: How does G-GNN perform in comparison to stateof-the-art learning methods on plain graphs?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on Attributed Graphs (Q1)</head><p>Herein, we will address the first question Q1 by comparing G-GNNs with different GNN kernels. Note that the code of all the GNN methods are based on the implementation released by DGL 2 . All results are the average value of 10 experiments with different random initialization seeds.  where the probability is 0.5. We set number of training epoches to 300, number of layers to 2, and the dimension of hidden states to 16. The self-loops are used.</p><p>• Graphsage (Hamilton, Ying, and Leskovec 2017): It is a general framework by sampling and aggregating features from a node's local neighborhood. We use the mean aggregate. We set the dropout rate to 0.5, the number of training epoches to 200, the number of layers to 2, and the dimension of hidden states to 16.</p><p>• APPNP <ref type="bibr" target="#b8">(Klicpera, Bojchevski, and Günnemann 2018)</ref>: This paper designs a new propagation procedure based on personalized PageRank, and hence can also model the long-distance information to a source node. We set the dropout rate to 0.5, the number of training epoches to 300, the number of propagation steps to 10, the teleport probability to 0.1 and the dimension of hidden states to 64.</p><p>All models are optimized with Adam (Kingma and Ba 2014) where the initial learning rate is 0.01 and the weight decay is 0.0005 per epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Training Details</head><p>In the unsupervised learning of global features, we conducted 10 iterations of random walk which started from each node. The walk length was 100. For each source node, the nearby nodes within 10 steps were considered as the neighborhood nodes. The dimensions of both the global structure and attribute vectors were 8. The number of negative sampling was 64.</p><p>In the semi-supervised learning, we used the three baseline models as kernels of G-GNNs, named G-GCN, G-Graphsage and so on. The parameters were exactly the same as those in the baseline methods.We search α and β between 0.001 to 0.05. The test results were reported when the best valid results were obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Results</head><p>The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. From the table, it is found that all baseline kernels with global information achieve substantial gains on the classification task. For example, G-GCN outperforms GCN with 2.24%, 0.26%, and 1.79% of precision on the three datasets respectively. The results demonstrate that the learning framework of G-GNNs can effectively and consistently enhance the learning ability of the corresponding GNN kernels.</p><p>APPNP is also designed for enlarging the reception field and can utilize global information. APPNP outperforms other baseline models. Although the improvement is not as large as those in G-GCN, G-APPNP still significantly outperforms its kernel of APPNP with 0.4%, 0.07% and 1.27% respectively. The result shows that even for a propagation method can powerful utilize global information, our learning schema of G-GNNs can still bring considerable precision gains. We believe the advantage comes from the pretrainingand-learning schema, since our global information are obtained via pre-training and no longer suffers from the limitation brought by weak supervision.</p><p>G-APPNP achieves the best results among all the methods. Note that its precisions on Cora (84.31%) and Pubmed (80.95%) are the new state-of-the-art results. To the best of our knowledge, the previous best results is GraphNAS In all, the results validate the effectiveness of the pretraining-and-learning schema, which can significantly improve the global information preserving ability of GNN based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Properties Analysis (Q2)</head><p>We want address the second question Q2. The parameter sensitivity and ablation analysis are given. Herein, we mainly use GCN as the kernel and the setting is attributed graph learning. <ref type="figure" target="#fig_2">Figure 2</ref> shows the precision w.r.t. α and β. Generally, different datasets require different α and β to achieve the best precision, and the value is often around 0.01. The precision will decrease quickly if we continue to increase the two parameters. The result shows that it is very necessary to introduce the two hyper-parameters to turn down the impact of pre-trained features. In fact, the component of GNN(X, A) will contribute almost nothing without the weight method. <ref type="figure">Figure 3</ref> shows the precision w.r.t. dimensions of the global features. The highest precision is achieved when the dimension is around 8 to 16. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Parameter Sensitivity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation Analysis</head><p>First, the effectiveness of parallel learning framework are investigated. Note that the simplest way to utilize all 3 feature matrices X (s) , X (a) and X is to concatenate them first, and then feed the concatenated feature matrix to a single GNN kernel. <ref type="table" target="#tab_2">Table 3</ref> compares the results with simple concatenation and our parallel method. We can find that the method of simple concatenation has already outperforms GCN, which demonstrates that the pretrainingand-learning schema can well utilize the global information. G-GCN makes further improvement than the method of simple concatenation, which validates the effectiveness of parallel learning method.</p><p>Second, we investigate the effect of the global features, as is shown in <ref type="table" target="#tab_3">Table 4</ref>. The results show that both pre-trained features can help to increase the model precision. The global  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Plain Graphs (Q3)</head><p>We will try to address the last question Q3 by comparing G-GNN with other plain graph learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Experiment Setup</head><p>We conducted the task of semisupervised classification on the two datasets of Cora and Citeseer. The results of the baseline methods are cited from their original papers. Pubmed is excluded from comparison since it is not used the baseline papers.</p><p>The training data was the entire plain graph(attributes X are excluded), and part of the node labels. We used 0.1, 0.2 ... 0.9 of node labels to train the model respectively, and reported the classification accuracy on the rest of data. All results were the average value of 10 experiments with different random split.</p><p>Our proposed model was G-GCN (plain), where the GCN was used as the kernel. In the unsupervised training, the dimension of global structure vectors was 32. In the semisupervised learning, the dimension of the hidden states was 256. No dropout was used. The rest of parameters were the same as those in 4.1.2.</p><p>Two state-of-the-art semi-supervised learning methods on plain graphs were used as baselines. Some other unsupervised or semi-supervised methods, such as Deepwalk <ref type="bibr" target="#b12">(Perozzi, Al-Rfou, and Skiena 2014)</ref>, Line <ref type="bibr" target="#b13">(Tang et al. 2015)</ref> and LSHM <ref type="bibr" target="#b6">(Jacob, Denoyer, and Gallinari 2014)</ref> are excluded from comparison since the baseline methods have demonstrated that they were outperformed by MMDW or PNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>The results are shown in <ref type="table" target="#tab_4">Table 5</ref>. G-GCN (plain) achieves the highest precision on half of the total data points (9 out of 18). Specially, the advantage is more obvious when the training ratio is arising. When the training ratio is small, the catagories with less instances may provide very small number of training instance, which makes GNN difficult to pass message from these nodes. We beileve this is the reason why G-GCN is less powerful when training ratio is small.</p><p>In all, the results show that the learning framework of G-GNN can be successfully applied to plain graphs, and achieve similar or better results than state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Several related works have tried to expand the reception field of GNN and increase the neighborhood available at each node. PPNP/APPNP <ref type="bibr" target="#b8">(Klicpera, Bojchevski, and Günnemann 2018)</ref> improved the message passing algorithms based on personalized PageRank. <ref type="bibr" target="#b14">(Xu et al. 2018)</ref> proposed jumping knowledge networks that can that flexibly leverage different neighborhood ranges for each node. N-GCN <ref type="bibr" target="#b1">(Abu-El-Haija et al. 2018</ref>) trained multiple instances of GCNs over node pairs discovered at different distances in random walks, and learned a combination of these instance outputs. However, because the semi-supervised settings lack of enough training, these methods have to control the model complexity carefully, which limits the learning ability in exploring the global information. For example, our experiments have shown that G-GNNs with kernel of APPNP can still achieve promising improvement.</p><p>Some studies also tried to introduce unsupervised learning in GNNs to alleviate the insufficient supervision problem. (Tran 2018) proposed a auto-encoder architecture that learned a joint representation of both local graph structure and available node features for the multi-task learning of link prediction and node classification. <ref type="bibr">GraphNAS(Gao et al. 2019</ref>) first generated variable-length strings that described the architectures of graph neural networks, and then maximized the expected accuracy of the generated architectures on a validation data based on reinforcement learning. However, these methods do not consider to utilize global information of the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In the paper, we propose a novel framework named G-GNN, which is able to conduct semi-supervised learning on both plain and attributed graphs. Our pretraining-and-learning schema marries two lines of work and benefits from their advantages: the global information preserving ability brought by unsupervised learning and the local information aggregation ability induced by GNN. Extensive experiments confirm the effectiveness of G-GNN.</p><p>For future work, we plan to test some more complicated methods that combine the hidden states, and study other unsupervised methods that can produce global features more suitable for the learning ability of GNN. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of the G-GNN method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(84.2%) (Gao et al. 2019) on Cora, and MixHop (80.8%) (Abu-El-Haija et al. ) on Pubmed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Precision w.r.t. α and β Figure 3: Precision w.r.t. Dimension of global features on Cora</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>MMDW (Tu et al. 2016): The method jointly optimized the max-margin classifier and the aimed social representation learning model. • PNE (Chen et al. 2017): The method embedded nodes and labels into the same latent space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="4">: The statistics of the datasets</cell></row><row><cell></cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell># Nodes</cell><cell>2708</cell><cell>3327</cell><cell>19717</cell></row><row><cell># Edges</cell><cell>5429</cell><cell>4732</cell><cell>44338</cell></row><row><cell># Attributes</cell><cell>1433</cell><cell>3703</cell><cell>500</cell></row><row><cell># Classes</cell><cell>7</cell><cell>6</cell><cell>3</cell></row><row><cell cols="2"># Training nodes 140</cell><cell>120</cell><cell>60</cell></row><row><cell># Valid nodes</cell><cell>500</cell><cell>500</cell><cell>500</cell></row><row><cell># Test nodes</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell></row><row><cell cols="4">4.1.1 Datasets and Baselines The statistics of the datasets</cell></row><row><cell cols="4">used in this study are shown in Table 1. The three stan-</cell></row><row><cell cols="4">dard citation graph benchmark datasets of Cora, Citeseer</cell></row><row><cell cols="4">and Pubmed (Sen et al. 2008) are widely used in various</cell></row><row><cell cols="4">of GNN studies (Veličković et al. 2017; Yang, Cohen, and</cell></row><row><cell cols="4">Salakhutdinov 2016). In the citation graphs, nodes denote</cell></row><row><cell cols="4">papers and links denote undirected citations. Node attributes</cell></row><row><cell cols="4">are the extracted elements of bag-of-words representation of</cell></row><row><cell cols="4">the documents. The class label is the research area of each</cell></row><row><cell>paper.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">The following baselines are compared in this paper.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">: Classification Precision (%) on Attributed Graphs</cell><cell></cell></row><row><cell></cell><cell>Cora</cell><cell></cell><cell cols="2">Citeseer</cell><cell cols="2">Pubmed</cell></row><row><cell></cell><cell cols="6">Precision Range Precision Range Precision Range</cell></row><row><cell>GCN</cell><cell>81.47</cell><cell>±2.6</cell><cell>71.01</cell><cell>±1.3</cell><cell>79.1</cell><cell>±1.1</cell></row><row><cell>G-GCN</cell><cell>83.71</cell><cell>±1.8</cell><cell>71.27</cell><cell>±1.4</cell><cell>80.88</cell><cell>±0.95</cell></row><row><cell>Graphsage</cell><cell>82.7</cell><cell>±1.65</cell><cell>69.87</cell><cell>±1.3</cell><cell>78.56</cell><cell>±0.65</cell></row><row><cell>G-Graphsage</cell><cell>83.84</cell><cell>±1.1</cell><cell>70.2</cell><cell>±1.15</cell><cell>78.89</cell><cell>±1.85</cell></row><row><cell>APPNP</cell><cell>83.91</cell><cell>±1.45</cell><cell>71.93</cell><cell>±1.55</cell><cell>79.68</cell><cell>±0.65</cell></row><row><cell>G-APPNP</cell><cell>84.31</cell><cell>±1.3</cell><cell>72</cell><cell>±1.45</cell><cell>80.95</cell><cell>±0.8</cell></row><row><cell cols="3">• Graph convolutional network(GCN) (Kipf and Welling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">2016): It is a simple type of GNN which have already been</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">introduced in details in section 2. We use dropout tech-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">nique to prevent from overfitting (Srivastava et al. 2014),</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">: The effect of parallel learning</cell><cell></cell></row><row><cell></cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell>GCN</cell><cell>81.47</cell><cell>71.01</cell><cell>79.1</cell></row><row><cell cols="2">Simple Concatenation 83.39</cell><cell>70.78</cell><cell>80.64</cell></row><row><cell>G-GCN</cell><cell>83.71</cell><cell>71.27</cell><cell>80.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The effect of the global structure and attribute features. The model is G-GCN. Each line gives the results of G-GCN with the corresponding feature matrices. Hence, the model in the first line is equivalent to GCN, and the last line is corresponding to the full G-GCN model.</figDesc><table><row><cell></cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell>X</cell><cell>81.47</cell><cell>71.01</cell><cell>79.1</cell></row><row><cell>X + X (s)</cell><cell>83.2</cell><cell>71.05</cell><cell>79.35</cell></row><row><cell>X + X (a)</cell><cell>82.77</cell><cell>71.27</cell><cell>80.73</cell></row><row><cell cols="2">X + X (s) + X (a) 83.71</cell><cell>71.27</cell><cell>80.88</cell></row><row><cell cols="4">structure features are more helpful on Cora, but less effective</cell></row><row><cell cols="4">than global attribute features on Pubmed and Citeseer. On</cell></row><row><cell cols="4">the dataset of Cora and Pubmed, the highest precisions are</cell></row><row><cell cols="4">obtained when all the features are used. However, in Cite-</cell></row><row><cell cols="4">seer, the global structure features cannot help to increase the</cell></row><row><cell cols="4">performance if the global attribute features are used. In all,</cell></row><row><cell cols="4">both pre-trained features can contribute to improve the re-</cell></row><row><cell cols="4">sults of our proposed model. However, the amount of im-</cell></row><row><cell cols="3">provement depends on specific datasets.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Classification Results (%) on Plain Graphs. The results of PNE and MWNE are cited from their original papers. 81.22 82.94 84.54 84.73 85.55 86.15 86.39 87.76 MWNE 74.94 80.83 82.83 83.68 84.71 85.51 87.01 87.27 88.19 G-GCN (Plain) 76.88 80.5 82.65 85.06 85.57 86.23 87.67 87.05 89.16 Citeseer PNE 54.79 60.87 64.67 66.95 68.59 70.00 72.06 73.41 74.76 MWNE 55.60 60.97 63.18 65.08 66.93 69.52 70.47 70.87 70.95 G-GCN (Plain) 54.24 60.31 64.16 66.41 69.36 70.77 72.12 74.41 75.89</figDesc><table><row><cell></cell><cell></cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell>Cora</cell><cell>PNE</cell><cell>77.58</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The data, code and pre-trained vectors to reproduce our results will be released on https://github.com/zhudanhao/g-gnn 2 https://github.com/dmlc/dgl/tree/master/examples/pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abu-El-Haija</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00067</idno>
		<title level="m">Higher-order graph convolution architectures via sparsified neighborhood mixing</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">N-gcn: Multi-scale graph convolution for semi-supervised node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>El-Haija</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08888</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
	<note>Internet: Diameter of the worldwide web</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph structure in the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer networks</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1-6</biblScope>
			<biblScope unit="page" from="309" to="320" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pne: label embedding enhanced network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09981</idno>
	</analytic>
	<monogr>
		<title level="m">Graphnas: Graph neural architecture search with reinforcement learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="547" to="560" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Pacific-Asia Conference on Knowledge Discovery and Data Mining</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>Leskovec; Hamilton, Ying, and Leskovec</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning latent representations of nodes for classifying in heterogeneous social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denoyer</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>and Welling</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predict then propagate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojchevski</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<idno>arXiv:1810.05997</idno>
	</analytic>
	<monogr>
		<title level="m">Semi-supervised classification with graph convolutional networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph neural networks meet personalized pagerank</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Rfou</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
	<note>AI magazine</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Max-margin deepwalk: discriminative learning of network representation</title>
	</analytic>
	<monogr>
		<title level="m">5th IEEE International Conference on Data Science and Advanced Analytics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International Joint Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<idno>arXiv:1806.03536</idno>
	</analytic>
	<monogr>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pcane: Preserving context attributes for network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cohen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="156" to="168" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Revisiting semisupervised learning with graph embeddings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
