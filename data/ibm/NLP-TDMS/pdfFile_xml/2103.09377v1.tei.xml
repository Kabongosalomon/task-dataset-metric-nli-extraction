<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Prize Lottery Ticket Hypothesis: FINDING ACCURATE BINARY NEURAL NETWORKS BY PRUNING A RANDOMLY WEIGHTED NETWORK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Diffenderfer</surname></persName>
							<email>diffenderfer2@llnl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Applied Scientific Computing</orgName>
								<orgName type="institution">Lawrence Livermore National Laboratory Livermore</orgName>
								<address>
									<postCode>94550</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
							<email>kailkhura1@llnl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Applied Scientific Computing</orgName>
								<orgName type="institution">Lawrence Livermore National Laboratory Livermore</orgName>
								<address>
									<postCode>94550</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Prize Lottery Ticket Hypothesis: FINDING ACCURATE BINARY NEURAL NETWORKS BY PRUNING A RANDOMLY WEIGHTED NETWORK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, <ref type="bibr" target="#b8">Frankle &amp; Carbin (2019)</ref> demonstrated that randomly-initialized dense networks contain subnetworks that once found can be trained to reach test accuracy comparable to the trained dense network. However, finding these high performing trainable subnetworks is expensive, requiring iterative process of training and pruning weights. In this paper, we propose (and prove) a stronger Multi-Prize Lottery Ticket Hypothesis: A sufficiently over-parameterized neural network with random weights contains several subnetworks (winning tickets) that (a) have comparable accuracy to a dense target network with learned weights (prize 1), (b) do not require any further training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of quantization (i.e., binary weights and/or activation) (prize 3). This provides a new paradigm for learning compact yet highly accurate binary neural networks simply by pruning and quantizing randomly weighted full precision neural networks. We also propose an algorithm for finding multi-prize tickets (MPTs) and test it by performing a series of experiments on CIFAR-10 and ImageNet datasets. Empirical results indicate that as models grow deeper and wider, multi-prize tickets start to reach similar (and sometimes even higher) test accuracy compared to their significantly larger and full-precision counterparts that have been weight-trained. Without ever updating the weight values, our MPTs-1/32 not only set new binary weight network state-of-the-art (SOTA) Top-1 accuracy -94.8% on CIFAR-10 and 74.03% on ImageNet -but also outperform their full-precision counterparts by 1.78% and 0.76%, respectively. Further, our MPT-1/1 achieves SOTA Top-1 accuracy (91.9%) for binary neural networks on CIFAR-10. Code and pre-trained models are available at:</p><p>Published as a conference paper at ICLR 2021 Figure 1: Multi-Prize Ticket Performance: Multi-prize tickets, obtained only by pruning and binarizing random networks, outperforms trained full precision and SOTA binary weight networks.</p><p>Although pruning and quantization 1 are typical approaches used for compressing DNNs (Neill, 2020), it is not clear under what conditions and to what extent compression can be achieved without sacrificing the accuracy. The most extreme form of quanitization is binarization, where weights and/or activations can only have two possible values, namely −1(0) or +1 (the interest of this paper). In addition to saving memory, binarization results in more power efficient networks with significant computation acceleration since expensive multiply-accumulate operations (MACs) can be replaced by cheap XNOR and bit-counting operations <ref type="bibr" target="#b35">(Qin et al., 2020a)</ref>. In light of these benefits, it is of interest to question if conditions exists such that a binarized DNN can be pruned to achieve accuracy comparable to the dense FP DNN. More importantly, even if these favourable conditions are met then how do we find these extremely compressed (or compact) and highly accurate subnetworks?</p><p>Traditional pruning schemes have shown that a pretrained DNN can be pruned without a significant loss in the performance. Recently, (Frankle &amp; Carbin, 2019) made a breakthrough by showing that dense network contain sparse subnetworks that can match the performance of the original network when trained from scratch with weights being reset to their initialization (Lottery Ticket Hypothesis). Although the original approach to find these subnetworks still required training the dense network, some efforts <ref type="bibr" target="#b44">(Wang et al., 2020b;</ref><ref type="bibr" target="#b50">You et al., 2019;</ref><ref type="bibr" target="#b43">Wang et al., 2020a)</ref> have been carried out to overcome this limitation. Recently a more intriguing phenomenon has been reported -a dense network with random initialization contains subnetworks that achieve high accuracy, without any further training <ref type="bibr" target="#b53">(Zhou et al., 2019;</ref><ref type="bibr" target="#b37">Ramanujan et al., 2020;</ref><ref type="bibr" target="#b29">Malach et al., 2020;</ref><ref type="bibr" target="#b33">Orseau et al., 2020)</ref>. These trends highlight good progress being made towards efficiently and accurately pruning DNNs.</p><p>In contrast to these positive developments for pruning, results on binarizing DNNs have been mostly negative. To the best of our knowledge, post-training schemes have not been successful in binarizing pretrained models without retraining. Even with training binary neural networks (BNNs) from scratch (though inefficient), the community has not been able to make BNNs achieve comparable results to their full precision counterparts. The main reason being that network structures and weight optimization techniques are predominantly developed for full precision DNNs and may not be suitable for training BNNs. Thus, closing the gap in accuracy between the full precision and the binarized version may require a paradigm shift. Furthermore, this also makes one wonder if efficiently and accurately binarizing DNNs similar to the recent trends in pruning is ever feasible.</p><p>In this paper, we show that a randomly initialized dense network contains extremely sparse binary subnetworks that without any weight training (i.e., efficient) have comparable performance to their trained dense and full-precision counterparts (i.e., accurate). Based on this, we state our hypothesis:</p><p>Multi-Prize Lottery Ticket Hypothesis. A sufficiently over-parameterized neural network with random weights contains several subnetworks (winning tickets) that (a) have comparable accuracy to a dense target network with learned weights (prize 1), (b) do not require any further training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of quantization (i.e., binary weights and/or activation) (prize 3).</p><p>Contributions. First, we propose the multi-prize lottery ticket hypothesis as a new perspective on finding neural networks with drastically reduced memory size, much faster test-time inference and</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning (DL) has made a significant breakthroughs in a wide range of applications <ref type="bibr" target="#b12">(Goodfellow et al., 2016)</ref>. These performance improvements can be attributed to the significant growth in the model size and the availability of massive computational resources to train such models. Therefore, these gains have come at the cost of large memory consumption, high inference time, and increased power consumption. This not only limits the potential applications where DL can make an impact but also have some serious consequences, such as, (a) generating huge carbon footprint, and (b) creating roadblocks to the democratization of AI. Note that significant parameter redundancy and a large number of floating-point operations are key factors incurring the these costs. Thus, for discarding the redundancy from DNNs, one can either (a) Prune: remove non-essential connections from an existing dense network, or (b) Quantize: constrain the full-precision (FP) weight and activation values to a set of discrete values which allows them to be represented using fewer bits. Further, one can exploit the complementary nature of pruning and quantization to combine their strengths. lower power consumption compared to their dense and full-precision counterparts. Next, we provide theoretical evidence of the existence of highly accurate binary subnetworks within a randomly weighted DNN (i.e., proving the multi-prize lottery ticket hypothesis). Specifically, we mathematically prove that we can find an ε-approximation of a fully-connected ReLU DNN with width n and depth using a sparse binary-weight DNN of sufficient width. Our proof indicates that this can be accomplished by pruning and binarizing the weights of a randomly weighted neural network that is a factor O(n 3/2 /ε) wider and 2 deeper. To the best of our knowledge, this is the first theoretical work proving the existence of highly accurate binary subnetworks within a sufficiently overparameterized randomly initialized neural network. Finally, we provide biprop (binarize-prune optimizer) in Algorithm 1 to identify MPTs within randomly weighted DNNs and empirically test our hypothesis. This provides a completely new way to learn BNNs without relying on weight-optimization.</p><p>Results. We explore two variants of multi-prize tickets -one with binary weights (MPT-1/32) and other with binary weights and activation (MPT-1/1) where x/y denotes x and y bits to represent weights and activation, respectively. MPTs we find have 60 − 80% fewer parameters than the original network. We perform a series of experiments on on small and large scale datasets for image recognition, namely CIFAR-10 <ref type="bibr" target="#b21">(Krizhevsky et al., 2009)</ref> and ImageNet <ref type="bibr" target="#b7">(Deng et al., 2009</ref>). On CIFAR-10, we test the performance of multi-prize tickets against the trend of making the model deeper and wider. We found that as models grow deeper and wider, both variants of multi-prize tickets start to reach similar (and sometimes even higher) test accuracy compared to the dense and full precision original network with learned weights. In other words, the performance of multiprize tickets improves with the amount of redundancy in the original network. We also carry out experiments with state-of-the-art (SOTA) architectures on CIFAR-10 and ImageNet datasets with an aim to investigate their redundancy. We find that within most randomly weighted SOTA DNNs reside extremely compact (i.e., sparse and binary) subnetworks which are smaller than, but match the performance of trained target dense and full precision networks. Furthermore, with minimal hyperparameter tuning, our MPTs achieve Top-1 accuracy comparable to (or higher than) SOTA BNNs. The performance of MPTs is further improved by allowing the parameters in BatchNorm layer to be learned. Finally, on both CIFAR-10 and ImageNet, MPT-1/32 subnetworks outperform their significantly larger and full-precision counterparts that have been weight-trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MULTI-PRIZE LOTTERY TICKETS: THEORY AND ALGORITHMS</head><p>We first prove the existence of MPTs in an overparameterized randomly weighted DNN. For ease of presentation, we state an informal version of Theorem 2 which can be found in Appendix B. We then explore two variants of tickets (MPT-1/32 and MPT-1/1) and provide an algorithm to find them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PROVING THE MULTI-PRIZE LOTTERY TICKETS HYPOTHESIS</head><p>In this section we seek to answer the following question: What is the required amount of overparameterization such that a randomly weighted neural network can be compressed to a sparse binary subnetwork that approximates a dense trained target network? Theorem 1. (Informal Statement of Theorem 2) Let ε, δ &gt; 0. For every fully-connected (FC) target network with ReLU activations of depth and width n with bounded weights, a random binary FC network with ReLU activations of depth 2 and width O ( n 3/2 /ε) + n log( n/δ) contains with probability (1 − δ) a binary subnetwork that approximates the target network with error at most ε.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sketch of Proof. Consider a FC ReLU network F</head><formula xml:id="formula_0">(x) = W ( ) σ(W ( −1) · · · σ(W (1) x)), where σ(x) = max{0, x}, x ∈ R d , W (i) ∈ R ki×ki−1 , k 0 = d, and i ∈ [ ].</formula><p>Additionally, consider a FC network with binary weights given by G(</p><formula xml:id="formula_1">x) = B ( ) σ(B ( −1) · · · σ(B (1) x)), where B (i) ∈ {−1, +1} k i ×k i−1 , k 0 = d, and i ∈ [ ].</formula><p>Our goal is to determine a lower bound on the depth, , and the widths, {k i } i=1 , such that with probability (1 − δ) the network G(x) contains a subnetwork G(x) satisfying G (x) − F (x) ≤ ε, for any ε &gt; 0 and δ ∈ (0, 1). We first establish lower bounds on the width of a network of the form g(x) = B (2) σ(B (1) x) such that with probability (1 − δ ) there exists a subnetworkg(x) of g(x) s.t. g(x) − σ(W x) ≤ ε , for any ε &gt; 0 and δ ∈ (0, 1). This process is carried out in detail in Lemmas 1, 2, and 3 in Appendix B. We have now approximated a single layer FC real-valued network using a subnetwork of a two-layer FC binary network. Hence, we can take = 2 and Lemma 3 provides lower bounds on the width of each intermediate layer such that with probability (1 − δ) there exists a subnetworkG(x) of G(x) satisfying G (x) − F (x) ≤ ε. This is accomplished in Theorem 2 in Appendix B.</p><p>To the best of our knowledge this is the first theoretical result proving that a sparse binary-weight DNN that can approximate a real-valued target DNN. As it has been established that real-valued DNNs are universal approximators <ref type="bibr" target="#b39">(Scarselli &amp; Tsoi, 1998)</ref>, our result carries the implication that sparse binary-weight DNNs are also universal approximators. In relation to the first result establishing the existence of real-valued subnetworks in a randomly weighted DNN approximating a realvalued target DNN <ref type="bibr" target="#b29">(Malach et al., 2020)</ref>, the lower bound on the width established in Theorem 2 is better than their lower bound of O 2 n 2 log( n/δ)/ε 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">FINDING MULTI-PRIZE WINNING TICKETS</head><p>Given the existence of multi-prize winning tickets from Theorem 2, a natural question arises -How should we find them? In this section, we answer this question by introducing an algorithm for finding multi-prize tickets. 2 Specifically, we explore two variants of multi-prize tickets in this paper -1) MPT-1/32 where weights are quantized to 1-bit with activations being real valued (i.e., 32-bits) and 2) MPT-1/1 where both weights and activations are quantized to 1-bit. We first outline a generic process for identifying MPTs along with some theoretical motivation for our approach.</p><p>Given a neural network g(x; W ) with weights W ∈ R m , we can express a subnetwork of g using a binary mask M ∈ {0, 1} m as g(x; M W ), where denotes the Hadamard product. Hence, a binary subnetwork can be expressed as g(x; M B), where B ∈ {−1, +1} m . Lemma 1 in Appendix B indicates that rescaling the binary weights to {−α, α} using a gain term α ∈ R is necessary to achieve good performance of the resulting subnetwork. We note that the use of gain terms is common in binary neural networks <ref type="bibr" target="#b35">(Qin et al., 2020a;</ref><ref type="bibr" target="#b30">Martinez et al., 2020;</ref><ref type="bibr" target="#b2">Bulat &amp; Tzimiropoulos, 2019)</ref>. Combining all this allows us to represent a binary subnetwork as g(x; α(M B)). Now we focus on how to update M , B, and α. Suppose f (x; W * ) is a target network with optimized weights W * that we wish to approximate. Assuming g(x; ·) is κ-Lipschitz continuous yields</p><formula xml:id="formula_2">g (x; α(M B)) − f (x; W * ) MPT error ≤ κ M (W − αB) Binarization error + g(x; M W ) − f (x; W * ) Subnetwork error . (1)</formula><p>Hence, the MPT error is bounded above by the error of the subnetwork of g with the original weights and the error from binarizing the current subnetwork. This informs our approach for identifying MPTs: 1) Update a pruning mask M that reduces the subnetwork error (lines 7 -9 in Algorithm 1), and 2) apply binarization with a gain term that minimizes the binarization error (lines 4 and 10).</p><p>We first discuss how to update M . While we could search for M by minimizing the subnetwork error in (1), this would require the use of a pretrained target network (i.e., f (x; W * )). To avoid requiring a target network in our method we instead aim to minimize the training loss w.r.t. M in the current binary subnetwork. Directly optimizing over the pruning mask is a combinatorial problem. So to update the pruning mask efficiently we optimize over a set of scores S ∈ R m corresponding to each randomly initialized weight in the network. In this approach, each component of the randomly initialized weights is assigned a pruning score. The pruning scores are updated via backpropagation by computing the gradient of the loss function over minibatches with respect to the pruning scores (line 7). Then the magnitude of the scores in absolute value are used to identify the P percent of weights in each layer that are least important to the success of the binary subnetwork (line 8). The components of the pruning mask corresponding to these indices are set to 0 and the remaining components are set to 1 (line 9). To avoid unintentionally pruning an entire layer of the network, we use a pruning mask for each layer that prunes P percent of the weights in that layer. The choice to use pruning scores to update the mask M was due to the fact that it is computationally efficient. The use of pruning scores is a well-established optimization technique used in a range of applications <ref type="bibr" target="#b19">(Joshi &amp; Boyd, 2009;</ref><ref type="bibr" target="#b37">Ramanujan et al., 2020)</ref>.</p><p>Algorithm 1 biprop: Finding multi-prize tickets in a randomly weighted neural network 1: Input: Neural network g(x; ·) with 1-or 32-bit activations; Network depth ; Layer widths {k j } j=1 ; Loss function L; Training data {(x (i) , y (i) )} N i=1 ; Pruning percentage P . 2: Randomly Initialize FP Parameters: Network weights {W (j) } j=1 ; Pruning scores {S (j) } j=1 . 3: Initialize Layerwise Pruning Masks: {M (j) } j=1 each to 1. 4: Initialize Binary Subnetwork Weights:</p><formula xml:id="formula_3">{B (j) } j=1 ← {sign(W (j) )} j=1 . 5: Initialize Layerwise Gain Terms: {α (j) } j=1 ← { M (j) W (j) 1 / M (j) 1 } j=1 . 6: for k = 1 to N epochs do 7: S (j) ← S (j) − η∇ S (j) L({α (j) (M (j) B (j) )} j=1 )</formula><p>Update pruning scores at layer j 8:</p><formula xml:id="formula_4">{τ (i)} kj i=1 ← Sorting of indices {i} kj i=1 s.t. |S (j) τ (i) | ≤ |S (j) τ (i+1) | Index sort over values |S (j) | 9: M (j) i ← 1 {τ (i)≥ kj P/100 } (i)</formula><p>Update pruning mask at layer j 10:</p><formula xml:id="formula_5">α (j) ← M (j) W (j) 1 / M (j) 1</formula><p>Update gain term at layer j 11: Output:</p><formula xml:id="formula_6">Return Binarized Subnetwork g(x; {α (j) (M (j) B (j) )} j=1 ).</formula><p>We now consider how to update B and α. By keeping M fixed, we can derive the following closed form expressions that minimize the binarization error in (1): B * = sign(W ) and α * = M W 1 / M 1 . These closed form expressions indicate that only the gain term needs to be recomputed after each update to M . Hence, B = sign(W ) throughout our entire approach (line 4). We update a gain term for each layer of the subnetwork in our approach based on the formula for α * (line 10). More details on the derivation of B * and α * are provided in Appendix C.</p><p>Pseudocode for our method biprop (binarize-prune optimizer) is provided in Algorithm 1 and crossentropy loss is used in our experiments. Note that the process for identifying MPT-1/32 and MPT-1/1 differs only in computation of the gradient. Next, we explain how these gradients can be computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">UPDATING PRUNING SCORES FOR BINARY-WEIGHT TICKETS (MPT-1/32)</head><p>As an example, for a FC network where the state at each layer is defined recursively by</p><formula xml:id="formula_7">U (1) = α (1) (B (1) M (1) )x and U (j) = α (j) (B (j) M (j) )σ(U (j−1) ) we have ∂L ∂S (j) p,q = ∂L ∂U (j) q ∂U (j) q ∂M (j) p,q ∂M (j) p,q ∂S (j) p,q</formula><p>. We use the straight-through estimator <ref type="bibr" target="#b1">(Bengio et al., 2013)</ref> for</p><formula xml:id="formula_8">∂M (j) p,q ∂S (j) p,q which yields ∂L ∂S (j) p,q = ∂L ∂U (j) q α (j) B (j) p,q σ U (j−1) p , where ∂L ∂U (j) q</formula><p>is computed via backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">UPDATING PRUNING SCORES FOR BINARY-ACTIVATION TICKETS (MPT-1/1)</head><p>Note that MPT-1/1 uses the sign activation function. From Section 2.2.1, it immediately follows that</p><formula xml:id="formula_9">∂L ∂S (j) p,q = ∂L ∂U (j) q α (j) B (j) p,q sign U (j−1) p . However, updating ∂L ∂U (j) q</formula><p>via backpropagation requires a gradient estimator for the sign activation function. To motivate our choice of estimator note that we can approximate the sign function using a quadratic spline parameterized by some t &gt; 0:</p><formula xml:id="formula_10">s t (x) =      −1 : x &lt; −t q 1 (x) : x ∈ [−t, 0) q 2 (x) : x ∈ [0, t) 1 : x ≥ t .<label>(2)</label></formula><p>In (2), q i (x) = a i x 2 + b i x + c i and suitable values for the coefficients are derived using the following zero-and first-order constraints: q 1 (−t) = −1, q 1 (0) = 0, q 2 (0) = 0, q 2 (t) = 1, q 1 (−t) = 0, q 1 (0) = q 2 (0), and q 2 (t) = 0. This yields q 1 (x) = (x/t) 2 + 2(x/t) and q 2 (x) = −(x/t) 2 + 2(x/t). As s t (x) approximates sign(x), we can use s t (x) as our gradient estimator. Since q 1 (x) = 2 t (1 + x t ) and q 2 (x) = 2 t (1 − x t ) it follows that s t (x) = as torch.clamp(2 * (1-torch.abs(x)/t)/t,min=0.0). We note that lim t→0 s t (x) = sign(x), which suggests that smaller values of t yield more suitable approximations. Our experiments use s 1 (x) as the gradient estimator since we found it to work well in practice. Finally, we note that taking t = 1 in our gradient estimator yields the same value as the gradient estimator in <ref type="bibr" target="#b27">(Liu et al., 2018a)</ref>, however, our implementation in PyTorch is 6× more memory efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL RESULTS</head><p>The primary goal of the experiments in Section 3.1 is to empirically verify our Multi-Prize Lottery Ticket Hypothesis. As a secondary objective, we would like to determine tunable factors that make randomly-initialized networks amenable to containing readily identifiable Multi-Prize Tickets (MPTs). Thus, we test our hypothesis against the general trend of increasing the model size (depth and width) and monitor the accuracy of the identified MPTs. After verifying our Multi-Prize Lottery Ticket Hypothesis, we consider the performance of MPTs compared to state-of-the-arts in binary neural networks and their dense counterparts on CIFAR-10 and ImageNet datasets in Section 3.2. Building upon edge-popup (Ramanujan et al., 2020), we implement Algorithm 1 to identify MPTs. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">WHERE CAN WE EXPECT TO FIND MULTI-PRIZE TICKETS?</head><p>In this section, we empirically test the effect of overparameterization on the performance of MPTs. We overparameterize networks by making them (a) deeper (Sec. 3.1.1) and (b) wider (Sec. 3.1.2). We use VGG <ref type="bibr" target="#b41">(Simonyan &amp; Zisserman, 2014)</ref> variants as our network architectures for searching for MPTs. In each randomly weighted network, we find winning tickets MPT-1/32 and MPT-1/1 for different pruning rates using Algorithm 1. We choose our baselines as dense full-precision models with learned weights. In all experiments, we use three independent initializations and report the average of Top-1 accuracy with with error bars extending to the lowest and highest Top-1 accuracy. Additional experiment configuration details are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">DO WINNING TICKETS EXIST IN DEEP NETWORKS?</head><p>In this experiment, we empirically test the following hypothesis: As a network grows deeper, the performance of multi-prize tickets in the randomly initialized network will approach the performance of the same network with learned weights. We are further interested in exploring the required network depth for our hypothesis to be true.</p><p>In <ref type="figure" target="#fig_0">Figure 2</ref>, we vary the depth of VGG architectures (d = 2 to 8) and compare the Top-1 accuracy of MPTs (at different pruning rates) with weight-trained dense network. We notice that there exist a range of pruning rates where the performance of MPTs are very similar, and beyond this range the performance drops quickly. Interestingly, as the network depth increases, more parameters can be pruned without hurting the performance of MPTs. For example, MPT-1/32 can match the performance of trained Conv-8 while having only ∼ 20% of its parameter count. Interestingly, the performance gap between MPT-1/32 and MPT-1/1 does not change much with depth across different pruning rates. We further note that the performance of MPTs improve when increasing the depth and both start to approach the performance of the dense model with learned weights. This gain starts to plateau beyond a certain depth, suggesting that the MPTs might be approaching the limit of their achievable accuracy. Surprisingly, MPT-1/32 performs equally good (or better) than the weight-trained model regardless of having 50 − 80% lesser parameters and weights being binarized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">DO WINNING TICKETS EXIST IN WIDE NETWORKS?</head><p>Figure 4: Effect of Varying Width on MPT-1/1: Comparing the Top-1 accuracy of sparse and binary MPT-1/1 to dense, full-precision, and weight-optimized network on CIFAR-10.</p><p>Similar to the previous experiment, in this experiment, we empirically test the following hypothesis:</p><p>As a network grows wider, the performance of multi-prize tickets in the randomly initialized network will approach the performance of the same network with learned weights. We are further interested in exploring the required layer width for our hypothesis to be true.</p><p>In <ref type="figure" target="#fig_1">Figures 3 and 4</ref>, we vary the width of different VGG architectures and compare the Top-1 accuracy of MPT-1/32 and MPT-1/1 tickets (at different pruning rates) with weight-trained dense network. A width multiplier of value 1 corresponds to the models in <ref type="figure" target="#fig_0">Figure 2</ref>. Performance of all the models improves when increasing the width and the performance of both MPT-1/32 and MPT-1/1 start to approach the performance of the dense model with learned weights. Although, this gain starts to plateau beyond a certain width. For both MPT-1/32 and MPT-1/1, as the width and depth increase the performance at different pruning rates approach the same value. This observed phenomenon yields a more significant gain in the performance for MPTs with higher pruning rates. Similar to the previous experiment, the performance of MPT-1/32 matches (or exceeds) the performance of dense models for a large range of pruning rates. Furthermore, in the high width regime, a large number of weights (∼ 90%) can be pruned without having a noticeable impact on the performance of MPTs. We also notice that the performance gap between MPT-1/32 and MPT-1/1 decreases significantly with an increase the width which is in sharp contrast with the with the depth experiments where the performance gap between MPT-1/32 and MPT-1/1 appeared to be largely independent of the depth.</p><p>Key Takeaways. Our experiments verify Multi-Prize Lottery Ticket Hypothesis and additionally convey the significance of choosing appropriate network depth and layer width for a given pruning rate. In particular, we find that a network with a large width can be pruned more aggressively without sacrificing much accuracy, while the accuracy of a network with smaller widths suffers when pruning a large percentage of the weights. Similar patterns hold for the depth of the networks as well. The amount of overparametrization needed to approach the performance of dense networks seems to differ for MPT variants -MPT-1/1 requires higher depth and width compared to MPT-1/32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HOW REDUNDANT ARE STATE-OF-THE-ART DEEP NEURAL NETWORKS?</head><p>Having shown that MPTs can perform equally good (or better) than overparameterized networks, this experiment aims to answer: Are state-of-the-art weight-trained DNNs overparametrized enough that significantly smaller multi-prize tickets can match (or beat) their performance?</p><p>Experimental Configuration. Instead of focusing on extremely large DNNs, we experiment with small to moderate size DNNs. Specifically, we analyze the redundancy of following backbone models: (1) VGG-Small and ResNet-18 on CIFAR-10, and <ref type="formula" target="#formula_10">(2)</ref> WideResNet-34 and WideResNet-50 on ImageNet. As we will show later that even these models are highly redundant, thus, our finding automatically extends to larger models. In this process, we also perform a comprehensive comparison of the performance of our multi-prize winning tickets with state-of-the-art in binary neural networks (BNNs). Details on the experimental configuration are provided in Appendix A.</p><p>This experiment uses Algorithm 1 to find MPTs within randomly initialized backbone networks. We compare the Top-1 accuracy and number of non-zero parameters for our MPT-1/32 and MPT-1/1 tickets with selected baselines in BNNs <ref type="bibr" target="#b35">(Qin et al., 2020a)</ref>. Results for CIFAR-10 and ImageNet are shown in <ref type="table" target="#tab_1">Tables 1, 2 and Tables 3, 4</ref>, respectively. Next to each MPT method we include the percentage of weights pruned in parentheses. Motivated by <ref type="bibr" target="#b9">(Frankle et al., 2020)</ref>, we also include models in which the BatchNorm parameters are learned when identifying the random subnetwork using biprop, indicated by +BN. A more comprehensive comparison can be found in Appendix D.    Our results highlight that SOTA DNN models are extremely redundant. For similar parameter count, our binary MPT-1/32 models outperform even full-precision models with learned weights. When compared to state-of-the-art in BNNs, with minimal hyperparameter tuning our multi-prize tickets achieve comparable (or higher) Top-1 accuracy. Specifically, our MPT-1/32 outperform trained binary weight networks on CIFAR-10 and ImageNet and our MPT-1/1 outperforms trained binary weight and activation networks on CIFAR-10. Further, on CIFAR-10 and ImageNet, MPT-1/32 networks with significantly reduced parameter counts outperform dense and full precision networks with learned weights. Searches for MPT-1/1 in BNN-specific architectures <ref type="bibr" target="#b20">(Kim et al., 2020;</ref><ref type="bibr" target="#b3">Bulat et al., 2020a)</ref> and adopting other commonly used tricks to improve model &amp; representation capacities <ref type="bibr" target="#b4">(Bulat et al., 2020b;</ref><ref type="bibr" target="#b24">Lin et al., 2020;</ref><ref type="bibr" target="#b25">2021)</ref> are likely to yield MPT-1/1 networks with improved performance. For example, up to a 7% gain in the MPT-1/1 accuracy was achieved by simply allowing BatchNorm parameters to be updated. Additionally, alternative approaches for updating the pruning mask in biprop could alleviate issues with back-propagating gradients through binary activation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION AND IMPLICATIONS</head><p>Existing compression approaches (e.g., pruning and binarization) typically rely on some form of weight-training. This paper showed that a sufficiently overparametrized randomly weighted network contains binary subnetworks that achieve high accuracy (comparable to dense and full precision original network with learned weights) without any training. We referred to this finding as the Multi-Prize Lottery Ticket Hypothesis. We also proved the existence of such winning tickets and presented a generic procedure to find them. Our comparison with state-of-the-art neural networks corroborated our hypothesis. With minimal hyperparameter tuning, our binary weight multi-prize tickets outperformed current state-of-the-art in BNNs and proved its practical importance. Our work has several important practical and theoretical implications.</p><p>Algorithmic. Our biprop framework enjoys certain advantages over traditional weightoptimization. First, contemporary experience suggests that sparse BNN training from scratch is challenging. Both sparseness and binarization bring their own challenges for gradient-based weight training -getting stuck at bad local minima in the sparse regime, incompatibility of backpropagation due to discontinuity in activation function, etc. Although we used gradient-based approaches in this paper, biprop is flexible to accommodate different class of algorithms that might avoid the pitfalls of gradient-based weight training. Next, in contrast to weight-optimization that requires large model size and massive compute resources to achieve high performance, our hypothesis suggests that one can achieve similar performance without ever training the large model. Therefore, strategies such as fast ticket search <ref type="bibr" target="#b50">(You et al., 2019)</ref> or forward ticket selection <ref type="bibr" target="#b49">(Ye et al., 2020)</ref> can be developed to enable more efficient ways of finding-or even designing-MPTs. Finally, as opposed to weight-optimization, biprop by design achieves compact yet accurate models.</p><p>Theoretical. MPTs achieve similar performance as the model with learned weights. First, this observation notes the benefit of overparameterization in the neural network learning and reinforces the idea that an important task of gradient descent (and learning in general) may be to effectively compress overparametrized models to find multi-prize tickets. Next, our results highlight the expressive power of MPTs -since we showed that compressed subnetworks can approximate any target neural network who are known to be universal approximators, our MPTs are also universal approximators. Finally, the multi-prize lottery ticket hypothesis also uncovers the generalization properties of DNNs. Generalization theory for DL is still in its infancy and its not clear what and how DNNs learn <ref type="bibr" target="#b32">(Neyshabur et al., 2017)</ref>. Multi-prize lottery ticket hypothesis may serve as a valuable tool for answering such questions as it indicates the dependence of generalization on the compressiblity.</p><p>Practical. Huge storage and heavy computation requirements of state-of-the-art deep neural networks inevitably limit their applications in practice. Multi-prize tickets are significantly lighter, faster, and efficient while maintaining performance. This unlocks a range of potential applications DL could be applied to (e.g., applications with resource-constrained devices such as mobile phones, embedded devices, etc.). Our results also indicate that existing SOTA models might be spending far more compute and power than is needed to achieve a certain performance. In other words, SOTA DL models have terrible energy efficiency and significant carbon footprint <ref type="bibr" target="#b42">(Strubell et al., 2019)</ref>. In this regard, MPTs have the potential to enable environmentally friendly artificial intelligence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A HYPERPARAMETER CONFIGURATIONS</head><p>A.1 HYPERPARAMETERS FOR SECTION 3.1</p><p>Experimental Configuration. For MPT-1/32 tickets, the network structure is not modified from the original. For MPT-1/1 tickets, the network structure is modified by moving the max-pooling layer directly after the convolution layer and adding a batch-normalization layer before the binary activation function, as is common in many BNN architectures . We choose our baselines as dense full precision models with learned weights. The baselines were obtained by training backbone networks using the Adam optimizer with learning rate of 0.0003 for 100 epochs and with a batch size of 60. In each randomly weighted backbone network, we find winning tickets MPT-1/32 and MPT-1/1 for different pruning rates using Algorithm 1. For both the weight-optimized and MPT networks, the weights are initialized using the Kaiming Normal distribution . All training routines make use of a cosine decay learning rate policy.    In the following analysis, note that we write Bin({−1, +1} m×n ) to denote matrices of dimension m × n whose components are independently sampled from a binomial distribution with elements {−1, +1} and probability p = 1/2.</p><formula xml:id="formula_11">Lemma 1. Let s ∈ [d], α ∈ − 1 √ s , 1 √ s , i ∈ [d]</formula><p>, and ε, δ ≥ 0 be given. Let B ∈ {−1, +1} k×d be chosen randomly from Bin({−1, 1} k×d ) and u ∈ {−1, +1} k be chosen randomly from</p><formula xml:id="formula_12">Bin({−1, +1} k ). If k ≥ 16 ε √ s + 16 log 2 δ ,<label>(3)</label></formula><p>then with probability at least 1 − δ there exist masksm ∈ {0, 1} k and M ∈ {0, 1} k×d such that the function g :</p><formula xml:id="formula_13">R d → R defined by g(x) = (m u) σ (ε(M B)x) ,<label>(4)</label></formula><p>satisfies</p><formula xml:id="formula_14">|g(x) − αx i | ≤ ε,<label>(5)</label></formula><p>for all x ∞ ≤ 1. Furthermore, m 0 = M 0 ≤ 2 ε √ s , and max 1≤j≤k M j,: 0 ≤ 1.</p><p>Proof. If |α| ≤ ε then taking M = 0 yields the desired result. Suppose that |α| &gt; ε. Then there exists a c i ∈ N such that</p><formula xml:id="formula_15">c i ε ≤ |α| ≤ (c i + 1)ε and |c i ε − |α|| ≤ ε.<label>(6)</label></formula><p>Hence, it follows that</p><formula xml:id="formula_16">|c i ε sign(α)x i − αx i | = |x i ||c i ε − |α|| ≤ ε,<label>(7)</label></formula><p>where the final inequality follows from (6) and the hypothesis that x ∞ ≤ 1. Our goal now is to show that with probability 1 − δ the random initialization of u and B yield masksm and M such that</p><formula xml:id="formula_17">g(x) = c i ε sign(α)x i . Now fix i ∈ [d]</formula><p>and take k = k 2 . First, we consider the probability P (|{j ∈ [k ] : u j = +1 and B j,i = sign(α)}| &lt; c i ) .</p><p>As u and B :,i are each sampled from a binomial distribution with k trials, the distribution that the pair (u j , B j,i ) is sampled from is a multinomial distribution with four possible events each having a probability of 1/4. Since we are only interested in the event (u j , B j,i ) = (+1, sign(α)) occurring, we can instead consider a binomial distribution where P ((u j , B j,i ) = (+1, sign(α)) = 1 4 and P ((u j , B j,i ) = (+1, sign(α)) = 3 4 . Hence, using Hoeffding's inequality we have that</p><formula xml:id="formula_19">P (|{j ∈ [k ] : u j = +1 and B j,i = sign(α)}| &lt; c i ) ≤ exp −2k 1 4 − c i k 2 (9) = exp − 1 8 k + c i − 2 c 2 i k (10) &lt; exp − 1 8 k + 2c i ,<label>(11)</label></formula><p>where the final inequality follows since exp() is an increasing function and −2 c 2 i k &lt; 0. From (6) and the fact that |α| ≤ 1 √ s , it follows that</p><formula xml:id="formula_20">c i ≤ 1 ε √ s .<label>(12)</label></formula><p>Combining our hypothesis in (3) with (12) yields that</p><formula xml:id="formula_21">− 1 8 k + c i = − 1 16 k + c i ≤ − 1 16 16 ε √ s + 16 log 2 δ + 1 ε √ s = log δ 2 .<label>(13)</label></formula><p>Substituting <ref type="formula" target="#formula_12">(13)</ref>  </p><p>Additionally, it follows from the same argument that</p><formula xml:id="formula_23">P (|{k &lt; j ≤ k : u j = −1 and B j,i = − sign(α)}| &lt; c i ) &lt; δ 2 .<label>(15)</label></formula><p>From <ref type="formula" target="#formula_13">(14)</ref> and <ref type="formula" target="#formula_14">(15)</ref> it follows with probability at least 1 − δ that there exist sets S + := {j : u j = +1 and B j,i = sign(α)} and S − := {j : u j = −1 and B j,i = − sign(α)} satisfying |S + | = |S − | = c i and S + ∩ S − = ∅. Using these sets, we define the components of the maskm and M bym</p><formula xml:id="formula_24">j = 1 : j ∈ S + ∪ S − 0 : otherwise<label>(16)</label></formula><p>and</p><formula xml:id="formula_25">M j, = 1 : j ∈ S + ∪ S − and = i 0 : otherwise .<label>(17)</label></formula><p>Using the definition of g(x) in (4) we now have that</p><formula xml:id="formula_26">g(x) = i∈S+ σ (ε sign(α)x i ) − i∈S− σ (−ε sign(α)x i ) (18) = c i σ (ε sign(α)x i ) − c i σ (−ε sign(α)x i ) (19) = c i ε sign(α)x i ,<label>(20)</label></formula><p>where the final equality follows from the identity σ(a) − σ(−a) = a, for all a ∈ R. This concludes the proof of (5).</p><p>Lastly, by our choice ofm in <ref type="formula" target="#formula_15">(16)</ref>, M in <ref type="formula" target="#formula_16">(17)</ref>, and <ref type="formula" target="#formula_10">(12)</ref>, it follows that</p><formula xml:id="formula_27">m 0 = M 0 = 2c i ≤ 2 ε √ s ,<label>(21)</label></formula><p>and max 1≤j≤k M j,: 0 ≤ 1,</p><p>which concludes the proof.</p><p>The next step is to consider an analogue for Lemma A.2 from <ref type="bibr" target="#b29">(Malach et al., 2020)</ref> which we provide in Lemma 2.</p><formula xml:id="formula_29">Lemma 2. Let s ∈ [d], w * ∈ − 1 √ s , 1 √ s d</formula><p>with w * 0 ≤ s, and ε, δ &gt; 0 be given. Let B ∈ {−1, +1} k×d be chosen randomly from Bin({−1, 1} k×d ) and u ∈ {−1, +1} k be chosen randomly from</p><formula xml:id="formula_30">Bin({−1, +1} k ). If k ≥ s · 16 √ s ε + 16 log 2s δ ,<label>(23)</label></formula><p>then with probability at least 1 − δ there exist masksm ∈ {0, 1} k and M ∈ {0, 1} k×d such that the function g : R d → R defined by</p><formula xml:id="formula_31">g(x) = (m u) σ (ε(M B)x) ,<label>(24)</label></formula><p>satisfies |g(x) − w * , x | ≤ ε, for all x ∞ ≤ 1. </p><formula xml:id="formula_32">u (i) := u k (i−1)+1 · · · u k i ∈ {−1, +1} k ×1 (26) m (i) := m k (i−1)+1 · · ·m k i ∈ {0, 1} k ×1 (27) B (i) :=    b (k (i−1)+1),1 · · · b (k (i−1)+1),d . . . . . . . . . b k i,1 · · · b k i,d    ∈ {−1, +1} k ×d<label>(28)</label></formula><formula xml:id="formula_33">M (i) :=    m (k (i−1)+1),1 · · · m (k (i−1)+1),d . . . . . . . . . m k i,1 · · · m k i,d    ∈ {0, 1} k ×d ,<label>(29)</label></formula><p>for i ∈ [s]. Note that these submatrices satisfy</p><formula xml:id="formula_34">u =    u (1) . . . u (s)    ,m =   m<label>(1)</label></formula><p>. . . </p><formula xml:id="formula_35">m (s)    , B =    B (1) . . . B (s)    , M =    M (1) . . . M (s)    .<label>(30)</label></formula><formula xml:id="formula_36">g i (x) := m (i) u (i) σ ε(M (i) B (i) )x<label>(31)</label></formula><p>By <ref type="formula" target="#formula_10">(23)</ref>, taking ε = ε s and δ = δ s yields that k ≥ 16 ε √ s + 16 log 2 δ . Hence, it follows from Lemma 1 that with probability at least 1 − δ there existm (i) ∈ {0, 1} k and M (i) ∈ {0, 1} k ×d such that</p><formula xml:id="formula_37">|g i (x) − w * i x i | ≤ ε = ε s ,<label>(32)</label></formula><p>for every x ∈ R d with x ∞ ≤ 1, and</p><formula xml:id="formula_38">m (i) 0 = M (i) 0 ≤ 2 ε √ s = 2 √ s ε and max k (i−1)+1≤j≤k i M (i) j,i 0 ≤ 1.<label>(33)</label></formula><p>By the definition of g(x) in (24), using (30) yields</p><formula xml:id="formula_39">g(x) = (m u) σ (ε(M B)x) = s i=1 m (i) u (i) σ ε(M (i) B (i) )x = s i=1 g i (x).<label>(34)</label></formula><p>Hence, combining (32) for all i ∈ [s], it follows that with probability at least 1 − δ we have</p><formula xml:id="formula_40">|g(x) − w * , x | = s i=1 g i (x) − s i=1 w * i x i ≤ s i=1 |g i (x) − w * i x i | ≤ ε.<label>(35)</label></formula><p>Finally, it follows from (30) and (33) that</p><formula xml:id="formula_41">m 0 = M 0 ≤ 2s √ s ε and max 1≤j≤k M j,: 0 ≤ 1,<label>(36)</label></formula><p>which concludes the proof.</p><p>We now state and prove an analogue to Lemma A.5 in <ref type="bibr" target="#b29">(Malach et al., 2020)</ref> which is the last lemma we will need to establish the desired result. </p><formula xml:id="formula_42">Lemma 3. Let s ∈ [d], W * ∈ − 1 √ s , 1 √ s n×d with W * 0 ≤ s, F : R d → R n defined by F i (x) = σ( w * i , x ),</formula><p>then with probability at least 1 − δ there exist masksM ∈ {0, 1} k×n and M ∈ {0, 1} k×d such that the function G : R d → R n defined by</p><formula xml:id="formula_44">G(x) = σ (M U ) σ (ε(M B)x) ,<label>(38)</label></formula><p>satisfies As in the proof of Lemma 2, we can split U ,M , B, and M into n submatrices, denoted</p><formula xml:id="formula_45">G(x) − F (x) 2 ≤ ε, for all x ∞ ≤ 1.<label>(39)</label></formula><formula xml:id="formula_46">U (i) ∈ {−1, +1} k ×n ,M (i) ∈ {−1, +1} k ×n , B (i) ∈ {−1, +1} k ×d , and M (i) ∈ {−1, +1} k ×d for i ∈ [n], such that U =    U (1)</formula><p>. . .</p><formula xml:id="formula_47">U (n)    ,M =   M (1)</formula><p>. . .</p><formula xml:id="formula_48">M (n)    , B =    B (1) . . . B (n)    , and M =    M (1) . . . M (n)    .<label>(40)</label></formula><p>To simplify notation in the following definition, we define the vectorsm (i) :=M </p><formula xml:id="formula_49">g i (x) = m (i) u (i) σ β(M (i) B (i) )x ,<label>(41)</label></formula><p>for each i ∈ [n]. Taking ε = ε √ n and δ = δ n , it follows from (37) that k ≥ s · 16 √ s ε + 16 log 2s δ . As the hypotheses of Lemma 2 are satisfied, with probability at least 1 − δ n there exist masksm (i) and M (i) with</p><formula xml:id="formula_50">m (i) 0 = M (i) 0 ≤ 2s √ s ε = 2s √ ns ε<label>(42)</label></formula><p>such that</p><formula xml:id="formula_51">|g i (x) − W * i , x | ≤ ε √ n , for all x ∞ ≤ 1.<label>(43)</label></formula><p>For each i ∈ [n], note that this results in choosing the columns of the maskM (i) bỹ</p><formula xml:id="formula_52">M (i) :, = m (i) : = i 0 : otherwise<label>(44)</label></formula><p>Combining this choice with (40) yields</p><formula xml:id="formula_53">(M U ) σ (β(M B)x) =    g 1 (x) . . . g n (x)    .<label>(45)</label></formula><p>By the definition of G(x) in (38), it follows from (45) that</p><formula xml:id="formula_54">G(x) =    σ(g 1 (x)) . . . σ(g n (x))    .<label>(46)</label></formula><p>Combining <ref type="formula" target="#formula_12">(43)</ref> and <ref type="formula" target="#formula_13">(46)</ref>, we have with probability at least 1 − δ that</p><formula xml:id="formula_55">G(x) − F (x) 2 2 = n i=1 (σ(g i (x)) − σ( w * i , x )) 2 ≤ n i=1 (g i (x) − W * i , x ) 2 ≤ ε 2 .<label>(47)</label></formula><p>Finally, it follows from <ref type="formula" target="#formula_10">(42)</ref> and <ref type="formula" target="#formula_13">(44)</ref> that</p><formula xml:id="formula_56">M 0 = M 0 ≤ 2ns √ ns ε<label>(48)</label></formula><p>which concludes the proof.</p><p>We are now ready to prove the main result in Theorem 2.</p><p>Theorem 2. Let , n, s ∈ N,</p><formula xml:id="formula_57">W (1) * ∈ − 1 √ s , 1 √ s d×n , {W (i) * } −1 i=2 ∈ − 1 √ n , 1 √ n n×n , and W ( ) * ∈ − 1 √ n , 1 √ n 1×n . Assume that for each i ∈ [ ] we have W (i) * 2 ≤ 1 and max j W (i) * j 0 ≤ s. Define F (x) := F ( ) • · · · • F (1) (x) where F (i) (x) = σ(W (i) * x) for i ∈ [ − 1] and F ( ) (x) = W ( ) * x. Fix ε, δ ∈ (0, 1). Let B (1) ∈ {−1, +1} k×d be sampled from Bin({−1, +1} k×d ), {B (i) } i=2 ∈ {−1, +1} k×n be sampled from Bin({−1, +1} k×n ), {U (i) } −1 i=1 ∈ {−1, +1} k×n be sampled from Bin({−1, +1} k×n ) and U ( ) ∈ {−1, +1} k×1 sampled from Bin({−1, +1} k×1 ). If k ≥ ns · 32 √ ns ε + 16 log 2ns δ ,<label>(49)</label></formula><p>then with probability at least 1 − δ there exist binary masks</p><formula xml:id="formula_58">{M (i) } i=1 and {M (i) } i=1 for {B (i) } i=1 and {U (i) } i=1</formula><p>, respectively, such that the function G :</p><formula xml:id="formula_59">R d → R defined by G(x) := G ( ) • · · · • G (1) (x),<label>(50)</label></formula><p>where</p><formula xml:id="formula_60">G (i) (x) := σ (M (i) U (i) ) σ(ε(M (i) B (i) )x) , for i ∈ [ − 1]<label>(51)</label></formula><formula xml:id="formula_61">G ( ) (x) := (M (i) U (i) ) σ(ε(M (i) B (i) )x),<label>(52)</label></formula><p>satisfies</p><formula xml:id="formula_62">|G(x) − F (x)| ≤ ε, for all x 2 .<label>(53)</label></formula><formula xml:id="formula_63">Additionally, M 0 = M 0 ≤ 4ns 2 √ ns ε . Proof. Let i ∈ [ − 1].</formula><p>Using Lemma 3 with ε = ε 2 and δ = δ , with probability at least 1 − δ there exist M (i) andM (i) such that</p><formula xml:id="formula_64">G (i) (x) − F (i) (x) 2 ≤ ε 2 , for all x ∞ ≤ 1<label>(54)</label></formula><p>and</p><formula xml:id="formula_65">M (i) 0 = M (i) 0 ≤ 2ns √ ns ε = 4ns √ ns ε .<label>(55)</label></formula><p>The remainder of the proof follows from applying the same argument as in the proof of Theorem A.6 from <ref type="bibr" target="#b29">(Malach et al., 2020)</ref>.     <ref type="figure" target="#fig_0">., 2020)</ref>). In this specific instance, biprop boils down to edgepopup with proper scaling. Next, we compare the performance of MPT-1/32 networks identified using these two approaches. Both networks presented below use the same hyperparameter configurations and are trained for 250 epochs on the CIFAR-10 dataset. We initialize the networks identified with edgepopup using the Signed Constant initialization as it yielded their best performance. MPT-1/32 networks identified using biprop are initialized using the Kaiming Normal initialization. We plot the average over three experiments for each pruning percentage and bars extending to the minimum and maximum accuracy for each pruning percentage. Additionally, for each network we include the Top-1 accuracy of a dense model with learned weights. These plots can be found in <ref type="table">Table 11</ref>: Comparison of MPT-1/1 with Trained Binary (1/1) Networks on ImageNet <ref type="figure" target="#fig_5">Figure 5</ref>. We find that the performance of MPT-1/32 identified with biprop outperforms networks identified using edgepopup. This highlights the benefit of binarization (in conjunction with pruning) as a learning strategy. We categorize pruning methods based on whether a model is pruned either after the training or before the training (see (Neill, 2020) for a comprehensive review).</p><p>Post-Training Pruning. The traditional pruning methods leverage a three-stage pipeline -pretraining (a large model), pruning, and fine-tuning. The main distinction lies among these approaches is what type of criteria is used for pruning. One of the most popular approach is the magnitude-based pruning where the weights with the magnitude below a certain threshold are discarded <ref type="bibr" target="#b14">(Hagiwara, 1993)</ref>. Further, certain penalty term (e.g., l 1 , l 2 or lasso weight regularization) can be used during training to encourage a model to learn certain smaller magnitude weights and removing them posttraining <ref type="bibr" target="#b45">(Weigend et al., 1991)</ref>. Models can also be pruned by measuring the importance of weights by computing the sensitivity of the loss function when weights are removed and prune those which cause the smallest change in the loss <ref type="bibr" target="#b22">(LeCun et al., 1990)</ref>.</p><p>Pruning Before Training. Thus far, we have have discussed methods for pruning pretrained DNNs.</p><p>Recently, <ref type="bibr" target="#b8">(Frankle &amp; Carbin, 2019)</ref> proposed the Lottery Ticket Hypothesis and showed that randomly-initialized neural networks contain sparse subnetworks that can be effectively trained from scratch when reset to their initialization. Further, <ref type="bibr" target="#b28">(Liu et al., 2018b)</ref> showed that the training an over-parameterized model is often not necessary to obtain an efficient final model and network architecture itself is more important than the remaining weights after pruning pretrained networks. These findings has revived interest in finding approaches for searching sparse and trainable subnetworks. For example, <ref type="bibr" target="#b23">(Lee et al., 2018;</ref><ref type="bibr" target="#b44">Wang et al., 2020b;</ref><ref type="bibr" target="#b50">You et al., 2019;</ref><ref type="bibr" target="#b43">Wang et al., 2020a)</ref> explored efficient approaches to search for these sparse and trainable subnetworks. Along this line of work, a striking finding was reported by <ref type="bibr" target="#b53">(Zhou et al., 2019;</ref><ref type="bibr" target="#b37">Ramanujan et al., 2020)</ref> showing that randomly-initialized neural networks contain sparse subnetworks that achieve good performance without any training. <ref type="bibr" target="#b29">(Malach et al., 2020;</ref><ref type="bibr" target="#b34">Pensia et al., 2020)</ref> provided theoretical evidences for this phenomenon and showed that one can approximate any target neural network, by pruning a sufficiently over-parameterized network of random weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 BINARIZATION</head><p>Similar to pruning, we categorize binarization methods based on whether a model is binarized either after the training or during the training (see <ref type="bibr" target="#b35">(Qin et al., 2020a)</ref> for a comprehensive review).</p><p>Post-Training Binarization. To the best of our knowledge, none of the post-training schemes have been successful in binarizing pretrained models with or without retraining to achieve reasonable test accuracy. Most existing works <ref type="bibr" target="#b16">(Han et al., 2015;</ref> are limited to ternary weight quantization.</p><p>Training-Aware Binarization. There are several efforts to improve the performance of BNN training. This is a challenging problem as binarization introduces discontinuities which makes differentiation during backpropogation difficult. Binaryconnect  established how to train networks with binary weights within the familiar back-propagation paradigm. Bina-ryNet  further quantize both the weights and the activations to 1-bit values. Unfortunately, these early schemes resulted in a staggering drop in the accuracy compared to their full precision counterparts. In an attempt to improve the performance, XNOR-Net  proposed to add a real-valued channel-wise scaling factor. Dorefa-Net <ref type="bibr" target="#b54">(Zhou et al., 2016)</ref> extends XNOR-Net to accelerate the training process using quantized gradients. ABC-Net  improved the performance by using more weight bases and activation bases at the cost of increase in memory and computation. There have also been efforts in making modifications to the network architectures to make them amenable for the binary neural network training. For example, Bireal-Net <ref type="bibr" target="#b27">(Liu et al., 2018a)</ref> added layer-wise identity short-cut, and AutoBNN  proposed to widen or squeeze the channels in an automatic manner.  proposed to learn to binarize neurons with noisy supervision. Some efforts also have been carried out to designing gradient estimators extending straight-through estimator (STE) <ref type="bibr" target="#b1">(Bengio et al., 2013)</ref> for accurate gradient back-propagation. DSQ  used differentiable soft quantization to have accurate gradients in backward propagation. On the other hand, <ref type="bibr">PCNN Gu et al. (2019)</ref> proposed a new discrete back-propagation via projection algorithm to build BNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 OTHER RELATED DIRECTIONS</head><p>Gaier &amp; Ha (2019) proposed a search method for neural network architectures that can already perform a task without any explicit weight training, i.e., each weight in the network has the same shared value. Recent work in randomly wired neural networks <ref type="bibr" target="#b47">(Xie et al., 2019)</ref> showed that constructing neural networks with random graph algorithms often outperforms a manually engineered architecture. As opposed to fixed wirings in <ref type="bibr" target="#b47">(Xie et al., 2019)</ref>, <ref type="bibr" target="#b46">(Wortsman et al., 2019)</ref> learned the network parameters as well as the structure. This show that finding a good architecture is akin to finding a sparse subnetwork of the complete graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Effect of Varying Depth and Pruning Rate: Comparing the Top-1 accuracy of small and binary MPTs to a large, full-precision, and weight-optimized network on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Effect of Varying Width on MPT-1/32: Comparing the Top-1 accuracy of sparse and binary MPT-1/32 to dense, full-precision, and weight-optimized network on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>into (11) yields P (|{j ∈ [k ] : u j = +1 and B j,i = sign(α)}| &lt; c i ) &lt; δ 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Furthermore, m 0 = M 0 ≤ 2s √ s ε and max 1≤j≤k M j,: 0 ≤ 1. Proof. Assume k = s · 16 √ s ε + 16 log 2s δ and set k = k s . Note that if k &gt; s · 16 √ s ε + 16 log 2s δ then the excess neurons can be masked yielding the desired value for k. We decompose u,m, B, and M into s equal size submatrices by defining</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. Now we define the functions g i : R d → R by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Comparing biprop and edgepopup: Comparing the Top-1 accuracy of MPT-1/32 to binary weight networks of the same size identified using edgepopup on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="3">Model Top-1 Params</cell></row><row><cell>ABC-Net</cell><cell>ResNet-18</cell><cell>62.8</cell><cell>11.2 M</cell></row><row><cell>BWN</cell><cell>ResNet-18</cell><cell>60.8</cell><cell>11.2 M</cell></row><row><cell>IR-Net</cell><cell>ResNet-34</cell><cell>70.4</cell><cell>21.8 M</cell></row><row><cell>Quant-Net</cell><cell>ResNet-50</cell><cell>72.8</cell><cell>25.6 M</cell></row><row><cell>Full-Precision</cell><cell>ResNet-34</cell><cell>73.27</cell><cell>21.8 M</cell></row><row><cell>MPT (80)</cell><cell>WRN-50</cell><cell>72.67</cell><cell>13.7 M</cell></row><row><cell>MPT (80) +BN</cell><cell>WRN-50</cell><cell>74.03</cell><cell>13.7 M</cell></row></table><note>Comparison of MPT-1/1 with trained binary-1/1 networks on CIFAR-10.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of MPT-1/32 with trained binary-1/32 networks on ImageNet.</figDesc><table><row><cell>Method</cell><cell cols="3">Model Top-1 Params</cell></row><row><cell>BNN</cell><cell>AlexNet</cell><cell>27.9</cell><cell>62.3 M</cell></row><row><cell>XNOR-Net</cell><cell>AlexNet</cell><cell>44.2</cell><cell>62.3 M</cell></row><row><cell>ABC-Net</cell><cell>ResNet-34</cell><cell>52.4</cell><cell>21.8 M</cell></row><row><cell>IR-Net</cell><cell>ResNet-34</cell><cell>62.9</cell><cell>21.8 M</cell></row><row><cell>Full-Precision</cell><cell>ResNet-34</cell><cell>73.27</cell><cell>21.8 M</cell></row><row><cell>MPT (60)</cell><cell>WRN-34</cell><cell>45.06</cell><cell>19.3 M</cell></row><row><cell>MPT (60) +BN</cell><cell>WRN-34</cell><cell>52.07</cell><cell>19.3 M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Comparison of MPT-1/1 with</cell></row><row><cell>trained binary-1/1 networks on ImageNet.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameter Configurations for CIFAR-10 Experiments A.2 HYPERPARAMETERS FOR SECTION 3.2In these experiments, the weights are initialized using the Kaiming Normal distribution for all the models except for MPT-1/32 on ImageNet where we use the Signed Constant initialization (Ramanujan et al., 2020) as it yielded slightly better performance. All training routines make use of a cosine decay learning rate policy. For ImageNet training we used a label smoothing value of 0.1 and a learning rate warmup length of 5 epochs.</figDesc><table><row><cell>Method</cell><cell>Model</cell><cell>Opt.</cell><cell cols="5">LR Momentum Weight Decay Batch Epochs</cell></row><row><cell>MPT-1/32</cell><cell>ResNet-18</cell><cell>SGD</cell><cell>0.1</cell><cell>0.9</cell><cell>5e-4</cell><cell>256</cell><cell>250</cell></row><row><cell>MPT-1/32 +BN</cell><cell>ResNet-18</cell><cell>SGD</cell><cell>0.1</cell><cell>0.9</cell><cell>5e-4</cell><cell>256</cell><cell>250</cell></row><row><cell>MPT-1/1</cell><cell cols="3">VGG-Small Adam 3.63e-3</cell><cell>-</cell><cell>17.335</cell><cell>128</cell><cell>600</cell></row><row><cell>MPT-1/1 +BN</cell><cell cols="3">VGG-Small Adam 3.63e-3</cell><cell>-</cell><cell>1e-4</cell><cell>128</cell><cell>600</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameter Configurations for CIFAR-10 Experiments</figDesc><table><row><cell>Method</cell><cell cols="2">Model Optimizer</cell><cell cols="2">LR Momentum</cell><cell cols="3">Weight Decay Batch Epochs</cell></row><row><cell>MPT-1/32</cell><cell>WRN-50</cell><cell>SGD</cell><cell>0.256</cell><cell cols="2">0.875 3.051757812e-5</cell><cell>256</cell><cell>120</cell></row><row><cell cols="2">MPT-1/32 +BN WRN-50</cell><cell>SGD</cell><cell>0.256</cell><cell cols="2">0.875 3.051757812e-5</cell><cell>256</cell><cell>120</cell></row><row><cell>MPT-1/1</cell><cell>WRN-34</cell><cell cols="2">Adam 2.56e-4</cell><cell cols="2">-3.051757812e-5</cell><cell>256</cell><cell>250</cell></row><row><cell>MPT-1/1 +BN</cell><cell>WRN-34</cell><cell cols="2">Adam 2.56e-4</cell><cell cols="2">-3.051757812e-5</cell><cell>256</cell><cell>250</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameter Configurations for ImageNet Experiments B EXISTENCE OF BINARY-WEIGHT SUBNETWORK APPROXIMATING TARGET NETWORK</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>By our hypothesis that w * 0 ≤ s, it follows that |I| ≤ s. WLOG, assume that I ⊆ [s]. Now fix i ∈ [s] and define g i : R d → R by</figDesc><table /><note>Now let I := {i ∈ [d] : w * i = 0}.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>and ε, δ &gt; 0 be given. Let B ∈ {−1, +1} k×d be chosen randomly from Bin({−1, 1} k×d ) and U ∈ {−1, +1} k×n be chosen randomly from Bin({−1, +1} k×n ). If</figDesc><table><row><cell>k ≥ ns ·</cell><cell>16</cell><cell>√ ε</cell><cell>ns</cell><cell>+ 16 log</cell><cell>2ns δ</cell><cell>,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="4">: Comparison of MPT-1/32 with Trained Binary (1/32) Networks on CIFAR-10</cell></row><row><cell>Method</cell><cell cols="3">Model Top-1 Params</cell></row><row><cell>BNN</cell><cell>VGG-Small</cell><cell>89.9</cell><cell>4.6 M</cell></row><row><cell>XNOR-Net</cell><cell>VGG-Small</cell><cell>89.8</cell><cell>4.6 M</cell></row><row><cell>DoReFa-Net</cell><cell>ResNet-20</cell><cell>79.3</cell><cell>0.27 M</cell></row><row><cell>BBG</cell><cell>ResNet-20</cell><cell>85.3</cell><cell>0.27 M</cell></row><row><cell>LAB</cell><cell>VGG-Small</cell><cell>87.7</cell><cell>4.6 M</cell></row><row><cell>DSQ</cell><cell>VGG-Small</cell><cell>91.7</cell><cell>4.6 M</cell></row><row><cell>IR-Net</cell><cell>ResNet-18</cell><cell>91.5</cell><cell>4.6 M</cell></row><row><cell>Full-Precision</cell><cell>VGG-Small</cell><cell>93.6</cell><cell>4.6 M</cell></row><row><cell>MPT (75, 1.25x)</cell><cell>VGG-Small</cell><cell>88.49</cell><cell>1.44 M</cell></row><row><cell cols="2">MPT (75, 1.25x) +BN VGG-Small</cell><cell>91.9</cell><cell>1.44 M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="4">: Comparison of MPT-1/1 with Trained Binary (1/1) Networks on CIFAR-10</cell></row><row><cell>Method</cell><cell cols="3">Model Top-1 Params</cell></row><row><cell>ABC-Net</cell><cell>ResNet-18</cell><cell>62.8</cell><cell>11.2 M</cell></row><row><cell>BWN</cell><cell>ResNet-18</cell><cell>60.8</cell><cell>11.2 M</cell></row><row><cell>BWNH</cell><cell>ResNet-18</cell><cell>64.3</cell><cell>11.2 M</cell></row><row><cell>PACT</cell><cell>ResNet-18</cell><cell>65.8</cell><cell>11.2 M</cell></row><row><cell>IR-Net</cell><cell>ResNet-34</cell><cell>70.4</cell><cell>21.8 M</cell></row><row><cell cols="2">Quantization-Networks ResNet-18</cell><cell>66.5</cell><cell>11.2 M</cell></row><row><cell cols="2">Quantization-Networks ResNet-50</cell><cell>72.8</cell><cell>25.6 M</cell></row><row><cell>Full-Precision</cell><cell>ResNet-34</cell><cell>73.27</cell><cell>21.8 M</cell></row><row><cell>MPT (80)</cell><cell>WRN-50</cell><cell>72.67</cell><cell>13.7 M</cell></row><row><cell>MPT (80) +BN</cell><cell>WRN-50</cell><cell>74.03</cell><cell>13.7 M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>: Comparison of MPT-1/32 with Trained Binary (1/32) Networks on ImageNet</cell></row><row><cell>E COMPARISON TO EDGEPOPUP FOR MPT-1/32</cell></row><row><cell>Note that binarization step of biprop can be avoided while finding MPT-1/32 -by initializing (and</cell></row><row><cell>pruning) our backbone neural network with binary initialization (e.g., edgepopup with Signed Con-</cell></row><row><cell>stant initialization (Ramanujan et al</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A detailed discussion on related work on pruning and quantization is provided in Appendix F.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although our results are derived under certain assumptions (e.g., fully-connected, ReLU neural network approximated by a subnetwork with binary weights), our algorithm is not restricted by these assumptions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">t 1 − |x| t 1 {x∈[−t,t]} (x). The choice to approximate sign using a quadratic spline instead of a cubic spline results in a gradient estimator that can be implemented efficiently in PyTorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A comparison of MPT-1/32 found using biprop and edgepopup is provided in Appendix E, which demonstrates that biprop outperforms edgepopup.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank Shreya Chaganti for her valuable contributions to the biprop open source code development and for her help on training MPT models for the final version of the paper.</p><p>This work was performed under the auspices of the U.S. Department of Energy by the Lawrence Livermore National Laboratory under Contract No. DE-AC52-07NA27344, Lawrence Livermore National Security, LLC. This document was prepared as an account of the work sponsored by an agency of the United States Government. Neither the United States Government nor Lawrence Livermore National Security, LLC, nor any of their employees makes any warranty, expressed or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or Lawrence Livermore National Security, LLC. The views and opinions of the authors expressed herein do not necessarily state or reflect those of the United States Government or Lawrence Livermore National Security, LLC, and shall not be used for advertising or product endorsement purposes. This work was supported by LLNL Laboratory Directed Research and Development project 20-ER-014 and released with LLNL tracking number LLNL-CONF-815432.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MOTIVATION FOR FRAMEWORK TO IDENTIFY MPTS</head><p>Suppose that f (x; W * ) with optimized weights W * is a target network that we wish to approximate. Let g(x; W ) denote the network in which we want to identify a MPT-1/32 that is an εapproximation of f (x; W * ), for some ε &gt; 0. Now assume that g(x; ·) is Lipschitz continuous with constant κ, B ∈ {−1, +1} m are binary parameters for g, and α ∈ R is gain term. It follows that</p><p>If we take M to be a fixed binary mask, we can minimize the error of binarizing the subnetwork parameters M W by solving the optimization problem</p><p>where M , W , and B are stacked into vectors of some length, say n. As the pruning mask M is applied to both W and B, solving problem <ref type="formula">(57)</ref> is equivalent to solving problem (2) in  with a different dimension. Hence, it immediately follows that one closed form solution for B in problem <ref type="formula">(57)</ref> is</p><p>Taking the derivative of the cost function in (57) with respect to α and setting it equal to zero yields</p><p>Recalling that M ∈ {0, 1} n and using (58), we have</p><p>and</p><p>Substituting <ref type="formula">(60)</ref> and <ref type="formula">(61)</ref> into <ref type="formula">(59)</ref> and solving for α yields the closed form solution</p><p>Hence, α * and B * minimize the right hand side of (56) and, consequently, reduce the approximation error of the MPT-1/32. So when the binarization error, (M W ) − α(M sign(W )) , and the subnetwork error, g(x; M W )−f (x; W * ) , are sufficiently small then the binarized subnetwork g (x; α(M sign(W ))) serves as a good approximation to the target network.</p><p>These closed form expressions for the gain term and the binarized weights are the updates used for the gain term and binary subnetwork weights in biprop after updating the binary pruning mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D COMPARISON OF MPTS WITH BINARY NEURAL NETWORK SOTA</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Proxquant: Quantized neural networks via proximal operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edo</forename><surname>Liberty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13863</idno>
		<title level="m">Xnor-net++: Improved binary neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bats: Binary architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01711</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03558</idno>
		<title level="m">High-capacity expert binary networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJl-b3RcF7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training batchnorm and only batchnorm: On the expressive power of random features in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weight agnostic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Gaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5364" to="5378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Differentiable soft quantization: Bridging full-precision and low-bit neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4852" to="4861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Projection convolutional neural networks for 1-bit cnns via discrete back propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8344" to="8351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Removal of hidden units and weights for back propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masafumi</forename><surname>Hagiwara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1993 International Conference on Neural Networks (IJCNN-93</title>
		<meeting>1993 International Conference on Neural Networks (IJCNN-93<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="351" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Training binary neural networks through learning with noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04871</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James T</forename><surname>Kwok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01600</idno>
		<title level="m">Loss-aware binarization of deep networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sensor selection via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSP.2008.2007095</idno>
		<ptr target="https://doi.org/10.1109/TSP.2008.2007095" />
	</analytic>
	<monogr>
		<title level="j">Trans. Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="451" to="462" />
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning architectures for binary networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratap</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="575" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02340</idno>
		<title level="m">Snip: Single-shot network pruning based on connection sensitivity</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13055</idno>
		<title level="m">Rotated binary neural network</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Siman: Sign-to-magnitude network binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07981</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards accurate binary convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="345" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="722" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05270</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00585</idno>
		<title level="m">Shai Shalev-Shwartz, and Ohad Shamir. Proving the lottery ticket hypothesis: Pruning is all you need</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11535</idno>
		<title level="m">Training binary neural networks with real-to-binary convolutions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An overview of neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03669</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5947" to="5956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Logarithmic pruning is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Orseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Rivasplata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Harit Vishwakarma, and Dimitris Papailiopoulos. Optimal lottery tickets via subsetsum: Logarithmic over-parameterization is sufficient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Pensia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alliot</forename><surname>Nagle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Binary neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107281</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Forward and backward information retention for accurate binary neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2250" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What&apos;s hidden in a randomly weighted neural network?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Vivek Ramanujan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11893" to="11902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Universal approximation using feedforward neural networks: A survey of some existing methods, and some new results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0893-6080(97)00097-X</idno>
		<ptr target="https://doi.org/10.1016/S0893-6080(97)00097-X" />
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="37" />
			<date type="published" when="1998-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Balanced binary neural networks with gated residual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4197" to="4201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv 1409.1556</idno>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02243</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Picking winning tickets before training by preserving gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07376</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pruning from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12273" to="12280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generalization by weightelimination with application to forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Weigend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo A</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="875" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Discovering neural wirings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2684" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1284" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08695</idno>
		<title level="m">Searching for low-bit weights in quantized neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Good subnetworks provably exist: Pruning via greedy forward selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Klivans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01794</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Drawing early-bird tickets: Towards more efficient training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11957</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqiangzi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Incremental network quantization: Towards lossless cnns with low-precision weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03044</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deconstructing lottery tickets: Zeros, signs, and the supermask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hattie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3597" to="3607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Courbariaux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IR-Net. Qin et al., 2020b), LAB (Hou et al.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Results for CIFAR-10 can be found in Tables 8 and 9 and results for ImageNet can be found in Tables 10 and 11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Proxquant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Next to the MPT method we include the percentage of weights pruned and the layer width multiplier</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
