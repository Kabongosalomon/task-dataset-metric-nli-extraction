<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self Paced Deep Learning for Weakly Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Self Paced Deep Learning for Weakly Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Weakly supervised learning</term>
					<term>object detection</term>
					<term>self-paced learning</term>
					<term>curriculum learning</term>
					<term>deep learning</term>
					<term>training protocol</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In a weakly-supervised scenario object detectors need to be trained using image-level annotation alone. Since bounding-box-level ground truth is not available, most of the solutions proposed so far are based on an iterative, Multiple Instance Learning framework in which the current classifier is used to select the highest-confidence boxes in each image, which are treated as pseudo-ground truth in the next training iteration. However, the errors of an immature classifier can make the process drift, usually introducing many of false positives in the training dataset. To alleviate this problem, we propose in this paper a training protocol based on the self-paced learning paradigm. The main idea is to iteratively select a subset of images and boxes that are the most reliable, and use them for training. While in the past few years similar strategies have been adopted for SVMs and other classifiers, we are the first showing that a self-paced approach can be used with deep-network-based classifiers in an end-to-end training pipeline. The method we propose is built on the fully-supervised Fast-RCNN architecture and can be applied to similar architectures which represent the input image as a bag of boxes. We show state-of-the-art results on Pascal VOC 2007, Pascal VOC 2010 and ILSVRC 2013. On ILSVRC 2013 our results based on a low-capacity AlexNet network outperform even those weakly-supervised approaches which are based on much higher-capacity networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A well known problem in object detection is the fact that collecting ground truth data (i.e., object-level annotations) for training is usually much more time consuming and expensive than collecting image-level labels for object classification. This problem is exacerbated in the context of the current deep networks, which need to be trained or "finetuned" using large amounts of data. Weakly-supervised techniques for object detection (WSD) can alleviate the problem by leveraging existing datasets which provide imagelevel annotations only.</p><p>In the common Multiple Instance Learning (MIL) formalization of the WSD problem, an image I, associated with a label of a given class y, is described as a "bag" of Bounding Boxes (BBs), where at least one BB is a positive sample for y and the others are samples of the other classes (e.g., the background class). The main problem is how can the classifier, while being trained, automatically guess what the positives in I are. A typical MIL-based solution alternates between 2 phases: (1) optimizing the classifier's parameters, assuming that the positive BBs in each image are known, and (2) using the current classifier to predict the most likely positives in each image <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b27">[28]</ref>. However, a well known problem of MIL-like solutions is that if the initial classifier is not strong enough, this process can easily drift. For instance, predicted false positives (e.g., BBs on the background) can make the classifier learn something different than the target class.</p><p>Kumar et al. <ref type="bibr" target="#b21">[22]</ref> propose to alleviate this problem using a self-paced learning strategy. The main idea is that a subset of "easy" samples can be automatically selected by the classifier in each iteration. Training is then performed using only this subset, which is less prone to drifting and is progressively increased in the subsequent iterations when the classifier becomes more mature. Self-paced learning, applied in many other studies <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b44">[45]</ref>, is related to curriculum learning <ref type="bibr" target="#b1">[2]</ref> and is biologically inspired by the common human process of gradual learning, starting with the simplest concepts.</p><p>In this paper we adopt a self-paced learning approach to handle the uncertainty related to the BB-level localization of the objects in the training images in a WSD scenario, and "easier" is interpreted as "more reliable" localization. We propose a new training protocol for deep networks in which the self-paced strategy is implemented by modifying the mini-batch-based selection of the training samples. As far as we know, this is the first self-paced learning approach directly embedded in a modern end-to-end deep-network training protocol.</p><p>More specifically, the solution we propose in this paper is based on the state-of-the-art (fully supervised) objectdetection architecture Fast-RCNN <ref type="bibr" target="#b11">[12]</ref>. Fast-RCNN naturally embeds the idea of an image as a bag of BBs (see Sec. <ref type="bibr" target="#b2">3)</ref>. Moreover, in the Fast-RCNN approach, each minibatch of the Stochastic Gradient Descent (SGD) procedure is sampled hierarchically, by first (randomly) sampling images and then sampling BBs within those images according to the BB-level ground truth information. We exploit this "image-centric" sampling but we modify the random image selection using a self-paced strategy in which the images containing the highest-confidence boxes associated with the annotated classes are selected the first. In more detail, given an image I with an image-level label y, we use the network trained in the previous iterations to associate a class-specific score s iy with each predicted BB p i . The highest-score box z I over all these predictions is selected in I. Note that, due to the spatial regression layer in Fast-RCNN, all the predicted BBs (z I included) are usually different from the set of input box proposals, i.e., the bag of BBs associated with I dynamically changes at every self-paced iteration. Once z I is chosen for each I in the training set, we select a subset of images according to the score associated with the corresponding z I and a mini-batch of positive and background BBs is extracted using z I . Moreover, since we train a multi-class classifier (a common approach in deep networks which exploit inter-category representation sharing <ref type="bibr" target="#b20">[21]</ref>), we exploit the competition among classifiers of different categories (i.e., among different classification-output neurons of the same network) and an image is chosen only when its label is consistent with the strongest classifier on that image. This image-based classifier competition is also used to progressively train different classifiers starting from the strongest ones. Since the predictions of the weak classifiers are usually not correct, we start training using only those samples corresponding to the strongest classifiers (i.e., the easiest classes), which are selected according to the number of images in which each classifier beats all the others. The benefit of this strategy is that, during the initial training phases, the network learns a visual representation of the objects in the shared layers (common to all the classes) together with a representation of the background class and these improved representations are used in the subsequent training phases when the network predicts the object localizations of the difficult classes.</p><p>Note that the basic Fast-RCNN architecture has been used, simulated or extended in many different applications, due to the flexibility of its region-based pooling layer, including a few very recent WSD architectures <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Since we propose a training protocol, our method is orthogonal to many of these works and can potentially be used in conjunction with more sophisticated architectures to obtain higher accuracy experimental results. However, we adopted the basic Fast-RCNN architecture <ref type="bibr" target="#b11">[12]</ref> in order to present a more general framework and we show empirical results using both a low-capacity AlexNet-like network <ref type="bibr" target="#b20">[21]</ref> and a much larger VGG-16 network <ref type="bibr" target="#b35">[36]</ref>. In common WSD benchmarks (Pascal VOC 2007 and 2010, and ILSVRC 2013) our approach largely outperforms the current state of the art. For instance, on ILSVRC 2013, our AlexNet-based results are even higher than those results obtained by other WSD methods which use much larger capacity networks (e.g., . Finally, as far as we know, this is the first work empirically showing that a self-paced selection of samples is useful in training a deep network. Recent works have shown that an anti-curriculum learning strategy (e.g., hard-negative mining) can be useful in a supervised scenario <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>. In <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b43">[44]</ref> a curriculum learning strategy is used to select easy samples for training. However, the sample order is not decided by the classifier being trained (i.e., the network) but it is provided using auxiliary data (e.g., the program complexity in <ref type="bibr" target="#b43">[44]</ref> and the object's scale estimate in <ref type="bibr" target="#b33">[34]</ref>). In our case, we do not use auxiliary data and we assume that only image-level labels are given at training time. The goal of the adopted self-paced strategy is to discard noisy training data (wrong pseudo-ground truth BBs) and we use the same detection network that is being trained in order to progressively select the most likely candidate pseudoground truth BBs.</p><p>In summary, our contributions are the following. <ref type="bibr">•</ref> We propose a computationally efficient self-paced learning protocol for training a deep network for WSD. During the training of the network, the same network, at different evolution stages, is used to predict the object-level localizations of the positive samples and to select a subset of images whose pseudo-ground truth is the most reliable. <ref type="bibr">•</ref> We propose to use the spatial regression layer of the network to dynamically change the initial bag of boxes. We empirically show that selecting z I over the set of current predictions ({p i }) rather than over the set of the initial box proposals ({b i }) can largely boost the final WSD accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We propose to use class-specific confidence and interclassifier competition to decrease the probability of selecting incorrect samples.</p><p>• We propose to extend the self-paced sample selection paradigm to a self-paced class selection using the inter-classifier competition to estimate the difficulty of each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We test our approach on Pascal VOC 2007, Pascal VOC 2010 and ILSVRC 2013, obtaining state-of-theart WSD results in all these benchmarks. This is the first work showing the usefulness of a self-paced sample selection strategy with an end-to-end trained deep network.</p><p>Our code and our trained models are publicly available 1 . The rest of the paper is organized as follows. In Sec. 2 we review the literature concerning self-paced learning, weakly supervised object detection and related areas. In Sec. 3 we analyse the main aspects of the Fast-RCNN architecture that are of interest for our work. In Secs. 4-5 we introduce and analyse our method, which is evaluated in Secs. 6-7 and finally we conclude in Sec. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Many recent studies have shown that selecting a subset of "good" samples for training a classifier can lead to better results than using all the samples <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref>. A pioneering work in this direction is the curriculum learning approach proposed in <ref type="bibr" target="#b1">[2]</ref>. The authors show that suitably sorting the training samples, from the easiest to the most difficult, and iteratively training a classifier starting with a subset of easy samples (progressively augmented with more and more difficult samples), can be useful to find better local minima. In <ref type="bibr" target="#b6">[7]</ref>, easy and difficult images (taken from datasets known to be more or less "difficult") are provided for training a Convolutional Neural Network (CNN) in order to learn generic CNN features using webly annotated data. In <ref type="bibr" target="#b42">[43]</ref>, different and progressively more complex CNNs are trained for a segmentation task, using more and more difficult data samples together with the output of the previously learned networks. It is worth noting that in these and in all the other curriculum-learningbased approaches, the order of the samples is decided using additional supervisory information usually provided by a human teacher. Unfortunately, these "image-easiness" metadata are not available for the common large-scale datasets.</p><p>Curriculum learning was extended to self-paced learning in <ref type="bibr" target="#b21">[22]</ref>. The main difference between the two paradigms 1. https://github.com/moinnabi/SelfPacedDeepLearning is that in self-paced learning the order of the samples is automatically computed and it is a priori unknown. The selection of the best "easy" sample set for training is, generally speaking, untractable (it is a subset selection problem). The solution proposed in <ref type="bibr" target="#b21">[22]</ref> is based on a continuous relaxation of the problem's constraints which leads to a biconvex optimization of a Structural SVM. Supancic et al. <ref type="bibr" target="#b38">[39]</ref> adopt a similar framework in a tracking by detection scenario and train a detector using a subset of video frames, showing that this selection is important to avoid drifting. Frames are selected by computing the SVM objective function for different candidate subsets of frames and then selecting the subset corresponding to the lowest objective value. In <ref type="bibr" target="#b17">[18]</ref> the authors pre-cluster the training data in order to balance the selection of the easiest samples with a sufficient intercluster diversity. However, the clusters and the feature space are fixed: they do not depend on the current self-paced training iteration and the adaptation of this method to a deeplearning scenario, where the feature space changes during learning, is not trivial. In <ref type="bibr" target="#b29">[30]</ref> a set of learning tasks is automatically sorted in order to allow for a gradual sharing of information among tasks. Our self-paced class selection aims at a similar goal but it is obtained with a radically different approach. Liang et al. <ref type="bibr" target="#b25">[26]</ref> use Exemplar SVMs (ESVMs) <ref type="bibr" target="#b26">[27]</ref> to train a classifier from a single positive sample. The trained ESVMs are then run on an unsupervised collection of videos in order to extract new positives which are gradually more and more different from the seed instances. ESVMs are also used in <ref type="bibr" target="#b22">[23]</ref> to assess the "training value" of each instance and then use this value to select the best subset of samples for training a classifier. In <ref type="bibr" target="#b23">[24]</ref>, the easiness of an image region is estimated using its "objectness" and the category context of its surrounding regions. In <ref type="bibr" target="#b44">[45]</ref> saliency is used to progressively select samples in WSD.</p><p>Although some of these self-paced methods use pretrained CNN-based features to represent samples (e.g., <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>), none of them uses a deep network as the classifier or formulates the self-paced strategy in an end-to-end deepnetwork training protocol as we do in this paper.</p><p>Concerning the broader WSD field, a few recent studies address the problem in a deep-learning framework. For instance, in <ref type="bibr" target="#b28">[29]</ref>, a final max-pooling layer selects the highest scoring position for an instance of an object in the input image and back-propagates the training error only to those network's weights that correspond to the highest scoring window. However, in this work the object is localized at testing time by providing only one 2D point. A similar max-pooling layer over different subwindows of the input image is adopted in <ref type="bibr" target="#b13">[14]</ref>, together with the Fast-RCNN architecture <ref type="bibr" target="#b11">[12]</ref>, to select the most significant context box in an action recognition task. Hoffman et al. <ref type="bibr" target="#b15">[16]</ref> use both weakly-supervised and strongly-supervised data (the latter being BB-level annotations) to adapt a CNN pre-trained for a classification task to work in a detection task. This work was extended in <ref type="bibr" target="#b16">[17]</ref> using a MIL-based SVM training. Encouraging results were obtained both in <ref type="bibr" target="#b15">[16]</ref> and in <ref type="bibr" target="#b16">[17]</ref> using the ILSVRC 2013 detection dataset. However, in both papers, auxiliary strongly-annotated data for half of the 200 ILSVRC 2013 categories were used for training, together with image-level-only annotations for the remaining categories.</p><p>Very recently, a few WSD approaches have been proposed for training a deep network in an end-to-end fashion which are based on specific network architectures. For instance, Bilen and Vedaldi <ref type="bibr" target="#b4">[5]</ref> extend a Fast-RCNNlike network using two different data streams, respectively computing a classification and a detection score for each candidate box of an image. Specifically, the detection score is obtained using a softmax operator which produces a probability distribution over all the input region proposals, thus avoiding the hard assignment of the pseudo-ground truth position to a specific box, common in MIL-like approaches. A similar soft assignment in a WSD scenario was previously developed in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, while the architecture proposed in <ref type="bibr" target="#b4">[5]</ref> is further extended in <ref type="bibr" target="#b18">[19]</ref> introducing specific regions which describe the context surrounding each candidate box. We compare with <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b18">[19]</ref> in Sec. 6.</p><p>Li et al. <ref type="bibr" target="#b24">[25]</ref> address the multi-label problem (the same image can contain objects belonging to different classes) by proposing a specific classification loss for training an image classification network. Then, this classification network is used to initialize a Fast-RCNN-based detector which is trained using a MIL framework. Following their approach we trained a similar classification network which is used as the initialization of our detector, trained using our selfpaced framework and tested on Pascal VOC, where we largely outperform the results obtained in <ref type="bibr" target="#b24">[25]</ref>. In <ref type="bibr" target="#b39">[40]</ref> for each candidate box of an image, an attention score is computed which estimates how likely that box contains the object of interest. While these works propose specific network architectures for solving the WSD problem, we take a different direction and we propose a training protocol which can be used with different architectures, provided that they have a region-pooling layer similar to Fast-RCNN and an image-based sampling strategy in computing the SGD minibatch (Sec. 3).</p><p>Finally, the closest work to this paper is probably <ref type="bibr" target="#b33">[34]</ref>, where the authors propose a curriculum-learning based training protocol for WSD, in which the size estimate of an object inside a given image is used as a proxy for assessing the "easiness" degree of that image. However, an additional training set, provided with the ground-truth size of each object, is necessary to train the size regressor, which makes this approach not directly comparable with other works using only weakly-supervised data. Moreover, the authors present results using either SVM classifiers or a deep network (Fast-RCNN). In the latter case, the deep network is trained using a simpler MIL approach in which the previous SVM-self paced based image selection is used only to select an initial set of pseudo-ground truth for training. Differently from <ref type="bibr" target="#b33">[34]</ref>, our pseudo-ground truth training set is modified during the Fast-RCNN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FAST-RCNN AND NOTATION</head><p>In this section we review the main aspects of the Fast-RCNN <ref type="bibr" target="#b11">[12]</ref> approach which are important to understand our proposal and we introduce notations, used in the rest of the paper.</p><p>The supervised state-of-the-art object-detection approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b34">[35]</ref> on Pascal VOC <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> are based on the Fast-RCNN architecture, whose main characteristic is the RoI pooling layer. This layer is used to extract box-specific information from the final convolutional maps and feed the final classification and regression branches of the network.</p><p>The network takes as input an image I (raw pixels) and a set of BBs on I: B(I) = {b 1 , ..., b n } . B(I) is computed using an external tool, which usually selects image subwindows taking into account their "objectness": for instance using Selective Search <ref type="bibr" target="#b40">[41]</ref> (also used in all our experiments). If f is the function computed by the network, its outcome is a set of detections:</p><formula xml:id="formula_0">f (I, B(I)) = {d ic } i=1,...,n,c=1,...,C ,<label>(1)</label></formula><p>where C is the number of object classes and, for each class c and each input box b i ∈ B(I), d ic = (s ic , p ic ), where s ic is the score and p ic the predicted box. Note that, usually, p ic = b i and p ic ∈ B(I), p ic being the result of a spatial regression applied to b i . The RoI pooling layer makes it possible to efficiently compute f (I, B(I)) and the dependence of the network's output on a set of boxes B(I) is important for our bag of BBs formulation. As mentioned in Sec. 1, another aspect of Fast-RCNN exploited in our training protocol is that each mini-batch (used in the mini-batch SGD procedure) is constructed using only a small number m of images, where m = 2 is indicated as a good compromise between quality of the samples and efficiency. Specifically, at training time a set T = {(I 1 , G 1 ), ..., (I j , G j )..., (I N , G N )} of N images and corresponding ground-truth is given, where:</p><formula xml:id="formula_1">G j = {(y 1 , b 1 ), (y 2 , b 2 ), ...} and, for each (y i , b i ), y i ∈ {1, .</formula><p>.., C} is the label and b i ∈ B(I) is the BB of the i-th object instance in I. In each SGD iteration m = 2 images are randomly extracted from T . If I j is one of these 2 images, for each (y i , b i ) ∈ G j , b i is matched with the boxes in B(I) using common spatial criteria (i.e., Intersection over Union between two BBs higher than a given threshold) in order to select those BBs in B(I) that will be used as positives for the y i class, as well as a set of "negatives" (i.e., samples for the background class y = 0).</p><p>For more details we refer the reader to <ref type="bibr" target="#b11">[12]</ref>. What is important to highlight here is that Fast-RCNN is a strongly supervised method. Conversely, in our weakly-supervised scenario, we do not have BB-level annotations. Hence, in the rest of the article we assume that our training set is T =</p><formula xml:id="formula_2">{(I 1 , Y 1 ), ..., (I j , Y j ), ..., (I N , Y N )}, where Y j = {y 1 , ...y nj }</formula><p>is the set of labels associated with image I j and the number of object classes contained in I j varies depending on the specific image (n j ≥ 1). Note that, for a given class y i ∈ Y j , more than one object of the same class can be contained in I j : for instance, two instances of the "dog" category; and the number of instances is unknown.</p><p>Since object-level ground truth is not given, we use the network (in the current self-paced training iteration) to compute the most likely positions of the objects in I j . In the next section we show how these locations are computed and how T is updated following a self-paced learning strategy. For each c ∈ {1, ..., C} compute e(c) using Eq. 3 7</p><p>C t := r t C 8</p><p>Let S = {c 1 , c 2 , ...} be the subset of the C t easiest classes according to e(c) 9</p><p>Remove from P those tuples (I, s, z, y) s.t. y ∈ S 10 N t := min(r t N, |P |) 11</p><p>Let P be the N t topmost tuples in P according to the s-score 12</p><p>For each (I, s, z, y) ∈ P :</p><formula xml:id="formula_3">T t := T t ∪ {(I, {(y, z)})} 13 V 0 = W t−1 14</formula><p>For t := 1 to N t /m: 15</p><p>Randomly select</p><formula xml:id="formula_4">(I 1 , {(y 1 , z 1 )}), ..., (I m , {(y m , z m )}) ∈ T t 16</formula><p>Compute a mini-batch M B of BBs using</p><formula xml:id="formula_5">(I 1 , {(y 1 , z 1 )}), ..., (I m , {(y m , z m )}) 17 Compute V t using M B and back-propagation on f V t −1 18 W t := V Nt/m 19 r t+1 = r t + 1−r1 M</formula><p>be obtained using any standard object classification network, trained using only image-level information. At the end of this section we provide more details on how W 0 is obtained. The proposed self-paced learning protocol of the network is composed of a sequence of self-paced iterations. At a self-paced iteration t we use the current network f Wt−1 in order to select a subset of easy classes and easy samples of these classes. The result is a new training set T t which is used to train a new model W t . W t is obtained using the "standard" training procedure of the Fast-RCNN (Sec. 3), based on mini-batch SGD, but it is applied to T t only and iterated for only N t /m mini-batch SGD iterations, N t being the cardinality of T t . Note that, being m the number of images used to build a mini-batch, N t /m corresponds to one epoch (a full iteration over T t ). Note also that a minibatch SGD iteration is different from a self-paced iteration and in each SGD iteration a mini-batch of BBs is formed using the pseudo-ground truth obtained using f Wt−1 . The proposed protocol is summarized in Alg. 1 and we provide the details below.</p><p>Computing the latent boxes. Given an image I, its label set Y and the current network f Wt−1 , first we compute:</p><formula xml:id="formula_6">(s I y , z I y ) = arg max (sic,pic)∈f W t−1 (I,B(I)) s ic<label>(2)</label></formula><p>In Eq. 2, (s I y , z I y ) is the detection in f (I, B(I)) with the highest score (s I y ) with respect to all the detections obtained starting from B(I) and the subscript y indicates the corresponding class. z I y is a latent box which specifies the most Inter-classifier competition. The two above figures show the behaviour of our self-paced learning algorithm on the same image I in two different iterations (t = 1 and t = 2). In both cases, the green box shows the highest-score box in I corresponding to z I y in Eq. 2. Conversely, the red box in the left figure shows the highest-score box in I corresponding to the car class (zcar). Since in t = 1 (left figure) s T V = s I y &gt; scar, and since T V ∈ Y , then I is not included in T 1 (see also <ref type="figure" target="#fig_0">Fig. 1</ref>). However, in t = 2 (right figure) s T V &lt; s I y = scar (where car ∈ Y ), thus (I, scar, zcar, car) is included in P (line Line 5 in Alg. 1): the "car" classifier in this iteration is strong enough and "wins" in I.</p><p>likely position of an object of the "winning" class y in image I according to f Wt−1 . Note that the background class is not included in f Wt−1 (see Eq. 1), thus y ∈ {1, ..., C}. Note also that z I y is computed using the regression layer of f Wt−1 and usually z I y ∈ B(I). Conversely, most of the existing WSD approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b39">[40]</ref> restrict the choice of the pseudo-ground truth over a pre-fixed and constant set of region proposals (B(I)). We empirically show in Sec. 7.1 the importance of using the regression part of the network to extend B(I). However, at the end of the current iteration t, z I y and all the other predictions {p ic } are discarded and, at iteration t + 1, Eq. 2 is minimized again starting from the original set of box proposals B(I).</p><p>If y ∈ Y (y is one of the labels of I), then we associate I with the latent box z I y and with the confidence score s I y (line 5), otherwise I is temporally discarded and will not be included in the current self-paced iteration training set T t .</p><p>Inter-classifier competition. Eq. 2 imposes a competition among classifiers, where a "classifier" for class c is the classification-output neuron of f specific for class c. Only if one of the classifiers corresponding to an image label y ∈ Y is "stronger" (more confident) than all the others, including y ∈ Y, y = 0, then I is considered for inclusion in T t (Line 5). We found this competition to be very important to decrease the risk of error and to enforce a self-paced learning strategy which prudently selects initially easy image samples. When the network becomes more mature (i.e., in the subsequent self-paced iterations), the risk of error gradually decreases and a previously weaker classifier can correctly "win" a previously discarded image (see <ref type="figure" target="#fig_1">Fig. 2</ref>). Note that a consequence of this classifier competition is that only one pseudo-ground truth box z I y can be selected from a given image I, regardless of the number of labels associated with I and the number of object instances of class y in I. In Sec. 7.2 we present multiple-label and multipleinstance relaxations of this inter-classifier competition.</p><p>Class selection. The classifier competition is used also to sort all the C classes from the easiest to the most difficult. This is obtained using the winning classifiers in each image as follows. Let P = {(I 1 , s 1 , z 1 , y 1 ), (I 2 , s 2 , z 2 , y 2 ), ...} be the set of non-discarded images at iteration t (Line 5) and let: be the "easiness" degree of class c, defined as the ratio between the cardinality of those tuples in P associated with the class label y = c over the overall frequency of the label c in T (p c ). The higher the value e(c) for a given class c, the stronger the corresponding classifier is and the easier that class according to f Wt−1 . We sort all the classes using e(c) (c = 1, ..., C) and we select the subset of the easiest r t C classes which are the only classes subject to training in the current self-paced iteration. The ratio r t ∈ [0, 1] is increased at each self-paced iteration (see below) and at iteration t + 1 more difficult classes will be included in T t+1 and presented to the network for training.</p><p>Selecting the easiest image samples. Once image samples associated with difficult classes have been removed from P (Line 9), we select a subset (T t ) of P corresponding to those images in which f Wt−1 is the most confident. With this aim we use the score s I y computed using Eq. 2 and we sort P in a descending order according to these scores. Then we select the first N t top-most elements, where N t = min(r t N, |P |), and r t N is an upper bound on the number of elements of T to be selected in the current self-paced iteration. At each selfpaced iteration r t is increased. Indeed, we adopt the strategy proposed in <ref type="bibr" target="#b21">[22]</ref> (also used in most of the self-paced learning approaches) to progressively increase the training set as the model is more and more mature (see <ref type="figure" target="#fig_0">Fig. 1</ref>). However, in our experiments we observed that usually |P | &lt; r t N , mainly because of the sample rejection step in Line 5, hence the learning "pace" is dominated by our classifier-competition constraint.</p><p>Details. The inner loop over t (Lines 14-17), whose number of iterations depends on the length of the current training set T t , is equivalent to the mini-batch SGD procedure adopted in <ref type="bibr" target="#b11">[12]</ref>, with a single important difference: Since we do not have BB-level ground truth, each mini-batch is computed using (y, z) as the pseudo-ground truth (Line 12). M B is built using BB samples which are collected using the same spatial criteria adopted in the supervised Fast-RCNN training protocol (see Sec. 3) and with the same positive/negative proportion (see <ref type="bibr" target="#b11">[12]</ref> for more details). Also the number of images m = 2 we use to compute a mini-batch of BBs is the same used in <ref type="bibr" target="#b11">[12]</ref>. In this loop, the weights of the network are called V t for notational convenience (their update depends on t and not on t), but there is only one network model, continuously evolving.</p><p>Inspired by <ref type="bibr" target="#b21">[22]</ref>, where half of the data are used in the first self-paced iteration and all data are used in the last iteration, we start with r 1 = 0.5 and we iterate for M = 4 iterations till r M = 1, linearly interpolating the intermediate increments <ref type="bibr">(Line 19)</ref>. Experiments with M = 5 obtained very similar results. All the other Fast-RCNN specific hyperparameters are the same used in <ref type="bibr" target="#b11">[12]</ref> for the fine-tuning of a pre-trained network, including the initial learning rate value (0.001), the size of M B (128), the weight decay (0.0005) and the momentum (0.9). No batch normalization is used and standard backpropagation with momentum is adopted. The only difference with respect to <ref type="bibr" target="#b11">[12]</ref> is that, in all our experiments, we divide (only once) the learning rate by a factor of 10 after the first self-paced iteration.</p><p>The reason for which we adopted the same hyperparameter values used in the supervised Fast-RCNN and we followed as strictly as possible the same design choices (e.g., how a mini-batch is computed, etc.) is that tuning the hyper-parameter values in a weakly supervised scenario is not easy because of the lack of validation data with BB-level ground truth. Moreover, in this way our training protocol can be more easily generalized to those WSD approaches which are based on the same Fast-RCNN architecture. All the above hyper-parameters (including those which are specific of our self-paced protocol, r 1 and M ) are kept fixed in our experiments on both Pascal VOC and ILSVRC.</p><p>Finally, in all our experiments, T is composed of the original images and their mirrored versions. No other data augmentation is performed.</p><p>Computational issues. From a computational point of view, the only additional demanding operation in our approach with respect to the Fast-RCNN training procedure is computing f (I, B(I)) for each I ∈ T , which involves passing I forward through all the layers of f . Fortunately, Fast-RCNN performs this operation in only ≈ 0.1 seconds per image (e.g., using a Tesla K40 GPU). For instance, with N = 20K, computing the latent boxes of all the images in T takes approximately 30 minutes. Note that this operation is repeated only M times during the whole training.</p><p>A simpler self-paced approach to train a Fast-RCNN is to fully train the network (for several epochs) with an initial, small "easy" dataset T 1 , then use the current network to compute the latent variables of a larger set T 2 , then fully fine-tune the network again, etc. However, this procedure is not only much slower than the epoch-based dataset update strategy we adopted (because it involves a full-training of the detector for each iteration), but it is also less effective. Our preliminary results using this approach showed that the network quickly overfits on the initial relatively small dataset T 1 and the final accuracy of the network is much lower than what we obtain using Alg. 1.</p><p>Initialization. The initial model W 0 can be obtained in different ways using only weakly-supervised annotation. Below we describe the steps we followed in our experiments on Pascal VOC and on ILSVRC as solution examples.</p><p>In both datasets we used a two-steps procedure: (1) training a Classification Network (CN) and (2) inspired by <ref type="bibr" target="#b21">[22]</ref>, where all the samples of the dataset are used for pretraining the classifier using a small number of iterations, we also pre-train the Detection Network (Fast-RCNN) using all the images of T for a fixed, small number of SGD iterations.</p><p>In case of the ILSVRC dataset, the CN is obtained following the steps suggested in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>: Starting from the AlexNet <ref type="bibr" target="#b20">[21]</ref>, pre-trained on ImageNet (1000 classes), we first fine-tune the network on the ILSVRC 2013 detection dataset <ref type="bibr" target="#b30">[31]</ref>, which is composed of C = 200 classes. This is done by removing the last layer from the AlexNet and replacing it with a 200-class output layer. For the fine-tuning we use a random subset of the train partition of ILSVRC 2013 (see Sec. 6), but we simulate a situation in which we have access to image-level labels only. We call h I this CN and W I CN its weights, where the superscript I stands for "trained on ILSVRC 2013". Note that h I takes as input a 227 × 227 image and outputs a 200-element score vector.</p><p>Using W I CN we initialize the Fast-RCNN architecture. To do so, the last layer needs again be removed and replaced by two (parallel) Fast-RCNN specific layers: a C + 1 classification layer and C × 4 regression layer <ref type="bibr" target="#b11">[12]</ref>. The weights of this layers are randomly initialized. Then, we train the Fast-RCNN Detection Network (DN) for 30K SGD iterations using all the images in T , where T is the val1 split of ILSVRC 2013 (see Sec. 6), mirrored images included. Since Fast-RCNN is a DN and needs BB-level annotation for training, we associate the images in T with a pseudo-ground truth by collecting the top-score boxes obtained using h I . More in detail, for each (I, Y ) ∈ T and each box b ∈ B(I), we rescale b to 227 × 227 (u(b)) and, for each y ∈ Y = {y 1 , ..., y k }, we compute:</p><formula xml:id="formula_7">z y = arg max b∈B(I) h I (u(b), y),<label>(4)</label></formula><p>where h I (·, y) is the y-class score. The final pseudo groundtruth corresponding to</p><formula xml:id="formula_8">I is G = {(y 1 , z y1 )..., (y k , z y k )} (see Sec.</formula><p>3). We call this training protocol Init and you can think of it as a one-shot MIL solution with only one iteration over the latent variables (i.e., the latent boxes are not recomputed while the network is trained and are kept fixed). Note that there is no classifier competition or class selection in Init and we use all the samples in T , inspired by Kumar et al. <ref type="bibr" target="#b21">[22]</ref>, confirming that this strategy leads to a good initialization for a self-paced learning approach. At the end of Init, we call the final network's weights W I 0 and we use W I 0 as input in Alg. 1.</p><p>In case of Pascal VOC we use a similar strategy and we train different CNs for different experiments, using as basic architecture either AlexNet or VGG-16 <ref type="bibr" target="#b35">[36]</ref>. Firstly, we simply use the above-mentioned h I trained on ILSVRC 2013: the weights (W I CN ) of h I are directly used to initialize the Fast-RCNN architecture (except the last randomly initialized layers, see above a surprisingly good accuracy achieved after the proposed self-paced training procedure (see Sec. 6).</p><p>In case of Pascal VOC, we also fine-tune a second CN, using only Pascal VOC 2007 trainval. Also in this case the basic network architecture is AlexNet, pre-trained on ImageNet (1000 classes). However, since Pascal VOC 2007 trainval is a much smaller dataset than the ILSVRC 2013 train split and, on average, a Pascal VOC image contains more objects with different-labels than an ILSVRC 2013 image <ref type="bibr" target="#b12">[13]</ref>, care should be taken in training a CN directly on Pascal VOC. For this reason we adopted the approach proposed in <ref type="bibr" target="#b24">[25]</ref> for training a CN on a multi-label dataset, where the authors replace the network softmax loss with a multi-label loss based on a 2C binary element vector label. The trained CN (h P ) and the corresponding weights (W P CN ) are used to collect pseudo-ground truth data and to initialize the Fast-RCNN for the Init stage (see above), using the same number (10K) of SGD iterations and the final weights are called W P 0 . Finally, in Sec. 6 we also show two experiments (Tab. 5 and Tab. 7) in which the basic architecture is VGG-16 and the initialization procedure is the same followed in case of W P 0 . To simplify our notation, we call the VGG-16 based initialization W P 0 as well and we will explicitly specify when the basic architecture is not AlexNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>As mentioned in Sec. 1, the goal of the proposed self-paced strategy is to discard noisy data in order to decrease the drifting problem in a MIL framework. Hence, the training protocol described in Alg. 1 can be seen as a "prudent" strategy, where "good" ("easy") data are preferred to lot of data. Moreover, Alg. 1 can terminate without using all T . In fact, in our experiments this often happens, and it is mainly due to the inter-classifier competition constraint. This is apparently in contrast with the common deep-learning training practice, where the trend is to use as much data as possible. However, in a WSD scenario, adding data which are wrong (noisy) most likely does not improve the training quality. For instance, <ref type="figure">Fig. 3</ref>, first column, shows the class-specific top score box z y for a set of random image samples, computed taking into account the corresponding image label (y) and without inter-classifier competition. In a typical MIL-like approach z y is selected as pseudo-ground truth, and in most of the cases these data are very noisy. Conversely, our selfpaced learning strategy leads to select (and use) z y only in later stages, most of the times decreasing the overall amount of noise (see <ref type="figure">Fig. 3</ref>, columns 1-4).</p><p>It is also worth noticing that our choice of using T t for only one epoch of SGD-training is important to avoid overfitting, especially when N t = |T t | is initially small (see the Computational issues paragraph).</p><p>Finally, the sequence of datasets T 1 , ...T M is not monotonic, meaning that an image I ∈ T t1 can be discarded when T t2 is created (with t 1 &lt; t 2 ). Empirically we observed frequent oscillations in the class-specific Average Precision (AP) when training both on Pascal VOC 07 and on ILSVRC 2013. However, the overall mean AP is almost monotonic with respect to the self-paced iterations, with some small oscillations (typically less than 1%), showing that the combined effect of weight sharing among classes and the com-mon background class (whose samples are included in each mini-batch, independently of the positive class y) leads the network as a whole to benefit from the progressive increase of training data through time. In Sec. 7.1 we analyse this progressive behaviour of our networks and we contrast it with respect to other MIL-like solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DETECTION PERFORMANCE</head><p>In this section we compare our method with other WSD approaches using the two most common WSD datasets (Pascal VOC and ILSVRC 2013). Comparing to each other different methods developed in the past years is not easy due to their heterogeneity, which has been increased after the introduction of deep-learning approaches. For instance, some methods <ref type="bibr" target="#b7">[8]</ref> use SVMs and hand-crafted features, others <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref> use SVMs and pre-trained CNNfeatures obtained with the AlexNet and very recently a few deep learning approaches based on an end-to-end training <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b39">[40]</ref> have been introduced (including ours). Moreover, the latter usually present results obtained with small (e.g. AlexNet <ref type="bibr" target="#b20">[21]</ref>), medium (e.g. VGG-CNN-M-1024 from <ref type="bibr" target="#b5">[6]</ref>) or large (e.g. VGG-16 <ref type="bibr" target="#b35">[36]</ref>) capacity networks or using an ensemble of three or more architectures.</p><p>In order to have a comparison which is the most fair as possible, we separate methods based on low-capacity, AlexNet-like networks from those based on higher-capacity, VGG-16-like architectures and we mark those approaches which use ensembles of networks. In the rest of the paper, if not otherwise explicitly specified, the basic architecture used for our experiments (initialization included) is AlexNet.</p><p>The ILSVRC 2013 detection dataset <ref type="bibr" target="#b30">[31]</ref> is a standard benchmark for object detection. It is partitioned in 3 main subsets: train, val and test. The train images are more objectcentric (one or very few objects per image on average) and represent more classification-style data than the images in the other 2 partitions <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. All the images of the ILSVRC 2013 detection dataset are annotated with objectlevel ground truth that we do not use. We use only the labels of the objects contained in each image (where each label ranges over C = 200 classes). Girshick et al. <ref type="bibr" target="#b12">[13]</ref> split val further in val1 and val2 and use at most 1000 randomly selected images per category from train. We use the same approach and ≈ 200K randomly selected images from train were used to fine-tune AlexNet and obtain h I as explained in Sec. 4 and following the procedure suggested in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Then, we use only val1 as our set T (N ≈ 20K, after image mirroring). T is the training dataset used both in Init and in Alg. 1. Finally, we evaluate on val2, whose cardinality is ≈ 10K (the test images are not mirrored). Note that this is a broadly adopted protocol, both for supervised (e.g., <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>) and weakly or semi-supervised (e.g., <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b24">[25]</ref>) object detection experiments.</p><p>Once our network is trained using Init and Alg. 1, it is used as a standard detector at testing time. In other words, given a test image I, we apply Non-Maxima Suppression on f (I, B(I)) as in the original Fast-RCNN approach <ref type="bibr" target="#b11">[12]</ref> and in this way we obtain multiple, spatially separated detections per category on I and we can compute Average Precision (AP) and mean Average Precision (mAP) following the standard object detection protocol <ref type="bibr" target="#b9">[10]</ref>.</p><p>In Tab. 1 we compare our approach with the previously published WSD results on ILSVRC 2013: Wang et al. <ref type="bibr" target="#b41">[42]</ref> and Li et al. <ref type="bibr" target="#b24">[25]</ref>. Our method largely outperforms the state of the art in this large dataset. Note that Li et al. <ref type="bibr" target="#b24">[25]</ref> report other results obtained using a VGG network <ref type="bibr" target="#b35">[36]</ref>: 10.8 mAP, which is lower than what we obtained with our self-paced training protocol using a much smaller network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>End   <ref type="bibr" target="#b24">[25]</ref> are computed on the val2 split.</p><p>Pascal VOC 2007 <ref type="bibr" target="#b9">[10]</ref> is another well known benchmark for object detection and it is widely used by different WSD methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>. The number of classes (C) is 20. We adopted the common training-testing protocol in which training is done on the trainval split and testing is performed on the test split. Hence, T is trainval in which only image-level labels were used. Since trainval is highly imbalanced toward the person class, we have subsampled this class, selecting the 262 topscore images used in case of Init. Since we used different CNs to initialize the weights of our network (see Sec. 4), we report results obtained using W 0 = W I,P 0 and W 0 = W P 0 in Alg. 1, respectively. The results are shown in Tab. 2, where all the methods (including ours) use AlexNet (or an AlexNet-like) as the basic architecture. Using a CN trained on ILSVRC (h I , see Sec. 4), the final mAP reached after the self-paced training (Alg. 1) is 32.1, which is comparable with other state-of-the-art WSD methods. Using a stronger initialization, in which the CN is trained using a multi-label loss <ref type="bibr" target="#b24">[25]</ref> directly on Pascal VOC (h P , Sec. 4), the final mAP after the self-paced training is 38.11, which is higher than all the other weakly supervised approaches tested on this dataset.</p><p>In order to show how large is the gain that can be obtained using the proposed self-paced training protocol, we compare the detection performance achieved when using only Init for fine-tuning the Fast-RCNN (see Sec. 4) with the performance obtained after the self-paced phase (SP) described in Alg. 1. Since Init is used to initialize Alg. 1, the accuracy difference shows the boost obtained by the selfpaced strategy. The results, reported in Tab. 4, show that, independently of the CN used to initialize Init, in both cases the SP-based boost over the Init phase is dramatic: +7.36 mAP in case of h I and +6.18 in case of h P . Moreover, since we trained h P as proposed by Li et al. <ref type="bibr" target="#b24">[25]</ref>, it is also interesting to directly compare the results we obtained using h P and the results obtained in <ref type="bibr" target="#b24">[25]</ref>, where a detection network is trained on top of the same CN. Our "simple" Init-based detector achieves a slightly better mAP (31.93) than the mAP (31) achieved by the DN proposed in <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>End-to-end aero bike bird boat bottle bus car cat chair cow  <ref type="bibr" target="#b33">[34]</ref> √     In Tab. 3 we report the CorLoc computed on the trainval split, which is a common metric adopted in many WSD approaches tested on Pascal VOC 07. Our CorLoc result corresponding to the W P 0 -based initialization is the second best after the values obtained by Shi et al. <ref type="bibr" target="#b33">[34]</ref>.</p><formula xml:id="formula_9">- - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_10">[34] √ - - - - - - - - - - - - - - - - - - - -</formula><p>Finally, in Tab. 5 we report the results obtained using a larger capacity network. Specifically, we use VGG-16 <ref type="bibr" target="#b35">[36]</ref> and we compare with those works which use the same (single) network. In our case, VGG-16 is used both at initialization time, where we train a CN (h P ) following the steps reported in Sec. 4, and for the Fast-RCNN DN, trained using our self-paced protocol (Init + SP ). As shown in Tab. 5, we obtain state-of-the-art results, largely outperforming the other methods. The importance of this experiments relies on the fact that it demonstrates that our self-paced protocol is not inclined to overfitting with a large-capacity network.</p><p>Pascal VOC 2010 <ref type="bibr" target="#b10">[11]</ref>. In Tabs. 6-7 we show the results obtained with the Pascal VOC 2010 test set. Tab. 6 refers to methods using an AlexNet-like network, while Tab. 7 refers to methods using a VGG-16 or an ensemble of VGG-16 and other networks. In both cases, our results are the state of the art. Specifically, our VGG-16 based results are even higher than previous results obtained with an ensemble of 3 networks [5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ANALYSIS OF DIFFERENT ASPECTS OF THE PRO-TOCOL</head><p>In this section we analyse the influence of different elements of our proposed training protocol by separately removing or modifying important parts of Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Simplified versions of the training protocol</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic-MIL.</head><p>In the experiments of this subsection we use both Pascal VOC 07 and ILSVRC 2013. We start with comparing our method (Self-Paced, SP) with a MIL-based solution (MIL), where: (a) all the images in T are used and (b) in each image the latent boxes are computed by iteratively maximizing the class-specific score of the current     <ref type="bibr" target="#b24">[25]</ref> -  iteration's model. Thus, we remove from Alg. 1 all those steps which concern image (and class) selection. Moreover, we also remove the inter-classifier competition, and we independently select the top score box for each label in Y . More in detail, given (I, Y ) ∈ T , for each y ∈ Y we separately compute:</p><formula xml:id="formula_11">[25] - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_12">- - - - - - - - - - - - - - - - - - -</formula><p>(s y , z y ) = arg max (sic,pic)∈f W t−1 (I,B(I)), c=y s ic</p><p>Note that Eq. 5 is different from Eq. 2 because only yspecific scores are taken into account. For instance, an image I associated with 3 different labels (Y = {y 1 , y 2 , y 3 }) will produce 3 corresponding class-specific latent boxes ({z y1 , z y2 , z y3 }) using Eq. 5 and iterating over y ∈ Y , but at most only one latent box z I y using Eq. 2 and the interclassifier competition constraint (see Sec. 4). In MIL we use Eq. 5 to associate each I with the set of its pseudo-ground truth boxes (I, {(y, z y )} y∈Y ), which are directly added to T t , skipping Lines 4-12. Note also that Eq. 5 is based on the current network f Wt−1 , including the regression part, which is used at each iteration t to compute the set {p ic } over which z y is selected. Thus, both in MIL and in SP the bag of boxes associated with I dynamically changes at each iteration t. Finally, note that MIL is different from Init, since in the latter the pseudo-ground truth is computed only once and using the CN. However, both in MIL and in Init, N t = N = |T | is kept fixed in every iteration t and no score-based image selection or class selection is done, hence we use all the images for training.</p><p>Curriculum. In contrast, we call Curriculum a simplified version of Alg. 1 in which information about "easiness" of images and boxes is given externally to the trained DN. The purpose of this experiment is to show the behaviour of a simplified progressive-image-selection protocol in which image selection is performed "statically" using the scores computed by the CN, as opposed to SP, where selection is performed "dynamically" using the current DN f Wt−1 .</p><p>In Curriculum we use the same pseudo-ground truth used in Init and we select images according to the static score values initially computed by the CN. Specifically, we use the boxes and the corresponding scores computed when we build the the training set of Init (Sec. 4): For each (I, Y ) ∈ T and for each y ∈ Y , (s y , z y ) is obtained using Eq. 4 and the CN (h I and h P for ILSVRC and Pascal, respectively). Thus, T is the same used in Init. However, at each iteration t, T is sorted using the pre-computed scores (s y ) and a subset T t is extracted from T as in Lines 10-11. When we select T t we use the same ratio sequence r 1 , ...r t , ...r M used in SP (see Alg. 1). However, no inter-classifier competition and thus no class-selection is used in Curriculum. Images associated with multiple labels are sorted according to their highest score label. Note that in this experiment, despite the values of the latent boxes and their scores are fixed, the model will observe more and more data (from "easy" to "difficult" images) while t increases.</p><p>The results are shown in Tab. 8 and 9 for the two datasets, where we report the mAP for different networks f Wt obtained at the end of different iterations t and for each of the training protocols. For all the training procedures M = 4 and r t+1 is computed as in Line 19 (when it applies). However, in the last column of the table we show the results obtained iterating all the protocols for one more iteration (using r M +1 = 1). The accuracy impact of the last iteration is generally marginal. The first column of the table (W 0 ) is the same for all the methods as it is the evaluation of Init, which is used as the pre-trained model for all the protocols. The results show that SP is able to increase its accuracy during time, confirming the self-paced assumption of a model which progressively becomes more mature and, as a consequence, is more and more reliable when it computes the values of its latent variables.</p><p>On the other hand, the other two simplified solutions do not really seem to be able to improve over time. In both datasets MIL achieves a final mAP even worse than Init (both when t = M and when t = M + 1). This is probably due to a drifting effect: At time t all the images in T are used for training and a large portion of them are noisy (i.e., are associated with wrong latent boxes). Thus W t is a weak model and it is used to compute T t+1 , most likely producing many errors that will be accumulated over time. In SP this problem is alleviated because only the "best" images in T t are used to train W t . We do not claim that any MIL-based solution should drift, and, for example, Cinbis et al. <ref type="bibr" target="#b8">[9]</ref> combined a standard MIL approach with a multifold strategy, observing a progressively increasing accuracy in their experiments. However, the baseline we adopted in our experiments, which is based on the "standard" MIL characteristics (see points (a) and (b) above), embedded in our framework, did not show significant progresses in the two datasets we used for the evaluation, confirming the difficulty of making MIL-like methods work in practice.</p><p>On the other hand, sample selection is performed in Curriculum, but, similarly to MIL, the mAP values oscillate without improving with respect to the starting point. This is probably due to the fact that there can be little progress if the model cannot update the initial predictions done by the CN. Moreover, the sample selection strategy in SP takes also into account the inter-classifier competition, prudently discarding those images in which the current model is uncertain, a constraint which is not used in Curriculum. In Sec. 7.3 we present another experiment showing the precision of the latent boxes computed by SP, which further confirms this hypothesis.</p><p>Class selection. In the same tables we show the results of SP-all-cls, obtained from SP by removing only the classselection part (Lines 6-9) and keeping all the rest unchanged (inter-classifier competition included). In both the datasets SP-all-cls achieved a lower final mAP with respect to SP, but this is more evident in the case of Pascal. In the latter dataset we performed an additional experiment, called SP-rnd-cls, where class selection is performed randomly, in such a way that the number of classes (C t , see Line 7 of Alg. 1) is the same used in SP and the results are reported in Tab. 8. SPrnd-cls is able to improve with respect to the initialization but, as expected, the final mAP is much lower than the corresponding mAP obtained with SP-all-cls. The reason of this behaviour is most likely due to the fact that, randomly discarding easy classes and the corresponding correct samples, has a negative effect on the final performance. Comparing the category-specific AP obtained with Init in ILSVRC with the corresponding AP obtained by SP (see Sec. A), 147 classes out of 200 (73.5%) have improved. This demonstrates that the combination of class-selection and interclassifier competition in SP does not prevent most of the classes to improve and that the learning process is not dominated by few strong classifiers. Finally, we list below the 10 classes selected when t = 1 in Pascal in descending order: S = {aeroplane, sheep, train, cow, bird, cat, boat, horse, motorbike, car}. As expected, this set includes those "classifiers" which are already strong enough both in h P and in f W P 0 , and it is largely independent of the cardinality of corresponding class samples in T .</p><p>Regression. The last part of this ablation study is dedicated to the importance of the regression part. In SP, z I y is selected over the set {p ic }, computed using the regression layer of Fast-RCNN. Choosing z I y over the BBs in B(I) at training time but using the regression layer at testing time, we obtain the results reported in Tab. 8 (No-reg-train). Note that we can use the regression layer at testing time because the weights of this layer are trained using the pseudo-ground truth, though that layer is not used for selecting z I y . Conversely, in No-reg-train-test we disable the regression layer also at testing time (in this case f W0 does not correspond to Init, being the results of Init obtained using the regression part). As shown in Tab. 8 there is a large accuracy gap with respect to SP. We repeated the Noreg-train-test experiment on ILSVRC obtaining a similar very large accuracy gap: a final mAP of 7.57, 4.56 points less than our best result on that dataset. These accuracy differences show the importance of the proposed iterative strategy in which the current network is used to compute the supposed locations of the objects inside the training images.   </p><formula xml:id="formula_14">Method W 0 W 1 W 2 W 3 W 4 W 5<label>MIL</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Multi-label versions of the training protocol</head><p>This subsection is dedicated to evaluating the importance of the inter-classifier competition. As explained in Sec. 4 the inter-classifier competition is used in SP to reduce the amount of noisy training boxes by selecting only one box z I y per image I, according to the current most confident classifier (y) on I. A disadvantage of this strategy is the sacrifice of possible other "good" boxes associated with either other classes (y = y, y ∈ Y ) or the same class. For instance, the latter situation occurs when more than one instance of the category person is contained in I (recall that we only know that person ∈ Y , without any information about the cardinality of the instances for each image). We report below our study of two different versions of SP: SP-SIML and SP-MIML, where we relax the inter-classifier competition for a multiple-label and a multiple-instance scenario. Multiple-label. SP-SIML is a Single-Instance-Multiple-Label version of Alg. 1, where in each image we mine one box per each label in Y . Similarly to MIL, given (I, Y ) ∈ T , for each y ∈ Y we use Eq. 5 to compute a set of candidate boxes associated with I: P I = (I, {(s y , z y , y)} y∈Y ). However, before adding P I to P , in Line 5 we remove from P I those boxes which have a score lower than the top-score box of all the other categories Y = {1, ..., C}\Y . Y is the complement of Y (it contains the set of all the labels not included in Y ) and it is used to compute s o y :</p><formula xml:id="formula_15">(s o y , z o y ) = arg max (sic,pic)∈f W t−1 (I,B(I)), c∈Y s ic<label>(6)</label></formula><p>The value s o y is used to prune from P I all those elements whose score s y is lower than s o y . The intuitive idea behind this procedure is that we relax the inter-classifier competition, collecting more boxes in those images associated with multiple labels (there is no competition among classes contained in Y ). However, we impose that a classifier for class y, y ∈ Y , should be more confident than the strongest noisy classifier firing on z o y . Note also that we do not need to modify the class-selection strategy because e(c) (Line 6) can be computed using Eq. 3 which applies also to the case in which a given image I ∈ P is associated with multiple boxes. Intuitively, in SP-SIML, both the class selection and intra-image box pruning are based on the competition between classifiers in Y and classifiers in Y . Apart from Lines 3-5, modified as explained above, SP-SIML is identical to SP, including score-based image selection.</p><p>Multiple-label and multiple-instance. SP-MIML is a Multiple-Instance-Multiple-Label extension of SP-SIML, where more than one box of the same category y ∈ Y can be collected from the same image I. SP-MIML is obtained applying standard Non-Maxima Suppression (NMS) over f Wt−1 <ref type="figure">(I, B(I)</ref>) and then, for each y ∈ Y , keeping all those boxes whose value is higher than s o y (thus, no ad hoc, manually tuned threshold on the box scores after NMS needs to be used). Also in this case, we use both scorebased image selection (Lines 10-11) and class-selection and we modify only Lines 3-5 in Alg. 1 to obtain SP-MIML.</p><p>We test SP-SIML and SP-MIML on Pascal VOC 2007 because Pascal VOC images contain more objects and more labels on average with respect to ILSVRC 2013. In Tab. 10 we show the results which have been obtained using W P 0 as the initialization and using the same hyper-parameter values used in the other experiments (see Sec. 4).  Unexpectedly, SP largely outperforms both SP-SIML and SP-MIML, despite more data are used in the latter two versions (because more boxes on average are collected from a single image I). Our interpretation of this result is that the inter-classifier competition is very important for a prudent choice of the latent boxes, forcing the system to select a box z I y only when its classification confidence s I y is very high. To confirm this hypothesis, we computed the average CorLoc over all the self-paced iterations and using SP, SP-SIML and SP-MIML. Consistently in all M = 4 iterations the CorLoc of both SP-SIML and SP-MIML is much smaller than the corresponding values obtained with SP, which means that the mined pseudo-ground truth is less precise. A visual inspection of randomly selected images further confirmed that most of the times the additional boxes selected in both SP-SIML and SP-MIML with respect to z I y are inaccurately localized or completely wrong. However, although the multi-label extensions of SP underperformed with respect to SP, their final mAP is comparable with the state of the art on Pascal VOC 07 (see Tab. 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Precision of the selected subsets of training data</head><p>In this subsection we evaluate the number of "correct" samples selected for training the network. To this aim we adopt the evaluation protocol suggested in <ref type="bibr" target="#b16">[17]</ref>, where the authors use ILSVRC 2013 val1 and a Precision metric. The latter is similar to CorLoc, the difference being that in CorLoc one latent box (z y ) is computed for each label y ∈ Y associated with a training image, while Precision is based on extracting one single latent box (z I y ) per image. Using Precision @0.5 IoU we can measure the quantity of latent boxes actually used during training which sufficiently overlap with a real ground truth box with the correct class.</p><p>In Tab. 11 we show the results, where Precision is the percentage of correct image samples over all the images included in the training set T t . In case of <ref type="bibr" target="#b16">[17]</ref>, Precision is computed with respect to the whole val1 because no subset selection is done in that work. T 1 in Tab. 11 is the dataset obtained with the initialization model W 0 and used to train W 1 , while T 4 is the dataset obtained with W 3 and used in the last iteration to train W 4 . As shown in the table, Precision in T 4 is largely improved with respect to Precision in T 1 . Precision in T 4 is much higher than the Precision obtained by <ref type="bibr" target="#b16">[17]</ref>, even when object-level annotation for 100 over 200 categories is used as auxiliary data during training.</p><p>In <ref type="figure">Fig. 3-4</ref> we show the evolution of the class-specific top-score box z y for the same image I over the four selfpaced iterations. The new localizations usually improve with respect to the previous ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>We proposed a self-paced learning based protocol for deep networks in a WSD scenario, aiming at reducing the amount of noise while training the DN. Our training protocol extends the self-paced learning paradigm by introducing:</p><p>(1) inter-classifier competition as a powerful mechanism to reduce noise, (2) class-selection, in which the easiest classes are trained first, and (3) the use of the Fast-RCNN regression layer for the implicit modification of the bag of boxes.</p><p>While in the past self-paced learning strategies have been successfully adopted for other classifier types (mainly SVMs), we are the first showing that this paradigm can be successfully utilized also for an end-to-end training of deep networks. Despite the reduced sizes of the initial training subsets, typical in a self-paced strategy, we have empirically shown that our training protocol not only does not suffer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Precision (IoU &gt; 0.5) Hoffman et al. <ref type="bibr" target="#b16">[17]</ref> without auxiliary strongly supervised data 26.10 Hoffman et al. <ref type="bibr" target="#b16">[17]</ref> with auxiliary strongly supervised data 28.81 SP (T 1 ) 20.55 SP (T 4 ) 37. <ref type="table" target="#tab_3">01   TABLE 11</ref>: Precision of the selected boxes used for training. In SP the Precision value is computed over the elements in T t , which is a subset of ILSVRC 2013 val1, while in <ref type="bibr" target="#b16">[17]</ref> Precision is computed over the whole val1. However, the comparison is fair because, differently from <ref type="bibr" target="#b16">[17]</ref>, we do not use the whole val1 for training but only the subset T t , thus the quality of the training boxes should be compared with only those samples actually used for training. <ref type="figure">Fig. 3</ref>. Qualitative results: visualizations of the class-specific top-score box zy in the four self-paced iterations (chronologically ordered from left to right) with respect to different training images and labels y (leftmost column). <ref type="figure">Fig. 4</ref>. Other qualitative results in which the evolution over time of the class-specific top-score box (zy) of the network did not succeed in localizing the true objects into the images.</p><p>for overfitting, but it benefits from the reduced noise, by largely boosting the final accuracy with respect to different initialization methods.</p><p>Using the proposed training protocol we achieved stateof-the-art results on common WSD benchmarks: ILSVRC 2013, Pascal VOC 2007 and VOC 2010.</p><p>Finally, we presented a detailed analysis of the main components of our proposed training protocol, comparing SP with both simplified and more sophisticated versions of the same approach, with the goal of showing the importance of our design choices and to allow other authors to build on our method, possibly choosing those components which best fit with other application scenarios. Enver Sangineto is a research fellow at University of Trento, Department of Information Engineering and Computer Science. He received his PhD in Computer Engineering from the University of Rome "La Sapienza". After that he has been a post-doctoral researcher at "La Sapienza" and at the Italian Institute of Technology in Genova. His research interests include object detection and learning with minimal human supervision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moin</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>In Tab. 12 we show the per-category AP obtained by our method on ILSVRC 2013 val2. Similarly to Sec. 7.1, we analyse 5 different nets (W 0 , ...W 4 ), corresponding to the 4 self-paced iterations of Alg. 1 plus the initialization model (W 0 = W I 0 , i.e., Init), used as a comparison. This is done in order to show that most of the categories progressively improve during training and that this improvement is generalized and not dominated by a few categories. In fact, the AP of 147 out of 200 categories increases when evaluated using f W4 with respect to the evaluation obtained using the initial model f W0 (see Sec. 7.1). This is not a trivial result, because the combination of class-selection and inter-classifier competition in SP could potentially lead the sample selection process to be dominated by a few strong categories in f W0 . For instance, categories like antelope or fox, which have AP 22.09 and 22.61 using f W0 , respectively, are already strong in the beginning of the learning process and could dominate the selection of new samples in T 1 , T 2 , .... Conversely, initially very weak classifiers like cream or oboe (AP 3.06 and 1.51 using f W0 , respectively) could be penalized because not able to win the inter-classifier competition or because excluded by the class-selection process. However, our empirical results show that this harmful domination of the initial strong classifiers does not happen and that learning is spread over most of the categories. We believe that this is due to the fact that good classifiers (e.g., antelope) do not have high scores in an image showing an oboe, because most of the BBs of the oboe's image (background boxes included) have an appearance different from an antelope. Thus even a weak classifier can win the competition on its own samples and "gain" new samples to add to the next training set T t which will finally lead to the improvement the weak classifier.   <ref type="figure">Init)</ref>, used as a comparison. This is done in order to show that most of the categories progressively improve during training and that this improvement is generalized and not dominated by a few categories. In fact, as mentioned in the main paper, the AP of 147 out of 200 categories increases when evaluated using f W4 with respect to the evaluation obtained using the initial model f W0 (see Sec. 7.1 of the main paper). This is not a trivial result, because the combination of class-selection and inter-classifier competition in SP could potentially lead the sample selection process to be dominated by a few strong categories in f W0 . For instance, categories like antelope or fox, which have AP 22.09 and 22.61 using f W0 , respectively, are already strong in the beginning of the learning process and could dominate the selection of new samples in T 1 , T 2 , .... Conversely, initially very weak classifiers like cream or oboe (AP 3.06 and 1.51 using f W0 , respectively) could be penalized because not able to win the inter-classifier competition or because excluded by the class-selection process. However, our empirical results show that this harmful domination of the initial strong classifiers does not happen and that learning is spread over most of the categories. We believe that this is due to the fact that good classifiers (e.g., antelope) do not have high scores in an image showing an oboe, because most of the BBs of the oboe's image (background boxes included) have an appearance different from an antelope. Thus even a weak classifier can win the competition on its own samples and "gain" new samples to add to the next training set T t which will finally lead to the improvement the weak classifier.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A schematic illustration of how the training dataset Tt (represented by the green rectangle) of our deep network evolves depending on t and on the progressively increasing recognition skills of the trained network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Inter-classifier competition. The two above figures show the behaviour of our self-paced learning algorithm on the same image I in two different iterations (t = 1 and t = 2). In both cases, the green box shows the highest-score box in I corresponding to z I y in Eq. 2. Conversely, the red box in the left figure shows the highest-score box in I corresponding to the car class (zcar). Since in t = 1 (left figure) s T V = s I y &gt; scar, and since T V ∈ Y , then I is not included in T 1 (see also Fig. 1). However, in t = 2 (right figure) s T V &lt; s I y = scar (where car ∈ Y ), thus (I, scar, zcar, car) is included in P (line Line 5 in Alg. 1): the "car" classifier in this iteration is strong enough and "wins" in I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>e(c) = |{(I, s, z, y) : (I, s, z, y) ∈ P, y = c}|/p c (3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Nabi is a Senior Research Scientist at SAP Machine Learning Research in Berlin. Before that, he was a postdoctoral research fellow in the Deep Relational Learning group at the University of Trento and a visiting researcher in the GRAIL lab at the University of Washington. He holds a PhD from the Italian Institute of Technology. His research lies at the intersection of machine learning, computer vision, and natural language processing with an emphasis on learning Deep Neural Networks with minimal supervision and/or limited data. Dubravko Culibrk is a Professor at the University of Novi Sad, Serbia and the director of the Industry/University Center for Collaborative Research of the Faculty of Technical Sciences. His interests are in the domain of computer vision, machine learning and knowledge discovery. Prof. Culibrk received his B.Eng. degree in Electrical Engineering and an M.Sc. degree in Computer Engineering from the University of Novi Sad, Serbia, in 2000 and 2003, respectively, as well as a Ph.D. degree in computer engineering from Florida Atlantic University, Boca Raton, USA, in 2006. Nicu Sebe is Professor with the University of Trento, Italy, leading the research in the areas of multimedia information retrieval and human behavior understanding. He was the General Chair of the IEEE FG Conference 2008, ACM Multimedia 2013 and ICMR 2017, and the Program Chair of the International Conference on Image and Video Retrieval in 2007 and 2010, ACM Multimedia 2007 and 2011, ECCV 2016 and ICCV 2017. He is the Program Chair of ICPR 2020. He is a fellow of the International Association for Pattern Recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). Moreover, the pseudo-ground truth for the Init stage is collected using Eq. 4 with an amputated version of h I , obtained by removing 180 over 200 output neurons and keeping only those f c8 original neurons roughly corresponding to the 20 Pascal VOC classes 2 . We run Init for 10K SGD iterations using the trainval split of Pascal VOC 2007 (see Sec. 6) as our T . We call W I,P 0 the final network's weights because they are obtained using a hybrid solution, based on training images from both ILSVRC 2013 and Pascal VOC 2007. Note that, due to the differences between ILSVRC and Pascal VOC in both the corresponding image distributions and the object classes [1], [13], [20], using h I to initialize Init corresponds to a quite weak initialization. Despite this, our experimental results show 2. Specifically, the adopted ILSVRC → VOC class mapping is: airplane → aeroplane, bicycle → bicycle, bird → bird, watercraft → boat, wine bottle → bottle, bus → bus, car → car, domestic cat → cat, chair → chair, cattle → cow, table → diningtable, dog → dog, horse → horse, motorcycle → motorbike, person → person, flower pot → pottedplant, sheep → sheep, sofa → sofa, train</figDesc><table /><note>→ train, tv or monitor → tvmonitor.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>Quantitative comparison (mAP %) on the ILSVRC 2013 detection dataset. All the methods are based on a single, low-capacity network (AlexNet). Wang et al.<ref type="bibr" target="#b41">[42]</ref> use AlexNet to extract CNN-features from the BBs but do not perform end-to-end training. ( * ) Note that the result of Wang et al.<ref type="bibr" target="#b41">[42]</ref> is obtained on the whole ILSVRC 2013 val set, while our result and the result of Li et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>table dog horse mbike persn plant sheep sofa train tv mAP Song et al. [37] 27.6 41.9 19.7 9.1 10.4 35.8 39.1 33.6 0.6 20.9 10 27.7 29.4 39.2 9.1 19.3 20.5 17.1 35.6 7.1 22.7 Song et al. [38] 36.3 47.6 23.3 12.3 11.1 36 46.6 25.4 0.7 23.5 12.5 23.5 27.9 40.9 14.8 19.2 24.2 17.1 37.7 11.6 24.6 Bilen et al. [3] 42.2 43.9 23.1 9.2 12.5 44.9 45.1 24.9 8.3 24 13.9 18.6 31.6 43.6 7.6 20.9 26.6 20.6 35.9 29.6 26.4 Bilen et al. [4] 46.2 46.9 24.1 16.4 12.2 42.2 47.1 35.2 7.8 28.3 12.7 21.5 30.1 42.4 7.8 20 26.8 20.8 35.8 29.6 27.7 Cinbis et al. [9] 39.3 43 28.8 20.4 8 45.5 47.9 22.1 8.4 33.5 23.6 29.2 38.5 47.9 20.3 20 35.8 30.8 41 20.1 30.2 Wang et al. ( * ) [42] 48.9 42.3 26.1 11.3 11.9 41.3 40.9 34.7 10.8 34.7 18.8 34.4 35.4 52.7 19.1 17.4 35.9 33.3 34.8 46.5 31.6 Li et al. [25] √ 49.7 33.6 30.8 19.9 13 40.5 54.3 37.4 14.8 39.8 9.4 28.8 38.1 49.8 14.5 24 27.1 12.1 42.3 39.7 31.0 Bilen et al. [5] √ 42.9 56 32 17.6 10.2 61.8 50.2 29 3.8 36.2 18.5 31.1 45.8 54.5 10.2 15.4 36.3 45.2 50.1 43.8 34.5 Shi et al. ( * * )</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2 :</head><label>2</label><figDesc>Quantitative comparison (AP %) on the Pascal VOC 2007 test set. All the methods are based on a single network with a capacity comparable with AlexNet, which is used either as a black-box to extract CNN-features or to perform end-to-end training (see the corresponding column). Note that: ( * ) Wang et al.<ref type="bibr" target="#b41">[42]</ref> specifically tune the number of latent categories for each class and ( * * ) Shi et al.<ref type="bibr" target="#b33">[34]</ref> use additional data (the Pascal VOC 2012 dataset) with BB-level annotation at training time.MethodEnd-to-end aero bike bird boat bottle bus car cat chair cowtable dog horse mbike persn plant sheep sofa train tv avg Bilen et al. [4] 66.4 59.3 42.7 20.4 21.3 63.4 74.3 59.6 21.1 58.2 14 38.5 49.5 60 19.8 39.2 41.7 30.1 50.2 44.1 43.7 Cinbis et al. [9] 65.3 55 52.4 48.3 18.2 66.4 77.8 35.6 26.5 67 46.9 48.4 70.5 69.1 35.2 35.2 69.6 43.4 64.6 43.7 52.0 Wang et al.</figDesc><table /><note>( * ) [42] 80.1 63.9 51.5 14.9 21 55.7 74.2 43.5 26.2 53.4 16.3 56.7 58.3 69.5 14.1 38.3 58.8 47.2 49.1 60.9 48.5 Li et al. [25]√ 77.3 62.6 53.3 41.4 28.7 58.6 76.2 61.1 24.5 59.6 18 49.9 56.8 71.4 20.9 44.5 59.4 22.3 60.9 48.8 49.8 Bilen et al. [5]√ 68.5 67.5 56.7 34.3 32.8 69.9 75 45.7 17.1 68.1 30.5 40.6 67.2 82.9 28.8 43.7 71.9 62 62.8 58.2 54.2 Shi et al. ( * * )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Classification Network</cell></row><row><cell></cell><cell>h I</cell><cell>h P</cell></row><row><cell>Init</cell><cell>24.74</cell><cell>31.93</cell></row><row><cell>Init + SP</cell><cell>32.1</cell><cell>38.11</cell></row></table><note>Quantitative comparison (CorLoc %) on the Pascal VOC 2007 trainval set (with a single AlexNet-like capacity network). Note that: ( * ) Wang et al. [42] tune the number of latent categories for each class and ( * * ) Shi et al. [34] use additional data with BB-level annotation at training time. However, our full-pipeline Init+SP obtains a much higher mAP value (38.11), showing the advantage of a self-paced training strategy. The gap with respect to [25] is even higher in other experiments (e.g., see Tabs. 6-7). In Sec. 7.1 (Tab. 9) we show the corresponding relative boost obtained by SP with respect to Init in case of the ILSVRC dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table /><note>mAP (%) on Pascal VOC 2007 test computed using only Init (first row) or Init + SP (second row) for fine- tuning the detector. The first column corresponds to the results obtained when Init is initialized with a CN trained with only ILSVRC data (h I ), while the second column corresponds to using a CN trained with Pascal VOC data and a multi-label loss [25] (h P ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP Li et al. [25] 54.5 47.4 41.3 20.8 17.7 51.9 63.5 46.1 21.8 57.1 22.1 34.4 50.5 61.8 16.2 29.9 40.7 15.9 55.3 40.2 39.5 Bilen et al. [5] 39.4 50.1 31.5 16.3 12.6 64.5 42.8 42.6 10.1 35.7 24.9 38.2 34.4 55.6 9.4 14.7 30.2 40.7 54.7 46.9 34.8 Shi et al.</figDesc><table><row><cell>( * ) [34] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-37.2</cell></row><row><cell>Ours (W P</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>0 ) 61.7 66.7 47.7 26.2 19.1 61.3 68 52.6 23.5 61.8 26.5 27.8 57.3 63.7 14.3 24.6 46.3 31.2 66.8 49.5 44.84</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 5 :</head><label>5</label><figDesc>Quantitative comparison (AP %) on the Pascal VOC 2007 test set using a single VGG-16 network. Note that ( * ) Shi et al. [34] use additional data with BB-level annotation at training time. Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP Cinbis et al. [9] 44.6 42.3 25.5 14.1 11 44.1 36.3 23.2 12.2 26.1 14 29.2 36 54.3 20.7 12.4 26.5 20.3 31.2 23.7 27.4 Li et al. ( * )</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 6 :</head><label>6</label><figDesc>Pascal VOC 2010 test set, single AlexNet network. Note that ( * ) Li et al.<ref type="bibr" target="#b24">[25]</ref> use the val split for the evaluation. Our results are available at: http://host.robots.ox.ac.uk:8080/anonymous/WHEGLB.html.</figDesc><table /><note>Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP Li et al. ( * )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 7 :</head><label>7</label><figDesc></figDesc><table /><note>Pascal VOC 2010 test set, VGG-16 network. Note that: ( * ) Li et al. [25] use the val split for the evaluation and ( * * ) Bilen et al. [5] use an ensemble of 3 networks: VGG-F [6], VGG-CNN-M-1024 [6] and VGG-16. Our results are available at: http://host.robots.ox.ac.uk:8080/anonymous/UBLBGP.html.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>No-reg-train-test 28.3 28.3 30.1 30.9 30.7 31.4 SP 31.9 35.3 37.6 37.8 38.1 38.1</figDesc><table><row><cell></cell><cell>31.9 33.6 32.1 32.2 30.8 30.9</cell></row><row><cell>Curriculum</cell><cell>31.9 31.3 33.8 31.6 31.3 30.5</cell></row><row><cell>SP-all-cls</cell><cell>31.9 36.6 36.9 36.6 36.9 36.9</cell></row><row><cell>SP-rnd-cls</cell><cell>31.9 32.3 31.6 32.4 32.7 33.8</cell></row><row><cell>No-reg-train</cell><cell>31.9 31.2 32.6 33.1 33.5 34.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 8 :</head><label>8</label><figDesc>mAP (%) on Pascal VOC 2007 test computed with different networks f Wt and with respect to different versions of our training protocol and M + 1 iterations.</figDesc><table><row><cell>Method</cell><cell>W 0</cell><cell>W 1</cell><cell>W 2</cell><cell>W 3</cell><cell>W 4</cell><cell>W 5</cell></row><row><cell>MIL</cell><cell>9.54</cell><cell>9.66</cell><cell>9.01</cell><cell>8.97</cell><cell>8.59</cell><cell>8.7</cell></row><row><cell cols="2">Curriculum 9.54</cell><cell>9.08</cell><cell>9.15</cell><cell>8.77</cell><cell>8.89</cell><cell>8.97</cell></row><row><cell>SP-all-cls</cell><cell cols="6">9.54 10.68 10.74 11.77 11.97 12.06</cell></row><row><cell>SP</cell><cell cols="6">9.54 10.88 11.87 12.01 12.13 11.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 9 :</head><label>9</label><figDesc></figDesc><table /><note>mAP (%) on ILSVRC 2013 val2 computed with dif- ferent networks f Wt and with respect to different versions of our training protocol and M + 1 iterations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE 10 :</head><label>10</label><figDesc>Results (mAP %) on the Pascal VOC 2007 test set using relaxed versions of the inter-classifier competition.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE 12 :</head><label>12</label><figDesc>Per-class AP on the ILSVRC13 detection val2.Self Paced Deep Learning for Weakly Supervised Object Detection -Supplemental Material Enver Sangineto † , Moin Nabi † , Dubravko Culibrk and Nicu Sebe, In Tab. I we show the per-category AP obtained by our method on ILSVRC 2013 val2. Similarly to Sec. 7.1 of the main paper, we analyse 5 different nets (W 0 , ...W 4 ), corresponding to the 4 self-paced iterations of Alg. 1 plus the initialization model (W 0 = W I 0 , i.e.,</figDesc><table><row><cell>#</cell><cell>Category</cell><cell>W0</cell><cell>W1</cell><cell>W2</cell><cell>W3</cell><cell>W4</cell></row><row><cell cols="2">51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 lamp croquet ball crutch cucumber cup or mug diaper digital clock dishwasher dog domestic cat dragonfly drum dumbbell electric fan elephant face powder fig filing cabinet flower pot flute fox french horn frog frying pan giant panda goldfish golf ball golfcart guacamole guitar hair dryer hair spray hamburger hammer hamster harmonica harp hat with a wide brim head cabbage helmet hippopotamus horizontal bar horse hotdog iPod isopod jellyfish koala bear ladle ladybug</cell><cell>1.72 1.34 5.22 7.51 1.46 4.62 7.79 16.80 4.10 18.44 0.23 2.56 28.65 31.17 9.96 7.56 3.60 0.24 0.08 22.61 9.17 22.53 5.08 34.60 5.37 17.15 32.61 13.27 5.49 3.74 1.71 15.57 0.72 41.87 0.49 21.65 6.10 4.92 5.08 21.24 0.04 7.34 4.06 15.80 10.06 7.32 37.38 0.47 11.74 0.99</cell><cell>0.66 1.96 4.52 7.28 4.60 13.05 0.79 15.59 3.87 16.02 0.42 1.49 16.71 38.46 7.85 7.69 6.26 0.39 1.68 28.19 10.71 28.31 4.03 46.05 9.75 22.84 24.31 8.80 5.35 0.44 2.57 31.05 0.54 31.78 0.94 30.85 10.00 8.11 4.10 24.43 0.07 6.74 4.74 26.91 13.45 3.41 46.53 1.75 13.94 0.87</cell><cell>0.28 1.79 3.59 11.99 3.13 20.51 0.96 17.34 3.15 26.51 0.57 1.43 16.83 34.92 6.27 6.70 7.72 0.28 1.67 28.73 10.50 29.88 4.41 46.31 7.69 23.18 28.82 8.33 6.69 0.86 3.85 25.96 0.64 42.78 0.76 34.71 11.51 8.76 4.51 30.54 0.07 9.30 4.12 26.39 12.88 2.76 50.72 0.52 13.06 0.61</cell><cell>0.16 1.85 3.65 12.10 3.43 19.04 0.70 19.32 4.87 25.22 0.56 1.32 20.57 37.21 5.19 4.59 6.92 0.27 1.60 32.96 16.79 29.37 5.79 26.62 9.40 23.10 31.24 8.65 4.91 1.36 3.75 17.97 1.27 39.35 0.62 38.22 9.85 9.63 5.16 27.03 0.05 9.06 4.13 25.08 13.31 4.39 56.96 0.57 13.69 0.53</cell><cell>0.25 2.78 3.41 12.67 2.96 19.35 0.67 17.35 3.23 25.29 0.57 1.36 21.10 34.44 4.65 4.10 7.74 0.27 1.57 36.23 12.28 30.08 5.70 26.71 9.42 22.91 33.59 8.28 4.82 1.28 4.94 17.92 1.58 39.29 0.50 37.95 11.37 9.64 5.43 27.58 0.05 8.90 3.77 25.70 10.11 4.36 52.95 0.58 13.35 0.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>TABLE I :</head><label>I</label><figDesc>Per-class AP on the ILSVRC13 detection val2.</figDesc><table><row><cell># 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 lamp Category croquet ball crutch cucumber cup or mug diaper digital clock dishwasher dog domestic cat dragonfly drum dumbbell electric fan elephant face powder fig filing cabinet flower pot flute fox french horn frog frying pan giant panda goldfish golf ball golfcart guacamole guitar hair dryer hair spray hamburger hammer hamster harmonica harp hat with a wide brim head cabbage helmet hippopotamus horizontal bar horse hotdog iPod isopod jellyfish koala bear ladle ladybug # Category # Category 101 laptop 102 lemon 103 lion 104 lipstick 105 lizard 106 lobster 107 maillot 108 maraca 109 microphone 110 microwave 111 milk can 112 miniskirt 113 monkey 114 motorcycle 115 mushroom 116 nail 117 neck brace 118 oboe 119 orange 120 otter 121 pencil box 122 pencil sharpener 123 perfume 124 person 125 piano 126 pineapple 127 ping-pong ball 128 pitcher 129 pizza 130 plastic bag 131 plate rack 132 pomegranate 133 popsicle 134 porcupine 135 power drill 136 pretzel 137 printer 138 puck 139 punching bag 140 purse 141 rabbit 142 racket 143 ray 144 red panda 145 refrigerator 146 remote control 147 rubber eraser 148 rugby ball 149 ruler 150 salt or pepper shaker 151 saxophone 152 scorpion 153 screwdriver 154 seal 155 sheep 156 ski 157 skunk 158 snail 159 snake 160 snowmobile 161 snowplow 162 soap dispenser 163 soccer ball 164 sofa 165 spatula 166 squirrel 167 starfish 168 stethoscope 169 stove 170 strainer 171 strawberry 172 stretcher 173 sunglasses 174 swimming trunks 175 swine 176 syringe 177 table 178 tape player 179 tennis ball 180 tick 181 tie 182 tiger 183 toaster 184 traffic light 185 train 186 trombone 187 trumpet 188 turtle 189 tv or monitor 190 unicycle 191 vacuum 192 violin 193 volleyball 194 waffle iron 195 washer 196 water bottle 197 watercraft 198 whale 199 wine bottle 200 zebra -mAP</cell><cell>W0 1.72 1.34 5.22 7.51 1.46 4.62 7.79 16.80 4.10 18.44 0.23 2.56 28.65 31.17 9.96 7.56 3.60 0.24 0.08 22.61 9.17 22.53 5.08 34.60 5.37 17.15 32.61 13.27 5.49 3.74 1.71 15.57 0.72 41.87 0.49 21.65 6.10 4.92 5.08 21.24 0.04 7.34 4.06 15.80 10.06 7.32 37.38 0.47 11.74 0.99 W0 W0 7.60 5.81 4.51 3.50 6.39 5.12 0.20 3.36 0.07 19.30 16.62 0.74 17.49 19.21 13.53 0.07 0.11 1.51 3.85 2.73 2.89 1.49 11.55 0.14 5.06 6.20 0.01 3.66 8.89 0.39 1.78 8.72 0.06 24.08 0.83 3.33 6.17 0.01 0.58 0.82 43.34 0.05 10.26 16.13 9.90 17.05 0.01 0.04 0.30 9.99 15.90 15.23 0.11 2.22 18.81 0.09 6.61 24.51 1.58 21.22 33.11 0.01 20.17 7.65 0.04 17.91 3.88 2.12 0.58 0.14 4.90 0.10 1.51 0.11 18.70 1.55 1.36 11.73 8.88 19.72 3.13 10.38 22.46 1.20 10.28 3.49 2.66 22.47 20.12 0.96 1.49 2.02 0.02 2.71 41.19 5.28 6.62 23.15 2.74 31.42 9.54</cell><cell>W1 0.66 1.96 4.52 7.28 4.60 13.05 0.79 15.59 3.87 16.02 0.42 1.49 16.71 38.46 7.85 7.69 6.26 0.39 1.68 28.19 10.71 28.31 4.03 46.05 9.75 22.84 24.31 8.80 5.35 0.44 2.57 31.05 0.54 31.78 0.94 30.85 10.00 8.11 4.10 24.43 0.07 6.74 4.74 26.91 13.45 3.41 46.53 1.75 13.94 0.87 W1 W1 14.38 10.57 0.80 3.74 11.37 8.48 1.64 2.65 0.03 18.18 19.41 0.89 18.81 19.08 16.11 0.44 6.26 7.16 1.69 2.19 6.76 2.35 2.33 0.27 3.21 12.69 0.01 5.98 17.77 2.21 1.08 8.89 0.04 26.20 1.01 4.79 2.79 0.01 2.33 1.23 48.27 0.06 14.08 27.36 10.05 26.71 0.01 0.06 1.89 8.50 14.08 17.18 0.17 3.18 14.57 0.02 9.70 11.68 5.20 31.10 38.49 0.50 19.04 8.22 0.04 10.72 9.89 2.97 1.32 0.26 7.82 0.01 2.52 0.03 23.02 2.62 3.18 8.12 7.11 18.95 2.91 15.88 22.21 1.14 22.47 4.04 4.12 27.24 29.23 1.51 0.50 2.06 0.09 2.03 35.88 7.32 6.55 27.97 1.75 42.00 10.88</cell><cell>W2 0.28 1.79 3.59 11.99 3.13 20.51 0.96 17.34 3.15 26.51 0.57 1.43 16.83 34.92 6.27 6.70 7.72 0.28 1.67 28.73 10.50 29.88 4.41 46.31 7.69 23.18 28.82 8.33 6.69 0.86 3.85 25.96 0.64 42.78 0.76 34.71 11.51 8.76 4.51 30.54 0.07 9.30 4.12 26.39 12.88 2.76 50.72 0.52 13.06 0.61 W2 W2 15.47 14.28 2.56 3.14 13.14 17.80 1.35 1.53 0.06 14.61 21.39 0.48 24.26 26.79 17.66 0.21 1.29 7.52 5.58 5.89 6.25 2.05 5.54 0.37 9.26 12.49 0.01 7.53 14.55 2.74 2.62 9.15 0.01 33.64 5.40 6.04 2.37 0.01 3.65 1.32 47.09 0.08 19.76 22.34 8.14 32.35 0.01 0.05 3.84 7.69 13.23 22.04 0.13 3.33 18.88 0.14 9.19 16.11 12.69 27.13 37.20 0.19 17.84 7.14 0.07 16.12 13.17 7.39 1.81 0.87 7.52 0.15 1.39 0.00 28.07 2.64 3.77 8.94 1.61 24.49 3.43 18.89 21.94 1.03 19.73 1.15 3.75 31.95 33.18 0.70 0.48 3.19 0.02 2.29 34.47 4.89 5.68 32.27 1.87 43.26 11.87</cell><cell>W3 0.16 1.85 3.65 12.10 3.43 19.04 0.70 19.32 4.87 25.22 0.56 1.32 20.57 37.21 5.19 4.59 6.92 0.27 1.60 32.96 16.79 29.37 5.79 26.62 9.40 23.10 31.24 8.65 4.91 1.36 3.75 17.97 1.27 39.35 0.62 38.22 9.85 9.63 5.16 27.03 0.05 9.06 4.13 25.08 13.31 4.39 56.96 0.57 13.69 0.53 W3 W3 18.66 14.60 2.04 4.04 13.30 19.80 1.05 1.57 0.10 14.25 14.75 0.19 26.50 26.96 18.54 0.13 0.21 7.47 6.42 8.51 4.28 1.42 5.09 0.47 11.43 11.82 0.01 8.35 11.71 2.82 6.29 9.24 0.01 29.51 7.14 6.20 2.42 0.01 4.35 1.28 48.31 0.08 20.22 25.36 9.21 24.36 0.01 0.04 3.66 5.45 11.66 18.47 0.68 3.30 19.25 0.06 12.30 19.22 15.81 25.74 41.74 0.20 18.06 7.66 0.11 25.52 17.33 7.67 2.18 3.64 7.33 0.10 1.35 0.00 34.95 2.64 3.25 9.92 1.75 17.99 2.25 20.83 21.79 0.82 17.29 1.72 3.83 31.68 33.81 0.54 0.67 2.42 0.02 2.98 30.45 6.23 3.65 33.57 1.34 40.14 12.01</cell><cell>W4 0.25 2.78 3.41 12.67 2.96 19.35 0.67 17.35 3.23 25.29 0.57 1.36 21.10 34.44 4.65 4.10 7.74 0.27 1.57 36.23 12.28 30.08 5.70 26.71 9.42 22.91 33.59 8.28 4.82 1.28 4.94 17.92 1.58 39.29 0.50 37.95 11.37 9.64 5.43 27.58 0.05 8.90 3.77 25.70 10.11 4.36 52.95 0.58 13.35 0.58 W4 W4 18.75 15.03 1.90 3.85 13.78 15.46 1.36 1.55 0.11 14.32 17.97 0.20 24.46 26.70 18.66 0.19 0.20 7.51 6.14 9.02 4.84 1.59 5.48 0.47 11.45 11.92 0.01 8.08 11.55 2.55 6.19 9.42 0.01 31.30 7.10 6.40 2.29 0.01 4.03 1.29 48.41 0.06 22.67 25.28 8.13 25.56 0.01 0.04 3.90 6.92 12.78 16.83 0.35 3.96 18.59 0.06 10.85 19.46 16.05 28.05 41.12 0.17 18.04 7.71 0.12 23.18 17.25 7.74 2.04 3.76 7.20 0.10 1.74 0.00 31.21 2.62 3.34 10.32 1.35 18.74 3.28 26.02 22.69 0.90 17.68 1.66 3.88 32.04 33.84 0.56 0.37 5.22 0.02 4.09 31.32 6.05 4.39 37.38 1.74 42.17 12.13</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">SELF-PACED LEARNING PROTOCOLWe call W the set of weights of all the layers of the network and we initialize our network with W 0 , which can</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We want to thank the NVIDIA Corporation for the donation of the GPUs used in this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-taught object localization with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-fold MIL training for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2409" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="203" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Zisserman. The Pascal Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>VOC 2007) Results</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zisserman. The Pascal Visual Object Classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contextual action recognition with R*CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LSDA: large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detector discovery in the wild: Joint multiple instance and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2883" to="2891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2078" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ContextLocNet: context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Are all training examples equally valuable?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>arxiv:1311.6510</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning the easy things first: Self-paced visual category discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1721" to="1728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards computational baby learning: A weakly-supervised approach for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ensemble of Exemplar-SVMs for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning discriminative localization from weakly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1523" to="1534" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Is object localization for free? -Weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Curriculum learning of multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5492" to="5500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno>arxiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Statistical and spatial consensus collection for detector adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="456" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization using size estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="105" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-paced learning for longterm tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2379" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Largescale weakly supervised object localization via latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1371" to="1385" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">STC: A simple to complex framework for weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bridging saliency detection to weakly supervised object detection based on selfpaced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3538" to="3544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">E-mail: {enver.sangineto,moin.nabi}@unitn.it; sebe@disi.unitn.it D. Culibrk is with the Department of Industrial Engineering and Management, University of Novi Sad, Serbia. E-mail: dculibrk@uns.ac.rs † These two authors contributed equally</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07651v3[cs.CV]21</idno>
		<imprint>
			<date type="published" when="2018-02" />
			<pubPlace>Trento, Italy</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Moin Nabi and Nicu Sebe are with the Department of Information Engineering and Computer Science (DISI), University of</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
